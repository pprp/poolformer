Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Model mobilenetv2_100 created, param count:3504872
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Using NVIDIA APEX AMP. Training in mixed precision.
Using NVIDIA APEX DistributedDataParallel.
Scheduled epochs: 310
Train: 0 [   0/1251 (  0%)]  Loss: 6.958 (6.96)  Time: 5.024s,  203.82/s  (5.024s,  203.82/s)  LR: 1.000e-06  Data: 2.365 (2.365)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 0 [  50/1251 (  4%)]  Loss: 6.961 (6.96)  Time: 0.701s, 1459.96/s  (0.295s, 3471.02/s)  LR: 1.000e-06  Data: 0.022 (0.076)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.943 (6.95)  Time: 0.162s, 6302.55/s  (0.262s, 3907.61/s)  LR: 1.000e-06  Data: 0.019 (0.056)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.953 (6.95)  Time: 0.158s, 6486.76/s  (0.254s, 4034.61/s)  LR: 1.000e-06  Data: 0.020 (0.059)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.933 (6.95)  Time: 0.145s, 7064.95/s  (0.252s, 4065.06/s)  LR: 1.000e-06  Data: 0.023 (0.068)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.942 (6.95)  Time: 0.155s, 6614.26/s  (0.249s, 4105.54/s)  LR: 1.000e-06  Data: 0.020 (0.070)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.936 (6.95)  Time: 0.146s, 7032.92/s  (0.255s, 4013.04/s)  LR: 1.000e-06  Data: 0.017 (0.065)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.933 (6.94)  Time: 0.145s, 7045.49/s  (0.252s, 4061.09/s)  LR: 1.000e-06  Data: 0.023 (0.059)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.940 (6.94)  Time: 0.152s, 6724.01/s  (0.249s, 4117.32/s)  LR: 1.000e-06  Data: 0.026 (0.055)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.929 (6.94)  Time: 0.159s, 6426.58/s  (0.247s, 4149.65/s)  LR: 1.000e-06  Data: 0.021 (0.052)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.934 (6.94)  Time: 0.161s, 6351.98/s  (0.247s, 4143.22/s)  LR: 1.000e-06  Data: 0.017 (0.049)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.936 (6.94)  Time: 0.167s, 6135.90/s  (0.247s, 4151.88/s)  LR: 1.000e-06  Data: 0.017 (0.047)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.934 (6.94)  Time: 0.154s, 6668.62/s  (0.245s, 4174.04/s)  LR: 1.000e-06  Data: 0.024 (0.045)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.924 (6.94)  Time: 0.154s, 6660.92/s  (0.245s, 4185.24/s)  LR: 1.000e-06  Data: 0.023 (0.043)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.928 (6.94)  Time: 0.643s, 1593.72/s  (0.245s, 4174.65/s)  LR: 1.000e-06  Data: 0.022 (0.041)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.917 (6.94)  Time: 0.151s, 6767.02/s  (0.244s, 4188.93/s)  LR: 1.000e-06  Data: 0.021 (0.040)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.933 (6.94)  Time: 1.225s,  835.79/s  (0.245s, 4175.41/s)  LR: 1.000e-06  Data: 0.017 (0.039)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.925 (6.94)  Time: 0.165s, 6199.02/s  (0.245s, 4185.32/s)  LR: 1.000e-06  Data: 0.020 (0.038)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.924 (6.94)  Time: 0.829s, 1235.31/s  (0.245s, 4179.72/s)  LR: 1.000e-06  Data: 0.017 (0.037)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.920 (6.94)  Time: 0.139s, 7348.10/s  (0.247s, 4150.56/s)  LR: 1.000e-06  Data: 0.019 (0.036)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.919 (6.93)  Time: 0.702s, 1457.93/s  (0.248s, 4135.74/s)  LR: 1.000e-06  Data: 0.033 (0.036)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.922 (6.93)  Time: 0.154s, 6659.94/s  (0.248s, 4132.39/s)  LR: 1.000e-06  Data: 0.027 (0.035)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.918 (6.93)  Time: 0.158s, 6467.08/s  (0.247s, 4144.32/s)  LR: 1.000e-06  Data: 0.018 (0.035)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.923 (6.93)  Time: 0.655s, 1563.80/s  (0.248s, 4129.05/s)  LR: 1.000e-06  Data: 0.538 (0.037)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.923 (6.93)  Time: 0.270s, 3788.40/s  (0.247s, 4137.79/s)  LR: 1.000e-06  Data: 0.021 (0.039)
Train: 0 [1250/1251 (100%)]  Loss: 6.917 (6.93)  Time: 0.114s, 8983.78/s  (0.247s, 4150.41/s)  LR: 1.000e-06  Data: 0.000 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.703 (2.703)  Loss:  6.8865 (6.8865)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0977 ( 0.0977)
Test: [  48/48]  Time: 0.843 (0.317)  Loss:  6.8797 (6.9178)  Acc@1:  0.1179 ( 0.0980)  Acc@5:  0.2358 ( 0.4900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 1 [   0/1251 (  0%)]  Loss: 6.920 (6.92)  Time: 1.884s,  543.56/s  (1.884s,  543.56/s)  LR: 2.008e-04  Data: 1.712 (1.712)
Train: 1 [  50/1251 (  4%)]  Loss: 6.929 (6.92)  Time: 0.164s, 6259.66/s  (0.225s, 4557.77/s)  LR: 2.008e-04  Data: 0.025 (0.083)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.921 (6.92)  Time: 0.165s, 6208.98/s  (0.208s, 4931.40/s)  LR: 2.008e-04  Data: 0.024 (0.064)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.901 (6.92)  Time: 0.175s, 5838.85/s  (0.202s, 5074.94/s)  LR: 2.008e-04  Data: 0.022 (0.053)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.898 (6.91)  Time: 0.342s, 2992.49/s  (0.198s, 5166.01/s)  LR: 2.008e-04  Data: 0.026 (0.047)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.876 (6.91)  Time: 0.176s, 5817.89/s  (0.196s, 5235.90/s)  LR: 2.008e-04  Data: 0.028 (0.043)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.844 (6.90)  Time: 0.155s, 6626.38/s  (0.195s, 5240.20/s)  LR: 2.008e-04  Data: 0.029 (0.043)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.846 (6.89)  Time: 0.155s, 6607.32/s  (0.193s, 5304.95/s)  LR: 2.008e-04  Data: 0.027 (0.042)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.815 (6.88)  Time: 0.178s, 5745.58/s  (0.194s, 5285.81/s)  LR: 2.008e-04  Data: 0.028 (0.043)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.837 (6.88)  Time: 0.175s, 5861.60/s  (0.193s, 5301.01/s)  LR: 2.008e-04  Data: 0.029 (0.043)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.783 (6.87)  Time: 0.177s, 5774.31/s  (0.193s, 5293.85/s)  LR: 2.008e-04  Data: 0.019 (0.043)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.751 (6.86)  Time: 0.171s, 6001.89/s  (0.193s, 5302.65/s)  LR: 2.008e-04  Data: 0.023 (0.044)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.807 (6.86)  Time: 0.166s, 6173.36/s  (0.193s, 5310.96/s)  LR: 2.008e-04  Data: 0.021 (0.044)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.750 (6.85)  Time: 0.160s, 6412.92/s  (0.193s, 5318.27/s)  LR: 2.008e-04  Data: 0.028 (0.044)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.793 (6.84)  Time: 0.165s, 6220.11/s  (0.193s, 5317.21/s)  LR: 2.008e-04  Data: 0.022 (0.044)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.744 (6.84)  Time: 0.156s, 6553.34/s  (0.192s, 5323.31/s)  LR: 2.008e-04  Data: 0.024 (0.044)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.741 (6.83)  Time: 0.154s, 6640.20/s  (0.192s, 5322.87/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.672 (6.82)  Time: 0.181s, 5643.45/s  (0.192s, 5324.01/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.721 (6.82)  Time: 0.160s, 6410.85/s  (0.193s, 5314.22/s)  LR: 2.008e-04  Data: 0.034 (0.045)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.688 (6.81)  Time: 0.169s, 6066.06/s  (0.193s, 5310.83/s)  LR: 2.008e-04  Data: 0.026 (0.046)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.684 (6.81)  Time: 0.196s, 5211.47/s  (0.193s, 5317.60/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.715 (6.80)  Time: 0.168s, 6105.35/s  (0.193s, 5313.12/s)  LR: 2.008e-04  Data: 0.028 (0.045)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.644 (6.79)  Time: 0.298s, 3432.03/s  (0.193s, 5311.70/s)  LR: 2.008e-04  Data: 0.170 (0.045)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.728 (6.79)  Time: 0.276s, 3713.60/s  (0.193s, 5307.11/s)  LR: 2.008e-04  Data: 0.019 (0.045)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.652 (6.79)  Time: 0.181s, 5662.75/s  (0.193s, 5297.80/s)  LR: 2.008e-04  Data: 0.025 (0.045)
Train: 1 [1250/1251 (100%)]  Loss: 6.628 (6.78)  Time: 0.113s, 9089.13/s  (0.193s, 5314.61/s)  LR: 2.008e-04  Data: 0.000 (0.044)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.788 (1.788)  Loss:  5.9561 (5.9561)  Acc@1:  3.0273 ( 3.0273)  Acc@5: 12.5977 (12.5977)
Test: [  48/48]  Time: 0.019 (0.208)  Loss:  5.5965 (6.1005)  Acc@1: 10.2594 ( 2.5260)  Acc@5: 19.4575 ( 8.0560)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 2 [   0/1251 (  0%)]  Loss: 6.632 (6.63)  Time: 1.885s,  543.25/s  (1.885s,  543.25/s)  LR: 4.006e-04  Data: 1.746 (1.746)
Train: 2 [  50/1251 (  4%)]  Loss: 6.705 (6.67)  Time: 0.178s, 5768.31/s  (0.225s, 4559.32/s)  LR: 4.006e-04  Data: 0.024 (0.077)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.646 (6.66)  Time: 0.160s, 6418.99/s  (0.207s, 4939.25/s)  LR: 4.006e-04  Data: 0.028 (0.052)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.697 (6.67)  Time: 0.172s, 5958.07/s  (0.202s, 5066.08/s)  LR: 4.006e-04  Data: 0.023 (0.043)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.651 (6.67)  Time: 0.168s, 6112.22/s  (0.198s, 5172.73/s)  LR: 4.006e-04  Data: 0.033 (0.039)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.623 (6.66)  Time: 0.160s, 6418.26/s  (0.196s, 5235.10/s)  LR: 4.006e-04  Data: 0.021 (0.037)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.569 (6.65)  Time: 0.188s, 5434.34/s  (0.196s, 5231.81/s)  LR: 4.006e-04  Data: 0.022 (0.035)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.607 (6.64)  Time: 0.161s, 6377.06/s  (0.196s, 5232.80/s)  LR: 4.006e-04  Data: 0.029 (0.034)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.563 (6.63)  Time: 0.166s, 6171.64/s  (0.194s, 5284.39/s)  LR: 4.006e-04  Data: 0.035 (0.033)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.581 (6.63)  Time: 0.166s, 6168.90/s  (0.194s, 5279.24/s)  LR: 4.006e-04  Data: 0.022 (0.032)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.435 (6.61)  Time: 0.172s, 5967.88/s  (0.194s, 5280.17/s)  LR: 4.006e-04  Data: 0.024 (0.032)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.497 (6.60)  Time: 0.180s, 5702.26/s  (0.193s, 5301.30/s)  LR: 4.006e-04  Data: 0.026 (0.031)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.601 (6.60)  Time: 0.173s, 5916.09/s  (0.193s, 5296.65/s)  LR: 4.006e-04  Data: 0.027 (0.031)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.540 (6.60)  Time: 0.158s, 6500.81/s  (0.193s, 5304.86/s)  LR: 4.006e-04  Data: 0.023 (0.030)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.489 (6.59)  Time: 0.423s, 2418.54/s  (0.193s, 5301.04/s)  LR: 4.006e-04  Data: 0.023 (0.030)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.503 (6.58)  Time: 0.167s, 6138.93/s  (0.192s, 5321.15/s)  LR: 4.006e-04  Data: 0.034 (0.030)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.476 (6.58)  Time: 0.457s, 2241.51/s  (0.192s, 5319.73/s)  LR: 4.006e-04  Data: 0.338 (0.031)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.424 (6.57)  Time: 0.178s, 5754.59/s  (0.193s, 5307.90/s)  LR: 4.006e-04  Data: 0.026 (0.032)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.428 (6.56)  Time: 0.171s, 5982.25/s  (0.193s, 5306.18/s)  LR: 4.006e-04  Data: 0.030 (0.033)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.438 (6.56)  Time: 0.168s, 6110.60/s  (0.193s, 5313.91/s)  LR: 4.006e-04  Data: 0.026 (0.033)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.428 (6.55)  Time: 0.166s, 6182.17/s  (0.193s, 5312.35/s)  LR: 4.006e-04  Data: 0.035 (0.032)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.500 (6.55)  Time: 0.362s, 2828.01/s  (0.193s, 5308.37/s)  LR: 4.006e-04  Data: 0.030 (0.032)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.410 (6.54)  Time: 0.200s, 5114.01/s  (0.193s, 5300.43/s)  LR: 4.006e-04  Data: 0.026 (0.032)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.324 (6.53)  Time: 0.157s, 6507.88/s  (0.193s, 5295.34/s)  LR: 4.006e-04  Data: 0.022 (0.031)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.420 (6.53)  Time: 0.163s, 6284.36/s  (0.194s, 5289.00/s)  LR: 4.006e-04  Data: 0.022 (0.031)
Train: 2 [1250/1251 (100%)]  Loss: 6.468 (6.53)  Time: 0.113s, 9039.70/s  (0.193s, 5311.93/s)  LR: 4.006e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.778 (1.778)  Loss:  5.1876 (5.1876)  Acc@1:  7.4219 ( 7.4219)  Acc@5: 22.8516 (22.8516)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  4.5310 (5.3344)  Acc@1: 21.5802 ( 7.0440)  Acc@5: 37.9717 (19.7260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 3 [   0/1251 (  0%)]  Loss: 6.478 (6.48)  Time: 1.681s,  609.08/s  (1.681s,  609.08/s)  LR: 6.004e-04  Data: 1.556 (1.556)
Train: 3 [  50/1251 (  4%)]  Loss: 6.528 (6.50)  Time: 0.161s, 6351.71/s  (0.223s, 4593.47/s)  LR: 6.004e-04  Data: 0.024 (0.079)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.382 (6.46)  Time: 0.159s, 6442.64/s  (0.207s, 4948.24/s)  LR: 6.004e-04  Data: 0.028 (0.063)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.431 (6.45)  Time: 0.194s, 5287.60/s  (0.201s, 5101.91/s)  LR: 6.004e-04  Data: 0.021 (0.054)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.369 (6.44)  Time: 0.189s, 5419.32/s  (0.199s, 5145.06/s)  LR: 6.004e-04  Data: 0.020 (0.046)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.281 (6.41)  Time: 0.157s, 6515.78/s  (0.196s, 5213.16/s)  LR: 6.004e-04  Data: 0.027 (0.043)
Train: 3 [ 300/1251 ( 24%)]  Loss: 6.302 (6.40)  Time: 0.177s, 5772.80/s  (0.195s, 5259.82/s)  LR: 6.004e-04  Data: 0.021 (0.040)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.393 (6.40)  Time: 0.169s, 6060.77/s  (0.194s, 5285.30/s)  LR: 6.004e-04  Data: 0.019 (0.039)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.307 (6.39)  Time: 0.172s, 5947.23/s  (0.194s, 5264.83/s)  LR: 6.004e-04  Data: 0.029 (0.037)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.289 (6.38)  Time: 0.162s, 6325.39/s  (0.194s, 5283.72/s)  LR: 6.004e-04  Data: 0.027 (0.036)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.293 (6.37)  Time: 0.179s, 5718.72/s  (0.193s, 5299.76/s)  LR: 6.004e-04  Data: 0.030 (0.035)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.232 (6.36)  Time: 0.167s, 6123.26/s  (0.193s, 5309.05/s)  LR: 6.004e-04  Data: 0.033 (0.034)
Train: 3 [ 600/1251 ( 48%)]  Loss: 6.418 (6.36)  Time: 0.165s, 6210.14/s  (0.192s, 5322.70/s)  LR: 6.004e-04  Data: 0.031 (0.034)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.265 (6.35)  Time: 0.164s, 6232.66/s  (0.193s, 5308.94/s)  LR: 6.004e-04  Data: 0.021 (0.033)
Train: 3 [ 700/1251 ( 56%)]  Loss: 6.353 (6.35)  Time: 0.250s, 4093.05/s  (0.193s, 5317.07/s)  LR: 6.004e-04  Data: 0.030 (0.033)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.219 (6.35)  Time: 0.162s, 6335.02/s  (0.192s, 5323.27/s)  LR: 6.004e-04  Data: 0.035 (0.033)
Train: 3 [ 800/1251 ( 64%)]  Loss: 6.215 (6.34)  Time: 0.156s, 6568.54/s  (0.193s, 5310.78/s)  LR: 6.004e-04  Data: 0.024 (0.032)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.115 (6.33)  Time: 0.155s, 6588.13/s  (0.193s, 5310.45/s)  LR: 6.004e-04  Data: 0.023 (0.032)
Train: 3 [ 900/1251 ( 72%)]  Loss: 6.279 (6.32)  Time: 0.169s, 6047.86/s  (0.193s, 5305.48/s)  LR: 6.004e-04  Data: 0.030 (0.031)
Train: 3 [ 950/1251 ( 76%)]  Loss: 6.349 (6.32)  Time: 0.166s, 6160.04/s  (0.193s, 5305.56/s)  LR: 6.004e-04  Data: 0.020 (0.031)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.307 (6.32)  Time: 0.170s, 6028.21/s  (0.193s, 5307.03/s)  LR: 6.004e-04  Data: 0.022 (0.031)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.165 (6.32)  Time: 0.203s, 5032.51/s  (0.193s, 5303.50/s)  LR: 6.004e-04  Data: 0.031 (0.031)
Train: 3 [1100/1251 ( 88%)]  Loss: 6.124 (6.31)  Time: 0.740s, 1383.81/s  (0.193s, 5293.25/s)  LR: 6.004e-04  Data: 0.626 (0.031)
Train: 3 [1150/1251 ( 92%)]  Loss: 6.253 (6.31)  Time: 0.163s, 6301.38/s  (0.193s, 5294.71/s)  LR: 6.004e-04  Data: 0.019 (0.032)
Train: 3 [1200/1251 ( 96%)]  Loss: 6.259 (6.30)  Time: 0.181s, 5651.12/s  (0.194s, 5288.82/s)  LR: 6.004e-04  Data: 0.029 (0.033)
Train: 3 [1250/1251 (100%)]  Loss: 6.159 (6.30)  Time: 0.114s, 9016.87/s  (0.193s, 5306.40/s)  LR: 6.004e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.911 (1.911)  Loss:  4.2315 (4.2315)  Acc@1: 17.7734 (17.7734)  Acc@5: 42.7734 (42.7734)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  3.7917 (4.6785)  Acc@1: 33.4906 (13.6480)  Acc@5: 48.4670 (31.7440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 4 [   0/1251 (  0%)]  Loss: 6.110 (6.11)  Time: 1.893s,  541.07/s  (1.893s,  541.07/s)  LR: 8.002e-04  Data: 1.762 (1.762)
Train: 4 [  50/1251 (  4%)]  Loss: 6.034 (6.07)  Time: 0.191s, 5362.35/s  (0.228s, 4482.13/s)  LR: 8.002e-04  Data: 0.022 (0.085)
Train: 4 [ 100/1251 (  8%)]  Loss: 6.241 (6.13)  Time: 0.176s, 5814.96/s  (0.209s, 4900.18/s)  LR: 8.002e-04  Data: 0.025 (0.064)
Train: 4 [ 150/1251 ( 12%)]  Loss: 6.306 (6.17)  Time: 0.171s, 5996.63/s  (0.201s, 5106.96/s)  LR: 8.002e-04  Data: 0.022 (0.052)
Train: 4 [ 200/1251 ( 16%)]  Loss: 6.016 (6.14)  Time: 0.185s, 5520.23/s  (0.198s, 5171.24/s)  LR: 8.002e-04  Data: 0.022 (0.046)
Train: 4 [ 250/1251 ( 20%)]  Loss: 6.111 (6.14)  Time: 0.190s, 5389.13/s  (0.196s, 5230.65/s)  LR: 8.002e-04  Data: 0.024 (0.044)
Train: 4 [ 300/1251 ( 24%)]  Loss: 6.061 (6.13)  Time: 0.165s, 6193.69/s  (0.195s, 5263.42/s)  LR: 8.002e-04  Data: 0.023 (0.043)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.952 (6.10)  Time: 0.166s, 6159.81/s  (0.193s, 5307.38/s)  LR: 8.002e-04  Data: 0.019 (0.042)
Train: 4 [ 400/1251 ( 32%)]  Loss: 6.131 (6.11)  Time: 0.207s, 4937.19/s  (0.193s, 5306.14/s)  LR: 8.002e-04  Data: 0.025 (0.040)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.134 (6.11)  Time: 0.201s, 5093.19/s  (0.193s, 5319.43/s)  LR: 8.002e-04  Data: 0.029 (0.039)
Train: 4 [ 500/1251 ( 40%)]  Loss: 6.136 (6.11)  Time: 0.203s, 5051.02/s  (0.192s, 5319.97/s)  LR: 8.002e-04  Data: 0.022 (0.039)
Train: 4 [ 550/1251 ( 44%)]  Loss: 6.071 (6.11)  Time: 0.172s, 5943.81/s  (0.192s, 5337.16/s)  LR: 8.002e-04  Data: 0.024 (0.039)
Train: 4 [ 600/1251 ( 48%)]  Loss: 6.088 (6.11)  Time: 0.190s, 5379.46/s  (0.192s, 5333.66/s)  LR: 8.002e-04  Data: 0.027 (0.039)
Train: 4 [ 650/1251 ( 52%)]  Loss: 6.114 (6.11)  Time: 0.172s, 5940.91/s  (0.192s, 5344.58/s)  LR: 8.002e-04  Data: 0.031 (0.039)
Train: 4 [ 700/1251 ( 56%)]  Loss: 6.115 (6.11)  Time: 0.191s, 5352.56/s  (0.191s, 5352.79/s)  LR: 8.002e-04  Data: 0.024 (0.038)
Train: 4 [ 750/1251 ( 60%)]  Loss: 6.072 (6.11)  Time: 0.172s, 5945.97/s  (0.192s, 5333.58/s)  LR: 8.002e-04  Data: 0.032 (0.037)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.863 (6.09)  Time: 0.198s, 5182.72/s  (0.192s, 5322.56/s)  LR: 8.002e-04  Data: 0.024 (0.037)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.845 (6.08)  Time: 0.170s, 6036.96/s  (0.192s, 5334.39/s)  LR: 8.002e-04  Data: 0.028 (0.036)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.910 (6.07)  Time: 0.172s, 5942.15/s  (0.192s, 5323.23/s)  LR: 8.002e-04  Data: 0.022 (0.035)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.982 (6.06)  Time: 0.178s, 5740.15/s  (0.192s, 5330.27/s)  LR: 8.002e-04  Data: 0.037 (0.035)
Train: 4 [1000/1251 ( 80%)]  Loss: 6.020 (6.06)  Time: 0.174s, 5890.28/s  (0.192s, 5326.43/s)  LR: 8.002e-04  Data: 0.025 (0.035)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.925 (6.06)  Time: 0.331s, 3091.36/s  (0.192s, 5323.34/s)  LR: 8.002e-04  Data: 0.025 (0.034)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.946 (6.05)  Time: 0.183s, 5594.82/s  (0.192s, 5325.91/s)  LR: 8.002e-04  Data: 0.021 (0.034)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.643 (6.03)  Time: 0.167s, 6146.75/s  (0.192s, 5322.64/s)  LR: 8.002e-04  Data: 0.031 (0.034)
Train: 4 [1200/1251 ( 96%)]  Loss: 6.082 (6.04)  Time: 0.182s, 5611.67/s  (0.193s, 5316.95/s)  LR: 8.002e-04  Data: 0.026 (0.033)
Train: 4 [1250/1251 (100%)]  Loss: 5.823 (6.03)  Time: 0.113s, 9039.03/s  (0.192s, 5322.59/s)  LR: 8.002e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.778 (1.778)  Loss:  3.4774 (3.4774)  Acc@1: 30.0781 (30.0781)  Acc@5: 58.5938 (58.5938)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  3.1454 (4.1957)  Acc@1: 42.2170 (19.4860)  Acc@5: 62.9717 (41.2960)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 5 [   0/1251 (  0%)]  Loss: 5.830 (5.83)  Time: 1.838s,  557.14/s  (1.838s,  557.14/s)  LR: 9.993e-04  Data: 1.721 (1.721)
Train: 5 [  50/1251 (  4%)]  Loss: 6.059 (5.94)  Time: 0.160s, 6418.27/s  (0.223s, 4589.01/s)  LR: 9.993e-04  Data: 0.028 (0.076)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.874 (5.92)  Time: 0.165s, 6187.58/s  (0.203s, 5048.16/s)  LR: 9.993e-04  Data: 0.022 (0.055)
Train: 5 [ 150/1251 ( 12%)]  Loss: 6.002 (5.94)  Time: 0.160s, 6385.13/s  (0.200s, 5130.37/s)  LR: 9.993e-04  Data: 0.028 (0.051)
Train: 5 [ 200/1251 ( 16%)]  Loss: 6.004 (5.95)  Time: 0.457s, 2241.75/s  (0.200s, 5121.54/s)  LR: 9.993e-04  Data: 0.026 (0.048)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.780 (5.92)  Time: 0.167s, 6145.62/s  (0.196s, 5224.15/s)  LR: 9.993e-04  Data: 0.026 (0.044)
Train: 5 [ 300/1251 ( 24%)]  Loss: 6.019 (5.94)  Time: 0.186s, 5501.64/s  (0.194s, 5266.98/s)  LR: 9.993e-04  Data: 0.025 (0.041)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.993 (5.95)  Time: 0.161s, 6367.02/s  (0.195s, 5243.34/s)  LR: 9.993e-04  Data: 0.026 (0.039)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.939 (5.94)  Time: 0.155s, 6618.56/s  (0.194s, 5276.43/s)  LR: 9.993e-04  Data: 0.024 (0.037)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.669 (5.92)  Time: 0.193s, 5299.80/s  (0.193s, 5305.11/s)  LR: 9.993e-04  Data: 0.026 (0.036)
Train: 5 [ 500/1251 ( 40%)]  Loss: 6.091 (5.93)  Time: 0.166s, 6184.77/s  (0.193s, 5303.43/s)  LR: 9.993e-04  Data: 0.035 (0.035)
Train: 5 [ 550/1251 ( 44%)]  Loss: 6.045 (5.94)  Time: 0.169s, 6056.19/s  (0.193s, 5313.37/s)  LR: 9.993e-04  Data: 0.024 (0.035)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.581 (5.91)  Time: 0.185s, 5526.71/s  (0.193s, 5313.46/s)  LR: 9.993e-04  Data: 0.021 (0.034)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.702 (5.90)  Time: 0.173s, 5911.57/s  (0.192s, 5322.25/s)  LR: 9.993e-04  Data: 0.025 (0.033)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.934 (5.90)  Time: 0.161s, 6353.77/s  (0.193s, 5317.41/s)  LR: 9.993e-04  Data: 0.025 (0.033)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.636 (5.88)  Time: 0.152s, 6726.55/s  (0.194s, 5291.76/s)  LR: 9.993e-04  Data: 0.024 (0.032)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.769 (5.88)  Time: 0.160s, 6398.90/s  (0.194s, 5290.42/s)  LR: 9.993e-04  Data: 0.025 (0.032)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.892 (5.88)  Time: 0.380s, 2691.48/s  (0.193s, 5294.36/s)  LR: 9.993e-04  Data: 0.021 (0.032)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.802 (5.87)  Time: 0.164s, 6241.79/s  (0.193s, 5302.40/s)  LR: 9.993e-04  Data: 0.025 (0.032)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.947 (5.88)  Time: 0.193s, 5306.02/s  (0.194s, 5289.12/s)  LR: 9.993e-04  Data: 0.028 (0.031)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.883 (5.88)  Time: 0.169s, 6044.97/s  (0.194s, 5288.37/s)  LR: 9.993e-04  Data: 0.039 (0.031)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.767 (5.87)  Time: 0.180s, 5702.71/s  (0.194s, 5286.27/s)  LR: 9.993e-04  Data: 0.031 (0.031)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.670 (5.86)  Time: 0.156s, 6559.06/s  (0.194s, 5272.24/s)  LR: 9.993e-04  Data: 0.028 (0.031)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.942 (5.87)  Time: 0.369s, 2777.87/s  (0.194s, 5279.91/s)  LR: 9.993e-04  Data: 0.030 (0.031)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.348 (5.85)  Time: 0.188s, 5461.33/s  (0.194s, 5282.53/s)  LR: 9.993e-04  Data: 0.024 (0.031)
Train: 5 [1250/1251 (100%)]  Loss: 5.513 (5.83)  Time: 0.113s, 9073.69/s  (0.193s, 5296.21/s)  LR: 9.993e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.794 (1.794)  Loss:  3.0450 (3.0450)  Acc@1: 38.0859 (38.0859)  Acc@5: 66.7969 (66.7969)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  2.6985 (3.8092)  Acc@1: 49.0566 (24.7440)  Acc@5: 68.8679 (48.5900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 6 [   0/1251 (  0%)]  Loss: 5.912 (5.91)  Time: 1.748s,  585.89/s  (1.748s,  585.89/s)  LR: 9.990e-04  Data: 1.622 (1.622)
Train: 6 [  50/1251 (  4%)]  Loss: 5.469 (5.69)  Time: 0.157s, 6537.91/s  (0.225s, 4560.15/s)  LR: 9.990e-04  Data: 0.026 (0.064)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.632 (5.67)  Time: 0.167s, 6147.79/s  (0.205s, 4985.76/s)  LR: 9.990e-04  Data: 0.029 (0.045)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.636 (5.66)  Time: 0.153s, 6686.75/s  (0.200s, 5113.45/s)  LR: 9.990e-04  Data: 0.023 (0.040)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.800 (5.69)  Time: 0.351s, 2920.94/s  (0.198s, 5162.74/s)  LR: 9.990e-04  Data: 0.026 (0.036)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.464 (5.65)  Time: 0.174s, 5880.14/s  (0.198s, 5182.02/s)  LR: 9.990e-04  Data: 0.022 (0.035)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.642 (5.65)  Time: 0.189s, 5423.06/s  (0.196s, 5223.12/s)  LR: 9.990e-04  Data: 0.038 (0.033)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.839 (5.67)  Time: 0.171s, 6002.58/s  (0.195s, 5250.76/s)  LR: 9.990e-04  Data: 0.018 (0.032)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.700 (5.68)  Time: 0.180s, 5690.68/s  (0.194s, 5278.55/s)  LR: 9.990e-04  Data: 0.029 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.722 (5.68)  Time: 0.176s, 5832.00/s  (0.194s, 5290.92/s)  LR: 9.990e-04  Data: 0.025 (0.031)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.824 (5.69)  Time: 0.187s, 5473.81/s  (0.193s, 5307.85/s)  LR: 9.990e-04  Data: 0.026 (0.031)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.829 (5.71)  Time: 0.158s, 6493.02/s  (0.193s, 5306.72/s)  LR: 9.990e-04  Data: 0.024 (0.030)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.768 (5.71)  Time: 0.212s, 4830.63/s  (0.193s, 5304.80/s)  LR: 9.990e-04  Data: 0.028 (0.030)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.414 (5.69)  Time: 0.369s, 2773.99/s  (0.194s, 5285.01/s)  LR: 9.990e-04  Data: 0.031 (0.030)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.754 (5.69)  Time: 0.169s, 6072.44/s  (0.194s, 5274.39/s)  LR: 9.990e-04  Data: 0.027 (0.029)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.635 (5.69)  Time: 0.169s, 6060.91/s  (0.194s, 5282.81/s)  LR: 9.990e-04  Data: 0.024 (0.029)
Train: 6 [ 800/1251 ( 64%)]  Loss: 5.940 (5.70)  Time: 0.177s, 5798.85/s  (0.194s, 5276.11/s)  LR: 9.990e-04  Data: 0.030 (0.029)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.810 (5.71)  Time: 0.160s, 6408.74/s  (0.194s, 5271.70/s)  LR: 9.990e-04  Data: 0.025 (0.029)
Train: 6 [ 900/1251 ( 72%)]  Loss: 5.505 (5.70)  Time: 0.177s, 5795.40/s  (0.194s, 5268.14/s)  LR: 9.990e-04  Data: 0.028 (0.029)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.740 (5.70)  Time: 0.169s, 6059.13/s  (0.194s, 5267.29/s)  LR: 9.990e-04  Data: 0.029 (0.029)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.468 (5.69)  Time: 0.178s, 5752.13/s  (0.195s, 5260.35/s)  LR: 9.990e-04  Data: 0.028 (0.029)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.530 (5.68)  Time: 0.184s, 5566.44/s  (0.195s, 5259.10/s)  LR: 9.990e-04  Data: 0.029 (0.029)
Train: 6 [1100/1251 ( 88%)]  Loss: 6.006 (5.70)  Time: 0.209s, 4896.02/s  (0.195s, 5259.86/s)  LR: 9.990e-04  Data: 0.033 (0.029)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.630 (5.69)  Time: 0.186s, 5507.34/s  (0.195s, 5255.65/s)  LR: 9.990e-04  Data: 0.030 (0.028)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.834 (5.70)  Time: 0.157s, 6522.08/s  (0.195s, 5244.93/s)  LR: 9.990e-04  Data: 0.021 (0.028)
Train: 6 [1250/1251 (100%)]  Loss: 5.647 (5.70)  Time: 0.113s, 9056.64/s  (0.195s, 5256.96/s)  LR: 9.990e-04  Data: 0.000 (0.028)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.786 (1.786)  Loss:  2.6040 (2.6040)  Acc@1: 44.3359 (44.3359)  Acc@5: 73.3398 (73.3398)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  2.2701 (3.4928)  Acc@1: 54.9528 (29.5040)  Acc@5: 75.7076 (54.9720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 7 [   0/1251 (  0%)]  Loss: 5.356 (5.36)  Time: 1.729s,  592.31/s  (1.729s,  592.31/s)  LR: 9.987e-04  Data: 1.577 (1.577)
Train: 7 [  50/1251 (  4%)]  Loss: 5.615 (5.49)  Time: 0.170s, 6024.69/s  (0.227s, 4506.79/s)  LR: 9.987e-04  Data: 0.021 (0.059)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.603 (5.52)  Time: 0.172s, 5941.43/s  (0.206s, 4965.18/s)  LR: 9.987e-04  Data: 0.021 (0.043)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.886 (5.62)  Time: 0.187s, 5465.71/s  (0.201s, 5094.65/s)  LR: 9.987e-04  Data: 0.020 (0.038)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.485 (5.59)  Time: 0.160s, 6414.75/s  (0.199s, 5137.34/s)  LR: 9.987e-04  Data: 0.025 (0.035)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.419 (5.56)  Time: 0.187s, 5464.25/s  (0.200s, 5132.23/s)  LR: 9.987e-04  Data: 0.025 (0.033)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.270 (5.52)  Time: 0.156s, 6565.20/s  (0.197s, 5208.34/s)  LR: 9.987e-04  Data: 0.021 (0.032)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.764 (5.55)  Time: 0.166s, 6170.98/s  (0.198s, 5180.70/s)  LR: 9.987e-04  Data: 0.026 (0.031)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.825 (5.58)  Time: 0.162s, 6338.48/s  (0.197s, 5208.26/s)  LR: 9.987e-04  Data: 0.027 (0.030)
Train: 7 [ 450/1251 ( 36%)]  Loss: 5.606 (5.58)  Time: 0.520s, 1970.62/s  (0.196s, 5219.02/s)  LR: 9.987e-04  Data: 0.023 (0.030)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.393 (5.57)  Time: 0.179s, 5728.17/s  (0.196s, 5234.12/s)  LR: 9.987e-04  Data: 0.025 (0.029)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.440 (5.56)  Time: 0.156s, 6562.56/s  (0.196s, 5236.48/s)  LR: 9.987e-04  Data: 0.030 (0.029)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.449 (5.55)  Time: 0.157s, 6529.27/s  (0.195s, 5241.62/s)  LR: 9.987e-04  Data: 0.029 (0.029)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.547 (5.55)  Time: 0.268s, 3820.89/s  (0.195s, 5247.11/s)  LR: 9.987e-04  Data: 0.024 (0.029)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.868 (5.57)  Time: 0.158s, 6500.05/s  (0.195s, 5261.70/s)  LR: 9.987e-04  Data: 0.026 (0.029)
Train: 7 [ 750/1251 ( 60%)]  Loss: 5.552 (5.57)  Time: 0.164s, 6255.50/s  (0.194s, 5266.10/s)  LR: 9.987e-04  Data: 0.031 (0.029)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.440 (5.56)  Time: 0.445s, 2301.16/s  (0.195s, 5245.76/s)  LR: 9.987e-04  Data: 0.323 (0.031)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.701 (5.57)  Time: 0.182s, 5618.17/s  (0.195s, 5250.20/s)  LR: 9.987e-04  Data: 0.024 (0.032)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.524 (5.57)  Time: 0.161s, 6350.07/s  (0.195s, 5259.83/s)  LR: 9.987e-04  Data: 0.029 (0.032)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.233 (5.55)  Time: 0.163s, 6282.21/s  (0.195s, 5257.29/s)  LR: 9.987e-04  Data: 0.034 (0.033)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.378 (5.54)  Time: 0.263s, 3898.02/s  (0.195s, 5248.40/s)  LR: 9.987e-04  Data: 0.136 (0.034)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.560 (5.54)  Time: 0.160s, 6411.46/s  (0.196s, 5236.06/s)  LR: 9.987e-04  Data: 0.023 (0.033)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.371 (5.53)  Time: 0.167s, 6114.62/s  (0.195s, 5239.70/s)  LR: 9.987e-04  Data: 0.031 (0.033)
Train: 7 [1150/1251 ( 92%)]  Loss: 5.249 (5.52)  Time: 0.157s, 6510.17/s  (0.197s, 5204.33/s)  LR: 9.987e-04  Data: 0.024 (0.033)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.476 (5.52)  Time: 0.183s, 5589.69/s  (0.198s, 5172.56/s)  LR: 9.987e-04  Data: 0.028 (0.033)
Train: 7 [1250/1251 (100%)]  Loss: 5.640 (5.53)  Time: 0.114s, 8996.75/s  (0.197s, 5192.45/s)  LR: 9.987e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.881 (1.881)  Loss:  2.4212 (2.4212)  Acc@1: 49.8047 (49.8047)  Acc@5: 77.9297 (77.9297)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  2.2655 (3.2475)  Acc@1: 56.4859 (33.9980)  Acc@5: 76.8868 (59.8440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 8 [   0/1251 (  0%)]  Loss: 5.728 (5.73)  Time: 1.938s,  528.38/s  (1.938s,  528.38/s)  LR: 9.982e-04  Data: 1.468 (1.468)
Train: 8 [  50/1251 (  4%)]  Loss: 5.248 (5.49)  Time: 0.154s, 6650.72/s  (0.237s, 4326.94/s)  LR: 9.982e-04  Data: 0.026 (0.054)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.755 (5.58)  Time: 0.161s, 6363.75/s  (0.214s, 4790.65/s)  LR: 9.982e-04  Data: 0.025 (0.040)
Train: 8 [ 150/1251 ( 12%)]  Loss: 5.058 (5.45)  Time: 0.167s, 6146.36/s  (0.204s, 5028.83/s)  LR: 9.982e-04  Data: 0.028 (0.036)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.273 (5.41)  Time: 0.191s, 5352.28/s  (0.199s, 5141.49/s)  LR: 9.982e-04  Data: 0.027 (0.034)
Train: 8 [ 250/1251 ( 20%)]  Loss: 5.303 (5.39)  Time: 0.186s, 5501.07/s  (0.198s, 5169.06/s)  LR: 9.982e-04  Data: 0.029 (0.033)
Train: 8 [ 300/1251 ( 24%)]  Loss: 5.249 (5.37)  Time: 0.264s, 3872.85/s  (0.197s, 5186.60/s)  LR: 9.982e-04  Data: 0.041 (0.032)
Train: 8 [ 350/1251 ( 28%)]  Loss: 5.284 (5.36)  Time: 0.172s, 5955.42/s  (0.196s, 5227.04/s)  LR: 9.982e-04  Data: 0.025 (0.031)
Train: 8 [ 400/1251 ( 32%)]  Loss: 5.514 (5.38)  Time: 0.163s, 6298.60/s  (0.195s, 5251.23/s)  LR: 9.982e-04  Data: 0.029 (0.032)
Train: 8 [ 450/1251 ( 36%)]  Loss: 5.168 (5.36)  Time: 0.164s, 6255.39/s  (0.195s, 5249.23/s)  LR: 9.982e-04  Data: 0.031 (0.032)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.579 (5.38)  Time: 0.159s, 6444.55/s  (0.197s, 5203.74/s)  LR: 9.982e-04  Data: 0.028 (0.031)
Train: 8 [ 550/1251 ( 44%)]  Loss: 5.316 (5.37)  Time: 0.175s, 5864.34/s  (0.196s, 5228.53/s)  LR: 9.982e-04  Data: 0.025 (0.031)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.608 (5.39)  Time: 0.145s, 7082.01/s  (0.196s, 5233.86/s)  LR: 9.982e-04  Data: 0.020 (0.031)
Train: 8 [ 650/1251 ( 52%)]  Loss: 4.938 (5.36)  Time: 0.184s, 5557.17/s  (0.195s, 5242.69/s)  LR: 9.982e-04  Data: 0.026 (0.030)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.641 (5.38)  Time: 0.158s, 6496.97/s  (0.196s, 5233.46/s)  LR: 9.982e-04  Data: 0.027 (0.030)
Train: 8 [ 750/1251 ( 60%)]  Loss: 5.352 (5.38)  Time: 0.177s, 5798.02/s  (0.196s, 5231.65/s)  LR: 9.982e-04  Data: 0.032 (0.030)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.180 (5.36)  Time: 0.162s, 6329.90/s  (0.196s, 5224.96/s)  LR: 9.982e-04  Data: 0.025 (0.030)
Train: 8 [ 850/1251 ( 68%)]  Loss: 5.461 (5.37)  Time: 0.175s, 5840.52/s  (0.196s, 5218.61/s)  LR: 9.982e-04  Data: 0.022 (0.029)
Train: 8 [ 900/1251 ( 72%)]  Loss: 5.301 (5.37)  Time: 0.159s, 6422.99/s  (0.196s, 5221.81/s)  LR: 9.982e-04  Data: 0.033 (0.029)
Train: 8 [ 950/1251 ( 76%)]  Loss: 5.174 (5.36)  Time: 0.163s, 6281.10/s  (0.196s, 5232.41/s)  LR: 9.982e-04  Data: 0.025 (0.029)
Train: 8 [1000/1251 ( 80%)]  Loss: 5.484 (5.36)  Time: 0.293s, 3495.68/s  (0.196s, 5229.49/s)  LR: 9.982e-04  Data: 0.030 (0.029)
Train: 8 [1050/1251 ( 84%)]  Loss: 5.327 (5.36)  Time: 0.173s, 5911.67/s  (0.196s, 5230.28/s)  LR: 9.982e-04  Data: 0.019 (0.029)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.711 (5.38)  Time: 0.176s, 5832.50/s  (0.196s, 5224.77/s)  LR: 9.982e-04  Data: 0.023 (0.029)
Train: 8 [1150/1251 ( 92%)]  Loss: 5.521 (5.38)  Time: 0.182s, 5629.21/s  (0.196s, 5215.84/s)  LR: 9.982e-04  Data: 0.027 (0.029)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.105 (5.37)  Time: 0.171s, 5998.73/s  (0.197s, 5210.02/s)  LR: 9.982e-04  Data: 0.027 (0.029)
Train: 8 [1250/1251 (100%)]  Loss: 5.468 (5.37)  Time: 0.114s, 8976.12/s  (0.196s, 5222.78/s)  LR: 9.982e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 4.029 (4.029)  Loss:  2.1832 (2.1832)  Acc@1: 53.7109 (53.7109)  Acc@5: 80.8594 (80.8594)
Test: [  48/48]  Time: 0.019 (0.262)  Loss:  2.0232 (3.0964)  Acc@1: 61.5566 (36.8840)  Acc@5: 79.4811 (62.9120)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 9 [   0/1251 (  0%)]  Loss: 5.303 (5.30)  Time: 1.919s,  533.50/s  (1.919s,  533.50/s)  LR: 9.978e-04  Data: 1.792 (1.792)
Train: 9 [  50/1251 (  4%)]  Loss: 5.595 (5.45)  Time: 0.152s, 6744.99/s  (0.224s, 4567.08/s)  LR: 9.978e-04  Data: 0.028 (0.072)
Train: 9 [ 100/1251 (  8%)]  Loss: 5.274 (5.39)  Time: 0.193s, 5309.30/s  (0.208s, 4931.32/s)  LR: 9.978e-04  Data: 0.023 (0.056)
Train: 9 [ 150/1251 ( 12%)]  Loss: 5.184 (5.34)  Time: 0.180s, 5673.46/s  (0.200s, 5107.92/s)  LR: 9.978e-04  Data: 0.024 (0.048)
Train: 9 [ 200/1251 ( 16%)]  Loss: 5.366 (5.34)  Time: 0.223s, 4586.86/s  (0.199s, 5155.84/s)  LR: 9.978e-04  Data: 0.100 (0.043)
Train: 9 [ 250/1251 ( 20%)]  Loss: 5.137 (5.31)  Time: 0.175s, 5853.28/s  (0.197s, 5188.80/s)  LR: 9.978e-04  Data: 0.023 (0.040)
Train: 9 [ 300/1251 ( 24%)]  Loss: 5.615 (5.35)  Time: 0.168s, 6086.46/s  (0.196s, 5224.75/s)  LR: 9.978e-04  Data: 0.024 (0.040)
Train: 9 [ 350/1251 ( 28%)]  Loss: 5.292 (5.35)  Time: 0.173s, 5906.15/s  (0.195s, 5251.95/s)  LR: 9.978e-04  Data: 0.024 (0.041)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.311 (5.34)  Time: 0.168s, 6089.07/s  (0.195s, 5256.22/s)  LR: 9.978e-04  Data: 0.024 (0.040)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.189 (5.33)  Time: 0.181s, 5672.77/s  (0.195s, 5258.48/s)  LR: 9.978e-04  Data: 0.021 (0.040)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.853 (5.28)  Time: 0.174s, 5901.92/s  (0.193s, 5292.62/s)  LR: 9.978e-04  Data: 0.030 (0.039)
Train: 9 [ 550/1251 ( 44%)]  Loss: 5.374 (5.29)  Time: 0.342s, 2991.86/s  (0.194s, 5287.22/s)  LR: 9.978e-04  Data: 0.213 (0.039)
Train: 9 [ 600/1251 ( 48%)]  Loss: 5.564 (5.31)  Time: 0.191s, 5352.72/s  (0.194s, 5284.88/s)  LR: 9.978e-04  Data: 0.024 (0.039)
Train: 9 [ 650/1251 ( 52%)]  Loss: 5.214 (5.30)  Time: 0.160s, 6399.08/s  (0.193s, 5293.71/s)  LR: 9.978e-04  Data: 0.019 (0.040)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.631 (5.33)  Time: 0.166s, 6160.64/s  (0.194s, 5282.22/s)  LR: 9.978e-04  Data: 0.026 (0.041)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.308 (5.33)  Time: 0.184s, 5567.91/s  (0.195s, 5264.44/s)  LR: 9.978e-04  Data: 0.024 (0.042)
Train: 9 [ 800/1251 ( 64%)]  Loss: 5.442 (5.33)  Time: 0.186s, 5493.63/s  (0.195s, 5264.43/s)  LR: 9.978e-04  Data: 0.028 (0.043)
Train: 9 [ 850/1251 ( 68%)]  Loss: 5.416 (5.34)  Time: 0.169s, 6052.09/s  (0.195s, 5261.11/s)  LR: 9.978e-04  Data: 0.025 (0.043)
Train: 9 [ 900/1251 ( 72%)]  Loss: 5.497 (5.35)  Time: 0.166s, 6163.82/s  (0.194s, 5267.10/s)  LR: 9.978e-04  Data: 0.021 (0.043)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.349 (5.35)  Time: 0.188s, 5446.34/s  (0.195s, 5252.76/s)  LR: 9.978e-04  Data: 0.023 (0.043)
Train: 9 [1000/1251 ( 80%)]  Loss: 5.467 (5.35)  Time: 0.161s, 6374.86/s  (0.195s, 5257.95/s)  LR: 9.978e-04  Data: 0.026 (0.043)
Train: 9 [1050/1251 ( 84%)]  Loss: 5.391 (5.35)  Time: 0.170s, 6013.88/s  (0.195s, 5253.10/s)  LR: 9.978e-04  Data: 0.025 (0.044)
Train: 9 [1100/1251 ( 88%)]  Loss: 5.130 (5.34)  Time: 0.182s, 5625.62/s  (0.195s, 5240.71/s)  LR: 9.978e-04  Data: 0.022 (0.045)
Train: 9 [1150/1251 ( 92%)]  Loss: 5.149 (5.34)  Time: 0.171s, 5980.56/s  (0.195s, 5240.06/s)  LR: 9.978e-04  Data: 0.026 (0.045)
Train: 9 [1200/1251 ( 96%)]  Loss: 5.052 (5.32)  Time: 0.172s, 5959.32/s  (0.196s, 5225.65/s)  LR: 9.978e-04  Data: 0.024 (0.044)
Train: 9 [1250/1251 (100%)]  Loss: 5.315 (5.32)  Time: 0.113s, 9042.38/s  (0.195s, 5239.37/s)  LR: 9.978e-04  Data: 0.000 (0.044)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.817 (1.817)  Loss:  2.0447 (2.0447)  Acc@1: 57.8125 (57.8125)  Acc@5: 82.6172 (82.6172)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.7527 (2.9423)  Acc@1: 66.3915 (39.7420)  Acc@5: 83.4906 (65.7860)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 10 [   0/1251 (  0%)]  Loss: 5.185 (5.18)  Time: 1.842s,  555.84/s  (1.842s,  555.84/s)  LR: 9.973e-04  Data: 1.711 (1.711)
Train: 10 [  50/1251 (  4%)]  Loss: 5.266 (5.23)  Time: 0.163s, 6293.20/s  (0.225s, 4553.77/s)  LR: 9.973e-04  Data: 0.026 (0.075)
Train: 10 [ 100/1251 (  8%)]  Loss: 5.171 (5.21)  Time: 0.170s, 6033.97/s  (0.207s, 4953.55/s)  LR: 9.973e-04  Data: 0.022 (0.060)
Train: 10 [ 150/1251 ( 12%)]  Loss: 5.356 (5.24)  Time: 0.183s, 5592.71/s  (0.204s, 5012.21/s)  LR: 9.973e-04  Data: 0.018 (0.050)
Train: 10 [ 200/1251 ( 16%)]  Loss: 5.287 (5.25)  Time: 0.170s, 6012.35/s  (0.199s, 5154.32/s)  LR: 9.973e-04  Data: 0.027 (0.045)
Train: 10 [ 250/1251 ( 20%)]  Loss: 5.353 (5.27)  Time: 0.167s, 6114.49/s  (0.197s, 5189.76/s)  LR: 9.973e-04  Data: 0.026 (0.041)
Train: 10 [ 300/1251 ( 24%)]  Loss: 5.407 (5.29)  Time: 0.171s, 6004.60/s  (0.196s, 5221.74/s)  LR: 9.973e-04  Data: 0.030 (0.039)
Train: 10 [ 350/1251 ( 28%)]  Loss: 5.230 (5.28)  Time: 0.166s, 6163.54/s  (0.196s, 5236.20/s)  LR: 9.973e-04  Data: 0.024 (0.037)
Train: 10 [ 400/1251 ( 32%)]  Loss: 5.388 (5.29)  Time: 0.175s, 5850.82/s  (0.195s, 5252.06/s)  LR: 9.973e-04  Data: 0.031 (0.036)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.859 (5.25)  Time: 0.224s, 4562.53/s  (0.195s, 5255.59/s)  LR: 9.973e-04  Data: 0.031 (0.035)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.210 (5.25)  Time: 0.192s, 5341.32/s  (0.195s, 5253.42/s)  LR: 9.973e-04  Data: 0.021 (0.035)
Train: 10 [ 550/1251 ( 44%)]  Loss: 5.083 (5.23)  Time: 0.170s, 6032.02/s  (0.195s, 5252.44/s)  LR: 9.973e-04  Data: 0.021 (0.037)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.194 (5.23)  Time: 0.188s, 5442.07/s  (0.195s, 5263.56/s)  LR: 9.973e-04  Data: 0.033 (0.037)
Train: 10 [ 650/1251 ( 52%)]  Loss: 5.230 (5.23)  Time: 0.186s, 5502.53/s  (0.194s, 5266.79/s)  LR: 9.973e-04  Data: 0.030 (0.036)
Train: 10 [ 700/1251 ( 56%)]  Loss: 5.119 (5.22)  Time: 0.160s, 6407.49/s  (0.194s, 5270.13/s)  LR: 9.973e-04  Data: 0.028 (0.035)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.965 (5.21)  Time: 0.164s, 6225.27/s  (0.194s, 5274.71/s)  LR: 9.973e-04  Data: 0.030 (0.036)
Train: 10 [ 800/1251 ( 64%)]  Loss: 5.342 (5.21)  Time: 0.175s, 5847.65/s  (0.194s, 5269.80/s)  LR: 9.973e-04  Data: 0.022 (0.037)
Train: 10 [ 850/1251 ( 68%)]  Loss: 5.140 (5.21)  Time: 0.173s, 5920.85/s  (0.195s, 5260.93/s)  LR: 9.973e-04  Data: 0.024 (0.038)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.731 (5.19)  Time: 0.192s, 5321.34/s  (0.195s, 5249.78/s)  LR: 9.973e-04  Data: 0.021 (0.039)
Train: 10 [ 950/1251 ( 76%)]  Loss: 5.347 (5.19)  Time: 0.164s, 6236.85/s  (0.195s, 5243.12/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1000/1251 ( 80%)]  Loss: 5.041 (5.19)  Time: 0.176s, 5816.33/s  (0.195s, 5246.77/s)  LR: 9.973e-04  Data: 0.024 (0.040)
Train: 10 [1050/1251 ( 84%)]  Loss: 5.005 (5.18)  Time: 0.191s, 5370.43/s  (0.195s, 5252.49/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1100/1251 ( 88%)]  Loss: 5.218 (5.18)  Time: 0.148s, 6915.77/s  (0.195s, 5245.64/s)  LR: 9.973e-04  Data: 0.018 (0.041)
Train: 10 [1150/1251 ( 92%)]  Loss: 5.040 (5.17)  Time: 0.148s, 6935.11/s  (0.195s, 5241.42/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.922 (5.16)  Time: 0.223s, 4598.99/s  (0.195s, 5241.46/s)  LR: 9.973e-04  Data: 0.025 (0.039)
Train: 10 [1250/1251 (100%)]  Loss: 5.318 (5.17)  Time: 0.114s, 8996.54/s  (0.195s, 5257.48/s)  LR: 9.973e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.925 (1.925)  Loss:  1.9760 (1.9760)  Acc@1: 60.5469 (60.5469)  Acc@5: 82.8125 (82.8125)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.8242 (2.8344)  Acc@1: 64.5047 (41.6600)  Acc@5: 81.8396 (67.5520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)

Train: 11 [   0/1251 (  0%)]  Loss: 5.336 (5.34)  Time: 1.853s,  552.55/s  (1.853s,  552.55/s)  LR: 9.967e-04  Data: 1.732 (1.732)
Train: 11 [  50/1251 (  4%)]  Loss: 5.435 (5.39)  Time: 0.181s, 5656.39/s  (0.222s, 4615.19/s)  LR: 9.967e-04  Data: 0.029 (0.065)
Train: 11 [ 100/1251 (  8%)]  Loss: 5.151 (5.31)  Time: 0.163s, 6296.71/s  (0.206s, 4982.79/s)  LR: 9.967e-04  Data: 0.027 (0.048)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.966 (5.22)  Time: 0.260s, 3932.55/s  (0.202s, 5057.10/s)  LR: 9.967e-04  Data: 0.027 (0.041)
Train: 11 [ 200/1251 ( 16%)]  Loss: 5.175 (5.21)  Time: 0.169s, 6060.82/s  (0.200s, 5125.52/s)  LR: 9.967e-04  Data: 0.028 (0.038)
Train: 11 [ 250/1251 ( 20%)]  Loss: 5.126 (5.20)  Time: 0.192s, 5337.79/s  (0.198s, 5178.99/s)  LR: 9.967e-04  Data: 0.031 (0.036)
Train: 11 [ 300/1251 ( 24%)]  Loss: 5.442 (5.23)  Time: 0.150s, 6846.35/s  (0.196s, 5229.42/s)  LR: 9.967e-04  Data: 0.023 (0.036)
Train: 11 [ 350/1251 ( 28%)]  Loss: 5.155 (5.22)  Time: 0.159s, 6443.28/s  (0.196s, 5233.22/s)  LR: 9.967e-04  Data: 0.026 (0.035)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.722 (5.17)  Time: 0.168s, 6097.85/s  (0.195s, 5255.91/s)  LR: 9.967e-04  Data: 0.026 (0.034)
Train: 11 [ 450/1251 ( 36%)]  Loss: 5.203 (5.17)  Time: 0.167s, 6116.37/s  (0.195s, 5256.60/s)  LR: 9.967e-04  Data: 0.031 (0.033)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.753 (5.13)  Time: 0.152s, 6740.92/s  (0.195s, 5252.96/s)  LR: 9.967e-04  Data: 0.023 (0.032)
Train: 11 [ 550/1251 ( 44%)]  Loss: 5.230 (5.14)  Time: 0.164s, 6230.34/s  (0.194s, 5275.35/s)  LR: 9.967e-04  Data: 0.030 (0.032)
Train: 11 [ 600/1251 ( 48%)]  Loss: 5.353 (5.16)  Time: 0.153s, 6711.52/s  (0.194s, 5280.31/s)  LR: 9.967e-04  Data: 0.026 (0.032)
Train: 11 [ 650/1251 ( 52%)]  Loss: 5.080 (5.15)  Time: 0.178s, 5768.25/s  (0.193s, 5299.05/s)  LR: 9.967e-04  Data: 0.026 (0.031)
Train: 11 [ 700/1251 ( 56%)]  Loss: 5.148 (5.15)  Time: 0.172s, 5962.28/s  (0.194s, 5290.31/s)  LR: 9.967e-04  Data: 0.025 (0.031)
Train: 11 [ 750/1251 ( 60%)]  Loss: 5.122 (5.15)  Time: 0.165s, 6217.76/s  (0.193s, 5293.52/s)  LR: 9.967e-04  Data: 0.027 (0.031)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.952 (5.14)  Time: 0.169s, 6074.21/s  (0.193s, 5302.21/s)  LR: 9.967e-04  Data: 0.024 (0.031)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.957 (5.13)  Time: 0.265s, 3870.04/s  (0.193s, 5298.85/s)  LR: 9.967e-04  Data: 0.020 (0.031)
Train: 11 [ 900/1251 ( 72%)]  Loss: 5.410 (5.14)  Time: 0.168s, 6082.06/s  (0.193s, 5296.18/s)  LR: 9.967e-04  Data: 0.029 (0.030)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.962 (5.13)  Time: 0.155s, 6587.22/s  (0.194s, 5284.29/s)  LR: 9.967e-04  Data: 0.023 (0.030)
Train: 11 [1000/1251 ( 80%)]  Loss: 5.038 (5.13)  Time: 0.374s, 2737.01/s  (0.194s, 5281.16/s)  LR: 9.967e-04  Data: 0.027 (0.030)
Train: 11 [1050/1251 ( 84%)]  Loss: 5.143 (5.13)  Time: 0.165s, 6200.10/s  (0.194s, 5282.29/s)  LR: 9.967e-04  Data: 0.034 (0.030)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.955 (5.12)  Time: 0.178s, 5765.06/s  (0.195s, 5242.14/s)  LR: 9.967e-04  Data: 0.028 (0.030)
Train: 11 [1150/1251 ( 92%)]  Loss: 5.265 (5.13)  Time: 0.180s, 5681.02/s  (0.195s, 5241.52/s)  LR: 9.967e-04  Data: 0.025 (0.030)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.903 (5.12)  Time: 0.371s, 2757.08/s  (0.195s, 5238.15/s)  LR: 9.967e-04  Data: 0.024 (0.029)
Train: 11 [1250/1251 (100%)]  Loss: 4.980 (5.11)  Time: 0.113s, 9022.60/s  (0.195s, 5251.52/s)  LR: 9.967e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.869 (1.869)  Loss:  1.8219 (1.8219)  Acc@1: 64.7461 (64.7461)  Acc@5: 85.4492 (85.4492)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.8821 (2.7561)  Acc@1: 66.1557 (43.6760)  Acc@5: 80.6604 (69.3620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)

Train: 12 [   0/1251 (  0%)]  Loss: 5.164 (5.16)  Time: 1.940s,  527.97/s  (1.940s,  527.97/s)  LR: 9.961e-04  Data: 1.807 (1.807)
Train: 12 [  50/1251 (  4%)]  Loss: 5.031 (5.10)  Time: 0.180s, 5703.15/s  (0.228s, 4499.60/s)  LR: 9.961e-04  Data: 0.028 (0.086)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.744 (4.98)  Time: 0.174s, 5893.09/s  (0.210s, 4868.72/s)  LR: 9.961e-04  Data: 0.025 (0.065)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.683 (4.91)  Time: 0.176s, 5821.09/s  (0.205s, 4995.49/s)  LR: 9.961e-04  Data: 0.022 (0.060)
Train: 12 [ 200/1251 ( 16%)]  Loss: 5.412 (5.01)  Time: 0.181s, 5662.66/s  (0.200s, 5117.40/s)  LR: 9.961e-04  Data: 0.035 (0.055)
Train: 12 [ 250/1251 ( 20%)]  Loss: 5.263 (5.05)  Time: 0.179s, 5707.68/s  (0.198s, 5177.64/s)  LR: 9.961e-04  Data: 0.025 (0.051)
Train: 12 [ 300/1251 ( 24%)]  Loss: 5.109 (5.06)  Time: 0.174s, 5899.01/s  (0.197s, 5209.20/s)  LR: 9.961e-04  Data: 0.022 (0.047)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.968 (5.05)  Time: 0.173s, 5919.37/s  (0.196s, 5227.40/s)  LR: 9.961e-04  Data: 0.024 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 12 [ 400/1251 ( 32%)]  Loss: 5.354 (5.08)  Time: 0.171s, 5980.27/s  (0.196s, 5237.18/s)  LR: 9.961e-04  Data: 0.025 (0.043)
Train: 12 [ 450/1251 ( 36%)]  Loss: 5.086 (5.08)  Time: 0.187s, 5490.52/s  (0.195s, 5255.79/s)  LR: 9.961e-04  Data: 0.035 (0.041)
Train: 12 [ 500/1251 ( 40%)]  Loss: 5.233 (5.10)  Time: 0.170s, 6022.33/s  (0.195s, 5260.06/s)  LR: 9.961e-04  Data: 0.037 (0.039)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.897 (5.08)  Time: 0.157s, 6501.87/s  (0.194s, 5277.15/s)  LR: 9.961e-04  Data: 0.029 (0.038)
Train: 12 [ 600/1251 ( 48%)]  Loss: 5.172 (5.09)  Time: 0.169s, 6073.78/s  (0.194s, 5270.57/s)  LR: 9.961e-04  Data: 0.023 (0.037)
Train: 12 [ 650/1251 ( 52%)]  Loss: 5.176 (5.09)  Time: 0.171s, 5971.29/s  (0.194s, 5275.41/s)  LR: 9.961e-04  Data: 0.027 (0.037)
Train: 12 [ 700/1251 ( 56%)]  Loss: 5.012 (5.09)  Time: 0.167s, 6120.02/s  (0.194s, 5274.03/s)  LR: 9.961e-04  Data: 0.028 (0.036)
Train: 12 [ 750/1251 ( 60%)]  Loss: 5.347 (5.10)  Time: 0.163s, 6274.02/s  (0.194s, 5269.04/s)  LR: 9.961e-04  Data: 0.024 (0.036)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.934 (5.09)  Time: 0.165s, 6205.64/s  (0.194s, 5267.60/s)  LR: 9.961e-04  Data: 0.029 (0.036)
Train: 12 [ 850/1251 ( 68%)]  Loss: 5.004 (5.09)  Time: 0.175s, 5835.71/s  (0.195s, 5261.68/s)  LR: 9.961e-04  Data: 0.027 (0.036)
Train: 12 [ 900/1251 ( 72%)]  Loss: 5.213 (5.09)  Time: 0.159s, 6453.10/s  (0.194s, 5265.77/s)  LR: 9.961e-04  Data: 0.024 (0.035)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.826 (5.08)  Time: 0.183s, 5593.06/s  (0.195s, 5261.99/s)  LR: 9.961e-04  Data: 0.031 (0.035)
Train: 12 [1000/1251 ( 80%)]  Loss: 5.081 (5.08)  Time: 0.407s, 2517.88/s  (0.195s, 5255.28/s)  LR: 9.961e-04  Data: 0.021 (0.034)
Train: 12 [1050/1251 ( 84%)]  Loss: 5.221 (5.09)  Time: 0.153s, 6702.40/s  (0.195s, 5260.98/s)  LR: 9.961e-04  Data: 0.024 (0.034)
Train: 12 [1100/1251 ( 88%)]  Loss: 5.274 (5.10)  Time: 0.225s, 4553.48/s  (0.195s, 5246.19/s)  LR: 9.961e-04  Data: 0.031 (0.034)
Train: 12 [1150/1251 ( 92%)]  Loss: 5.274 (5.10)  Time: 0.388s, 2640.59/s  (0.195s, 5242.75/s)  LR: 9.961e-04  Data: 0.028 (0.033)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.968 (5.10)  Time: 0.158s, 6477.83/s  (0.196s, 5233.51/s)  LR: 9.961e-04  Data: 0.031 (0.033)
Train: 12 [1250/1251 (100%)]  Loss: 4.899 (5.09)  Time: 0.114s, 8984.54/s  (0.195s, 5242.83/s)  LR: 9.961e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.777 (1.777)  Loss:  1.8492 (1.8492)  Acc@1: 62.5977 (62.5977)  Acc@5: 84.4727 (84.4727)
Test: [  48/48]  Time: 0.019 (0.228)  Loss:  1.6095 (2.6183)  Acc@1: 69.4576 (45.7740)  Acc@5: 85.2594 (71.6360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)

Train: 13 [   0/1251 (  0%)]  Loss: 5.309 (5.31)  Time: 1.975s,  518.58/s  (1.975s,  518.58/s)  LR: 9.954e-04  Data: 1.670 (1.670)
Train: 13 [  50/1251 (  4%)]  Loss: 4.837 (5.07)  Time: 0.174s, 5885.22/s  (0.225s, 4543.35/s)  LR: 9.954e-04  Data: 0.026 (0.059)
Train: 13 [ 100/1251 (  8%)]  Loss: 5.347 (5.16)  Time: 0.165s, 6216.79/s  (0.213s, 4818.72/s)  LR: 9.954e-04  Data: 0.024 (0.043)
Train: 13 [ 150/1251 ( 12%)]  Loss: 5.151 (5.16)  Time: 0.165s, 6198.63/s  (0.206s, 4973.62/s)  LR: 9.954e-04  Data: 0.026 (0.038)
Train: 13 [ 200/1251 ( 16%)]  Loss: 5.070 (5.14)  Time: 0.161s, 6353.27/s  (0.201s, 5094.66/s)  LR: 9.954e-04  Data: 0.032 (0.035)
Train: 13 [ 250/1251 ( 20%)]  Loss: 5.056 (5.13)  Time: 0.157s, 6542.19/s  (0.199s, 5138.36/s)  LR: 9.954e-04  Data: 0.030 (0.033)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.966 (5.11)  Time: 0.164s, 6237.55/s  (0.198s, 5184.31/s)  LR: 9.954e-04  Data: 0.020 (0.032)
Train: 13 [ 350/1251 ( 28%)]  Loss: 5.133 (5.11)  Time: 0.174s, 5889.43/s  (0.197s, 5194.06/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [ 400/1251 ( 32%)]  Loss: 5.214 (5.12)  Time: 0.185s, 5549.95/s  (0.196s, 5228.23/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [ 450/1251 ( 36%)]  Loss: 5.065 (5.11)  Time: 0.161s, 6361.81/s  (0.195s, 5242.76/s)  LR: 9.954e-04  Data: 0.025 (0.031)
Train: 13 [ 500/1251 ( 40%)]  Loss: 5.172 (5.12)  Time: 0.174s, 5882.84/s  (0.195s, 5248.31/s)  LR: 9.954e-04  Data: 0.022 (0.030)
Train: 13 [ 550/1251 ( 44%)]  Loss: 5.224 (5.13)  Time: 0.164s, 6230.49/s  (0.195s, 5240.98/s)  LR: 9.954e-04  Data: 0.023 (0.030)
Train: 13 [ 600/1251 ( 48%)]  Loss: 5.233 (5.14)  Time: 0.177s, 5784.80/s  (0.195s, 5240.55/s)  LR: 9.954e-04  Data: 0.028 (0.030)
Train: 13 [ 650/1251 ( 52%)]  Loss: 5.081 (5.13)  Time: 0.167s, 6138.92/s  (0.195s, 5254.91/s)  LR: 9.954e-04  Data: 0.029 (0.030)
Train: 13 [ 700/1251 ( 56%)]  Loss: 5.225 (5.14)  Time: 0.156s, 6567.80/s  (0.195s, 5240.16/s)  LR: 9.954e-04  Data: 0.026 (0.030)
Train: 13 [ 750/1251 ( 60%)]  Loss: 5.093 (5.14)  Time: 0.190s, 5396.13/s  (0.196s, 5237.02/s)  LR: 9.954e-04  Data: 0.021 (0.030)
Train: 13 [ 800/1251 ( 64%)]  Loss: 5.359 (5.15)  Time: 0.195s, 5250.47/s  (0.196s, 5228.29/s)  LR: 9.954e-04  Data: 0.026 (0.030)
Train: 13 [ 850/1251 ( 68%)]  Loss: 4.945 (5.14)  Time: 0.163s, 6289.01/s  (0.196s, 5235.40/s)  LR: 9.954e-04  Data: 0.027 (0.030)
Train: 13 [ 900/1251 ( 72%)]  Loss: 5.257 (5.14)  Time: 0.181s, 5650.39/s  (0.195s, 5240.54/s)  LR: 9.954e-04  Data: 0.027 (0.031)
Train: 13 [ 950/1251 ( 76%)]  Loss: 5.202 (5.15)  Time: 0.170s, 6021.15/s  (0.195s, 5238.17/s)  LR: 9.954e-04  Data: 0.029 (0.031)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.949 (5.14)  Time: 0.163s, 6295.89/s  (0.196s, 5225.50/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [1050/1251 ( 84%)]  Loss: 5.179 (5.14)  Time: 0.407s, 2516.99/s  (0.197s, 5210.26/s)  LR: 9.954e-04  Data: 0.028 (0.031)
Train: 13 [1100/1251 ( 88%)]  Loss: 5.187 (5.14)  Time: 0.181s, 5658.14/s  (0.196s, 5212.86/s)  LR: 9.954e-04  Data: 0.031 (0.031)
Train: 13 [1150/1251 ( 92%)]  Loss: 5.398 (5.15)  Time: 0.160s, 6394.56/s  (0.197s, 5211.05/s)  LR: 9.954e-04  Data: 0.030 (0.030)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.883 (5.14)  Time: 0.169s, 6045.33/s  (0.197s, 5208.67/s)  LR: 9.954e-04  Data: 0.036 (0.030)
Train: 13 [1250/1251 (100%)]  Loss: 5.102 (5.14)  Time: 0.130s, 7881.57/s  (0.196s, 5221.99/s)  LR: 9.954e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.979 (1.979)  Loss:  1.6583 (1.6583)  Acc@1: 68.4570 (68.4570)  Acc@5: 87.2070 (87.2070)
Test: [  48/48]  Time: 0.019 (0.227)  Loss:  1.6653 (2.5695)  Acc@1: 68.3962 (47.2420)  Acc@5: 84.1981 (72.8000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)

Train: 14 [   0/1251 (  0%)]  Loss: 5.234 (5.23)  Time: 1.989s,  514.85/s  (1.989s,  514.85/s)  LR: 9.946e-04  Data: 1.865 (1.865)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 14 [  50/1251 (  4%)]  Loss: 5.030 (5.13)  Time: 0.182s, 5635.06/s  (0.234s, 4369.45/s)  LR: 9.946e-04  Data: 0.030 (0.090)
Train: 14 [ 100/1251 (  8%)]  Loss: 4.912 (5.06)  Time: 0.158s, 6467.43/s  (0.212s, 4833.67/s)  LR: 9.946e-04  Data: 0.026 (0.063)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.848 (5.01)  Time: 0.167s, 6116.75/s  (0.203s, 5046.57/s)  LR: 9.946e-04  Data: 0.029 (0.051)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.930 (4.99)  Time: 0.168s, 6091.30/s  (0.199s, 5136.84/s)  LR: 9.946e-04  Data: 0.025 (0.046)
Train: 14 [ 250/1251 ( 20%)]  Loss: 5.082 (5.01)  Time: 0.164s, 6252.79/s  (0.197s, 5187.61/s)  LR: 9.946e-04  Data: 0.029 (0.042)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.897 (4.99)  Time: 0.168s, 6106.83/s  (0.197s, 5206.17/s)  LR: 9.946e-04  Data: 0.030 (0.041)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.904 (4.98)  Time: 0.187s, 5464.79/s  (0.195s, 5238.65/s)  LR: 9.946e-04  Data: 0.025 (0.040)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.973 (4.98)  Time: 0.183s, 5592.01/s  (0.196s, 5233.04/s)  LR: 9.946e-04  Data: 0.024 (0.039)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.765 (4.96)  Time: 0.191s, 5348.97/s  (0.195s, 5256.96/s)  LR: 9.946e-04  Data: 0.026 (0.038)
Train: 14 [ 500/1251 ( 40%)]  Loss: 5.045 (4.97)  Time: 0.167s, 6124.29/s  (0.195s, 5264.51/s)  LR: 9.946e-04  Data: 0.029 (0.037)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.664 (4.94)  Time: 0.245s, 4184.84/s  (0.195s, 5252.30/s)  LR: 9.946e-04  Data: 0.025 (0.036)
Train: 14 [ 600/1251 ( 48%)]  Loss: 5.268 (4.97)  Time: 0.169s, 6051.46/s  (0.195s, 5264.06/s)  LR: 9.946e-04  Data: 0.027 (0.035)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.999 (4.97)  Time: 0.183s, 5606.64/s  (0.194s, 5271.56/s)  LR: 9.946e-04  Data: 0.024 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.765 (4.95)  Time: 0.360s, 2846.81/s  (0.195s, 5257.87/s)  LR: 9.946e-04  Data: 0.032 (0.034)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.726 (4.94)  Time: 0.173s, 5905.19/s  (0.195s, 5249.30/s)  LR: 9.946e-04  Data: 0.021 (0.034)
Train: 14 [ 800/1251 ( 64%)]  Loss: 5.243 (4.96)  Time: 0.174s, 5869.02/s  (0.195s, 5251.90/s)  LR: 9.946e-04  Data: 0.028 (0.033)
Train: 14 [ 850/1251 ( 68%)]  Loss: 4.961 (4.96)  Time: 0.201s, 5095.20/s  (0.195s, 5251.93/s)  LR: 9.946e-04  Data: 0.028 (0.033)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.751 (4.95)  Time: 0.204s, 5011.65/s  (0.195s, 5251.41/s)  LR: 9.946e-04  Data: 0.024 (0.033)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.821 (4.94)  Time: 0.159s, 6436.81/s  (0.195s, 5254.82/s)  LR: 9.946e-04  Data: 0.029 (0.033)
Train: 14 [1000/1251 ( 80%)]  Loss: 5.177 (4.95)  Time: 0.184s, 5550.49/s  (0.195s, 5252.75/s)  LR: 9.946e-04  Data: 0.024 (0.032)
Train: 14 [1050/1251 ( 84%)]  Loss: 5.316 (4.97)  Time: 0.169s, 6051.48/s  (0.195s, 5257.56/s)  LR: 9.946e-04  Data: 0.024 (0.032)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.697 (4.96)  Time: 0.183s, 5603.15/s  (0.195s, 5250.25/s)  LR: 9.946e-04  Data: 0.020 (0.032)
Train: 14 [1150/1251 ( 92%)]  Loss: 5.019 (4.96)  Time: 0.198s, 5181.91/s  (0.195s, 5248.22/s)  LR: 9.946e-04  Data: 0.073 (0.032)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.541 (4.94)  Time: 0.159s, 6422.15/s  (0.199s, 5135.82/s)  LR: 9.946e-04  Data: 0.027 (0.032)
Train: 14 [1250/1251 (100%)]  Loss: 5.060 (4.95)  Time: 0.114s, 8999.37/s  (0.199s, 5155.13/s)  LR: 9.946e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.899 (1.899)  Loss:  1.6816 (1.6816)  Acc@1: 66.7969 (66.7969)  Acc@5: 87.3047 (87.3047)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.6490 (2.5300)  Acc@1: 68.1604 (48.1040)  Acc@5: 85.6132 (73.7400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)

Train: 15 [   0/1251 (  0%)]  Loss: 4.841 (4.84)  Time: 1.765s,  580.02/s  (1.765s,  580.02/s)  LR: 9.939e-04  Data: 1.550 (1.550)
Train: 15 [  50/1251 (  4%)]  Loss: 5.019 (4.93)  Time: 0.149s, 6857.75/s  (0.227s, 4509.09/s)  LR: 9.939e-04  Data: 0.023 (0.058)
Train: 15 [ 100/1251 (  8%)]  Loss: 5.043 (4.97)  Time: 0.163s, 6289.21/s  (0.207s, 4936.26/s)  LR: 9.939e-04  Data: 0.023 (0.043)
Train: 15 [ 150/1251 ( 12%)]  Loss: 5.191 (5.02)  Time: 0.190s, 5388.96/s  (0.203s, 5039.19/s)  LR: 9.939e-04  Data: 0.032 (0.038)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.853 (4.99)  Time: 0.179s, 5706.23/s  (0.201s, 5085.35/s)  LR: 9.939e-04  Data: 0.032 (0.035)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.694 (4.94)  Time: 0.176s, 5829.01/s  (0.199s, 5154.57/s)  LR: 9.939e-04  Data: 0.027 (0.033)
Train: 15 [ 300/1251 ( 24%)]  Loss: 5.032 (4.95)  Time: 0.174s, 5893.77/s  (0.197s, 5198.29/s)  LR: 9.939e-04  Data: 0.026 (0.032)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.831 (4.94)  Time: 0.245s, 4174.36/s  (0.196s, 5211.65/s)  LR: 9.939e-04  Data: 0.031 (0.032)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.665 (4.91)  Time: 0.165s, 6195.33/s  (0.195s, 5237.91/s)  LR: 9.939e-04  Data: 0.035 (0.031)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.648 (4.88)  Time: 0.175s, 5863.55/s  (0.195s, 5254.06/s)  LR: 9.939e-04  Data: 0.027 (0.031)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.754 (4.87)  Time: 0.173s, 5907.20/s  (0.196s, 5228.63/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.896 (4.87)  Time: 0.165s, 6188.31/s  (0.195s, 5244.24/s)  LR: 9.939e-04  Data: 0.028 (0.030)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.848 (4.87)  Time: 0.163s, 6281.75/s  (0.195s, 5247.85/s)  LR: 9.939e-04  Data: 0.031 (0.030)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.901 (4.87)  Time: 0.187s, 5487.85/s  (0.195s, 5261.87/s)  LR: 9.939e-04  Data: 0.056 (0.030)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.937 (4.88)  Time: 0.159s, 6433.91/s  (0.195s, 5260.01/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 750/1251 ( 60%)]  Loss: 5.197 (4.90)  Time: 0.170s, 6011.14/s  (0.195s, 5254.16/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.846 (4.89)  Time: 0.397s, 2582.58/s  (0.195s, 5250.20/s)  LR: 9.939e-04  Data: 0.025 (0.030)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.958 (4.90)  Time: 0.176s, 5831.63/s  (0.195s, 5253.59/s)  LR: 9.939e-04  Data: 0.032 (0.030)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.851 (4.90)  Time: 0.215s, 4761.96/s  (0.195s, 5251.90/s)  LR: 9.939e-04  Data: 0.024 (0.030)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.896 (4.90)  Time: 0.187s, 5473.46/s  (0.195s, 5246.98/s)  LR: 9.939e-04  Data: 0.029 (0.030)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.887 (4.89)  Time: 0.339s, 3024.51/s  (0.195s, 5252.13/s)  LR: 9.939e-04  Data: 0.024 (0.030)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.689 (4.89)  Time: 0.168s, 6098.75/s  (0.195s, 5250.13/s)  LR: 9.939e-04  Data: 0.026 (0.030)
Train: 15 [1100/1251 ( 88%)]  Loss: 5.008 (4.89)  Time: 0.175s, 5858.64/s  (0.195s, 5258.15/s)  LR: 9.939e-04  Data: 0.033 (0.030)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.931 (4.89)  Time: 0.179s, 5710.26/s  (0.195s, 5257.93/s)  LR: 9.939e-04  Data: 0.033 (0.030)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.802 (4.89)  Time: 0.160s, 6417.04/s  (0.195s, 5260.07/s)  LR: 9.939e-04  Data: 0.024 (0.029)
Train: 15 [1250/1251 (100%)]  Loss: 5.082 (4.90)  Time: 0.113s, 9039.03/s  (0.194s, 5267.52/s)  LR: 9.939e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.784 (1.784)  Loss:  1.5635 (1.5635)  Acc@1: 68.0664 (68.0664)  Acc@5: 88.4766 (88.4766)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.5166 (2.4494)  Acc@1: 70.7547 (49.3220)  Acc@5: 85.3774 (74.5940)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)

Train: 16 [   0/1251 (  0%)]  Loss: 4.888 (4.89)  Time: 1.612s,  635.07/s  (1.612s,  635.07/s)  LR: 9.930e-04  Data: 1.488 (1.488)
Train: 16 [  50/1251 (  4%)]  Loss: 5.054 (4.97)  Time: 0.178s, 5766.25/s  (0.219s, 4684.55/s)  LR: 9.930e-04  Data: 0.031 (0.066)
Train: 16 [ 100/1251 (  8%)]  Loss: 4.902 (4.95)  Time: 0.171s, 5984.02/s  (0.206s, 4976.49/s)  LR: 9.930e-04  Data: 0.028 (0.050)
Train: 16 [ 150/1251 ( 12%)]  Loss: 5.156 (5.00)  Time: 0.164s, 6238.09/s  (0.201s, 5105.88/s)  LR: 9.930e-04  Data: 0.033 (0.042)
Train: 16 [ 200/1251 ( 16%)]  Loss: 5.250 (5.05)  Time: 0.319s, 3214.73/s  (0.197s, 5186.76/s)  LR: 9.930e-04  Data: 0.033 (0.039)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.914 (5.03)  Time: 0.183s, 5595.14/s  (0.196s, 5231.85/s)  LR: 9.930e-04  Data: 0.032 (0.037)
Train: 16 [ 300/1251 ( 24%)]  Loss: 5.380 (5.08)  Time: 0.164s, 6254.42/s  (0.195s, 5256.28/s)  LR: 9.930e-04  Data: 0.026 (0.035)
Train: 16 [ 350/1251 ( 28%)]  Loss: 5.229 (5.10)  Time: 0.189s, 5408.85/s  (0.194s, 5279.01/s)  LR: 9.930e-04  Data: 0.028 (0.034)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.755 (5.06)  Time: 0.179s, 5721.78/s  (0.194s, 5290.23/s)  LR: 9.930e-04  Data: 0.029 (0.033)
Train: 16 [ 450/1251 ( 36%)]  Loss: 5.121 (5.06)  Time: 0.191s, 5368.19/s  (0.193s, 5296.82/s)  LR: 9.930e-04  Data: 0.028 (0.033)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.978 (5.06)  Time: 0.168s, 6084.75/s  (0.192s, 5322.14/s)  LR: 9.930e-04  Data: 0.026 (0.032)
Train: 16 [ 550/1251 ( 44%)]  Loss: 5.093 (5.06)  Time: 0.178s, 5750.11/s  (0.193s, 5317.55/s)  LR: 9.930e-04  Data: 0.027 (0.032)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.899 (5.05)  Time: 0.273s, 3756.86/s  (0.193s, 5313.26/s)  LR: 9.930e-04  Data: 0.034 (0.031)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.981 (5.04)  Time: 0.163s, 6275.77/s  (0.193s, 5311.89/s)  LR: 9.930e-04  Data: 0.032 (0.031)
Train: 16 [ 700/1251 ( 56%)]  Loss: 5.054 (5.04)  Time: 0.203s, 5037.68/s  (0.193s, 5302.79/s)  LR: 9.930e-04  Data: 0.023 (0.031)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.692 (5.02)  Time: 0.171s, 5982.72/s  (0.193s, 5298.51/s)  LR: 9.930e-04  Data: 0.019 (0.031)
Train: 16 [ 800/1251 ( 64%)]  Loss: 5.118 (5.03)  Time: 0.402s, 2547.84/s  (0.193s, 5293.51/s)  LR: 9.930e-04  Data: 0.025 (0.031)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.663 (5.01)  Time: 0.159s, 6422.04/s  (0.193s, 5303.85/s)  LR: 9.930e-04  Data: 0.026 (0.030)
Train: 16 [ 900/1251 ( 72%)]  Loss: 5.032 (5.01)  Time: 0.162s, 6309.03/s  (0.193s, 5309.01/s)  LR: 9.930e-04  Data: 0.031 (0.030)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.417 (4.98)  Time: 0.178s, 5750.91/s  (0.193s, 5308.25/s)  LR: 9.930e-04  Data: 0.021 (0.030)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.954 (4.98)  Time: 0.182s, 5632.03/s  (0.195s, 5257.39/s)  LR: 9.930e-04  Data: 0.022 (0.030)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.744 (4.97)  Time: 0.153s, 6704.85/s  (0.195s, 5261.16/s)  LR: 9.930e-04  Data: 0.024 (0.030)
Train: 16 [1100/1251 ( 88%)]  Loss: 5.052 (4.97)  Time: 0.164s, 6256.53/s  (0.195s, 5251.16/s)  LR: 9.930e-04  Data: 0.028 (0.030)
Train: 16 [1150/1251 ( 92%)]  Loss: 5.558 (5.00)  Time: 0.166s, 6155.52/s  (0.195s, 5252.11/s)  LR: 9.930e-04  Data: 0.024 (0.030)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.441 (4.97)  Time: 0.175s, 5845.82/s  (0.195s, 5253.17/s)  LR: 9.930e-04  Data: 0.035 (0.029)
Train: 16 [1250/1251 (100%)]  Loss: 5.030 (4.98)  Time: 0.114s, 9018.30/s  (0.195s, 5264.57/s)  LR: 9.930e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.789 (1.789)  Loss:  1.6334 (1.6334)  Acc@1: 68.1641 (68.1641)  Acc@5: 89.0625 (89.0625)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.5166 (2.4221)  Acc@1: 71.6981 (50.3340)  Acc@5: 86.5566 (75.4720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)

Train: 17 [   0/1251 (  0%)]  Loss: 4.900 (4.90)  Time: 1.724s,  593.94/s  (1.724s,  593.94/s)  LR: 9.921e-04  Data: 1.502 (1.502)
Train: 17 [  50/1251 (  4%)]  Loss: 5.004 (4.95)  Time: 0.168s, 6092.12/s  (0.222s, 4614.59/s)  LR: 9.921e-04  Data: 0.026 (0.058)
Train: 17 [ 100/1251 (  8%)]  Loss: 4.870 (4.92)  Time: 0.170s, 6020.91/s  (0.212s, 4824.17/s)  LR: 9.921e-04  Data: 0.024 (0.053)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.500 (4.82)  Time: 0.176s, 5811.12/s  (0.204s, 5015.53/s)  LR: 9.921e-04  Data: 0.025 (0.050)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.853 (4.83)  Time: 0.164s, 6250.03/s  (0.200s, 5109.68/s)  LR: 9.921e-04  Data: 0.027 (0.048)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.667 (4.80)  Time: 0.166s, 6184.64/s  (0.199s, 5146.12/s)  LR: 9.921e-04  Data: 0.021 (0.045)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.837 (4.80)  Time: 0.178s, 5743.65/s  (0.197s, 5198.69/s)  LR: 9.921e-04  Data: 0.029 (0.043)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.802 (4.80)  Time: 0.170s, 6017.43/s  (0.196s, 5234.42/s)  LR: 9.921e-04  Data: 0.020 (0.041)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.926 (4.82)  Time: 0.170s, 6020.93/s  (0.195s, 5250.17/s)  LR: 9.921e-04  Data: 0.030 (0.041)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.824 (4.82)  Time: 0.168s, 6105.27/s  (0.194s, 5266.62/s)  LR: 9.921e-04  Data: 0.030 (0.040)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.899 (4.83)  Time: 0.169s, 6071.04/s  (0.194s, 5274.67/s)  LR: 9.921e-04  Data: 0.019 (0.039)
Train: 17 [ 550/1251 ( 44%)]  Loss: 5.032 (4.84)  Time: 0.171s, 5973.65/s  (0.194s, 5283.03/s)  LR: 9.921e-04  Data: 0.025 (0.037)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.459 (4.81)  Time: 0.164s, 6233.58/s  (0.194s, 5285.39/s)  LR: 9.921e-04  Data: 0.024 (0.037)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.823 (4.81)  Time: 0.176s, 5833.96/s  (0.194s, 5280.31/s)  LR: 9.921e-04  Data: 0.025 (0.036)
Train: 17 [ 700/1251 ( 56%)]  Loss: 5.051 (4.83)  Time: 0.167s, 6121.25/s  (0.194s, 5287.36/s)  LR: 9.921e-04  Data: 0.031 (0.035)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.913 (4.84)  Time: 0.161s, 6357.63/s  (0.194s, 5290.83/s)  LR: 9.921e-04  Data: 0.027 (0.035)
Train: 17 [ 800/1251 ( 64%)]  Loss: 5.269 (4.86)  Time: 0.161s, 6361.86/s  (0.193s, 5299.20/s)  LR: 9.921e-04  Data: 0.029 (0.035)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.854 (4.86)  Time: 0.169s, 6062.97/s  (0.193s, 5299.75/s)  LR: 9.921e-04  Data: 0.029 (0.035)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.843 (4.86)  Time: 0.181s, 5641.96/s  (0.194s, 5291.53/s)  LR: 9.921e-04  Data: 0.040 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.859 (4.86)  Time: 0.166s, 6154.51/s  (0.193s, 5297.34/s)  LR: 9.921e-04  Data: 0.024 (0.034)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.649 (4.85)  Time: 0.163s, 6280.72/s  (0.193s, 5292.59/s)  LR: 9.921e-04  Data: 0.030 (0.034)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.902 (4.85)  Time: 0.266s, 3847.15/s  (0.194s, 5285.49/s)  LR: 9.921e-04  Data: 0.027 (0.034)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.831 (4.85)  Time: 0.174s, 5871.39/s  (0.194s, 5282.19/s)  LR: 9.921e-04  Data: 0.022 (0.033)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.833 (4.85)  Time: 0.177s, 5797.47/s  (0.194s, 5276.89/s)  LR: 9.921e-04  Data: 0.030 (0.033)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.849 (4.85)  Time: 0.163s, 6279.21/s  (0.194s, 5271.44/s)  LR: 9.921e-04  Data: 0.024 (0.033)
Train: 17 [1250/1251 (100%)]  Loss: 5.141 (4.86)  Time: 0.114s, 9008.62/s  (0.194s, 5284.97/s)  LR: 9.921e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.914 (1.914)  Loss:  1.5740 (1.5740)  Acc@1: 70.9961 (70.9961)  Acc@5: 89.1602 (89.1602)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4577 (2.4026)  Acc@1: 72.1698 (51.3420)  Acc@5: 87.9717 (76.3420)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)

Train: 18 [   0/1251 (  0%)]  Loss: 5.159 (5.16)  Time: 1.805s,  567.35/s  (1.805s,  567.35/s)  LR: 9.912e-04  Data: 1.675 (1.675)
Train: 18 [  50/1251 (  4%)]  Loss: 4.945 (5.05)  Time: 0.174s, 5872.49/s  (0.222s, 4615.61/s)  LR: 9.912e-04  Data: 0.032 (0.077)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.680 (4.93)  Time: 0.167s, 6141.90/s  (0.210s, 4872.53/s)  LR: 9.912e-04  Data: 0.031 (0.053)
Train: 18 [ 150/1251 ( 12%)]  Loss: 5.083 (4.97)  Time: 0.164s, 6233.86/s  (0.205s, 4996.87/s)  LR: 9.912e-04  Data: 0.021 (0.044)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.511 (4.88)  Time: 0.154s, 6664.85/s  (0.201s, 5097.35/s)  LR: 9.912e-04  Data: 0.024 (0.040)
Train: 18 [ 250/1251 ( 20%)]  Loss: 5.289 (4.94)  Time: 0.172s, 5943.69/s  (0.199s, 5133.35/s)  LR: 9.912e-04  Data: 0.024 (0.042)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.775 (4.92)  Time: 0.193s, 5294.26/s  (0.197s, 5208.78/s)  LR: 9.912e-04  Data: 0.019 (0.041)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.950 (4.92)  Time: 0.174s, 5896.51/s  (0.196s, 5228.06/s)  LR: 9.912e-04  Data: 0.027 (0.039)
Train: 18 [ 400/1251 ( 32%)]  Loss: 5.042 (4.94)  Time: 0.163s, 6270.52/s  (0.197s, 5210.91/s)  LR: 9.912e-04  Data: 0.028 (0.038)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.719 (4.92)  Time: 0.194s, 5274.90/s  (0.196s, 5215.01/s)  LR: 9.912e-04  Data: 0.026 (0.037)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.625 (4.89)  Time: 0.192s, 5343.92/s  (0.196s, 5221.81/s)  LR: 9.912e-04  Data: 0.027 (0.036)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.775 (4.88)  Time: 0.159s, 6453.91/s  (0.195s, 5244.26/s)  LR: 9.912e-04  Data: 0.029 (0.035)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.925 (4.88)  Time: 0.406s, 2525.10/s  (0.195s, 5246.37/s)  LR: 9.912e-04  Data: 0.029 (0.035)
Train: 18 [ 650/1251 ( 52%)]  Loss: 5.244 (4.91)  Time: 0.178s, 5741.84/s  (0.195s, 5256.68/s)  LR: 9.912e-04  Data: 0.024 (0.034)
Train: 18 [ 700/1251 ( 56%)]  Loss: 4.893 (4.91)  Time: 0.171s, 5972.03/s  (0.194s, 5271.22/s)  LR: 9.912e-04  Data: 0.031 (0.034)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.945 (4.91)  Time: 0.162s, 6336.14/s  (0.194s, 5276.36/s)  LR: 9.912e-04  Data: 0.024 (0.035)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.694 (4.90)  Time: 0.171s, 5987.65/s  (0.194s, 5286.17/s)  LR: 9.912e-04  Data: 0.022 (0.036)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.802 (4.89)  Time: 0.182s, 5631.14/s  (0.194s, 5271.05/s)  LR: 9.912e-04  Data: 0.030 (0.036)
Train: 18 [ 900/1251 ( 72%)]  Loss: 5.010 (4.90)  Time: 0.163s, 6299.12/s  (0.194s, 5277.05/s)  LR: 9.912e-04  Data: 0.023 (0.035)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.604 (4.88)  Time: 0.164s, 6259.83/s  (0.194s, 5279.18/s)  LR: 9.912e-04  Data: 0.024 (0.035)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.954 (4.89)  Time: 0.154s, 6670.60/s  (0.194s, 5282.83/s)  LR: 9.912e-04  Data: 0.028 (0.035)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.159 (4.85)  Time: 0.195s, 5247.84/s  (0.194s, 5271.99/s)  LR: 9.912e-04  Data: 0.019 (0.034)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.821 (4.85)  Time: 0.164s, 6245.63/s  (0.194s, 5269.18/s)  LR: 9.912e-04  Data: 0.025 (0.034)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.596 (4.84)  Time: 0.161s, 6358.94/s  (0.194s, 5274.18/s)  LR: 9.912e-04  Data: 0.024 (0.034)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.882 (4.84)  Time: 0.173s, 5926.76/s  (0.194s, 5268.53/s)  LR: 9.912e-04  Data: 0.030 (0.034)
Train: 18 [1250/1251 (100%)]  Loss: 5.035 (4.85)  Time: 0.114s, 9016.51/s  (0.194s, 5285.39/s)  LR: 9.912e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.789 (1.789)  Loss:  1.5596 (1.5596)  Acc@1: 69.6289 (69.6289)  Acc@5: 88.3789 (88.3789)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4804 (2.3719)  Acc@1: 71.1085 (51.4680)  Acc@5: 87.7358 (76.2440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)

Train: 19 [   0/1251 (  0%)]  Loss: 4.998 (5.00)  Time: 2.011s,  509.18/s  (2.011s,  509.18/s)  LR: 9.901e-04  Data: 1.884 (1.884)
Train: 19 [  50/1251 (  4%)]  Loss: 4.622 (4.81)  Time: 0.179s, 5720.81/s  (0.223s, 4600.85/s)  LR: 9.901e-04  Data: 0.021 (0.076)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.928 (4.85)  Time: 0.172s, 5956.27/s  (0.207s, 4947.36/s)  LR: 9.901e-04  Data: 0.027 (0.060)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.935 (4.87)  Time: 0.163s, 6280.75/s  (0.201s, 5085.64/s)  LR: 9.901e-04  Data: 0.026 (0.054)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.907 (4.88)  Time: 0.322s, 3181.37/s  (0.199s, 5140.03/s)  LR: 9.901e-04  Data: 0.195 (0.053)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.738 (4.85)  Time: 0.395s, 2590.19/s  (0.199s, 5156.86/s)  LR: 9.901e-04  Data: 0.028 (0.048)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.915 (4.86)  Time: 0.172s, 5946.19/s  (0.198s, 5175.55/s)  LR: 9.901e-04  Data: 0.037 (0.044)
Train: 19 [ 350/1251 ( 28%)]  Loss: 5.099 (4.89)  Time: 0.208s, 4920.44/s  (0.197s, 5206.92/s)  LR: 9.901e-04  Data: 0.020 (0.042)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.824 (4.89)  Time: 0.174s, 5873.27/s  (0.196s, 5222.06/s)  LR: 9.901e-04  Data: 0.025 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Train: 19 [ 450/1251 ( 36%)]  Loss: 4.573 (4.85)  Time: 0.314s, 3264.94/s  (0.196s, 5231.04/s)  LR: 9.901e-04  Data: 0.025 (0.039)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.989 (4.87)  Time: 0.177s, 5797.49/s  (0.195s, 5255.08/s)  LR: 9.901e-04  Data: 0.032 (0.037)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.715 (4.85)  Time: 0.165s, 6199.45/s  (0.195s, 5255.95/s)  LR: 9.901e-04  Data: 0.030 (0.037)
Train: 19 [ 600/1251 ( 48%)]  Loss: 5.200 (4.88)  Time: 0.177s, 5774.75/s  (0.195s, 5254.72/s)  LR: 9.901e-04  Data: 0.020 (0.036)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.780 (4.87)  Time: 0.185s, 5531.79/s  (0.195s, 5255.23/s)  LR: 9.901e-04  Data: 0.023 (0.035)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.634 (4.86)  Time: 0.182s, 5626.35/s  (0.195s, 5251.88/s)  LR: 9.901e-04  Data: 0.021 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 19 [ 750/1251 ( 60%)]  Loss: 5.003 (4.87)  Time: 0.154s, 6643.55/s  (0.195s, 5243.80/s)  LR: 9.901e-04  Data: 0.025 (0.034)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.851 (4.87)  Time: 0.169s, 6067.49/s  (0.195s, 5244.89/s)  LR: 9.901e-04  Data: 0.029 (0.034)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.648 (4.85)  Time: 0.165s, 6213.42/s  (0.195s, 5252.41/s)  LR: 9.901e-04  Data: 0.029 (0.033)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.981 (4.86)  Time: 0.184s, 5571.65/s  (0.195s, 5254.48/s)  LR: 9.901e-04  Data: 0.032 (0.033)
Train: 19 [ 950/1251 ( 76%)]  Loss: 5.224 (4.88)  Time: 0.176s, 5807.25/s  (0.195s, 5249.71/s)  LR: 9.901e-04  Data: 0.026 (0.033)
Train: 19 [1000/1251 ( 80%)]  Loss: 5.005 (4.88)  Time: 0.167s, 6114.22/s  (0.195s, 5247.99/s)  LR: 9.901e-04  Data: 0.024 (0.033)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.579 (4.87)  Time: 0.148s, 6939.15/s  (0.195s, 5253.89/s)  LR: 9.901e-04  Data: 0.026 (0.032)
Train: 19 [1100/1251 ( 88%)]  Loss: 5.132 (4.88)  Time: 0.150s, 6832.31/s  (0.195s, 5253.14/s)  LR: 9.901e-04  Data: 0.023 (0.032)
Train: 19 [1150/1251 ( 92%)]  Loss: 5.065 (4.89)  Time: 0.160s, 6417.24/s  (0.195s, 5248.35/s)  LR: 9.901e-04  Data: 0.030 (0.032)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.871 (4.89)  Time: 0.175s, 5855.67/s  (0.195s, 5245.20/s)  LR: 9.901e-04  Data: 0.031 (0.032)
Train: 19 [1250/1251 (100%)]  Loss: 4.647 (4.88)  Time: 0.114s, 8994.43/s  (0.195s, 5259.91/s)  LR: 9.901e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.883 (1.883)  Loss:  1.4531 (1.4531)  Acc@1: 71.0938 (71.0938)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.5398 (2.3122)  Acc@1: 71.6981 (52.1760)  Acc@5: 87.1462 (76.9760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)

Train: 20 [   0/1251 (  0%)]  Loss: 4.769 (4.77)  Time: 1.912s,  535.54/s  (1.912s,  535.54/s)  LR: 9.891e-04  Data: 1.625 (1.625)
Train: 20 [  50/1251 (  4%)]  Loss: 4.953 (4.86)  Time: 0.154s, 6651.71/s  (0.226s, 4530.93/s)  LR: 9.891e-04  Data: 0.027 (0.061)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.486 (4.74)  Time: 0.194s, 5284.71/s  (0.211s, 4853.95/s)  LR: 9.891e-04  Data: 0.025 (0.046)
Train: 20 [ 150/1251 ( 12%)]  Loss: 5.216 (4.86)  Time: 0.153s, 6676.86/s  (0.202s, 5061.78/s)  LR: 9.891e-04  Data: 0.028 (0.040)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.390 (4.76)  Time: 0.247s, 4146.96/s  (0.200s, 5120.86/s)  LR: 9.891e-04  Data: 0.025 (0.037)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.526 (4.72)  Time: 0.187s, 5479.20/s  (0.198s, 5162.74/s)  LR: 9.891e-04  Data: 0.027 (0.036)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.835 (4.74)  Time: 0.190s, 5377.76/s  (0.197s, 5200.98/s)  LR: 9.891e-04  Data: 0.023 (0.035)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.675 (4.73)  Time: 0.164s, 6262.59/s  (0.196s, 5237.59/s)  LR: 9.891e-04  Data: 0.033 (0.034)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.936 (4.75)  Time: 0.282s, 3629.25/s  (0.194s, 5268.22/s)  LR: 9.891e-04  Data: 0.029 (0.033)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.928 (4.77)  Time: 0.172s, 5939.55/s  (0.195s, 5264.01/s)  LR: 9.891e-04  Data: 0.031 (0.032)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.842 (4.78)  Time: 0.175s, 5842.37/s  (0.194s, 5279.28/s)  LR: 9.891e-04  Data: 0.029 (0.032)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.744 (4.77)  Time: 0.184s, 5571.65/s  (0.196s, 5234.99/s)  LR: 9.891e-04  Data: 0.054 (0.031)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.965 (4.79)  Time: 0.166s, 6173.40/s  (0.195s, 5252.29/s)  LR: 9.891e-04  Data: 0.038 (0.031)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.767 (4.79)  Time: 0.164s, 6246.07/s  (0.195s, 5257.65/s)  LR: 9.891e-04  Data: 0.030 (0.031)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.824 (4.79)  Time: 0.154s, 6646.09/s  (0.195s, 5263.84/s)  LR: 9.891e-04  Data: 0.026 (0.031)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.897 (4.80)  Time: 0.431s, 2374.54/s  (0.195s, 5259.86/s)  LR: 9.891e-04  Data: 0.029 (0.030)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.746 (4.79)  Time: 0.160s, 6399.03/s  (0.194s, 5265.99/s)  LR: 9.891e-04  Data: 0.026 (0.030)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.810 (4.79)  Time: 0.185s, 5535.86/s  (0.195s, 5264.26/s)  LR: 9.891e-04  Data: 0.034 (0.030)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.881 (4.80)  Time: 0.176s, 5818.87/s  (0.195s, 5264.67/s)  LR: 9.891e-04  Data: 0.025 (0.030)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.830 (4.80)  Time: 0.172s, 5957.64/s  (0.195s, 5255.10/s)  LR: 9.891e-04  Data: 0.032 (0.030)
Train: 20 [1000/1251 ( 80%)]  Loss: 5.048 (4.81)  Time: 0.161s, 6375.70/s  (0.195s, 5253.00/s)  LR: 9.891e-04  Data: 0.023 (0.030)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.421 (4.79)  Time: 0.452s, 2267.67/s  (0.195s, 5248.70/s)  LR: 9.891e-04  Data: 0.035 (0.030)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.862 (4.80)  Time: 0.165s, 6210.12/s  (0.195s, 5250.17/s)  LR: 9.891e-04  Data: 0.030 (0.030)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.466 (4.78)  Time: 0.167s, 6121.42/s  (0.196s, 5215.99/s)  LR: 9.891e-04  Data: 0.020 (0.030)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.867 (4.79)  Time: 0.188s, 5434.20/s  (0.197s, 5188.76/s)  LR: 9.891e-04  Data: 0.029 (0.030)
Train: 20 [1250/1251 (100%)]  Loss: 4.584 (4.78)  Time: 0.114s, 9003.80/s  (0.197s, 5204.35/s)  LR: 9.891e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.896 (1.896)  Loss:  1.4645 (1.4645)  Acc@1: 73.0469 (73.0469)  Acc@5: 89.6484 (89.6484)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.4092 (2.2538)  Acc@1: 73.2311 (53.4980)  Acc@5: 88.3255 (77.9080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)

Train: 21 [   0/1251 (  0%)]  Loss: 4.360 (4.36)  Time: 1.779s,  575.53/s  (1.779s,  575.53/s)  LR: 9.880e-04  Data: 1.650 (1.650)
Train: 21 [  50/1251 (  4%)]  Loss: 4.768 (4.56)  Time: 0.175s, 5849.37/s  (0.222s, 4603.02/s)  LR: 9.880e-04  Data: 0.021 (0.075)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.594 (4.57)  Time: 0.161s, 6364.77/s  (0.207s, 4945.27/s)  LR: 9.880e-04  Data: 0.025 (0.055)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.628 (4.59)  Time: 0.155s, 6587.75/s  (0.201s, 5099.57/s)  LR: 9.880e-04  Data: 0.024 (0.050)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.465 (4.56)  Time: 0.166s, 6150.45/s  (0.199s, 5151.08/s)  LR: 9.880e-04  Data: 0.035 (0.045)
Train: 21 [ 250/1251 ( 20%)]  Loss: 5.284 (4.68)  Time: 0.162s, 6322.39/s  (0.197s, 5199.77/s)  LR: 9.880e-04  Data: 0.027 (0.041)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.701 (4.69)  Time: 0.167s, 6116.05/s  (0.197s, 5192.32/s)  LR: 9.880e-04  Data: 0.025 (0.039)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.907 (4.71)  Time: 0.171s, 5981.67/s  (0.196s, 5234.88/s)  LR: 9.880e-04  Data: 0.029 (0.038)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.784 (4.72)  Time: 0.364s, 2811.04/s  (0.195s, 5240.83/s)  LR: 9.880e-04  Data: 0.023 (0.036)
Train: 21 [ 450/1251 ( 36%)]  Loss: 5.332 (4.78)  Time: 0.172s, 5955.08/s  (0.195s, 5255.66/s)  LR: 9.880e-04  Data: 0.026 (0.036)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.718 (4.78)  Time: 0.171s, 5993.43/s  (0.194s, 5270.71/s)  LR: 9.880e-04  Data: 0.028 (0.035)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.941 (4.79)  Time: 0.169s, 6055.92/s  (0.194s, 5267.60/s)  LR: 9.880e-04  Data: 0.029 (0.035)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.619 (4.78)  Time: 0.162s, 6323.05/s  (0.194s, 5273.13/s)  LR: 9.880e-04  Data: 0.030 (0.034)
Train: 21 [ 650/1251 ( 52%)]  Loss: 5.013 (4.79)  Time: 0.188s, 5459.47/s  (0.194s, 5269.07/s)  LR: 9.880e-04  Data: 0.033 (0.034)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.816 (4.80)  Time: 0.188s, 5448.16/s  (0.194s, 5268.83/s)  LR: 9.880e-04  Data: 0.025 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Train: 21 [ 750/1251 ( 60%)]  Loss: 4.644 (4.79)  Time: 0.166s, 6152.65/s  (0.194s, 5286.08/s)  LR: 9.880e-04  Data: 0.033 (0.033)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.855 (4.79)  Time: 0.174s, 5900.57/s  (0.194s, 5279.79/s)  LR: 9.880e-04  Data: 0.030 (0.033)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.868 (4.79)  Time: 0.243s, 4209.72/s  (0.194s, 5276.99/s)  LR: 9.880e-04  Data: 0.028 (0.033)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.693 (4.79)  Time: 0.159s, 6434.00/s  (0.194s, 5274.66/s)  LR: 9.880e-04  Data: 0.023 (0.032)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.996 (4.80)  Time: 0.166s, 6150.60/s  (0.194s, 5274.43/s)  LR: 9.880e-04  Data: 0.024 (0.032)
Train: 21 [1000/1251 ( 80%)]  Loss: 4.729 (4.80)  Time: 0.202s, 5062.22/s  (0.194s, 5277.48/s)  LR: 9.880e-04  Data: 0.026 (0.032)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.909 (4.80)  Time: 0.174s, 5882.16/s  (0.194s, 5273.60/s)  LR: 9.880e-04  Data: 0.037 (0.032)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.897 (4.81)  Time: 0.188s, 5442.10/s  (0.195s, 5263.55/s)  LR: 9.880e-04  Data: 0.034 (0.032)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.648 (4.80)  Time: 0.160s, 6411.23/s  (0.194s, 5273.32/s)  LR: 9.880e-04  Data: 0.027 (0.031)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.542 (4.79)  Time: 0.190s, 5377.25/s  (0.195s, 5264.16/s)  LR: 9.880e-04  Data: 0.026 (0.031)
Train: 21 [1250/1251 (100%)]  Loss: 4.668 (4.78)  Time: 0.113s, 9025.07/s  (0.194s, 5276.98/s)  LR: 9.880e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.791 (1.791)  Loss:  1.4235 (1.4235)  Acc@1: 72.5586 (72.5586)  Acc@5: 89.7461 (89.7461)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3573 (2.2244)  Acc@1: 72.2877 (53.5500)  Acc@5: 89.2689 (78.0860)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)

Train: 22 [   0/1251 (  0%)]  Loss: 4.848 (4.85)  Time: 1.795s,  570.36/s  (1.795s,  570.36/s)  LR: 9.868e-04  Data: 1.486 (1.486)
Train: 22 [  50/1251 (  4%)]  Loss: 4.630 (4.74)  Time: 0.182s, 5640.86/s  (0.227s, 4505.18/s)  LR: 9.868e-04  Data: 0.026 (0.055)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.779 (4.75)  Time: 0.168s, 6109.37/s  (0.209s, 4907.19/s)  LR: 9.868e-04  Data: 0.023 (0.041)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.585 (4.71)  Time: 0.182s, 5626.94/s  (0.201s, 5103.99/s)  LR: 9.868e-04  Data: 0.033 (0.036)
Train: 22 [ 200/1251 ( 16%)]  Loss: 5.039 (4.78)  Time: 0.169s, 6050.33/s  (0.201s, 5103.64/s)  LR: 9.868e-04  Data: 0.025 (0.034)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.735 (4.77)  Time: 0.182s, 5615.63/s  (0.201s, 5096.75/s)  LR: 9.868e-04  Data: 0.032 (0.033)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.851 (4.78)  Time: 0.184s, 5573.80/s  (0.199s, 5156.46/s)  LR: 9.868e-04  Data: 0.020 (0.032)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.508 (4.75)  Time: 0.169s, 6056.77/s  (0.198s, 5162.13/s)  LR: 9.868e-04  Data: 0.021 (0.031)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.184 (4.68)  Time: 0.192s, 5321.59/s  (0.198s, 5179.63/s)  LR: 9.868e-04  Data: 0.027 (0.031)
Train: 22 [ 450/1251 ( 36%)]  Loss: 5.161 (4.73)  Time: 0.177s, 5788.13/s  (0.197s, 5207.02/s)  LR: 9.868e-04  Data: 0.026 (0.031)
Train: 22 [ 500/1251 ( 40%)]  Loss: 5.136 (4.77)  Time: 0.190s, 5385.78/s  (0.196s, 5215.44/s)  LR: 9.868e-04  Data: 0.022 (0.031)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.599 (4.75)  Time: 0.187s, 5466.78/s  (0.196s, 5236.10/s)  LR: 9.868e-04  Data: 0.022 (0.031)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.862 (4.76)  Time: 0.336s, 3049.53/s  (0.196s, 5235.45/s)  LR: 9.868e-04  Data: 0.023 (0.031)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.729 (4.76)  Time: 0.176s, 5804.59/s  (0.195s, 5250.13/s)  LR: 9.868e-04  Data: 0.024 (0.030)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.738 (4.76)  Time: 0.172s, 5958.99/s  (0.195s, 5254.36/s)  LR: 9.868e-04  Data: 0.029 (0.030)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.530 (4.74)  Time: 0.356s, 2880.43/s  (0.195s, 5256.14/s)  LR: 9.868e-04  Data: 0.028 (0.030)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.738 (4.74)  Time: 0.177s, 5782.22/s  (0.195s, 5260.48/s)  LR: 9.868e-04  Data: 0.025 (0.030)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.554 (4.73)  Time: 0.175s, 5834.89/s  (0.195s, 5262.11/s)  LR: 9.868e-04  Data: 0.030 (0.030)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.462 (4.72)  Time: 0.196s, 5227.49/s  (0.195s, 5261.68/s)  LR: 9.868e-04  Data: 0.036 (0.030)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.809 (4.72)  Time: 0.320s, 3200.20/s  (0.195s, 5250.10/s)  LR: 9.868e-04  Data: 0.024 (0.030)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.478 (4.71)  Time: 0.484s, 2116.96/s  (0.195s, 5243.82/s)  LR: 9.868e-04  Data: 0.026 (0.030)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.826 (4.72)  Time: 0.164s, 6257.15/s  (0.195s, 5250.64/s)  LR: 9.868e-04  Data: 0.025 (0.030)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.716 (4.72)  Time: 0.192s, 5331.93/s  (0.195s, 5246.28/s)  LR: 9.868e-04  Data: 0.028 (0.030)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.818 (4.72)  Time: 0.164s, 6250.47/s  (0.195s, 5242.91/s)  LR: 9.868e-04  Data: 0.029 (0.030)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.919 (4.73)  Time: 0.401s, 2554.15/s  (0.195s, 5241.31/s)  LR: 9.868e-04  Data: 0.026 (0.030)
Train: 22 [1250/1251 (100%)]  Loss: 4.877 (4.74)  Time: 0.113s, 9086.42/s  (0.195s, 5254.35/s)  LR: 9.868e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.859 (1.859)  Loss:  1.4890 (1.4890)  Acc@1: 72.1680 (72.1680)  Acc@5: 88.8672 (88.8672)
Test: [  48/48]  Time: 0.019 (0.233)  Loss:  1.4443 (2.2145)  Acc@1: 71.9340 (54.6000)  Acc@5: 87.1462 (78.5060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)

Train: 23 [   0/1251 (  0%)]  Loss: 4.672 (4.67)  Time: 1.819s,  562.90/s  (1.819s,  562.90/s)  LR: 9.856e-04  Data: 1.690 (1.690)
Train: 23 [  50/1251 (  4%)]  Loss: 4.203 (4.44)  Time: 0.163s, 6282.98/s  (0.218s, 4697.02/s)  LR: 9.856e-04  Data: 0.029 (0.070)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.745 (4.54)  Time: 0.176s, 5813.30/s  (0.203s, 5042.18/s)  LR: 9.856e-04  Data: 0.022 (0.054)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.490 (4.53)  Time: 0.176s, 5805.80/s  (0.200s, 5116.88/s)  LR: 9.856e-04  Data: 0.026 (0.051)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.619 (4.55)  Time: 0.180s, 5697.51/s  (0.199s, 5147.73/s)  LR: 9.856e-04  Data: 0.032 (0.049)
Train: 23 [ 250/1251 ( 20%)]  Loss: 5.020 (4.62)  Time: 0.186s, 5513.80/s  (0.196s, 5225.70/s)  LR: 9.856e-04  Data: 0.027 (0.045)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.820 (4.65)  Time: 0.184s, 5558.91/s  (0.195s, 5259.37/s)  LR: 9.856e-04  Data: 0.024 (0.045)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.646 (4.65)  Time: 0.175s, 5850.17/s  (0.194s, 5273.49/s)  LR: 9.856e-04  Data: 0.025 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.746 (4.66)  Time: 0.177s, 5793.70/s  (0.194s, 5276.97/s)  LR: 9.856e-04  Data: 0.030 (0.045)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.829 (4.68)  Time: 0.175s, 5849.56/s  (0.194s, 5270.77/s)  LR: 9.856e-04  Data: 0.029 (0.045)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.554 (4.67)  Time: 0.165s, 6221.84/s  (0.193s, 5293.96/s)  LR: 9.856e-04  Data: 0.026 (0.044)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.700 (4.67)  Time: 0.163s, 6274.53/s  (0.193s, 5297.72/s)  LR: 9.856e-04  Data: 0.030 (0.042)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.601 (4.66)  Time: 0.196s, 5236.18/s  (0.194s, 5288.56/s)  LR: 9.856e-04  Data: 0.027 (0.041)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.865 (4.68)  Time: 0.175s, 5839.82/s  (0.193s, 5293.23/s)  LR: 9.856e-04  Data: 0.026 (0.040)
Train: 23 [ 700/1251 ( 56%)]  Loss: 4.901 (4.69)  Time: 0.167s, 6124.42/s  (0.193s, 5296.02/s)  LR: 9.856e-04  Data: 0.030 (0.039)
Train: 23 [ 750/1251 ( 60%)]  Loss: 4.781 (4.70)  Time: 0.185s, 5544.70/s  (0.193s, 5297.85/s)  LR: 9.856e-04  Data: 0.024 (0.039)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.789 (4.70)  Time: 0.186s, 5498.32/s  (0.193s, 5299.43/s)  LR: 9.856e-04  Data: 0.028 (0.038)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.731 (4.71)  Time: 0.175s, 5861.02/s  (0.193s, 5292.31/s)  LR: 9.856e-04  Data: 0.021 (0.037)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.733 (4.71)  Time: 0.177s, 5774.99/s  (0.194s, 5287.22/s)  LR: 9.856e-04  Data: 0.038 (0.037)
Train: 23 [ 950/1251 ( 76%)]  Loss: 5.034 (4.72)  Time: 0.159s, 6432.92/s  (0.194s, 5289.04/s)  LR: 9.856e-04  Data: 0.027 (0.036)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.784 (4.73)  Time: 0.160s, 6389.63/s  (0.194s, 5268.57/s)  LR: 9.856e-04  Data: 0.029 (0.036)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.730 (4.73)  Time: 0.163s, 6288.97/s  (0.195s, 5263.33/s)  LR: 9.856e-04  Data: 0.031 (0.036)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.683 (4.72)  Time: 0.163s, 6286.61/s  (0.195s, 5254.79/s)  LR: 9.856e-04  Data: 0.026 (0.035)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.550 (4.72)  Time: 0.156s, 6550.01/s  (0.195s, 5241.26/s)  LR: 9.856e-04  Data: 0.033 (0.035)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.434 (4.71)  Time: 0.157s, 6538.85/s  (0.195s, 5247.46/s)  LR: 9.856e-04  Data: 0.030 (0.035)
Train: 23 [1250/1251 (100%)]  Loss: 4.954 (4.72)  Time: 0.114s, 9001.26/s  (0.195s, 5263.40/s)  LR: 9.856e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.811 (1.811)  Loss:  1.5588 (1.5588)  Acc@1: 70.9961 (70.9961)  Acc@5: 90.1367 (90.1367)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.4963 (2.2015)  Acc@1: 72.2877 (54.6160)  Acc@5: 88.2076 (78.9700)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)

Train: 24 [   0/1251 (  0%)]  Loss: 4.834 (4.83)  Time: 1.748s,  585.94/s  (1.748s,  585.94/s)  LR: 9.843e-04  Data: 1.500 (1.500)
Train: 24 [  50/1251 (  4%)]  Loss: 4.547 (4.69)  Time: 0.167s, 6118.74/s  (0.223s, 4585.87/s)  LR: 9.843e-04  Data: 0.027 (0.060)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.420 (4.60)  Time: 0.198s, 5171.85/s  (0.207s, 4954.32/s)  LR: 9.843e-04  Data: 0.030 (0.045)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.708 (4.63)  Time: 0.169s, 6041.74/s  (0.202s, 5069.31/s)  LR: 9.843e-04  Data: 0.029 (0.039)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.508 (4.60)  Time: 0.196s, 5234.95/s  (0.201s, 5092.76/s)  LR: 9.843e-04  Data: 0.029 (0.037)
Train: 24 [ 250/1251 ( 20%)]  Loss: 4.977 (4.67)  Time: 0.165s, 6191.78/s  (0.198s, 5165.65/s)  LR: 9.843e-04  Data: 0.037 (0.035)
Train: 24 [ 300/1251 ( 24%)]  Loss: 4.584 (4.65)  Time: 0.165s, 6215.01/s  (0.196s, 5231.73/s)  LR: 9.843e-04  Data: 0.027 (0.034)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.960 (4.69)  Time: 0.177s, 5774.70/s  (0.195s, 5247.33/s)  LR: 9.843e-04  Data: 0.032 (0.033)
Train: 24 [ 400/1251 ( 32%)]  Loss: 4.666 (4.69)  Time: 0.180s, 5685.46/s  (0.194s, 5265.71/s)  LR: 9.843e-04  Data: 0.028 (0.033)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.746 (4.70)  Time: 0.172s, 5938.94/s  (0.195s, 5253.07/s)  LR: 9.843e-04  Data: 0.025 (0.032)
Train: 24 [ 500/1251 ( 40%)]  Loss: 4.789 (4.70)  Time: 0.164s, 6241.33/s  (0.194s, 5289.16/s)  LR: 9.843e-04  Data: 0.026 (0.032)
Train: 24 [ 550/1251 ( 44%)]  Loss: 4.636 (4.70)  Time: 0.195s, 5260.29/s  (0.194s, 5282.20/s)  LR: 9.843e-04  Data: 0.023 (0.031)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.584 (4.69)  Time: 0.358s, 2859.25/s  (0.194s, 5276.45/s)  LR: 9.843e-04  Data: 0.025 (0.031)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.969 (4.71)  Time: 0.173s, 5902.66/s  (0.194s, 5291.73/s)  LR: 9.843e-04  Data: 0.028 (0.031)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.604 (4.70)  Time: 0.173s, 5933.24/s  (0.193s, 5292.24/s)  LR: 9.843e-04  Data: 0.037 (0.031)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.849 (4.71)  Time: 0.165s, 6222.80/s  (0.194s, 5288.55/s)  LR: 9.843e-04  Data: 0.022 (0.030)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.729 (4.71)  Time: 0.153s, 6700.68/s  (0.195s, 5247.80/s)  LR: 9.843e-04  Data: 0.030 (0.030)
Train: 24 [ 850/1251 ( 68%)]  Loss: 4.694 (4.71)  Time: 0.159s, 6445.61/s  (0.195s, 5252.53/s)  LR: 9.843e-04  Data: 0.024 (0.030)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.564 (4.70)  Time: 0.184s, 5568.70/s  (0.195s, 5244.01/s)  LR: 9.843e-04  Data: 0.056 (0.030)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.742 (4.71)  Time: 0.173s, 5923.25/s  (0.196s, 5237.17/s)  LR: 9.843e-04  Data: 0.031 (0.030)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.731 (4.71)  Time: 0.179s, 5725.88/s  (0.195s, 5238.12/s)  LR: 9.843e-04  Data: 0.023 (0.030)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.914 (4.72)  Time: 0.175s, 5867.39/s  (0.195s, 5245.64/s)  LR: 9.843e-04  Data: 0.025 (0.030)
Train: 24 [1100/1251 ( 88%)]  Loss: 4.449 (4.70)  Time: 0.167s, 6141.76/s  (0.195s, 5240.53/s)  LR: 9.843e-04  Data: 0.029 (0.030)
Train: 24 [1150/1251 ( 92%)]  Loss: 5.066 (4.72)  Time: 0.149s, 6859.63/s  (0.195s, 5240.87/s)  LR: 9.843e-04  Data: 0.029 (0.030)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.288 (4.70)  Time: 0.397s, 2577.63/s  (0.195s, 5237.92/s)  LR: 9.843e-04  Data: 0.028 (0.029)
Train: 24 [1250/1251 (100%)]  Loss: 4.635 (4.70)  Time: 0.114s, 8994.36/s  (0.195s, 5253.81/s)  LR: 9.843e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.792 (1.792)  Loss:  1.4283 (1.4283)  Acc@1: 73.7305 (73.7305)  Acc@5: 90.7227 (90.7227)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3861 (2.1953)  Acc@1: 73.5849 (54.7220)  Acc@5: 89.0330 (78.9380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)

Train: 25 [   0/1251 (  0%)]  Loss: 4.738 (4.74)  Time: 1.732s,  591.24/s  (1.732s,  591.24/s)  LR: 9.830e-04  Data: 1.602 (1.602)
Train: 25 [  50/1251 (  4%)]  Loss: 4.231 (4.48)  Time: 0.155s, 6619.60/s  (0.229s, 4476.10/s)  LR: 9.830e-04  Data: 0.023 (0.078)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.834 (4.60)  Time: 0.166s, 6164.79/s  (0.208s, 4934.87/s)  LR: 9.830e-04  Data: 0.028 (0.059)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.369 (4.54)  Time: 0.167s, 6133.21/s  (0.202s, 5066.04/s)  LR: 9.830e-04  Data: 0.030 (0.049)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.129 (4.46)  Time: 0.173s, 5909.01/s  (0.196s, 5215.76/s)  LR: 9.830e-04  Data: 0.025 (0.044)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.641 (4.49)  Time: 0.154s, 6668.24/s  (0.196s, 5219.29/s)  LR: 9.830e-04  Data: 0.029 (0.041)
Train: 25 [ 300/1251 ( 24%)]  Loss: 4.417 (4.48)  Time: 0.164s, 6227.55/s  (0.195s, 5262.16/s)  LR: 9.830e-04  Data: 0.022 (0.039)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.743 (4.51)  Time: 0.161s, 6367.98/s  (0.194s, 5272.81/s)  LR: 9.830e-04  Data: 0.027 (0.037)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.773 (4.54)  Time: 0.299s, 3429.60/s  (0.193s, 5294.87/s)  LR: 9.830e-04  Data: 0.029 (0.036)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.516 (4.54)  Time: 0.177s, 5769.10/s  (0.193s, 5312.63/s)  LR: 9.830e-04  Data: 0.027 (0.035)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.392 (4.53)  Time: 0.173s, 5911.14/s  (0.192s, 5322.04/s)  LR: 9.830e-04  Data: 0.026 (0.034)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.665 (4.54)  Time: 0.151s, 6763.27/s  (0.192s, 5319.74/s)  LR: 9.830e-04  Data: 0.025 (0.034)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.900 (4.57)  Time: 0.171s, 5972.95/s  (0.193s, 5295.48/s)  LR: 9.830e-04  Data: 0.025 (0.034)
Train: 25 [ 650/1251 ( 52%)]  Loss: 4.551 (4.56)  Time: 0.179s, 5714.32/s  (0.193s, 5304.08/s)  LR: 9.830e-04  Data: 0.026 (0.033)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.929 (4.59)  Time: 0.163s, 6284.28/s  (0.193s, 5311.80/s)  LR: 9.830e-04  Data: 0.027 (0.033)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.647 (4.59)  Time: 0.169s, 6072.59/s  (0.193s, 5310.55/s)  LR: 9.830e-04  Data: 0.028 (0.033)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.606 (4.59)  Time: 0.156s, 6557.23/s  (0.193s, 5312.35/s)  LR: 9.830e-04  Data: 0.024 (0.032)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.589 (4.59)  Time: 0.176s, 5828.89/s  (0.193s, 5302.96/s)  LR: 9.830e-04  Data: 0.024 (0.032)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.558 (4.59)  Time: 0.357s, 2868.47/s  (0.193s, 5295.30/s)  LR: 9.830e-04  Data: 0.026 (0.032)
Train: 25 [ 950/1251 ( 76%)]  Loss: 4.954 (4.61)  Time: 0.167s, 6131.68/s  (0.194s, 5291.75/s)  LR: 9.830e-04  Data: 0.025 (0.031)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.798 (4.62)  Time: 0.159s, 6443.42/s  (0.193s, 5295.71/s)  LR: 9.830e-04  Data: 0.028 (0.031)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.453 (4.61)  Time: 0.180s, 5673.44/s  (0.193s, 5299.92/s)  LR: 9.830e-04  Data: 0.025 (0.031)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.745 (4.62)  Time: 0.170s, 6036.60/s  (0.194s, 5289.72/s)  LR: 9.830e-04  Data: 0.029 (0.031)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.457 (4.61)  Time: 0.156s, 6581.51/s  (0.193s, 5294.74/s)  LR: 9.830e-04  Data: 0.026 (0.031)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.669 (4.61)  Time: 0.173s, 5919.37/s  (0.194s, 5291.82/s)  LR: 9.830e-04  Data: 0.028 (0.031)
Train: 25 [1250/1251 (100%)]  Loss: 4.694 (4.62)  Time: 0.114s, 9001.22/s  (0.193s, 5301.24/s)  LR: 9.830e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.823 (1.823)  Loss:  1.6186 (1.6186)  Acc@1: 71.3867 (71.3867)  Acc@5: 89.9414 (89.9414)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.5512 (2.2422)  Acc@1: 73.8208 (55.3380)  Acc@5: 88.4434 (79.3780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)

Train: 26 [   0/1251 (  0%)]  Loss: 4.806 (4.81)  Time: 1.664s,  615.45/s  (1.664s,  615.45/s)  LR: 9.816e-04  Data: 1.537 (1.537)
Train: 26 [  50/1251 (  4%)]  Loss: 4.278 (4.54)  Time: 0.179s, 5729.08/s  (0.223s, 4593.50/s)  LR: 9.816e-04  Data: 0.026 (0.079)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.789 (4.62)  Time: 0.187s, 5487.08/s  (0.207s, 4947.52/s)  LR: 9.816e-04  Data: 0.031 (0.064)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.905 (4.69)  Time: 0.151s, 6802.14/s  (0.201s, 5083.80/s)  LR: 9.816e-04  Data: 0.022 (0.058)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.393 (4.63)  Time: 0.163s, 6269.05/s  (0.199s, 5134.31/s)  LR: 9.816e-04  Data: 0.030 (0.055)
Train: 26 [ 250/1251 ( 20%)]  Loss: 4.858 (4.67)  Time: 0.166s, 6165.55/s  (0.195s, 5239.14/s)  LR: 9.816e-04  Data: 0.034 (0.051)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.962 (4.71)  Time: 0.161s, 6351.45/s  (0.195s, 5261.17/s)  LR: 9.816e-04  Data: 0.026 (0.049)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.767 (4.72)  Time: 0.169s, 6073.43/s  (0.194s, 5274.46/s)  LR: 9.816e-04  Data: 0.026 (0.047)
Train: 26 [ 400/1251 ( 32%)]  Loss: 5.047 (4.76)  Time: 0.534s, 1917.77/s  (0.195s, 5251.85/s)  LR: 9.816e-04  Data: 0.410 (0.046)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.741 (4.75)  Time: 0.164s, 6259.02/s  (0.195s, 5256.94/s)  LR: 9.816e-04  Data: 0.029 (0.046)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.845 (4.76)  Time: 0.182s, 5627.86/s  (0.194s, 5282.60/s)  LR: 9.816e-04  Data: 0.022 (0.045)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.848 (4.77)  Time: 0.172s, 5953.34/s  (0.195s, 5254.28/s)  LR: 9.816e-04  Data: 0.020 (0.046)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.679 (4.76)  Time: 0.392s, 2609.69/s  (0.195s, 5250.95/s)  LR: 9.816e-04  Data: 0.044 (0.045)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.738 (4.76)  Time: 0.188s, 5460.75/s  (0.195s, 5250.57/s)  LR: 9.816e-04  Data: 0.027 (0.044)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.525 (4.75)  Time: 0.152s, 6736.72/s  (0.194s, 5273.35/s)  LR: 9.816e-04  Data: 0.025 (0.043)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.511 (4.73)  Time: 0.192s, 5324.41/s  (0.193s, 5293.98/s)  LR: 9.816e-04  Data: 0.025 (0.042)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.790 (4.73)  Time: 0.241s, 4256.56/s  (0.194s, 5287.16/s)  LR: 9.816e-04  Data: 0.029 (0.041)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.697 (4.73)  Time: 0.168s, 6111.35/s  (0.194s, 5286.50/s)  LR: 9.816e-04  Data: 0.026 (0.041)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.353 (4.71)  Time: 0.164s, 6227.88/s  (0.194s, 5274.72/s)  LR: 9.816e-04  Data: 0.032 (0.041)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.642 (4.71)  Time: 0.175s, 5840.28/s  (0.194s, 5280.77/s)  LR: 9.816e-04  Data: 0.023 (0.040)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.618 (4.70)  Time: 0.172s, 5952.71/s  (0.194s, 5272.50/s)  LR: 9.816e-04  Data: 0.027 (0.041)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.388 (4.69)  Time: 0.183s, 5581.44/s  (0.194s, 5275.92/s)  LR: 9.816e-04  Data: 0.021 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Train: 26 [1100/1251 ( 88%)]  Loss: 4.097 (4.66)  Time: 0.180s, 5683.62/s  (0.194s, 5279.07/s)  LR: 9.816e-04  Data: 0.021 (0.040)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.728 (4.67)  Time: 0.177s, 5789.40/s  (0.194s, 5274.24/s)  LR: 9.816e-04  Data: 0.027 (0.039)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.781 (4.67)  Time: 0.172s, 5961.10/s  (0.194s, 5269.87/s)  LR: 9.816e-04  Data: 0.033 (0.039)
Train: 26 [1250/1251 (100%)]  Loss: 4.377 (4.66)  Time: 0.112s, 9126.17/s  (0.194s, 5276.35/s)  LR: 9.816e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.932 (1.932)  Loss:  1.4827 (1.4827)  Acc@1: 73.6328 (73.6328)  Acc@5: 90.6250 (90.6250)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.4961 (2.1920)  Acc@1: 73.4670 (56.2160)  Acc@5: 88.6792 (79.8680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)

Train: 27 [   0/1251 (  0%)]  Loss: 4.779 (4.78)  Time: 1.996s,  513.02/s  (1.996s,  513.02/s)  LR: 9.802e-04  Data: 1.867 (1.867)
Train: 27 [  50/1251 (  4%)]  Loss: 4.610 (4.69)  Time: 0.193s, 5297.71/s  (0.225s, 4555.93/s)  LR: 9.802e-04  Data: 0.020 (0.079)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.565 (4.65)  Time: 0.188s, 5459.16/s  (0.208s, 4922.90/s)  LR: 9.802e-04  Data: 0.036 (0.063)
Train: 27 [ 150/1251 ( 12%)]  Loss: 4.939 (4.72)  Time: 0.169s, 6066.66/s  (0.201s, 5092.40/s)  LR: 9.802e-04  Data: 0.027 (0.053)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.823 (4.74)  Time: 0.164s, 6231.90/s  (0.198s, 5163.86/s)  LR: 9.802e-04  Data: 0.034 (0.047)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.673 (4.73)  Time: 0.193s, 5303.42/s  (0.196s, 5227.43/s)  LR: 9.802e-04  Data: 0.027 (0.043)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.610 (4.71)  Time: 0.163s, 6276.12/s  (0.195s, 5250.38/s)  LR: 9.802e-04  Data: 0.025 (0.041)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.953 (4.74)  Time: 0.188s, 5453.40/s  (0.194s, 5280.68/s)  LR: 9.802e-04  Data: 0.027 (0.039)
Train: 27 [ 400/1251 ( 32%)]  Loss: 4.743 (4.74)  Time: 0.273s, 3755.52/s  (0.194s, 5283.58/s)  LR: 9.802e-04  Data: 0.029 (0.038)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.533 (4.72)  Time: 0.174s, 5892.54/s  (0.194s, 5283.82/s)  LR: 9.802e-04  Data: 0.022 (0.038)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.496 (4.70)  Time: 0.173s, 5911.98/s  (0.194s, 5286.85/s)  LR: 9.802e-04  Data: 0.026 (0.038)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.563 (4.69)  Time: 0.163s, 6269.95/s  (0.194s, 5288.70/s)  LR: 9.802e-04  Data: 0.030 (0.039)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.408 (4.67)  Time: 0.315s, 3246.63/s  (0.193s, 5293.44/s)  LR: 9.802e-04  Data: 0.187 (0.039)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.493 (4.66)  Time: 0.164s, 6244.66/s  (0.193s, 5314.12/s)  LR: 9.802e-04  Data: 0.027 (0.039)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.514 (4.65)  Time: 0.180s, 5683.22/s  (0.193s, 5319.19/s)  LR: 9.802e-04  Data: 0.023 (0.038)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.518 (4.64)  Time: 0.173s, 5930.00/s  (0.193s, 5311.48/s)  LR: 9.802e-04  Data: 0.042 (0.037)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.716 (4.64)  Time: 0.172s, 5946.14/s  (0.193s, 5305.29/s)  LR: 9.802e-04  Data: 0.031 (0.037)
Train: 27 [ 850/1251 ( 68%)]  Loss: 5.017 (4.66)  Time: 0.150s, 6848.82/s  (0.193s, 5308.46/s)  LR: 9.802e-04  Data: 0.023 (0.036)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.774 (4.67)  Time: 0.162s, 6301.88/s  (0.193s, 5310.49/s)  LR: 9.802e-04  Data: 0.023 (0.036)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.786 (4.68)  Time: 0.162s, 6326.53/s  (0.193s, 5310.84/s)  LR: 9.802e-04  Data: 0.021 (0.035)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.861 (4.68)  Time: 0.181s, 5649.71/s  (0.193s, 5313.23/s)  LR: 9.802e-04  Data: 0.021 (0.035)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.538 (4.68)  Time: 0.156s, 6544.14/s  (0.193s, 5306.46/s)  LR: 9.802e-04  Data: 0.028 (0.035)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.852 (4.69)  Time: 0.168s, 6084.22/s  (0.193s, 5294.85/s)  LR: 9.802e-04  Data: 0.027 (0.035)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.452 (4.68)  Time: 0.198s, 5159.05/s  (0.194s, 5289.96/s)  LR: 9.802e-04  Data: 0.025 (0.035)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.785 (4.68)  Time: 0.184s, 5552.40/s  (0.194s, 5289.98/s)  LR: 9.802e-04  Data: 0.030 (0.034)
Train: 27 [1250/1251 (100%)]  Loss: 4.681 (4.68)  Time: 0.114s, 8967.33/s  (0.193s, 5297.96/s)  LR: 9.802e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.085 (2.085)  Loss:  1.4167 (1.4167)  Acc@1: 72.2656 (72.2656)  Acc@5: 90.3320 (90.3320)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.4034 (2.1559)  Acc@1: 72.6415 (55.7820)  Acc@5: 88.3255 (79.7800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)

Train: 28 [   0/1251 (  0%)]  Loss: 4.501 (4.50)  Time: 1.934s,  529.61/s  (1.934s,  529.61/s)  LR: 9.787e-04  Data: 1.803 (1.803)
Train: 28 [  50/1251 (  4%)]  Loss: 4.453 (4.48)  Time: 0.161s, 6376.11/s  (0.220s, 4648.54/s)  LR: 9.787e-04  Data: 0.028 (0.076)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.290 (4.41)  Time: 0.162s, 6324.77/s  (0.207s, 4940.97/s)  LR: 9.787e-04  Data: 0.028 (0.062)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.742 (4.50)  Time: 0.177s, 5769.93/s  (0.200s, 5108.37/s)  LR: 9.787e-04  Data: 0.027 (0.055)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.684 (4.53)  Time: 0.334s, 3066.98/s  (0.199s, 5148.17/s)  LR: 9.787e-04  Data: 0.025 (0.049)
Train: 28 [ 250/1251 ( 20%)]  Loss: 4.742 (4.57)  Time: 0.174s, 5891.19/s  (0.196s, 5232.27/s)  LR: 9.787e-04  Data: 0.020 (0.045)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.550 (4.57)  Time: 0.168s, 6112.00/s  (0.194s, 5276.68/s)  LR: 9.787e-04  Data: 0.023 (0.042)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.593 (4.57)  Time: 0.175s, 5849.61/s  (0.194s, 5288.50/s)  LR: 9.787e-04  Data: 0.024 (0.040)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.932 (4.61)  Time: 0.174s, 5891.99/s  (0.194s, 5275.88/s)  LR: 9.787e-04  Data: 0.026 (0.039)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.713 (4.62)  Time: 0.160s, 6417.40/s  (0.193s, 5299.35/s)  LR: 9.787e-04  Data: 0.028 (0.037)
Train: 28 [ 500/1251 ( 40%)]  Loss: 4.829 (4.64)  Time: 0.168s, 6087.09/s  (0.193s, 5310.01/s)  LR: 9.787e-04  Data: 0.028 (0.037)
Train: 28 [ 550/1251 ( 44%)]  Loss: 4.670 (4.64)  Time: 0.167s, 6140.79/s  (0.193s, 5315.08/s)  LR: 9.787e-04  Data: 0.024 (0.036)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.555 (4.63)  Time: 0.505s, 2029.28/s  (0.193s, 5298.61/s)  LR: 9.787e-04  Data: 0.020 (0.035)
Train: 28 [ 650/1251 ( 52%)]  Loss: 4.852 (4.65)  Time: 0.164s, 6230.29/s  (0.193s, 5299.71/s)  LR: 9.787e-04  Data: 0.028 (0.034)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.912 (4.67)  Time: 0.164s, 6258.85/s  (0.193s, 5312.20/s)  LR: 9.787e-04  Data: 0.031 (0.034)
Train: 28 [ 750/1251 ( 60%)]  Loss: 4.236 (4.64)  Time: 0.191s, 5353.02/s  (0.193s, 5302.17/s)  LR: 9.787e-04  Data: 0.029 (0.034)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.823 (4.65)  Time: 0.328s, 3126.05/s  (0.193s, 5298.20/s)  LR: 9.787e-04  Data: 0.020 (0.033)
Train: 28 [ 850/1251 ( 68%)]  Loss: 4.837 (4.66)  Time: 0.182s, 5616.72/s  (0.193s, 5299.85/s)  LR: 9.787e-04  Data: 0.027 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.750 (4.67)  Time: 0.161s, 6370.73/s  (0.193s, 5312.15/s)  LR: 9.787e-04  Data: 0.024 (0.033)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.816 (4.67)  Time: 0.166s, 6161.99/s  (0.193s, 5310.78/s)  LR: 9.787e-04  Data: 0.024 (0.032)
Train: 28 [1000/1251 ( 80%)]  Loss: 4.343 (4.66)  Time: 0.187s, 5464.73/s  (0.193s, 5305.04/s)  LR: 9.787e-04  Data: 0.026 (0.033)
Train: 28 [1050/1251 ( 84%)]  Loss: 4.393 (4.65)  Time: 0.178s, 5741.52/s  (0.193s, 5299.04/s)  LR: 9.787e-04  Data: 0.025 (0.034)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.558 (4.64)  Time: 0.467s, 2192.95/s  (0.193s, 5295.39/s)  LR: 9.787e-04  Data: 0.341 (0.034)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.823 (4.65)  Time: 0.156s, 6559.62/s  (0.193s, 5293.77/s)  LR: 9.787e-04  Data: 0.026 (0.034)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.258 (4.63)  Time: 0.201s, 5103.83/s  (0.193s, 5294.86/s)  LR: 9.787e-04  Data: 0.021 (0.035)
Train: 28 [1250/1251 (100%)]  Loss: 4.839 (4.64)  Time: 0.114s, 9016.87/s  (0.193s, 5303.67/s)  LR: 9.787e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.802 (1.802)  Loss:  1.2850 (1.2850)  Acc@1: 75.5859 (75.5859)  Acc@5: 91.6992 (91.6992)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.4462 (2.1537)  Acc@1: 73.2311 (56.1920)  Acc@5: 87.9717 (80.1040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)

Train: 29 [   0/1251 (  0%)]  Loss: 4.754 (4.75)  Time: 1.672s,  612.55/s  (1.672s,  612.55/s)  LR: 9.771e-04  Data: 1.549 (1.549)
Train: 29 [  50/1251 (  4%)]  Loss: 4.422 (4.59)  Time: 0.168s, 6095.74/s  (0.228s, 4499.52/s)  LR: 9.771e-04  Data: 0.028 (0.066)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.953 (4.71)  Time: 0.196s, 5223.90/s  (0.209s, 4907.49/s)  LR: 9.771e-04  Data: 0.027 (0.048)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.617 (4.69)  Time: 0.171s, 5980.27/s  (0.202s, 5074.49/s)  LR: 9.771e-04  Data: 0.026 (0.041)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.688 (4.69)  Time: 0.155s, 6591.31/s  (0.199s, 5157.87/s)  LR: 9.771e-04  Data: 0.023 (0.038)
Train: 29 [ 250/1251 ( 20%)]  Loss: 4.827 (4.71)  Time: 0.177s, 5799.36/s  (0.196s, 5225.04/s)  LR: 9.771e-04  Data: 0.025 (0.036)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.431 (4.67)  Time: 0.166s, 6173.01/s  (0.195s, 5262.90/s)  LR: 9.771e-04  Data: 0.025 (0.034)
Train: 29 [ 350/1251 ( 28%)]  Loss: 4.486 (4.65)  Time: 0.170s, 6033.44/s  (0.195s, 5254.47/s)  LR: 9.771e-04  Data: 0.026 (0.034)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.356 (4.61)  Time: 0.169s, 6061.95/s  (0.195s, 5263.35/s)  LR: 9.771e-04  Data: 0.025 (0.033)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.778 (4.63)  Time: 0.173s, 5921.04/s  (0.194s, 5291.89/s)  LR: 9.771e-04  Data: 0.025 (0.032)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.800 (4.65)  Time: 0.163s, 6272.74/s  (0.193s, 5301.17/s)  LR: 9.771e-04  Data: 0.025 (0.032)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.999 (4.68)  Time: 0.155s, 6595.43/s  (0.193s, 5297.03/s)  LR: 9.771e-04  Data: 0.020 (0.032)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.407 (4.66)  Time: 0.216s, 4741.42/s  (0.193s, 5309.70/s)  LR: 9.771e-04  Data: 0.080 (0.032)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.870 (4.67)  Time: 0.155s, 6589.93/s  (0.193s, 5310.54/s)  LR: 9.771e-04  Data: 0.024 (0.033)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.621 (4.67)  Time: 0.188s, 5444.19/s  (0.193s, 5308.88/s)  LR: 9.771e-04  Data: 0.030 (0.034)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.792 (4.68)  Time: 0.167s, 6129.04/s  (0.193s, 5317.22/s)  LR: 9.771e-04  Data: 0.028 (0.035)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.697 (4.68)  Time: 0.258s, 3975.94/s  (0.193s, 5316.65/s)  LR: 9.771e-04  Data: 0.138 (0.035)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.715 (4.68)  Time: 0.171s, 5986.30/s  (0.193s, 5303.90/s)  LR: 9.771e-04  Data: 0.021 (0.036)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.544 (4.67)  Time: 0.177s, 5801.59/s  (0.193s, 5309.58/s)  LR: 9.771e-04  Data: 0.029 (0.036)
Train: 29 [ 950/1251 ( 76%)]  Loss: 4.705 (4.67)  Time: 0.173s, 5923.77/s  (0.194s, 5284.30/s)  LR: 9.771e-04  Data: 0.024 (0.037)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.462 (4.66)  Time: 0.168s, 6090.67/s  (0.193s, 5294.97/s)  LR: 9.771e-04  Data: 0.029 (0.037)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.593 (4.66)  Time: 0.176s, 5803.55/s  (0.194s, 5283.58/s)  LR: 9.771e-04  Data: 0.018 (0.037)
Train: 29 [1100/1251 ( 88%)]  Loss: 4.579 (4.66)  Time: 0.567s, 1806.05/s  (0.194s, 5275.86/s)  LR: 9.771e-04  Data: 0.020 (0.036)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.635 (4.66)  Time: 0.204s, 5019.69/s  (0.194s, 5282.24/s)  LR: 9.771e-04  Data: 0.027 (0.036)
Train: 29 [1200/1251 ( 96%)]  Loss: 4.900 (4.67)  Time: 0.173s, 5935.29/s  (0.194s, 5277.33/s)  LR: 9.771e-04  Data: 0.031 (0.036)
Train: 29 [1250/1251 (100%)]  Loss: 4.525 (4.66)  Time: 0.113s, 9044.70/s  (0.194s, 5291.20/s)  LR: 9.771e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.829 (1.829)  Loss:  1.5946 (1.5946)  Acc@1: 71.5820 (71.5820)  Acc@5: 89.2578 (89.2578)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.3838 (2.1340)  Acc@1: 74.7642 (56.0420)  Acc@5: 90.4481 (80.0780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)

Train: 30 [   0/1251 (  0%)]  Loss: 4.699 (4.70)  Time: 1.817s,  563.43/s  (1.817s,  563.43/s)  LR: 9.756e-04  Data: 1.695 (1.695)
Train: 30 [  50/1251 (  4%)]  Loss: 4.556 (4.63)  Time: 0.166s, 6182.67/s  (0.221s, 4631.78/s)  LR: 9.756e-04  Data: 0.027 (0.077)
Train: 30 [ 100/1251 (  8%)]  Loss: 4.711 (4.66)  Time: 0.153s, 6684.77/s  (0.208s, 4914.74/s)  LR: 9.756e-04  Data: 0.022 (0.053)
Train: 30 [ 150/1251 ( 12%)]  Loss: 4.695 (4.67)  Time: 0.173s, 5931.99/s  (0.203s, 5042.58/s)  LR: 9.756e-04  Data: 0.033 (0.045)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.710 (4.67)  Time: 0.181s, 5654.81/s  (0.200s, 5130.10/s)  LR: 9.756e-04  Data: 0.031 (0.041)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.074 (4.57)  Time: 0.184s, 5570.67/s  (0.197s, 5191.47/s)  LR: 9.756e-04  Data: 0.031 (0.038)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.880 (4.62)  Time: 0.164s, 6252.06/s  (0.197s, 5195.76/s)  LR: 9.756e-04  Data: 0.021 (0.036)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.506 (4.60)  Time: 0.167s, 6116.91/s  (0.196s, 5223.46/s)  LR: 9.756e-04  Data: 0.024 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.480 (4.59)  Time: 0.187s, 5486.06/s  (0.195s, 5247.35/s)  LR: 9.756e-04  Data: 0.029 (0.034)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.789 (4.61)  Time: 0.522s, 1960.74/s  (0.195s, 5250.68/s)  LR: 9.756e-04  Data: 0.025 (0.033)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.533 (4.60)  Time: 0.172s, 5962.48/s  (0.195s, 5255.42/s)  LR: 9.756e-04  Data: 0.020 (0.033)
Train: 30 [ 550/1251 ( 44%)]  Loss: 4.630 (4.61)  Time: 0.176s, 5823.31/s  (0.194s, 5267.70/s)  LR: 9.756e-04  Data: 0.027 (0.033)
Train: 30 [ 600/1251 ( 48%)]  Loss: 4.366 (4.59)  Time: 0.184s, 5574.46/s  (0.194s, 5280.41/s)  LR: 9.756e-04  Data: 0.028 (0.032)
Train: 30 [ 650/1251 ( 52%)]  Loss: 5.110 (4.62)  Time: 0.207s, 4938.46/s  (0.194s, 5281.54/s)  LR: 9.756e-04  Data: 0.032 (0.032)
Train: 30 [ 700/1251 ( 56%)]  Loss: 4.623 (4.62)  Time: 0.172s, 5957.46/s  (0.194s, 5285.15/s)  LR: 9.756e-04  Data: 0.028 (0.032)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.495 (4.62)  Time: 0.187s, 5472.44/s  (0.194s, 5277.53/s)  LR: 9.756e-04  Data: 0.026 (0.031)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.205 (4.59)  Time: 0.165s, 6203.97/s  (0.194s, 5269.73/s)  LR: 9.756e-04  Data: 0.022 (0.031)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.366 (4.58)  Time: 0.185s, 5531.57/s  (0.194s, 5275.41/s)  LR: 9.756e-04  Data: 0.024 (0.031)
Train: 30 [ 900/1251 ( 72%)]  Loss: 5.088 (4.61)  Time: 0.178s, 5741.26/s  (0.194s, 5274.32/s)  LR: 9.756e-04  Data: 0.026 (0.031)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.757 (4.61)  Time: 0.184s, 5576.97/s  (0.194s, 5269.90/s)  LR: 9.756e-04  Data: 0.028 (0.031)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.690 (4.62)  Time: 0.169s, 6064.60/s  (0.194s, 5271.32/s)  LR: 9.756e-04  Data: 0.029 (0.031)
Train: 30 [1050/1251 ( 84%)]  Loss: 4.474 (4.61)  Time: 0.183s, 5605.70/s  (0.194s, 5276.81/s)  LR: 9.756e-04  Data: 0.028 (0.030)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.119 (4.59)  Time: 0.185s, 5528.33/s  (0.194s, 5270.46/s)  LR: 9.756e-04  Data: 0.034 (0.030)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.223 (4.57)  Time: 0.187s, 5480.95/s  (0.195s, 5249.05/s)  LR: 9.756e-04  Data: 0.019 (0.031)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.549 (4.57)  Time: 0.166s, 6163.25/s  (0.195s, 5255.27/s)  LR: 9.756e-04  Data: 0.020 (0.031)
Train: 30 [1250/1251 (100%)]  Loss: 4.578 (4.57)  Time: 0.113s, 9038.25/s  (0.195s, 5260.36/s)  LR: 9.756e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.751 (1.751)  Loss:  1.5023 (1.5023)  Acc@1: 73.6328 (73.6328)  Acc@5: 90.4297 (90.4297)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.4832 (2.1485)  Acc@1: 73.5849 (56.7560)  Acc@5: 89.9764 (80.7280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)

Train: 31 [   0/1251 (  0%)]  Loss: 4.695 (4.69)  Time: 1.889s,  541.96/s  (1.889s,  541.96/s)  LR: 9.739e-04  Data: 1.769 (1.769)
Train: 31 [  50/1251 (  4%)]  Loss: 4.899 (4.80)  Time: 0.175s, 5866.28/s  (0.227s, 4503.56/s)  LR: 9.739e-04  Data: 0.031 (0.080)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.969 (4.85)  Time: 0.174s, 5885.61/s  (0.210s, 4881.69/s)  LR: 9.739e-04  Data: 0.032 (0.055)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.365 (4.73)  Time: 0.179s, 5710.66/s  (0.206s, 4963.29/s)  LR: 9.739e-04  Data: 0.031 (0.046)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.635 (4.71)  Time: 0.178s, 5743.97/s  (0.201s, 5092.78/s)  LR: 9.739e-04  Data: 0.028 (0.041)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.734 (4.72)  Time: 0.176s, 5826.29/s  (0.198s, 5163.44/s)  LR: 9.739e-04  Data: 0.022 (0.038)
Train: 31 [ 300/1251 ( 24%)]  Loss: 4.833 (4.73)  Time: 0.377s, 2714.62/s  (0.197s, 5210.38/s)  LR: 9.739e-04  Data: 0.241 (0.038)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.474 (4.70)  Time: 0.170s, 6009.03/s  (0.196s, 5230.03/s)  LR: 9.739e-04  Data: 0.027 (0.038)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.752 (4.71)  Time: 0.171s, 5989.56/s  (0.196s, 5237.03/s)  LR: 9.739e-04  Data: 0.024 (0.040)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.449 (4.68)  Time: 0.188s, 5447.61/s  (0.195s, 5246.55/s)  LR: 9.739e-04  Data: 0.025 (0.039)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.775 (4.69)  Time: 0.174s, 5879.27/s  (0.194s, 5264.83/s)  LR: 9.739e-04  Data: 0.040 (0.038)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.883 (4.71)  Time: 0.175s, 5851.29/s  (0.194s, 5281.81/s)  LR: 9.739e-04  Data: 0.025 (0.038)
Train: 31 [ 600/1251 ( 48%)]  Loss: 4.312 (4.68)  Time: 0.173s, 5914.63/s  (0.194s, 5275.89/s)  LR: 9.739e-04  Data: 0.035 (0.037)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.706 (4.68)  Time: 0.172s, 5949.38/s  (0.194s, 5287.88/s)  LR: 9.739e-04  Data: 0.029 (0.036)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.611 (4.67)  Time: 0.170s, 6033.08/s  (0.193s, 5294.20/s)  LR: 9.739e-04  Data: 0.035 (0.036)
Train: 31 [ 750/1251 ( 60%)]  Loss: 4.635 (4.67)  Time: 0.180s, 5690.39/s  (0.194s, 5291.31/s)  LR: 9.739e-04  Data: 0.025 (0.035)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.930 (4.69)  Time: 0.187s, 5489.52/s  (0.194s, 5280.93/s)  LR: 9.739e-04  Data: 0.028 (0.035)
Train: 31 [ 850/1251 ( 68%)]  Loss: 4.699 (4.69)  Time: 0.162s, 6338.85/s  (0.193s, 5292.37/s)  LR: 9.739e-04  Data: 0.024 (0.034)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.821 (4.69)  Time: 0.157s, 6519.50/s  (0.194s, 5286.26/s)  LR: 9.739e-04  Data: 0.036 (0.034)
Train: 31 [ 950/1251 ( 76%)]  Loss: 4.393 (4.68)  Time: 0.167s, 6126.96/s  (0.194s, 5291.73/s)  LR: 9.739e-04  Data: 0.027 (0.033)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.278 (4.66)  Time: 0.167s, 6141.05/s  (0.193s, 5294.43/s)  LR: 9.739e-04  Data: 0.022 (0.033)
Train: 31 [1050/1251 ( 84%)]  Loss: 4.666 (4.66)  Time: 0.166s, 6167.85/s  (0.193s, 5293.44/s)  LR: 9.739e-04  Data: 0.040 (0.033)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.856 (4.67)  Time: 0.183s, 5587.32/s  (0.194s, 5289.12/s)  LR: 9.739e-04  Data: 0.024 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 31 [1150/1251 ( 92%)]  Loss: 4.558 (4.66)  Time: 0.187s, 5483.80/s  (0.194s, 5285.47/s)  LR: 9.739e-04  Data: 0.030 (0.032)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.978 (4.68)  Time: 0.168s, 6083.85/s  (0.194s, 5283.81/s)  LR: 9.739e-04  Data: 0.035 (0.032)
Train: 31 [1250/1251 (100%)]  Loss: 4.340 (4.66)  Time: 0.114s, 8998.14/s  (0.193s, 5295.99/s)  LR: 9.739e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.941 (1.941)  Loss:  1.4403 (1.4403)  Acc@1: 72.0703 (72.0703)  Acc@5: 90.7227 (90.7227)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.4116 (2.0946)  Acc@1: 73.3491 (56.9260)  Acc@5: 87.6179 (80.7080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)

Train: 32 [   0/1251 (  0%)]  Loss: 5.208 (5.21)  Time: 1.861s,  550.37/s  (1.861s,  550.37/s)  LR: 9.722e-04  Data: 1.731 (1.731)
Train: 32 [  50/1251 (  4%)]  Loss: 4.865 (5.04)  Time: 0.163s, 6294.35/s  (0.225s, 4542.04/s)  LR: 9.722e-04  Data: 0.025 (0.075)
Train: 32 [ 100/1251 (  8%)]  Loss: 4.724 (4.93)  Time: 0.171s, 6003.07/s  (0.205s, 4985.10/s)  LR: 9.722e-04  Data: 0.020 (0.056)
Train: 32 [ 150/1251 ( 12%)]  Loss: 4.527 (4.83)  Time: 0.172s, 5959.27/s  (0.199s, 5142.22/s)  LR: 9.722e-04  Data: 0.024 (0.051)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.301 (4.73)  Time: 0.177s, 5773.77/s  (0.197s, 5187.67/s)  LR: 9.722e-04  Data: 0.034 (0.049)
Train: 32 [ 250/1251 ( 20%)]  Loss: 4.709 (4.72)  Time: 0.161s, 6341.18/s  (0.195s, 5240.90/s)  LR: 9.722e-04  Data: 0.029 (0.048)
Train: 32 [ 300/1251 ( 24%)]  Loss: 5.125 (4.78)  Time: 0.151s, 6772.48/s  (0.195s, 5261.02/s)  LR: 9.722e-04  Data: 0.027 (0.047)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.601 (4.76)  Time: 0.177s, 5770.22/s  (0.193s, 5292.31/s)  LR: 9.722e-04  Data: 0.043 (0.047)
Train: 32 [ 400/1251 ( 32%)]  Loss: 4.622 (4.74)  Time: 0.212s, 4825.14/s  (0.194s, 5291.65/s)  LR: 9.722e-04  Data: 0.021 (0.045)
Train: 32 [ 450/1251 ( 36%)]  Loss: 4.724 (4.74)  Time: 0.155s, 6621.33/s  (0.193s, 5307.75/s)  LR: 9.722e-04  Data: 0.024 (0.043)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.769 (4.74)  Time: 0.177s, 5770.63/s  (0.192s, 5321.80/s)  LR: 9.722e-04  Data: 0.033 (0.042)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.751 (4.74)  Time: 0.149s, 6880.85/s  (0.192s, 5342.92/s)  LR: 9.722e-04  Data: 0.023 (0.041)
Train: 32 [ 600/1251 ( 48%)]  Loss: 4.590 (4.73)  Time: 0.160s, 6391.00/s  (0.192s, 5321.38/s)  LR: 9.722e-04  Data: 0.026 (0.041)
Train: 32 [ 650/1251 ( 52%)]  Loss: 4.436 (4.71)  Time: 0.169s, 6062.10/s  (0.192s, 5322.03/s)  LR: 9.722e-04  Data: 0.030 (0.041)
Train: 32 [ 700/1251 ( 56%)]  Loss: 4.518 (4.70)  Time: 0.180s, 5701.52/s  (0.192s, 5332.01/s)  LR: 9.722e-04  Data: 0.024 (0.040)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.551 (4.69)  Time: 0.181s, 5651.42/s  (0.192s, 5336.29/s)  LR: 9.722e-04  Data: 0.028 (0.039)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.651 (4.69)  Time: 0.177s, 5770.66/s  (0.192s, 5333.27/s)  LR: 9.722e-04  Data: 0.028 (0.038)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.738 (4.69)  Time: 0.188s, 5454.43/s  (0.192s, 5324.24/s)  LR: 9.722e-04  Data: 0.025 (0.038)
Train: 32 [ 900/1251 ( 72%)]  Loss: 4.569 (4.68)  Time: 0.191s, 5348.29/s  (0.193s, 5314.08/s)  LR: 9.722e-04  Data: 0.020 (0.037)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.473 (4.67)  Time: 0.173s, 5907.61/s  (0.193s, 5304.82/s)  LR: 9.722e-04  Data: 0.030 (0.037)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.980 (4.69)  Time: 0.169s, 6055.07/s  (0.193s, 5299.13/s)  LR: 9.722e-04  Data: 0.022 (0.036)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.870 (4.70)  Time: 0.187s, 5487.00/s  (0.193s, 5297.06/s)  LR: 9.722e-04  Data: 0.030 (0.036)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.510 (4.69)  Time: 0.177s, 5793.85/s  (0.193s, 5302.70/s)  LR: 9.722e-04  Data: 0.031 (0.036)
Train: 32 [1150/1251 ( 92%)]  Loss: 4.887 (4.70)  Time: 0.167s, 6146.76/s  (0.193s, 5299.80/s)  LR: 9.722e-04  Data: 0.042 (0.035)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.495 (4.69)  Time: 0.189s, 5423.03/s  (0.194s, 5286.25/s)  LR: 9.722e-04  Data: 0.021 (0.036)
Train: 32 [1250/1251 (100%)]  Loss: 4.698 (4.69)  Time: 0.114s, 8983.50/s  (0.193s, 5301.79/s)  LR: 9.722e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.884 (1.884)  Loss:  1.3608 (1.3608)  Acc@1: 76.2695 (76.2695)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3497 (2.0897)  Acc@1: 75.9434 (57.3940)  Acc@5: 90.5660 (80.9640)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)

Train: 33 [   0/1251 (  0%)]  Loss: 4.738 (4.74)  Time: 1.651s,  620.11/s  (1.651s,  620.11/s)  LR: 9.705e-04  Data: 1.522 (1.522)
Train: 33 [  50/1251 (  4%)]  Loss: 4.797 (4.77)  Time: 0.177s, 5778.64/s  (0.226s, 4534.17/s)  LR: 9.705e-04  Data: 0.026 (0.059)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.610 (4.72)  Time: 0.186s, 5516.14/s  (0.208s, 4927.27/s)  LR: 9.705e-04  Data: 0.036 (0.044)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.613 (4.69)  Time: 0.188s, 5450.68/s  (0.206s, 4982.54/s)  LR: 9.705e-04  Data: 0.020 (0.039)
Train: 33 [ 200/1251 ( 16%)]  Loss: 4.899 (4.73)  Time: 0.329s, 3110.03/s  (0.200s, 5109.09/s)  LR: 9.705e-04  Data: 0.024 (0.037)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.588 (4.71)  Time: 0.172s, 5959.74/s  (0.198s, 5172.55/s)  LR: 9.705e-04  Data: 0.033 (0.035)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.600 (4.69)  Time: 0.180s, 5698.11/s  (0.196s, 5227.77/s)  LR: 9.705e-04  Data: 0.028 (0.034)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.642 (4.69)  Time: 0.183s, 5581.38/s  (0.195s, 5243.53/s)  LR: 9.705e-04  Data: 0.032 (0.033)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.172 (4.63)  Time: 0.185s, 5546.31/s  (0.194s, 5265.33/s)  LR: 9.705e-04  Data: 0.028 (0.032)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.838 (4.65)  Time: 0.157s, 6504.17/s  (0.194s, 5290.14/s)  LR: 9.705e-04  Data: 0.029 (0.032)
Train: 33 [ 500/1251 ( 40%)]  Loss: 4.714 (4.66)  Time: 0.190s, 5382.30/s  (0.194s, 5285.02/s)  LR: 9.705e-04  Data: 0.034 (0.033)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.658 (4.66)  Time: 0.177s, 5795.50/s  (0.194s, 5291.56/s)  LR: 9.705e-04  Data: 0.022 (0.034)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.832 (4.67)  Time: 0.203s, 5036.51/s  (0.193s, 5300.82/s)  LR: 9.705e-04  Data: 0.020 (0.034)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.489 (4.66)  Time: 0.161s, 6358.09/s  (0.193s, 5298.93/s)  LR: 9.705e-04  Data: 0.022 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 33 [ 700/1251 ( 56%)]  Loss: 4.516 (4.65)  Time: 0.313s, 3272.91/s  (0.193s, 5299.42/s)  LR: 9.705e-04  Data: 0.039 (0.033)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.871 (4.66)  Time: 0.182s, 5636.62/s  (0.193s, 5296.27/s)  LR: 9.705e-04  Data: 0.021 (0.033)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.729 (4.67)  Time: 0.181s, 5668.83/s  (0.193s, 5297.15/s)  LR: 9.705e-04  Data: 0.024 (0.033)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.377 (4.65)  Time: 0.174s, 5896.17/s  (0.193s, 5293.29/s)  LR: 9.705e-04  Data: 0.028 (0.032)
Train: 33 [ 900/1251 ( 72%)]  Loss: 4.633 (4.65)  Time: 0.175s, 5849.54/s  (0.193s, 5295.31/s)  LR: 9.705e-04  Data: 0.034 (0.032)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.398 (4.64)  Time: 0.173s, 5935.52/s  (0.193s, 5293.00/s)  LR: 9.705e-04  Data: 0.033 (0.032)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.914 (4.65)  Time: 0.183s, 5607.14/s  (0.193s, 5294.63/s)  LR: 9.705e-04  Data: 0.025 (0.032)
Train: 33 [1050/1251 ( 84%)]  Loss: 4.623 (4.65)  Time: 0.323s, 3171.16/s  (0.194s, 5289.52/s)  LR: 9.705e-04  Data: 0.036 (0.032)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.089 (4.62)  Time: 0.177s, 5793.64/s  (0.194s, 5284.19/s)  LR: 9.705e-04  Data: 0.030 (0.031)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.130 (4.60)  Time: 0.183s, 5590.45/s  (0.194s, 5278.55/s)  LR: 9.705e-04  Data: 0.025 (0.031)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.578 (4.60)  Time: 0.188s, 5461.26/s  (0.194s, 5268.32/s)  LR: 9.705e-04  Data: 0.030 (0.031)
Train: 33 [1250/1251 (100%)]  Loss: 4.391 (4.59)  Time: 0.114s, 9008.86/s  (0.194s, 5283.81/s)  LR: 9.705e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.721 (1.721)  Loss:  1.4203 (1.4203)  Acc@1: 74.5117 (74.5117)  Acc@5: 90.8203 (90.8203)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3158 (2.0935)  Acc@1: 74.6462 (57.0140)  Acc@5: 90.2123 (80.8220)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)

Train: 34 [   0/1251 (  0%)]  Loss: 4.730 (4.73)  Time: 1.702s,  601.69/s  (1.702s,  601.69/s)  LR: 9.687e-04  Data: 1.562 (1.562)
Train: 34 [  50/1251 (  4%)]  Loss: 4.540 (4.63)  Time: 0.161s, 6351.32/s  (0.226s, 4532.62/s)  LR: 9.687e-04  Data: 0.030 (0.064)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.207 (4.49)  Time: 0.164s, 6235.36/s  (0.208s, 4929.95/s)  LR: 9.687e-04  Data: 0.027 (0.046)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.758 (4.56)  Time: 0.197s, 5185.50/s  (0.202s, 5065.68/s)  LR: 9.687e-04  Data: 0.025 (0.040)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.685 (4.58)  Time: 0.178s, 5765.98/s  (0.198s, 5168.94/s)  LR: 9.687e-04  Data: 0.033 (0.037)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.617 (4.59)  Time: 0.151s, 6760.93/s  (0.197s, 5185.14/s)  LR: 9.687e-04  Data: 0.022 (0.037)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.701 (4.61)  Time: 0.177s, 5792.14/s  (0.196s, 5211.67/s)  LR: 9.687e-04  Data: 0.026 (0.036)
Train: 34 [ 350/1251 ( 28%)]  Loss: 4.672 (4.61)  Time: 0.163s, 6298.45/s  (0.196s, 5218.70/s)  LR: 9.687e-04  Data: 0.027 (0.034)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.633 (4.62)  Time: 0.174s, 5890.27/s  (0.195s, 5247.74/s)  LR: 9.687e-04  Data: 0.025 (0.034)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.683 (4.62)  Time: 0.166s, 6163.42/s  (0.195s, 5255.35/s)  LR: 9.687e-04  Data: 0.029 (0.033)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.652 (4.63)  Time: 0.214s, 4795.05/s  (0.194s, 5268.36/s)  LR: 9.687e-04  Data: 0.023 (0.032)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.761 (4.64)  Time: 0.188s, 5444.45/s  (0.194s, 5280.73/s)  LR: 9.687e-04  Data: 0.031 (0.032)
Train: 34 [ 600/1251 ( 48%)]  Loss: 4.954 (4.66)  Time: 0.166s, 6154.91/s  (0.194s, 5270.59/s)  LR: 9.687e-04  Data: 0.030 (0.032)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.467 (4.65)  Time: 0.173s, 5911.55/s  (0.195s, 5261.82/s)  LR: 9.687e-04  Data: 0.036 (0.032)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.530 (4.64)  Time: 0.178s, 5737.04/s  (0.194s, 5269.01/s)  LR: 9.687e-04  Data: 0.032 (0.031)
Train: 34 [ 750/1251 ( 60%)]  Loss: 4.783 (4.65)  Time: 0.175s, 5858.85/s  (0.194s, 5275.64/s)  LR: 9.687e-04  Data: 0.030 (0.031)
Train: 34 [ 800/1251 ( 64%)]  Loss: 4.811 (4.66)  Time: 0.189s, 5410.27/s  (0.195s, 5257.80/s)  LR: 9.687e-04  Data: 0.036 (0.031)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.780 (4.66)  Time: 0.172s, 5944.67/s  (0.195s, 5244.46/s)  LR: 9.687e-04  Data: 0.020 (0.031)
Train: 34 [ 900/1251 ( 72%)]  Loss: 4.596 (4.66)  Time: 0.156s, 6546.08/s  (0.195s, 5244.34/s)  LR: 9.687e-04  Data: 0.027 (0.031)
Train: 34 [ 950/1251 ( 76%)]  Loss: 4.760 (4.67)  Time: 0.182s, 5625.29/s  (0.195s, 5259.52/s)  LR: 9.687e-04  Data: 0.028 (0.031)
Train: 34 [1000/1251 ( 80%)]  Loss: 4.760 (4.67)  Time: 0.187s, 5469.15/s  (0.195s, 5249.53/s)  LR: 9.687e-04  Data: 0.022 (0.031)
Train: 34 [1050/1251 ( 84%)]  Loss: 4.706 (4.67)  Time: 0.224s, 4562.67/s  (0.195s, 5248.60/s)  LR: 9.687e-04  Data: 0.028 (0.030)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.442 (4.66)  Time: 0.175s, 5838.00/s  (0.195s, 5244.62/s)  LR: 9.687e-04  Data: 0.020 (0.030)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.536 (4.66)  Time: 0.159s, 6438.97/s  (0.195s, 5239.03/s)  LR: 9.687e-04  Data: 0.026 (0.030)
Train: 34 [1200/1251 ( 96%)]  Loss: 4.383 (4.65)  Time: 0.168s, 6096.00/s  (0.195s, 5241.40/s)  LR: 9.687e-04  Data: 0.023 (0.030)
Train: 34 [1250/1251 (100%)]  Loss: 4.505 (4.64)  Time: 0.113s, 9043.98/s  (0.195s, 5260.11/s)  LR: 9.687e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.821 (1.821)  Loss:  1.4551 (1.4551)  Acc@1: 76.2695 (76.2695)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.5025 (2.1310)  Acc@1: 75.7075 (58.2760)  Acc@5: 91.1557 (81.2820)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)

Train: 35 [   0/1251 (  0%)]  Loss: 4.752 (4.75)  Time: 1.853s,  552.47/s  (1.853s,  552.47/s)  LR: 9.668e-04  Data: 1.716 (1.716)
Train: 35 [  50/1251 (  4%)]  Loss: 4.604 (4.68)  Time: 0.162s, 6327.56/s  (0.219s, 4683.44/s)  LR: 9.668e-04  Data: 0.036 (0.070)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.724 (4.69)  Time: 0.175s, 5856.71/s  (0.205s, 4992.52/s)  LR: 9.668e-04  Data: 0.038 (0.049)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.726 (4.70)  Time: 0.172s, 5941.67/s  (0.203s, 5046.16/s)  LR: 9.668e-04  Data: 0.026 (0.042)
Train: 35 [ 200/1251 ( 16%)]  Loss: 4.816 (4.72)  Time: 0.176s, 5831.39/s  (0.200s, 5111.32/s)  LR: 9.668e-04  Data: 0.025 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.392 (4.67)  Time: 0.184s, 5575.24/s  (0.199s, 5150.25/s)  LR: 9.668e-04  Data: 0.033 (0.036)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.998 (4.72)  Time: 0.185s, 5520.50/s  (0.196s, 5217.06/s)  LR: 9.668e-04  Data: 0.027 (0.035)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.523 (4.69)  Time: 0.175s, 5857.85/s  (0.197s, 5197.36/s)  LR: 9.668e-04  Data: 0.028 (0.034)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.287 (4.65)  Time: 0.170s, 6023.13/s  (0.196s, 5224.18/s)  LR: 9.668e-04  Data: 0.027 (0.033)
Train: 35 [ 450/1251 ( 36%)]  Loss: 4.672 (4.65)  Time: 0.165s, 6219.56/s  (0.196s, 5233.29/s)  LR: 9.668e-04  Data: 0.030 (0.033)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.884 (4.67)  Time: 0.167s, 6138.08/s  (0.197s, 5203.43/s)  LR: 9.668e-04  Data: 0.030 (0.032)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.528 (4.66)  Time: 0.184s, 5578.09/s  (0.197s, 5196.29/s)  LR: 9.668e-04  Data: 0.021 (0.033)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.612 (4.66)  Time: 0.166s, 6169.38/s  (0.196s, 5224.47/s)  LR: 9.668e-04  Data: 0.020 (0.034)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.720 (4.66)  Time: 0.164s, 6251.47/s  (0.196s, 5231.30/s)  LR: 9.668e-04  Data: 0.029 (0.035)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.713 (4.66)  Time: 0.390s, 2627.02/s  (0.196s, 5220.66/s)  LR: 9.668e-04  Data: 0.270 (0.037)
Train: 35 [ 750/1251 ( 60%)]  Loss: 4.523 (4.65)  Time: 0.173s, 5910.63/s  (0.196s, 5230.73/s)  LR: 9.668e-04  Data: 0.028 (0.037)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.605 (4.65)  Time: 0.167s, 6117.52/s  (0.196s, 5232.59/s)  LR: 9.668e-04  Data: 0.030 (0.036)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.772 (4.66)  Time: 0.170s, 6017.66/s  (0.195s, 5238.67/s)  LR: 9.668e-04  Data: 0.031 (0.036)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.558 (4.65)  Time: 0.371s, 2760.88/s  (0.196s, 5230.08/s)  LR: 9.668e-04  Data: 0.248 (0.036)
Train: 35 [ 950/1251 ( 76%)]  Loss: 3.900 (4.62)  Time: 0.182s, 5629.44/s  (0.196s, 5228.32/s)  LR: 9.668e-04  Data: 0.021 (0.037)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.844 (4.63)  Time: 0.171s, 5996.10/s  (0.196s, 5226.72/s)  LR: 9.668e-04  Data: 0.022 (0.037)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.405 (4.62)  Time: 0.159s, 6454.36/s  (0.196s, 5228.42/s)  LR: 9.668e-04  Data: 0.030 (0.038)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.562 (4.61)  Time: 0.166s, 6179.20/s  (0.196s, 5232.56/s)  LR: 9.668e-04  Data: 0.031 (0.038)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.921 (4.63)  Time: 0.176s, 5810.98/s  (0.196s, 5217.27/s)  LR: 9.668e-04  Data: 0.024 (0.039)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.902 (4.64)  Time: 0.174s, 5899.92/s  (0.196s, 5212.43/s)  LR: 9.668e-04  Data: 0.021 (0.040)
Train: 35 [1250/1251 (100%)]  Loss: 4.509 (4.63)  Time: 0.114s, 9021.26/s  (0.196s, 5226.49/s)  LR: 9.668e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.848 (1.848)  Loss:  1.3998 (1.3998)  Acc@1: 75.3906 (75.3906)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3662 (2.0761)  Acc@1: 75.0000 (57.3940)  Acc@5: 88.6793 (81.0740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)

Train: 36 [   0/1251 (  0%)]  Loss: 4.819 (4.82)  Time: 1.695s,  603.96/s  (1.695s,  603.96/s)  LR: 9.649e-04  Data: 1.523 (1.523)
Train: 36 [  50/1251 (  4%)]  Loss: 4.589 (4.70)  Time: 0.169s, 6063.84/s  (0.224s, 4576.81/s)  LR: 9.649e-04  Data: 0.024 (0.061)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.438 (4.62)  Time: 0.156s, 6550.82/s  (0.211s, 4859.55/s)  LR: 9.649e-04  Data: 0.024 (0.044)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.613 (4.61)  Time: 0.171s, 5976.18/s  (0.205s, 4993.83/s)  LR: 9.649e-04  Data: 0.034 (0.039)
Train: 36 [ 200/1251 ( 16%)]  Loss: 4.684 (4.63)  Time: 0.155s, 6589.86/s  (0.200s, 5131.77/s)  LR: 9.649e-04  Data: 0.028 (0.036)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.867 (4.67)  Time: 0.182s, 5637.77/s  (0.197s, 5190.24/s)  LR: 9.649e-04  Data: 0.023 (0.035)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.626 (4.66)  Time: 0.194s, 5274.02/s  (0.196s, 5229.22/s)  LR: 9.649e-04  Data: 0.021 (0.034)
Train: 36 [ 350/1251 ( 28%)]  Loss: 4.604 (4.65)  Time: 0.178s, 5767.74/s  (0.195s, 5240.53/s)  LR: 9.649e-04  Data: 0.026 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 36 [ 400/1251 ( 32%)]  Loss: 4.833 (4.67)  Time: 0.364s, 2811.33/s  (0.195s, 5255.70/s)  LR: 9.649e-04  Data: 0.025 (0.032)
Train: 36 [ 450/1251 ( 36%)]  Loss: 4.877 (4.69)  Time: 0.165s, 6209.83/s  (0.194s, 5269.82/s)  LR: 9.649e-04  Data: 0.028 (0.033)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.507 (4.68)  Time: 0.180s, 5674.19/s  (0.194s, 5274.45/s)  LR: 9.649e-04  Data: 0.024 (0.034)
Train: 36 [ 550/1251 ( 44%)]  Loss: 4.600 (4.67)  Time: 0.168s, 6095.52/s  (0.194s, 5287.05/s)  LR: 9.649e-04  Data: 0.019 (0.035)
Train: 36 [ 600/1251 ( 48%)]  Loss: 4.715 (4.67)  Time: 0.180s, 5692.93/s  (0.195s, 5251.07/s)  LR: 9.649e-04  Data: 0.050 (0.034)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.671 (4.67)  Time: 0.172s, 5951.01/s  (0.194s, 5267.65/s)  LR: 9.649e-04  Data: 0.035 (0.034)
Train: 36 [ 700/1251 ( 56%)]  Loss: 4.564 (4.67)  Time: 0.194s, 5289.17/s  (0.195s, 5251.57/s)  LR: 9.649e-04  Data: 0.024 (0.033)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.788 (4.67)  Time: 0.174s, 5872.95/s  (0.195s, 5262.93/s)  LR: 9.649e-04  Data: 0.028 (0.033)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.540 (4.67)  Time: 0.466s, 2196.43/s  (0.195s, 5255.96/s)  LR: 9.649e-04  Data: 0.024 (0.033)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.757 (4.67)  Time: 0.185s, 5537.54/s  (0.194s, 5266.72/s)  LR: 9.649e-04  Data: 0.027 (0.032)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.389 (4.66)  Time: 0.161s, 6370.66/s  (0.194s, 5269.44/s)  LR: 9.649e-04  Data: 0.022 (0.032)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.806 (4.66)  Time: 0.172s, 5967.55/s  (0.194s, 5265.71/s)  LR: 9.649e-04  Data: 0.020 (0.032)
Train: 36 [1000/1251 ( 80%)]  Loss: 4.475 (4.66)  Time: 0.293s, 3490.82/s  (0.195s, 5260.09/s)  LR: 9.649e-04  Data: 0.026 (0.032)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.507 (4.65)  Time: 0.409s, 2501.65/s  (0.195s, 5248.88/s)  LR: 9.649e-04  Data: 0.027 (0.031)
Train: 36 [1100/1251 ( 88%)]  Loss: 4.512 (4.64)  Time: 0.165s, 6208.24/s  (0.195s, 5253.03/s)  LR: 9.649e-04  Data: 0.038 (0.031)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.507 (4.64)  Time: 0.181s, 5645.23/s  (0.195s, 5241.93/s)  LR: 9.649e-04  Data: 0.021 (0.032)
Train: 36 [1200/1251 ( 96%)]  Loss: 4.563 (4.63)  Time: 0.164s, 6252.42/s  (0.197s, 5188.53/s)  LR: 9.649e-04  Data: 0.025 (0.032)
Train: 36 [1250/1251 (100%)]  Loss: 4.526 (4.63)  Time: 0.114s, 8989.48/s  (0.197s, 5202.48/s)  LR: 9.649e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.817 (1.817)  Loss:  1.4197 (1.4197)  Acc@1: 75.5859 (75.5859)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.227)  Loss:  1.3823 (2.0555)  Acc@1: 76.1792 (58.9820)  Acc@5: 90.4481 (81.9440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)

Train: 37 [   0/1251 (  0%)]  Loss: 4.416 (4.42)  Time: 1.737s,  589.41/s  (1.737s,  589.41/s)  LR: 9.630e-04  Data: 1.607 (1.607)
Train: 37 [  50/1251 (  4%)]  Loss: 4.662 (4.54)  Time: 0.179s, 5731.38/s  (0.230s, 4451.28/s)  LR: 9.630e-04  Data: 0.019 (0.066)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.892 (4.66)  Time: 0.156s, 6578.43/s  (0.210s, 4884.15/s)  LR: 9.630e-04  Data: 0.025 (0.046)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.430 (4.60)  Time: 0.159s, 6451.91/s  (0.204s, 5030.65/s)  LR: 9.630e-04  Data: 0.032 (0.040)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.507 (4.58)  Time: 0.225s, 4550.55/s  (0.200s, 5116.16/s)  LR: 9.630e-04  Data: 0.031 (0.037)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.319 (4.54)  Time: 0.207s, 4949.27/s  (0.199s, 5149.11/s)  LR: 9.630e-04  Data: 0.025 (0.035)
Train: 37 [ 300/1251 ( 24%)]  Loss: 4.577 (4.54)  Time: 0.171s, 5978.79/s  (0.196s, 5218.62/s)  LR: 9.630e-04  Data: 0.025 (0.034)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.929 (4.59)  Time: 0.186s, 5497.11/s  (0.197s, 5209.78/s)  LR: 9.630e-04  Data: 0.030 (0.033)
Train: 37 [ 400/1251 ( 32%)]  Loss: 4.775 (4.61)  Time: 0.240s, 4274.16/s  (0.195s, 5250.51/s)  LR: 9.630e-04  Data: 0.031 (0.032)
Train: 37 [ 450/1251 ( 36%)]  Loss: 4.911 (4.64)  Time: 0.162s, 6337.32/s  (0.196s, 5234.71/s)  LR: 9.630e-04  Data: 0.029 (0.032)
Train: 37 [ 500/1251 ( 40%)]  Loss: 4.390 (4.62)  Time: 0.157s, 6530.69/s  (0.195s, 5252.21/s)  LR: 9.630e-04  Data: 0.028 (0.032)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.501 (4.61)  Time: 0.164s, 6245.07/s  (0.194s, 5275.30/s)  LR: 9.630e-04  Data: 0.033 (0.031)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.117 (4.57)  Time: 0.185s, 5528.81/s  (0.194s, 5272.63/s)  LR: 9.630e-04  Data: 0.037 (0.031)
Train: 37 [ 650/1251 ( 52%)]  Loss: 4.431 (4.56)  Time: 0.349s, 2931.22/s  (0.194s, 5272.58/s)  LR: 9.630e-04  Data: 0.030 (0.031)
Train: 37 [ 700/1251 ( 56%)]  Loss: 4.642 (4.57)  Time: 0.168s, 6082.08/s  (0.194s, 5272.96/s)  LR: 9.630e-04  Data: 0.027 (0.030)
Train: 37 [ 750/1251 ( 60%)]  Loss: 4.638 (4.57)  Time: 0.182s, 5627.29/s  (0.198s, 5168.46/s)  LR: 9.630e-04  Data: 0.027 (0.035)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.556 (4.57)  Time: 0.175s, 5835.12/s  (0.197s, 5186.02/s)  LR: 9.630e-04  Data: 0.027 (0.034)
Train: 37 [ 850/1251 ( 68%)]  Loss: 3.981 (4.54)  Time: 0.160s, 6391.32/s  (0.197s, 5186.02/s)  LR: 9.630e-04  Data: 0.019 (0.034)
Train: 37 [ 900/1251 ( 72%)]  Loss: 4.501 (4.54)  Time: 0.161s, 6358.10/s  (0.198s, 5182.64/s)  LR: 9.630e-04  Data: 0.021 (0.034)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.653 (4.54)  Time: 0.346s, 2956.56/s  (0.198s, 5179.92/s)  LR: 9.630e-04  Data: 0.023 (0.033)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.587 (4.54)  Time: 0.170s, 6012.78/s  (0.198s, 5183.44/s)  LR: 9.630e-04  Data: 0.028 (0.033)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.552 (4.54)  Time: 0.156s, 6567.20/s  (0.197s, 5186.62/s)  LR: 9.630e-04  Data: 0.023 (0.033)
Train: 37 [1100/1251 ( 88%)]  Loss: 4.335 (4.53)  Time: 0.171s, 5978.77/s  (0.197s, 5185.33/s)  LR: 9.630e-04  Data: 0.031 (0.032)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.488 (4.53)  Time: 0.171s, 5996.06/s  (0.197s, 5191.87/s)  LR: 9.630e-04  Data: 0.032 (0.032)
Train: 37 [1200/1251 ( 96%)]  Loss: 4.287 (4.52)  Time: 0.284s, 3607.56/s  (0.197s, 5190.86/s)  LR: 9.630e-04  Data: 0.024 (0.032)
Train: 37 [1250/1251 (100%)]  Loss: 4.409 (4.52)  Time: 0.114s, 9019.91/s  (0.197s, 5204.59/s)  LR: 9.630e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.892 (1.892)  Loss:  1.4256 (1.4256)  Acc@1: 72.3633 (72.3633)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.5069 (2.2327)  Acc@1: 75.0000 (55.1820)  Acc@5: 89.2689 (79.4920)
Train: 38 [   0/1251 (  0%)]  Loss: 4.744 (4.74)  Time: 2.225s,  460.20/s  (2.225s,  460.20/s)  LR: 9.610e-04  Data: 2.095 (2.095)
Train: 38 [  50/1251 (  4%)]  Loss: 4.782 (4.76)  Time: 0.164s, 6262.97/s  (0.227s, 4514.09/s)  LR: 9.610e-04  Data: 0.026 (0.083)
Train: 38 [ 100/1251 (  8%)]  Loss: 4.441 (4.66)  Time: 0.161s, 6370.45/s  (0.207s, 4945.71/s)  LR: 9.610e-04  Data: 0.033 (0.056)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.677 (4.66)  Time: 0.174s, 5889.04/s  (0.203s, 5047.42/s)  LR: 9.610e-04  Data: 0.032 (0.047)
Train: 38 [ 200/1251 ( 16%)]  Loss: 4.444 (4.62)  Time: 0.317s, 3227.72/s  (0.201s, 5085.38/s)  LR: 9.610e-04  Data: 0.028 (0.042)
Train: 38 [ 250/1251 ( 20%)]  Loss: 4.780 (4.64)  Time: 0.169s, 6060.59/s  (0.197s, 5193.70/s)  LR: 9.610e-04  Data: 0.027 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 38 [ 300/1251 ( 24%)]  Loss: 4.567 (4.63)  Time: 0.174s, 5884.22/s  (0.196s, 5236.63/s)  LR: 9.610e-04  Data: 0.044 (0.039)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.411 (4.61)  Time: 0.170s, 6025.31/s  (0.196s, 5237.29/s)  LR: 9.610e-04  Data: 0.023 (0.041)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.734 (4.62)  Time: 0.176s, 5804.56/s  (0.196s, 5224.46/s)  LR: 9.610e-04  Data: 0.021 (0.042)
Train: 38 [ 450/1251 ( 36%)]  Loss: 4.167 (4.57)  Time: 0.166s, 6156.04/s  (0.194s, 5265.78/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.426 (4.56)  Time: 0.257s, 3982.75/s  (0.194s, 5275.05/s)  LR: 9.610e-04  Data: 0.118 (0.042)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.694 (4.57)  Time: 0.171s, 5973.44/s  (0.194s, 5291.20/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.511 (4.57)  Time: 0.163s, 6286.09/s  (0.193s, 5294.29/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 650/1251 ( 52%)]  Loss: 4.072 (4.53)  Time: 0.181s, 5652.25/s  (0.194s, 5277.26/s)  LR: 9.610e-04  Data: 0.024 (0.042)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.740 (4.55)  Time: 0.287s, 3569.05/s  (0.194s, 5286.65/s)  LR: 9.610e-04  Data: 0.052 (0.042)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.572 (4.55)  Time: 0.310s, 3302.26/s  (0.194s, 5275.70/s)  LR: 9.610e-04  Data: 0.023 (0.041)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.268 (4.53)  Time: 0.175s, 5848.66/s  (0.194s, 5288.13/s)  LR: 9.610e-04  Data: 0.024 (0.040)
Train: 38 [ 850/1251 ( 68%)]  Loss: 4.779 (4.54)  Time: 0.182s, 5611.10/s  (0.194s, 5280.61/s)  LR: 9.610e-04  Data: 0.028 (0.040)
Train: 38 [ 900/1251 ( 72%)]  Loss: 4.187 (4.53)  Time: 0.221s, 4642.56/s  (0.194s, 5282.19/s)  LR: 9.610e-04  Data: 0.099 (0.040)
Train: 38 [ 950/1251 ( 76%)]  Loss: 4.381 (4.52)  Time: 0.166s, 6180.65/s  (0.194s, 5283.07/s)  LR: 9.610e-04  Data: 0.025 (0.041)
Train: 38 [1000/1251 ( 80%)]  Loss: 4.455 (4.52)  Time: 0.199s, 5157.69/s  (0.194s, 5284.56/s)  LR: 9.610e-04  Data: 0.026 (0.041)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.366 (4.51)  Time: 0.178s, 5740.79/s  (0.194s, 5270.99/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [1100/1251 ( 88%)]  Loss: 4.080 (4.49)  Time: 0.203s, 5044.74/s  (0.194s, 5270.71/s)  LR: 9.610e-04  Data: 0.043 (0.042)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.450 (4.49)  Time: 0.192s, 5337.76/s  (0.194s, 5276.70/s)  LR: 9.610e-04  Data: 0.031 (0.042)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.740 (4.50)  Time: 0.172s, 5937.61/s  (0.194s, 5274.01/s)  LR: 9.610e-04  Data: 0.026 (0.042)
Train: 38 [1250/1251 (100%)]  Loss: 4.604 (4.50)  Time: 0.125s, 8189.25/s  (0.194s, 5275.93/s)  LR: 9.610e-04  Data: 0.000 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.740 (1.740)  Loss:  1.3190 (1.3190)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3890 (2.0356)  Acc@1: 75.4717 (58.6360)  Acc@5: 89.8585 (81.8840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)

Train: 39 [   0/1251 (  0%)]  Loss: 4.663 (4.66)  Time: 1.713s,  597.79/s  (1.713s,  597.79/s)  LR: 9.589e-04  Data: 1.576 (1.576)
Train: 39 [  50/1251 (  4%)]  Loss: 4.240 (4.45)  Time: 0.191s, 5364.36/s  (0.223s, 4592.64/s)  LR: 9.589e-04  Data: 0.032 (0.074)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.381 (4.43)  Time: 0.164s, 6232.39/s  (0.209s, 4904.47/s)  LR: 9.589e-04  Data: 0.030 (0.062)
Train: 39 [ 150/1251 ( 12%)]  Loss: 4.698 (4.50)  Time: 0.179s, 5727.65/s  (0.200s, 5109.75/s)  LR: 9.589e-04  Data: 0.027 (0.053)
Train: 39 [ 200/1251 ( 16%)]  Loss: 4.375 (4.47)  Time: 0.164s, 6238.66/s  (0.198s, 5181.66/s)  LR: 9.589e-04  Data: 0.031 (0.051)
Train: 39 [ 250/1251 ( 20%)]  Loss: 4.391 (4.46)  Time: 0.174s, 5871.48/s  (0.196s, 5213.33/s)  LR: 9.589e-04  Data: 0.025 (0.048)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.293 (4.43)  Time: 0.248s, 4122.76/s  (0.195s, 5239.42/s)  LR: 9.589e-04  Data: 0.026 (0.045)
Train: 39 [ 350/1251 ( 28%)]  Loss: 4.419 (4.43)  Time: 0.172s, 5957.87/s  (0.195s, 5262.26/s)  LR: 9.589e-04  Data: 0.030 (0.043)
Train: 39 [ 400/1251 ( 32%)]  Loss: 4.435 (4.43)  Time: 0.182s, 5614.50/s  (0.193s, 5300.47/s)  LR: 9.589e-04  Data: 0.034 (0.041)
Train: 39 [ 450/1251 ( 36%)]  Loss: 4.308 (4.42)  Time: 0.217s, 4718.49/s  (0.193s, 5310.32/s)  LR: 9.589e-04  Data: 0.024 (0.039)
Train: 39 [ 500/1251 ( 40%)]  Loss: 4.562 (4.43)  Time: 0.179s, 5719.84/s  (0.193s, 5309.10/s)  LR: 9.589e-04  Data: 0.026 (0.038)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.746 (4.46)  Time: 0.159s, 6458.12/s  (0.193s, 5308.50/s)  LR: 9.589e-04  Data: 0.024 (0.037)
Train: 39 [ 600/1251 ( 48%)]  Loss: 4.536 (4.47)  Time: 0.192s, 5345.92/s  (0.193s, 5303.58/s)  LR: 9.589e-04  Data: 0.023 (0.038)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.486 (4.47)  Time: 0.157s, 6505.62/s  (0.193s, 5297.87/s)  LR: 9.589e-04  Data: 0.022 (0.039)
Train: 39 [ 700/1251 ( 56%)]  Loss: 4.194 (4.45)  Time: 0.186s, 5496.92/s  (0.193s, 5297.71/s)  LR: 9.589e-04  Data: 0.024 (0.040)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.957 (4.48)  Time: 0.172s, 5946.09/s  (0.195s, 5259.61/s)  LR: 9.589e-04  Data: 0.028 (0.042)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.358 (4.47)  Time: 0.191s, 5362.00/s  (0.195s, 5256.03/s)  LR: 9.589e-04  Data: 0.024 (0.042)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.659 (4.48)  Time: 0.180s, 5693.12/s  (0.194s, 5270.72/s)  LR: 9.589e-04  Data: 0.024 (0.042)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.480 (4.48)  Time: 0.172s, 5965.06/s  (0.194s, 5269.58/s)  LR: 9.589e-04  Data: 0.027 (0.042)
Train: 39 [ 950/1251 ( 76%)]  Loss: 4.403 (4.48)  Time: 0.186s, 5492.52/s  (0.195s, 5263.33/s)  LR: 9.589e-04  Data: 0.027 (0.042)
Train: 39 [1000/1251 ( 80%)]  Loss: 4.365 (4.47)  Time: 0.177s, 5782.80/s  (0.195s, 5264.22/s)  LR: 9.589e-04  Data: 0.028 (0.041)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.304 (4.47)  Time: 0.170s, 6019.54/s  (0.194s, 5265.68/s)  LR: 9.589e-04  Data: 0.024 (0.041)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.671 (4.47)  Time: 0.183s, 5585.50/s  (0.195s, 5260.86/s)  LR: 9.589e-04  Data: 0.020 (0.040)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.678 (4.48)  Time: 0.305s, 3354.42/s  (0.195s, 5260.40/s)  LR: 9.589e-04  Data: 0.020 (0.040)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.750 (4.49)  Time: 0.172s, 5962.56/s  (0.195s, 5256.12/s)  LR: 9.589e-04  Data: 0.022 (0.039)
Train: 39 [1250/1251 (100%)]  Loss: 4.682 (4.50)  Time: 0.120s, 8521.56/s  (0.194s, 5270.20/s)  LR: 9.589e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.770 (1.770)  Loss:  1.2820 (1.2820)  Acc@1: 74.9023 (74.9023)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3291 (2.0133)  Acc@1: 76.2972 (58.5220)  Acc@5: 90.8019 (81.7600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)

Train: 40 [   0/1251 (  0%)]  Loss: 4.916 (4.92)  Time: 1.887s,  542.69/s  (1.887s,  542.69/s)  LR: 9.568e-04  Data: 1.750 (1.750)
Train: 40 [  50/1251 (  4%)]  Loss: 4.299 (4.61)  Time: 0.164s, 6251.59/s  (0.227s, 4512.16/s)  LR: 9.568e-04  Data: 0.033 (0.079)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.735 (4.65)  Time: 0.205s, 4987.40/s  (0.211s, 4860.18/s)  LR: 9.568e-04  Data: 0.027 (0.054)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.771 (4.68)  Time: 0.175s, 5848.38/s  (0.203s, 5054.93/s)  LR: 9.568e-04  Data: 0.026 (0.049)
Train: 40 [ 200/1251 ( 16%)]  Loss: 4.392 (4.62)  Time: 0.164s, 6224.92/s  (0.198s, 5167.03/s)  LR: 9.568e-04  Data: 0.024 (0.047)
Train: 40 [ 250/1251 ( 20%)]  Loss: 4.355 (4.58)  Time: 0.151s, 6777.28/s  (0.196s, 5230.16/s)  LR: 9.568e-04  Data: 0.023 (0.045)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.732 (4.60)  Time: 0.158s, 6480.95/s  (0.195s, 5251.73/s)  LR: 9.568e-04  Data: 0.025 (0.045)
Train: 40 [ 350/1251 ( 28%)]  Loss: 4.715 (4.61)  Time: 0.184s, 5564.63/s  (0.195s, 5262.24/s)  LR: 9.568e-04  Data: 0.023 (0.045)
Train: 40 [ 400/1251 ( 32%)]  Loss: 4.539 (4.61)  Time: 0.315s, 3250.54/s  (0.196s, 5237.81/s)  LR: 9.568e-04  Data: 0.187 (0.047)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.448 (4.59)  Time: 0.179s, 5722.74/s  (0.194s, 5273.33/s)  LR: 9.568e-04  Data: 0.026 (0.046)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.838 (4.61)  Time: 0.155s, 6600.39/s  (0.196s, 5219.88/s)  LR: 9.568e-04  Data: 0.028 (0.047)
Train: 40 [ 550/1251 ( 44%)]  Loss: 4.336 (4.59)  Time: 0.165s, 6218.70/s  (0.196s, 5236.18/s)  LR: 9.568e-04  Data: 0.027 (0.046)
Train: 40 [ 600/1251 ( 48%)]  Loss: 4.460 (4.58)  Time: 0.160s, 6406.88/s  (0.196s, 5221.39/s)  LR: 9.568e-04  Data: 0.036 (0.047)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.436 (4.57)  Time: 0.158s, 6497.48/s  (0.196s, 5237.56/s)  LR: 9.568e-04  Data: 0.028 (0.047)
Train: 40 [ 700/1251 ( 56%)]  Loss: 4.516 (4.57)  Time: 0.167s, 6116.24/s  (0.196s, 5237.84/s)  LR: 9.568e-04  Data: 0.023 (0.047)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.261 (4.55)  Time: 0.169s, 6043.52/s  (0.195s, 5238.28/s)  LR: 9.568e-04  Data: 0.026 (0.047)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.098 (4.52)  Time: 0.257s, 3981.79/s  (0.195s, 5238.89/s)  LR: 9.568e-04  Data: 0.132 (0.048)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.327 (4.51)  Time: 0.184s, 5576.54/s  (0.195s, 5241.71/s)  LR: 9.568e-04  Data: 0.026 (0.047)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 40 [ 900/1251 ( 72%)]  Loss: 4.467 (4.51)  Time: 0.172s, 5940.73/s  (0.195s, 5243.78/s)  LR: 9.568e-04  Data: 0.025 (0.046)
Train: 40 [ 950/1251 ( 76%)]  Loss: 4.208 (4.49)  Time: 0.168s, 6104.33/s  (0.195s, 5249.91/s)  LR: 9.568e-04  Data: 0.030 (0.045)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.844 (4.51)  Time: 0.158s, 6494.99/s  (0.196s, 5234.64/s)  LR: 9.568e-04  Data: 0.032 (0.044)
Train: 40 [1050/1251 ( 84%)]  Loss: 4.445 (4.51)  Time: 0.160s, 6411.23/s  (0.196s, 5236.56/s)  LR: 9.568e-04  Data: 0.024 (0.043)
Train: 40 [1100/1251 ( 88%)]  Loss: 4.599 (4.51)  Time: 0.171s, 5981.44/s  (0.195s, 5240.31/s)  LR: 9.568e-04  Data: 0.019 (0.043)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.537 (4.51)  Time: 0.331s, 3092.78/s  (0.195s, 5240.59/s)  LR: 9.568e-04  Data: 0.191 (0.043)
Train: 40 [1200/1251 ( 96%)]  Loss: 4.744 (4.52)  Time: 0.182s, 5624.21/s  (0.196s, 5217.57/s)  LR: 9.568e-04  Data: 0.035 (0.044)
Train: 40 [1250/1251 (100%)]  Loss: 4.440 (4.52)  Time: 0.114s, 8988.69/s  (0.195s, 5240.75/s)  LR: 9.568e-04  Data: 0.000 (0.043)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.924 (1.924)  Loss:  1.2202 (1.2202)  Acc@1: 77.8320 (77.8320)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.3308 (1.9965)  Acc@1: 76.2972 (58.9900)  Acc@5: 89.9764 (82.0460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)

Train: 41 [   0/1251 (  0%)]  Loss: 4.470 (4.47)  Time: 1.774s,  577.33/s  (1.774s,  577.33/s)  LR: 9.547e-04  Data: 1.658 (1.658)
Train: 41 [  50/1251 (  4%)]  Loss: 4.347 (4.41)  Time: 0.175s, 5836.22/s  (0.225s, 4557.34/s)  LR: 9.547e-04  Data: 0.033 (0.081)
Train: 41 [ 100/1251 (  8%)]  Loss: 4.761 (4.53)  Time: 0.164s, 6228.43/s  (0.211s, 4845.02/s)  LR: 9.547e-04  Data: 0.026 (0.060)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.177 (4.44)  Time: 0.182s, 5618.12/s  (0.206s, 4969.05/s)  LR: 9.547e-04  Data: 0.059 (0.050)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.460 (4.44)  Time: 0.177s, 5770.81/s  (0.202s, 5060.86/s)  LR: 9.547e-04  Data: 0.025 (0.045)
Train: 41 [ 250/1251 ( 20%)]  Loss: 4.770 (4.50)  Time: 0.181s, 5656.37/s  (0.203s, 5050.05/s)  LR: 9.547e-04  Data: 0.027 (0.041)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.386 (4.48)  Time: 0.180s, 5677.58/s  (0.200s, 5115.82/s)  LR: 9.547e-04  Data: 0.029 (0.039)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.428 (4.48)  Time: 0.169s, 6042.45/s  (0.199s, 5157.78/s)  LR: 9.547e-04  Data: 0.025 (0.037)
Train: 41 [ 400/1251 ( 32%)]  Loss: 4.110 (4.43)  Time: 0.156s, 6583.91/s  (0.197s, 5184.96/s)  LR: 9.547e-04  Data: 0.022 (0.036)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.284 (4.42)  Time: 0.197s, 5208.08/s  (0.197s, 5198.40/s)  LR: 9.547e-04  Data: 0.026 (0.035)
Train: 41 [ 500/1251 ( 40%)]  Loss: 4.636 (4.44)  Time: 0.191s, 5374.62/s  (0.196s, 5222.33/s)  LR: 9.547e-04  Data: 0.043 (0.035)
Train: 41 [ 550/1251 ( 44%)]  Loss: 4.687 (4.46)  Time: 0.527s, 1943.99/s  (0.198s, 5182.82/s)  LR: 9.547e-04  Data: 0.028 (0.034)
Train: 41 [ 600/1251 ( 48%)]  Loss: 4.419 (4.46)  Time: 0.169s, 6062.08/s  (0.197s, 5196.35/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 650/1251 ( 52%)]  Loss: 4.827 (4.48)  Time: 0.173s, 5925.71/s  (0.197s, 5200.57/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.020 (4.45)  Time: 0.171s, 5994.67/s  (0.197s, 5211.11/s)  LR: 9.547e-04  Data: 0.033 (0.033)
Train: 41 [ 750/1251 ( 60%)]  Loss: 4.393 (4.45)  Time: 0.346s, 2961.55/s  (0.196s, 5217.91/s)  LR: 9.547e-04  Data: 0.033 (0.032)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.288 (4.44)  Time: 0.185s, 5540.45/s  (0.196s, 5222.14/s)  LR: 9.547e-04  Data: 0.033 (0.032)
Train: 41 [ 850/1251 ( 68%)]  Loss: 4.585 (4.45)  Time: 0.187s, 5490.12/s  (0.196s, 5227.79/s)  LR: 9.547e-04  Data: 0.020 (0.032)
Train: 41 [ 900/1251 ( 72%)]  Loss: 4.801 (4.47)  Time: 0.167s, 6146.97/s  (0.196s, 5231.20/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 950/1251 ( 76%)]  Loss: 4.770 (4.48)  Time: 0.188s, 5448.16/s  (0.197s, 5188.59/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1000/1251 ( 80%)]  Loss: 4.665 (4.49)  Time: 0.191s, 5360.67/s  (0.198s, 5184.76/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.877 (4.51)  Time: 0.156s, 6546.73/s  (0.197s, 5191.77/s)  LR: 9.547e-04  Data: 0.024 (0.032)
Train: 41 [1100/1251 ( 88%)]  Loss: 4.437 (4.50)  Time: 0.189s, 5423.99/s  (0.197s, 5185.15/s)  LR: 9.547e-04  Data: 0.025 (0.032)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.349 (4.50)  Time: 0.194s, 5267.79/s  (0.197s, 5189.00/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.548 (4.50)  Time: 0.156s, 6561.89/s  (0.197s, 5185.15/s)  LR: 9.547e-04  Data: 0.028 (0.031)
Train: 41 [1250/1251 (100%)]  Loss: 4.827 (4.51)  Time: 0.113s, 9033.99/s  (0.197s, 5200.97/s)  LR: 9.547e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.797 (1.797)  Loss:  1.3068 (1.3068)  Acc@1: 75.0977 (75.0977)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2053 (2.0709)  Acc@1: 79.2453 (58.8100)  Acc@5: 92.6887 (82.0060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)

Train: 42 [   0/1251 (  0%)]  Loss: 4.621 (4.62)  Time: 1.980s,  517.07/s  (1.980s,  517.07/s)  LR: 9.525e-04  Data: 1.836 (1.836)
Train: 42 [  50/1251 (  4%)]  Loss: 4.281 (4.45)  Time: 0.151s, 6799.74/s  (0.228s, 4489.39/s)  LR: 9.525e-04  Data: 0.023 (0.081)
Train: 42 [ 100/1251 (  8%)]  Loss: 4.729 (4.54)  Time: 0.180s, 5687.61/s  (0.207s, 4954.55/s)  LR: 9.525e-04  Data: 0.031 (0.059)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.306 (4.48)  Time: 0.168s, 6096.94/s  (0.204s, 5014.01/s)  LR: 9.525e-04  Data: 0.027 (0.057)
Train: 42 [ 200/1251 ( 16%)]  Loss: 4.377 (4.46)  Time: 0.160s, 6412.55/s  (0.201s, 5098.30/s)  LR: 9.525e-04  Data: 0.023 (0.054)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.721 (4.51)  Time: 0.218s, 4688.67/s  (0.198s, 5160.23/s)  LR: 9.525e-04  Data: 0.020 (0.051)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.816 (4.55)  Time: 0.175s, 5865.21/s  (0.197s, 5191.50/s)  LR: 9.525e-04  Data: 0.024 (0.050)
Train: 42 [ 350/1251 ( 28%)]  Loss: 4.213 (4.51)  Time: 0.179s, 5730.34/s  (0.197s, 5203.19/s)  LR: 9.525e-04  Data: 0.023 (0.049)
Train: 42 [ 400/1251 ( 32%)]  Loss: 4.675 (4.53)  Time: 0.174s, 5892.53/s  (0.196s, 5232.18/s)  LR: 9.525e-04  Data: 0.020 (0.046)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.580 (4.53)  Time: 1.238s,  827.45/s  (0.198s, 5181.86/s)  LR: 9.525e-04  Data: 0.019 (0.044)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.871 (4.56)  Time: 0.162s, 6331.24/s  (0.197s, 5198.98/s)  LR: 9.525e-04  Data: 0.033 (0.042)
Train: 42 [ 550/1251 ( 44%)]  Loss: 4.637 (4.57)  Time: 0.167s, 6118.40/s  (0.196s, 5217.76/s)  LR: 9.525e-04  Data: 0.026 (0.041)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.880 (4.59)  Time: 0.156s, 6549.71/s  (0.197s, 5208.09/s)  LR: 9.525e-04  Data: 0.025 (0.043)
Train: 42 [ 650/1251 ( 52%)]  Loss: 4.369 (4.58)  Time: 0.174s, 5890.48/s  (0.196s, 5230.36/s)  LR: 9.525e-04  Data: 0.029 (0.043)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.428 (4.57)  Time: 0.179s, 5736.32/s  (0.196s, 5225.34/s)  LR: 9.525e-04  Data: 0.028 (0.042)
Train: 42 [ 750/1251 ( 60%)]  Loss: 4.800 (4.58)  Time: 0.174s, 5888.44/s  (0.196s, 5223.20/s)  LR: 9.525e-04  Data: 0.024 (0.041)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.716 (4.59)  Time: 0.176s, 5820.84/s  (0.196s, 5219.92/s)  LR: 9.525e-04  Data: 0.031 (0.040)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.994 (4.61)  Time: 0.190s, 5396.72/s  (0.196s, 5221.91/s)  LR: 9.525e-04  Data: 0.059 (0.040)
Train: 42 [ 900/1251 ( 72%)]  Loss: 4.676 (4.62)  Time: 0.168s, 6087.12/s  (0.196s, 5214.77/s)  LR: 9.525e-04  Data: 0.028 (0.039)
Train: 42 [ 950/1251 ( 76%)]  Loss: 4.347 (4.60)  Time: 0.178s, 5754.20/s  (0.196s, 5219.44/s)  LR: 9.525e-04  Data: 0.029 (0.038)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.346 (4.59)  Time: 0.156s, 6561.62/s  (0.196s, 5229.99/s)  LR: 9.525e-04  Data: 0.027 (0.038)
Train: 42 [1050/1251 ( 84%)]  Loss: 4.254 (4.57)  Time: 0.182s, 5615.03/s  (0.195s, 5241.82/s)  LR: 9.525e-04  Data: 0.040 (0.038)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.442 (4.57)  Time: 0.163s, 6290.74/s  (0.196s, 5230.20/s)  LR: 9.525e-04  Data: 0.019 (0.037)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.326 (4.56)  Time: 0.155s, 6591.96/s  (0.196s, 5234.41/s)  LR: 9.525e-04  Data: 0.030 (0.037)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.285 (4.55)  Time: 0.225s, 4557.84/s  (0.196s, 5227.40/s)  LR: 9.525e-04  Data: 0.036 (0.036)
Train: 42 [1250/1251 (100%)]  Loss: 4.615 (4.55)  Time: 0.113s, 9041.62/s  (0.195s, 5242.08/s)  LR: 9.525e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.879 (1.879)  Loss:  1.4100 (1.4100)  Acc@1: 73.7305 (73.7305)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2896 (2.0460)  Acc@1: 76.8868 (58.5360)  Acc@5: 90.6840 (81.7980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)

Train: 43 [   0/1251 (  0%)]  Loss: 4.545 (4.55)  Time: 1.822s,  561.92/s  (1.822s,  561.92/s)  LR: 9.502e-04  Data: 1.616 (1.616)
Train: 43 [  50/1251 (  4%)]  Loss: 4.831 (4.69)  Time: 0.170s, 6035.17/s  (0.226s, 4535.49/s)  LR: 9.502e-04  Data: 0.020 (0.080)
Train: 43 [ 100/1251 (  8%)]  Loss: 4.315 (4.56)  Time: 0.181s, 5662.02/s  (0.207s, 4936.55/s)  LR: 9.502e-04  Data: 0.029 (0.062)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.481 (4.54)  Time: 0.189s, 5406.34/s  (0.200s, 5119.19/s)  LR: 9.502e-04  Data: 0.026 (0.052)
Train: 43 [ 200/1251 ( 16%)]  Loss: 4.163 (4.47)  Time: 0.169s, 6060.88/s  (0.200s, 5120.47/s)  LR: 9.502e-04  Data: 0.024 (0.046)
Train: 43 [ 250/1251 ( 20%)]  Loss: 4.421 (4.46)  Time: 0.167s, 6147.73/s  (0.198s, 5179.84/s)  LR: 9.502e-04  Data: 0.029 (0.042)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.542 (4.47)  Time: 0.172s, 5952.94/s  (0.196s, 5228.32/s)  LR: 9.502e-04  Data: 0.021 (0.040)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.634 (4.49)  Time: 0.159s, 6454.74/s  (0.195s, 5253.59/s)  LR: 9.502e-04  Data: 0.026 (0.038)
Train: 43 [ 400/1251 ( 32%)]  Loss: 4.746 (4.52)  Time: 0.165s, 6204.98/s  (0.194s, 5286.81/s)  LR: 9.502e-04  Data: 0.020 (0.037)
Train: 43 [ 450/1251 ( 36%)]  Loss: 4.397 (4.51)  Time: 0.163s, 6300.72/s  (0.193s, 5295.80/s)  LR: 9.502e-04  Data: 0.029 (0.036)
Train: 43 [ 500/1251 ( 40%)]  Loss: 4.633 (4.52)  Time: 0.169s, 6048.53/s  (0.194s, 5283.64/s)  LR: 9.502e-04  Data: 0.031 (0.035)
Train: 43 [ 550/1251 ( 44%)]  Loss: 4.482 (4.52)  Time: 0.279s, 3670.83/s  (0.193s, 5297.45/s)  LR: 9.502e-04  Data: 0.031 (0.034)
Train: 43 [ 600/1251 ( 48%)]  Loss: 4.181 (4.49)  Time: 0.168s, 6093.70/s  (0.194s, 5278.24/s)  LR: 9.502e-04  Data: 0.025 (0.036)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.338 (4.48)  Time: 0.168s, 6098.16/s  (0.193s, 5299.22/s)  LR: 9.502e-04  Data: 0.027 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 43 [ 700/1251 ( 56%)]  Loss: 4.507 (4.48)  Time: 0.183s, 5609.69/s  (0.193s, 5313.95/s)  LR: 9.502e-04  Data: 0.026 (0.037)
Train: 43 [ 750/1251 ( 60%)]  Loss: 4.962 (4.51)  Time: 0.292s, 3506.71/s  (0.193s, 5310.93/s)  LR: 9.502e-04  Data: 0.179 (0.038)
Train: 43 [ 800/1251 ( 64%)]  Loss: 4.548 (4.51)  Time: 0.165s, 6220.36/s  (0.194s, 5289.09/s)  LR: 9.502e-04  Data: 0.025 (0.038)
Train: 43 [ 850/1251 ( 68%)]  Loss: 4.544 (4.51)  Time: 0.179s, 5735.20/s  (0.193s, 5305.18/s)  LR: 9.502e-04  Data: 0.033 (0.037)
Train: 43 [ 900/1251 ( 72%)]  Loss: 4.896 (4.54)  Time: 0.179s, 5711.83/s  (0.193s, 5303.95/s)  LR: 9.502e-04  Data: 0.031 (0.037)
Train: 43 [ 950/1251 ( 76%)]  Loss: 4.432 (4.53)  Time: 0.203s, 5054.02/s  (0.193s, 5295.42/s)  LR: 9.502e-04  Data: 0.037 (0.037)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.611 (4.53)  Time: 0.165s, 6189.18/s  (0.193s, 5293.78/s)  LR: 9.502e-04  Data: 0.027 (0.036)
Train: 43 [1050/1251 ( 84%)]  Loss: 4.747 (4.54)  Time: 0.155s, 6594.60/s  (0.194s, 5280.03/s)  LR: 9.502e-04  Data: 0.022 (0.036)
Train: 43 [1100/1251 ( 88%)]  Loss: 4.766 (4.55)  Time: 0.168s, 6086.70/s  (0.194s, 5290.51/s)  LR: 9.502e-04  Data: 0.025 (0.035)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.846 (4.57)  Time: 0.257s, 3985.02/s  (0.194s, 5286.76/s)  LR: 9.502e-04  Data: 0.029 (0.035)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.700 (4.57)  Time: 0.162s, 6329.52/s  (0.194s, 5290.92/s)  LR: 9.502e-04  Data: 0.027 (0.035)
Train: 43 [1250/1251 (100%)]  Loss: 4.463 (4.57)  Time: 0.114s, 8991.40/s  (0.193s, 5305.59/s)  LR: 9.502e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.901 (1.901)  Loss:  1.2677 (1.2677)  Acc@1: 75.2930 (75.2930)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3233 (2.0123)  Acc@1: 75.9434 (59.2380)  Acc@5: 91.0377 (82.5200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)

Train: 44 [   0/1251 (  0%)]  Loss: 4.448 (4.45)  Time: 1.800s,  568.86/s  (1.800s,  568.86/s)  LR: 9.479e-04  Data: 1.631 (1.631)
Train: 44 [  50/1251 (  4%)]  Loss: 4.321 (4.38)  Time: 0.160s, 6403.19/s  (0.221s, 4642.10/s)  LR: 9.479e-04  Data: 0.027 (0.073)
Train: 44 [ 100/1251 (  8%)]  Loss: 4.941 (4.57)  Time: 0.168s, 6087.12/s  (0.206s, 4979.45/s)  LR: 9.479e-04  Data: 0.028 (0.060)
Train: 44 [ 150/1251 ( 12%)]  Loss: 4.406 (4.53)  Time: 0.156s, 6572.35/s  (0.200s, 5127.32/s)  LR: 9.479e-04  Data: 0.026 (0.053)
Train: 44 [ 200/1251 ( 16%)]  Loss: 4.539 (4.53)  Time: 0.159s, 6428.77/s  (0.197s, 5193.37/s)  LR: 9.479e-04  Data: 0.028 (0.047)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.243 (4.48)  Time: 0.170s, 6025.82/s  (0.196s, 5225.24/s)  LR: 9.479e-04  Data: 0.027 (0.043)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.657 (4.51)  Time: 0.197s, 5211.14/s  (0.195s, 5240.98/s)  LR: 9.479e-04  Data: 0.031 (0.041)
Train: 44 [ 350/1251 ( 28%)]  Loss: 4.347 (4.49)  Time: 0.180s, 5691.02/s  (0.195s, 5239.85/s)  LR: 9.479e-04  Data: 0.032 (0.039)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.434 (4.48)  Time: 0.170s, 6039.80/s  (0.195s, 5246.78/s)  LR: 9.479e-04  Data: 0.028 (0.038)
Train: 44 [ 450/1251 ( 36%)]  Loss: 4.530 (4.49)  Time: 0.196s, 5237.16/s  (0.194s, 5270.18/s)  LR: 9.479e-04  Data: 0.025 (0.037)
Train: 44 [ 500/1251 ( 40%)]  Loss: 4.514 (4.49)  Time: 0.164s, 6235.54/s  (0.194s, 5288.01/s)  LR: 9.479e-04  Data: 0.026 (0.036)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.347 (4.48)  Time: 0.169s, 6045.71/s  (0.194s, 5286.43/s)  LR: 9.479e-04  Data: 0.024 (0.036)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.815 (4.50)  Time: 0.155s, 6612.01/s  (0.194s, 5291.98/s)  LR: 9.479e-04  Data: 0.029 (0.035)
Train: 44 [ 650/1251 ( 52%)]  Loss: 4.201 (4.48)  Time: 0.177s, 5777.42/s  (0.193s, 5297.19/s)  LR: 9.479e-04  Data: 0.025 (0.035)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.686 (4.50)  Time: 0.190s, 5400.33/s  (0.194s, 5287.17/s)  LR: 9.479e-04  Data: 0.027 (0.034)
Train: 44 [ 750/1251 ( 60%)]  Loss: 4.701 (4.51)  Time: 0.170s, 6035.91/s  (0.193s, 5292.76/s)  LR: 9.479e-04  Data: 0.029 (0.034)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.627 (4.52)  Time: 0.152s, 6741.02/s  (0.193s, 5304.59/s)  LR: 9.479e-04  Data: 0.031 (0.034)
Train: 44 [ 850/1251 ( 68%)]  Loss: 4.513 (4.52)  Time: 0.169s, 6057.31/s  (0.193s, 5302.77/s)  LR: 9.479e-04  Data: 0.027 (0.033)
Train: 44 [ 900/1251 ( 72%)]  Loss: 4.681 (4.52)  Time: 0.195s, 5254.15/s  (0.193s, 5301.22/s)  LR: 9.479e-04  Data: 0.032 (0.033)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.647 (4.53)  Time: 0.180s, 5700.63/s  (0.193s, 5295.96/s)  LR: 9.479e-04  Data: 0.022 (0.033)
Train: 44 [1000/1251 ( 80%)]  Loss: 4.623 (4.53)  Time: 0.165s, 6205.34/s  (0.193s, 5304.04/s)  LR: 9.479e-04  Data: 0.027 (0.033)
Train: 44 [1050/1251 ( 84%)]  Loss: 4.576 (4.54)  Time: 0.205s, 5000.34/s  (0.193s, 5300.56/s)  LR: 9.479e-04  Data: 0.033 (0.032)
Train: 44 [1100/1251 ( 88%)]  Loss: 4.340 (4.53)  Time: 0.170s, 6016.15/s  (0.193s, 5292.24/s)  LR: 9.479e-04  Data: 0.024 (0.033)
Train: 44 [1150/1251 ( 92%)]  Loss: 4.372 (4.52)  Time: 0.170s, 6016.46/s  (0.194s, 5286.94/s)  LR: 9.479e-04  Data: 0.024 (0.034)
Train: 44 [1200/1251 ( 96%)]  Loss: 4.587 (4.52)  Time: 0.174s, 5868.55/s  (0.194s, 5289.81/s)  LR: 9.479e-04  Data: 0.023 (0.034)
Train: 44 [1250/1251 (100%)]  Loss: 4.165 (4.51)  Time: 0.114s, 9010.72/s  (0.193s, 5303.31/s)  LR: 9.479e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.815 (1.815)  Loss:  1.3240 (1.3240)  Acc@1: 75.0000 (75.0000)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.019 (0.211)  Loss:  1.3077 (1.9754)  Acc@1: 75.5896 (59.4540)  Acc@5: 89.6226 (82.6480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)

Train: 45 [   0/1251 (  0%)]  Loss: 4.566 (4.57)  Time: 1.856s,  551.73/s  (1.856s,  551.73/s)  LR: 9.456e-04  Data: 1.725 (1.725)
Train: 45 [  50/1251 (  4%)]  Loss: 4.586 (4.58)  Time: 0.166s, 6177.53/s  (0.225s, 4545.95/s)  LR: 9.456e-04  Data: 0.025 (0.069)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.313 (4.49)  Time: 0.161s, 6344.90/s  (0.207s, 4958.06/s)  LR: 9.456e-04  Data: 0.031 (0.049)
Train: 45 [ 150/1251 ( 12%)]  Loss: 4.823 (4.57)  Time: 0.192s, 5335.17/s  (0.202s, 5063.61/s)  LR: 9.456e-04  Data: 0.022 (0.044)
Train: 45 [ 200/1251 ( 16%)]  Loss: 5.011 (4.66)  Time: 0.168s, 6101.84/s  (0.197s, 5185.72/s)  LR: 9.456e-04  Data: 0.026 (0.042)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.306 (4.60)  Time: 0.146s, 7033.65/s  (0.196s, 5226.78/s)  LR: 9.456e-04  Data: 0.025 (0.042)
Train: 45 [ 300/1251 ( 24%)]  Loss: 4.384 (4.57)  Time: 0.159s, 6423.55/s  (0.196s, 5233.27/s)  LR: 9.456e-04  Data: 0.028 (0.042)
Train: 45 [ 350/1251 ( 28%)]  Loss: 4.653 (4.58)  Time: 0.175s, 5846.17/s  (0.195s, 5250.76/s)  LR: 9.456e-04  Data: 0.030 (0.041)
Train: 45 [ 400/1251 ( 32%)]  Loss: 4.847 (4.61)  Time: 0.163s, 6263.45/s  (0.195s, 5258.75/s)  LR: 9.456e-04  Data: 0.026 (0.039)
Train: 45 [ 450/1251 ( 36%)]  Loss: 4.657 (4.61)  Time: 0.184s, 5579.29/s  (0.194s, 5265.06/s)  LR: 9.456e-04  Data: 0.025 (0.038)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.417 (4.60)  Time: 0.182s, 5614.97/s  (0.193s, 5292.02/s)  LR: 9.456e-04  Data: 0.029 (0.037)
Train: 45 [ 550/1251 ( 44%)]  Loss: 4.101 (4.56)  Time: 0.158s, 6487.98/s  (0.193s, 5300.05/s)  LR: 9.456e-04  Data: 0.026 (0.036)
Train: 45 [ 600/1251 ( 48%)]  Loss: 4.785 (4.57)  Time: 0.194s, 5274.56/s  (0.193s, 5294.31/s)  LR: 9.456e-04  Data: 0.028 (0.036)
Train: 45 [ 650/1251 ( 52%)]  Loss: 4.728 (4.58)  Time: 0.178s, 5754.13/s  (0.194s, 5286.67/s)  LR: 9.456e-04  Data: 0.022 (0.035)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.720 (4.59)  Time: 0.180s, 5682.72/s  (0.194s, 5273.87/s)  LR: 9.456e-04  Data: 0.029 (0.035)
Train: 45 [ 750/1251 ( 60%)]  Loss: 4.753 (4.60)  Time: 0.177s, 5796.39/s  (0.193s, 5301.70/s)  LR: 9.456e-04  Data: 0.032 (0.034)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.446 (4.59)  Time: 0.177s, 5777.05/s  (0.194s, 5289.25/s)  LR: 9.456e-04  Data: 0.040 (0.034)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.590 (4.59)  Time: 0.181s, 5645.78/s  (0.194s, 5286.91/s)  LR: 9.456e-04  Data: 0.031 (0.034)
Train: 45 [ 900/1251 ( 72%)]  Loss: 4.263 (4.58)  Time: 0.165s, 6217.06/s  (0.194s, 5283.54/s)  LR: 9.456e-04  Data: 0.027 (0.033)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.441 (4.57)  Time: 0.167s, 6117.71/s  (0.194s, 5279.82/s)  LR: 9.456e-04  Data: 0.031 (0.033)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.700 (4.58)  Time: 0.186s, 5513.79/s  (0.194s, 5268.21/s)  LR: 9.456e-04  Data: 0.024 (0.033)
Train: 45 [1050/1251 ( 84%)]  Loss: 4.484 (4.57)  Time: 0.190s, 5379.70/s  (0.194s, 5266.52/s)  LR: 9.456e-04  Data: 0.064 (0.033)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.567 (4.57)  Time: 0.194s, 5268.93/s  (0.195s, 5260.91/s)  LR: 9.456e-04  Data: 0.034 (0.032)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.769 (4.58)  Time: 0.161s, 6375.80/s  (0.194s, 5268.64/s)  LR: 9.456e-04  Data: 0.031 (0.032)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.946 (4.59)  Time: 0.163s, 6270.29/s  (0.195s, 5259.61/s)  LR: 9.456e-04  Data: 0.029 (0.032)
Train: 45 [1250/1251 (100%)]  Loss: 4.809 (4.60)  Time: 0.113s, 9024.44/s  (0.194s, 5272.63/s)  LR: 9.456e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.899 (1.899)  Loss:  1.3263 (1.3263)  Acc@1: 75.6836 (75.6836)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3491 (2.0973)  Acc@1: 75.1179 (58.4420)  Acc@5: 90.5660 (82.0240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-45.pth.tar', 58.441999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)

Train: 46 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 1.701s,  601.87/s  (1.701s,  601.87/s)  LR: 9.432e-04  Data: 1.569 (1.569)
Train: 46 [  50/1251 (  4%)]  Loss: 4.392 (4.45)  Time: 0.161s, 6367.05/s  (0.223s, 4597.92/s)  LR: 9.432e-04  Data: 0.029 (0.063)
Train: 46 [ 100/1251 (  8%)]  Loss: 4.523 (4.47)  Time: 0.164s, 6237.95/s  (0.207s, 4935.35/s)  LR: 9.432e-04  Data: 0.029 (0.046)
Train: 46 [ 150/1251 ( 12%)]  Loss: 4.698 (4.53)  Time: 0.163s, 6277.80/s  (0.202s, 5062.99/s)  LR: 9.432e-04  Data: 0.034 (0.042)
Train: 46 [ 200/1251 ( 16%)]  Loss: 4.334 (4.49)  Time: 0.140s, 7305.21/s  (0.199s, 5145.45/s)  LR: 9.432e-04  Data: 0.023 (0.039)
Train: 46 [ 250/1251 ( 20%)]  Loss: 4.120 (4.43)  Time: 0.171s, 5988.01/s  (0.197s, 5206.58/s)  LR: 9.432e-04  Data: 0.026 (0.037)
Train: 46 [ 300/1251 ( 24%)]  Loss: 4.826 (4.49)  Time: 0.186s, 5519.35/s  (0.197s, 5191.67/s)  LR: 9.432e-04  Data: 0.041 (0.035)
Train: 46 [ 350/1251 ( 28%)]  Loss: 4.701 (4.51)  Time: 0.176s, 5828.58/s  (0.197s, 5202.51/s)  LR: 9.432e-04  Data: 0.026 (0.034)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.482 (4.51)  Time: 0.164s, 6247.47/s  (0.195s, 5240.09/s)  LR: 9.432e-04  Data: 0.032 (0.033)
Train: 46 [ 450/1251 ( 36%)]  Loss: 4.498 (4.51)  Time: 0.345s, 2968.62/s  (0.195s, 5258.30/s)  LR: 9.432e-04  Data: 0.025 (0.033)
Train: 46 [ 500/1251 ( 40%)]  Loss: 4.153 (4.48)  Time: 0.172s, 5936.56/s  (0.194s, 5272.59/s)  LR: 9.432e-04  Data: 0.022 (0.032)
Train: 46 [ 550/1251 ( 44%)]  Loss: 4.530 (4.48)  Time: 0.169s, 6055.46/s  (0.194s, 5265.25/s)  LR: 9.432e-04  Data: 0.023 (0.032)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.760 (4.50)  Time: 0.223s, 4600.30/s  (0.194s, 5280.13/s)  LR: 9.432e-04  Data: 0.026 (0.031)
Train: 46 [ 650/1251 ( 52%)]  Loss: 4.065 (4.47)  Time: 0.158s, 6499.82/s  (0.194s, 5284.93/s)  LR: 9.432e-04  Data: 0.027 (0.031)
Train: 46 [ 700/1251 ( 56%)]  Loss: 4.813 (4.49)  Time: 0.164s, 6228.29/s  (0.194s, 5277.31/s)  LR: 9.432e-04  Data: 0.024 (0.031)
Train: 46 [ 750/1251 ( 60%)]  Loss: 4.323 (4.48)  Time: 0.199s, 5136.19/s  (0.194s, 5274.29/s)  LR: 9.432e-04  Data: 0.025 (0.031)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.625 (4.49)  Time: 0.174s, 5877.54/s  (0.194s, 5287.98/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.460 (4.49)  Time: 0.202s, 5060.71/s  (0.194s, 5281.30/s)  LR: 9.432e-04  Data: 0.024 (0.030)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.510 (4.49)  Time: 0.194s, 5271.52/s  (0.194s, 5281.92/s)  LR: 9.432e-04  Data: 0.029 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 46 [ 950/1251 ( 76%)]  Loss: 4.466 (4.49)  Time: 0.158s, 6466.97/s  (0.194s, 5270.81/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [1000/1251 ( 80%)]  Loss: 4.768 (4.50)  Time: 0.164s, 6251.14/s  (0.194s, 5272.45/s)  LR: 9.432e-04  Data: 0.029 (0.030)
Train: 46 [1050/1251 ( 84%)]  Loss: 4.386 (4.50)  Time: 0.173s, 5915.15/s  (0.194s, 5268.56/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.508 (4.50)  Time: 0.160s, 6380.95/s  (0.194s, 5266.66/s)  LR: 9.432e-04  Data: 0.024 (0.030)
Train: 46 [1150/1251 ( 92%)]  Loss: 4.109 (4.48)  Time: 0.165s, 6193.35/s  (0.195s, 5261.24/s)  LR: 9.432e-04  Data: 0.029 (0.029)
Train: 46 [1200/1251 ( 96%)]  Loss: 4.126 (4.47)  Time: 0.221s, 4629.52/s  (0.195s, 5255.01/s)  LR: 9.432e-04  Data: 0.028 (0.029)
Train: 46 [1250/1251 (100%)]  Loss: 4.804 (4.48)  Time: 0.113s, 9057.08/s  (0.194s, 5270.18/s)  LR: 9.432e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.771 (1.771)  Loss:  1.3549 (1.3549)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.3843 (2.0625)  Acc@1: 78.0660 (59.4940)  Acc@5: 90.5660 (82.3840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-45.pth.tar', 58.441999921875)

Train: 47 [   0/1251 (  0%)]  Loss: 4.554 (4.55)  Time: 1.706s,  600.12/s  (1.706s,  600.12/s)  LR: 9.407e-04  Data: 1.580 (1.580)
Train: 47 [  50/1251 (  4%)]  Loss: 4.372 (4.46)  Time: 0.182s, 5634.43/s  (0.223s, 4588.47/s)  LR: 9.407e-04  Data: 0.024 (0.072)
Train: 47 [ 100/1251 (  8%)]  Loss: 4.607 (4.51)  Time: 0.185s, 5528.17/s  (0.209s, 4899.47/s)  LR: 9.407e-04  Data: 0.024 (0.055)
Train: 47 [ 150/1251 ( 12%)]  Loss: 4.668 (4.55)  Time: 0.185s, 5528.73/s  (0.203s, 5050.72/s)  LR: 9.407e-04  Data: 0.024 (0.046)
Train: 47 [ 200/1251 ( 16%)]  Loss: 4.350 (4.51)  Time: 0.179s, 5716.93/s  (0.200s, 5118.01/s)  LR: 9.407e-04  Data: 0.028 (0.042)
Train: 47 [ 250/1251 ( 20%)]  Loss: 4.429 (4.50)  Time: 0.161s, 6372.30/s  (0.199s, 5153.02/s)  LR: 9.407e-04  Data: 0.031 (0.040)
Train: 47 [ 300/1251 ( 24%)]  Loss: 4.657 (4.52)  Time: 0.161s, 6367.41/s  (0.196s, 5215.18/s)  LR: 9.407e-04  Data: 0.026 (0.038)
Train: 47 [ 350/1251 ( 28%)]  Loss: 4.525 (4.52)  Time: 0.175s, 5843.07/s  (0.197s, 5204.90/s)  LR: 9.407e-04  Data: 0.027 (0.036)
Train: 47 [ 400/1251 ( 32%)]  Loss: 4.637 (4.53)  Time: 0.168s, 6100.48/s  (0.196s, 5229.14/s)  LR: 9.407e-04  Data: 0.038 (0.036)
Train: 47 [ 450/1251 ( 36%)]  Loss: 4.285 (4.51)  Time: 0.202s, 5060.52/s  (0.196s, 5237.73/s)  LR: 9.407e-04  Data: 0.026 (0.035)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.480 (4.51)  Time: 0.156s, 6572.18/s  (0.194s, 5282.74/s)  LR: 9.407e-04  Data: 0.031 (0.034)
Train: 47 [ 550/1251 ( 44%)]  Loss: 4.530 (4.51)  Time: 0.192s, 5331.41/s  (0.194s, 5267.31/s)  LR: 9.407e-04  Data: 0.052 (0.034)
Train: 47 [ 600/1251 ( 48%)]  Loss: 3.996 (4.47)  Time: 0.189s, 5407.22/s  (0.194s, 5272.89/s)  LR: 9.407e-04  Data: 0.037 (0.033)
Train: 47 [ 650/1251 ( 52%)]  Loss: 4.308 (4.46)  Time: 0.174s, 5893.33/s  (0.194s, 5278.49/s)  LR: 9.407e-04  Data: 0.025 (0.033)
Train: 47 [ 700/1251 ( 56%)]  Loss: 4.458 (4.46)  Time: 0.157s, 6529.69/s  (0.194s, 5286.66/s)  LR: 9.407e-04  Data: 0.028 (0.033)
Train: 47 [ 750/1251 ( 60%)]  Loss: 4.374 (4.45)  Time: 0.390s, 2626.17/s  (0.194s, 5278.39/s)  LR: 9.407e-04  Data: 0.247 (0.035)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.577 (4.46)  Time: 0.178s, 5753.09/s  (0.194s, 5282.56/s)  LR: 9.407e-04  Data: 0.025 (0.035)
Train: 47 [ 850/1251 ( 68%)]  Loss: 4.510 (4.46)  Time: 0.160s, 6387.94/s  (0.194s, 5278.62/s)  LR: 9.407e-04  Data: 0.031 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.319 (4.45)  Time: 0.278s, 3681.37/s  (0.194s, 5273.87/s)  LR: 9.407e-04  Data: 0.028 (0.035)
Train: 47 [ 950/1251 ( 76%)]  Loss: 4.457 (4.45)  Time: 0.189s, 5414.44/s  (0.194s, 5271.06/s)  LR: 9.407e-04  Data: 0.022 (0.034)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.703 (4.47)  Time: 0.177s, 5786.88/s  (0.194s, 5267.57/s)  LR: 9.407e-04  Data: 0.027 (0.034)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.631 (4.47)  Time: 0.186s, 5500.67/s  (0.194s, 5266.90/s)  LR: 9.407e-04  Data: 0.023 (0.034)
Train: 47 [1100/1251 ( 88%)]  Loss: 4.479 (4.47)  Time: 0.833s, 1229.76/s  (0.195s, 5250.69/s)  LR: 9.407e-04  Data: 0.024 (0.033)
Train: 47 [1150/1251 ( 92%)]  Loss: 4.116 (4.46)  Time: 0.163s, 6265.22/s  (0.196s, 5224.86/s)  LR: 9.407e-04  Data: 0.022 (0.033)
Train: 47 [1200/1251 ( 96%)]  Loss: 4.680 (4.47)  Time: 0.159s, 6460.17/s  (0.196s, 5216.21/s)  LR: 9.407e-04  Data: 0.018 (0.033)
Train: 47 [1250/1251 (100%)]  Loss: 4.346 (4.46)  Time: 0.114s, 9015.16/s  (0.196s, 5229.01/s)  LR: 9.407e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.833 (1.833)  Loss:  1.3168 (1.3168)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2297 (1.9813)  Acc@1: 78.1840 (59.7660)  Acc@5: 90.8019 (82.8580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)

Train: 48 [   0/1251 (  0%)]  Loss: 4.264 (4.26)  Time: 1.801s,  568.47/s  (1.801s,  568.47/s)  LR: 9.382e-04  Data: 1.681 (1.681)
Train: 48 [  50/1251 (  4%)]  Loss: 4.861 (4.56)  Time: 0.166s, 6156.04/s  (0.229s, 4478.79/s)  LR: 9.382e-04  Data: 0.036 (0.063)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.562 (4.56)  Time: 0.184s, 5569.50/s  (0.208s, 4914.14/s)  LR: 9.382e-04  Data: 0.022 (0.049)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.825 (4.63)  Time: 0.182s, 5621.58/s  (0.204s, 5030.19/s)  LR: 9.382e-04  Data: 0.019 (0.041)
Train: 48 [ 200/1251 ( 16%)]  Loss: 4.619 (4.63)  Time: 0.180s, 5703.80/s  (0.200s, 5126.73/s)  LR: 9.382e-04  Data: 0.029 (0.038)
Train: 48 [ 250/1251 ( 20%)]  Loss: 4.226 (4.56)  Time: 0.174s, 5893.36/s  (0.196s, 5237.39/s)  LR: 9.382e-04  Data: 0.027 (0.036)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.320 (4.53)  Time: 0.167s, 6140.54/s  (0.195s, 5260.15/s)  LR: 9.382e-04  Data: 0.033 (0.035)
Train: 48 [ 350/1251 ( 28%)]  Loss: 4.568 (4.53)  Time: 0.163s, 6284.85/s  (0.194s, 5274.55/s)  LR: 9.382e-04  Data: 0.026 (0.033)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.687 (4.55)  Time: 0.187s, 5482.81/s  (0.195s, 5255.64/s)  LR: 9.382e-04  Data: 0.035 (0.033)
Train: 48 [ 450/1251 ( 36%)]  Loss: 4.266 (4.52)  Time: 0.180s, 5699.80/s  (0.194s, 5268.91/s)  LR: 9.382e-04  Data: 0.051 (0.032)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.686 (4.54)  Time: 0.181s, 5666.52/s  (0.195s, 5261.76/s)  LR: 9.382e-04  Data: 0.025 (0.032)
Train: 48 [ 550/1251 ( 44%)]  Loss: 4.537 (4.54)  Time: 0.179s, 5728.95/s  (0.194s, 5271.34/s)  LR: 9.382e-04  Data: 0.031 (0.031)
Train: 48 [ 600/1251 ( 48%)]  Loss: 4.443 (4.53)  Time: 0.172s, 5952.53/s  (0.195s, 5262.43/s)  LR: 9.382e-04  Data: 0.036 (0.031)
Train: 48 [ 650/1251 ( 52%)]  Loss: 4.370 (4.52)  Time: 0.176s, 5807.72/s  (0.194s, 5275.31/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [ 700/1251 ( 56%)]  Loss: 4.895 (4.54)  Time: 0.164s, 6246.40/s  (0.194s, 5284.56/s)  LR: 9.382e-04  Data: 0.029 (0.031)
Train: 48 [ 750/1251 ( 60%)]  Loss: 4.518 (4.54)  Time: 0.245s, 4181.67/s  (0.194s, 5283.66/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [ 800/1251 ( 64%)]  Loss: 4.755 (4.55)  Time: 0.248s, 4131.22/s  (0.193s, 5295.49/s)  LR: 9.382e-04  Data: 0.118 (0.031)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.404 (4.54)  Time: 0.199s, 5150.78/s  (0.194s, 5277.98/s)  LR: 9.382e-04  Data: 0.021 (0.031)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.087 (4.52)  Time: 0.181s, 5654.93/s  (0.194s, 5272.89/s)  LR: 9.382e-04  Data: 0.019 (0.030)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.896 (4.54)  Time: 0.182s, 5629.58/s  (0.194s, 5282.17/s)  LR: 9.382e-04  Data: 0.029 (0.030)
Train: 48 [1000/1251 ( 80%)]  Loss: 4.445 (4.54)  Time: 0.184s, 5578.40/s  (0.194s, 5282.54/s)  LR: 9.382e-04  Data: 0.032 (0.030)
Train: 48 [1050/1251 ( 84%)]  Loss: 4.702 (4.54)  Time: 0.183s, 5607.07/s  (0.194s, 5285.23/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.912 (4.56)  Time: 0.163s, 6297.92/s  (0.194s, 5267.17/s)  LR: 9.382e-04  Data: 0.024 (0.032)
Train: 48 [1150/1251 ( 92%)]  Loss: 4.311 (4.55)  Time: 0.192s, 5339.45/s  (0.195s, 5261.22/s)  LR: 9.382e-04  Data: 0.031 (0.033)
Train: 48 [1200/1251 ( 96%)]  Loss: 4.813 (4.56)  Time: 0.174s, 5899.45/s  (0.194s, 5272.02/s)  LR: 9.382e-04  Data: 0.027 (0.033)
Train: 48 [1250/1251 (100%)]  Loss: 4.336 (4.55)  Time: 0.137s, 7491.40/s  (0.194s, 5272.36/s)  LR: 9.382e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.847 (1.847)  Loss:  1.4455 (1.4455)  Acc@1: 76.6602 (76.6602)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3859 (2.0725)  Acc@1: 77.5943 (59.2380)  Acc@5: 91.0377 (82.5580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)

Train: 49 [   0/1251 (  0%)]  Loss: 4.590 (4.59)  Time: 1.842s,  556.00/s  (1.842s,  556.00/s)  LR: 9.357e-04  Data: 1.719 (1.719)
Train: 49 [  50/1251 (  4%)]  Loss: 4.801 (4.70)  Time: 0.183s, 5585.53/s  (0.229s, 4475.60/s)  LR: 9.357e-04  Data: 0.019 (0.079)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.561 (4.65)  Time: 0.186s, 5495.12/s  (0.208s, 4925.52/s)  LR: 9.357e-04  Data: 0.024 (0.054)
Train: 49 [ 150/1251 ( 12%)]  Loss: 4.578 (4.63)  Time: 0.157s, 6513.38/s  (0.203s, 5041.32/s)  LR: 9.357e-04  Data: 0.026 (0.046)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.452 (4.60)  Time: 0.300s, 3410.54/s  (0.202s, 5057.54/s)  LR: 9.357e-04  Data: 0.023 (0.042)
Train: 49 [ 250/1251 ( 20%)]  Loss: 4.666 (4.61)  Time: 0.204s, 5016.35/s  (0.198s, 5166.09/s)  LR: 9.357e-04  Data: 0.025 (0.039)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.781 (4.63)  Time: 0.162s, 6317.06/s  (0.196s, 5220.99/s)  LR: 9.357e-04  Data: 0.029 (0.037)
Train: 49 [ 350/1251 ( 28%)]  Loss: 4.581 (4.63)  Time: 0.169s, 6068.72/s  (0.196s, 5220.18/s)  LR: 9.357e-04  Data: 0.038 (0.036)
Train: 49 [ 400/1251 ( 32%)]  Loss: 4.245 (4.58)  Time: 0.231s, 4428.82/s  (0.196s, 5227.60/s)  LR: 9.357e-04  Data: 0.025 (0.035)
Train: 49 [ 450/1251 ( 36%)]  Loss: 4.589 (4.58)  Time: 0.162s, 6316.77/s  (0.194s, 5267.96/s)  LR: 9.357e-04  Data: 0.024 (0.034)
Train: 49 [ 500/1251 ( 40%)]  Loss: 4.683 (4.59)  Time: 0.174s, 5876.51/s  (0.194s, 5266.94/s)  LR: 9.357e-04  Data: 0.028 (0.034)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.609 (4.59)  Time: 0.160s, 6415.38/s  (0.194s, 5264.90/s)  LR: 9.357e-04  Data: 0.030 (0.034)
Train: 49 [ 600/1251 ( 48%)]  Loss: 4.198 (4.56)  Time: 0.178s, 5758.78/s  (0.194s, 5266.70/s)  LR: 9.357e-04  Data: 0.025 (0.034)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.159 (4.54)  Time: 0.174s, 5878.59/s  (0.194s, 5270.61/s)  LR: 9.357e-04  Data: 0.027 (0.034)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.474 (4.53)  Time: 0.172s, 5946.65/s  (0.194s, 5272.41/s)  LR: 9.357e-04  Data: 0.022 (0.033)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.654 (4.54)  Time: 0.164s, 6232.43/s  (0.195s, 5259.16/s)  LR: 9.357e-04  Data: 0.040 (0.033)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.641 (4.54)  Time: 0.569s, 1801.07/s  (0.195s, 5249.90/s)  LR: 9.357e-04  Data: 0.025 (0.033)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.539 (4.54)  Time: 0.156s, 6557.02/s  (0.195s, 5256.39/s)  LR: 9.357e-04  Data: 0.020 (0.032)
Train: 49 [ 900/1251 ( 72%)]  Loss: 4.448 (4.54)  Time: 0.193s, 5301.82/s  (0.195s, 5249.94/s)  LR: 9.357e-04  Data: 0.029 (0.032)
Train: 49 [ 950/1251 ( 76%)]  Loss: 4.367 (4.53)  Time: 0.173s, 5912.14/s  (0.196s, 5231.20/s)  LR: 9.357e-04  Data: 0.028 (0.032)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.372 (4.52)  Time: 0.183s, 5605.70/s  (0.196s, 5225.57/s)  LR: 9.357e-04  Data: 0.022 (0.032)
Train: 49 [1050/1251 ( 84%)]  Loss: 4.313 (4.51)  Time: 0.153s, 6706.00/s  (0.196s, 5224.72/s)  LR: 9.357e-04  Data: 0.023 (0.031)
Train: 49 [1100/1251 ( 88%)]  Loss: 4.330 (4.51)  Time: 0.169s, 6054.39/s  (0.196s, 5220.25/s)  LR: 9.357e-04  Data: 0.033 (0.031)
Train: 49 [1150/1251 ( 92%)]  Loss: 4.554 (4.51)  Time: 0.177s, 5774.61/s  (0.196s, 5211.82/s)  LR: 9.357e-04  Data: 0.025 (0.031)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.321 (4.50)  Time: 0.166s, 6177.56/s  (0.196s, 5213.48/s)  LR: 9.357e-04  Data: 0.022 (0.031)
Train: 49 [1250/1251 (100%)]  Loss: 4.620 (4.50)  Time: 0.114s, 9020.50/s  (0.196s, 5229.46/s)  LR: 9.357e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  1.4064 (1.4064)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3966 (2.0698)  Acc@1: 77.4764 (59.6220)  Acc@5: 91.6274 (82.5100)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)

Train: 50 [   0/1251 (  0%)]  Loss: 4.279 (4.28)  Time: 1.764s,  580.38/s  (1.764s,  580.38/s)  LR: 9.331e-04  Data: 1.636 (1.636)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 50 [  50/1251 (  4%)]  Loss: 4.575 (4.43)  Time: 0.172s, 5949.52/s  (0.226s, 4528.27/s)  LR: 9.331e-04  Data: 0.022 (0.064)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.314 (4.39)  Time: 0.166s, 6164.46/s  (0.207s, 4955.49/s)  LR: 9.331e-04  Data: 0.031 (0.048)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.059 (4.31)  Time: 0.181s, 5648.43/s  (0.201s, 5087.69/s)  LR: 9.331e-04  Data: 0.022 (0.041)
Train: 50 [ 200/1251 ( 16%)]  Loss: 4.239 (4.29)  Time: 0.180s, 5690.46/s  (0.199s, 5139.07/s)  LR: 9.331e-04  Data: 0.032 (0.038)
Train: 50 [ 250/1251 ( 20%)]  Loss: 4.416 (4.31)  Time: 0.169s, 6047.73/s  (0.197s, 5201.40/s)  LR: 9.331e-04  Data: 0.021 (0.037)
Train: 50 [ 300/1251 ( 24%)]  Loss: 4.492 (4.34)  Time: 0.165s, 6223.62/s  (0.196s, 5222.22/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 350/1251 ( 28%)]  Loss: 4.348 (4.34)  Time: 0.187s, 5466.16/s  (0.195s, 5244.43/s)  LR: 9.331e-04  Data: 0.025 (0.034)
Train: 50 [ 400/1251 ( 32%)]  Loss: 4.750 (4.39)  Time: 0.174s, 5873.08/s  (0.195s, 5260.51/s)  LR: 9.331e-04  Data: 0.034 (0.035)
Train: 50 [ 450/1251 ( 36%)]  Loss: 4.482 (4.40)  Time: 0.180s, 5678.13/s  (0.194s, 5269.40/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.722 (4.43)  Time: 0.192s, 5338.76/s  (0.194s, 5271.04/s)  LR: 9.331e-04  Data: 0.020 (0.037)
Train: 50 [ 550/1251 ( 44%)]  Loss: 4.773 (4.45)  Time: 0.170s, 6014.96/s  (0.193s, 5298.33/s)  LR: 9.331e-04  Data: 0.032 (0.037)
Train: 50 [ 600/1251 ( 48%)]  Loss: 4.367 (4.45)  Time: 0.186s, 5503.12/s  (0.193s, 5300.37/s)  LR: 9.331e-04  Data: 0.036 (0.036)
Train: 50 [ 650/1251 ( 52%)]  Loss: 4.178 (4.43)  Time: 0.274s, 3731.82/s  (0.193s, 5296.13/s)  LR: 9.331e-04  Data: 0.030 (0.036)
Train: 50 [ 700/1251 ( 56%)]  Loss: 4.633 (4.44)  Time: 0.161s, 6366.15/s  (0.193s, 5302.25/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.363 (4.44)  Time: 0.184s, 5574.94/s  (0.193s, 5301.15/s)  LR: 9.331e-04  Data: 0.021 (0.035)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.720 (4.45)  Time: 0.158s, 6489.26/s  (0.194s, 5291.91/s)  LR: 9.331e-04  Data: 0.023 (0.034)
Train: 50 [ 850/1251 ( 68%)]  Loss: 4.946 (4.48)  Time: 0.157s, 6510.96/s  (0.193s, 5297.49/s)  LR: 9.331e-04  Data: 0.030 (0.034)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.634 (4.49)  Time: 0.158s, 6497.39/s  (0.193s, 5311.82/s)  LR: 9.331e-04  Data: 0.025 (0.034)
Train: 50 [ 950/1251 ( 76%)]  Loss: 4.911 (4.51)  Time: 0.264s, 3877.32/s  (0.193s, 5302.92/s)  LR: 9.331e-04  Data: 0.036 (0.033)
Train: 50 [1000/1251 ( 80%)]  Loss: 4.276 (4.50)  Time: 0.195s, 5244.21/s  (0.193s, 5295.43/s)  LR: 9.331e-04  Data: 0.035 (0.033)
Train: 50 [1050/1251 ( 84%)]  Loss: 4.511 (4.50)  Time: 0.171s, 5986.88/s  (0.196s, 5234.30/s)  LR: 9.331e-04  Data: 0.021 (0.033)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.620 (4.50)  Time: 0.182s, 5634.17/s  (0.196s, 5221.35/s)  LR: 9.331e-04  Data: 0.028 (0.032)
Train: 50 [1150/1251 ( 92%)]  Loss: 4.542 (4.51)  Time: 0.175s, 5851.74/s  (0.196s, 5223.45/s)  LR: 9.331e-04  Data: 0.024 (0.032)
Train: 50 [1200/1251 ( 96%)]  Loss: 4.339 (4.50)  Time: 0.163s, 6286.03/s  (0.196s, 5223.94/s)  LR: 9.331e-04  Data: 0.020 (0.032)
Train: 50 [1250/1251 (100%)]  Loss: 4.301 (4.49)  Time: 0.117s, 8726.66/s  (0.196s, 5234.30/s)  LR: 9.331e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.832 (1.832)  Loss:  1.1946 (1.1946)  Acc@1: 76.0742 (76.0742)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.2061 (1.8838)  Acc@1: 77.4764 (60.4180)  Acc@5: 91.3915 (83.0720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)

Train: 51 [   0/1251 (  0%)]  Loss: 4.571 (4.57)  Time: 1.992s,  514.10/s  (1.992s,  514.10/s)  LR: 9.304e-04  Data: 1.846 (1.846)
Train: 51 [  50/1251 (  4%)]  Loss: 4.197 (4.38)  Time: 0.178s, 5766.61/s  (0.222s, 4618.07/s)  LR: 9.304e-04  Data: 0.031 (0.076)
Train: 51 [ 100/1251 (  8%)]  Loss: 4.795 (4.52)  Time: 0.167s, 6127.56/s  (0.211s, 4861.56/s)  LR: 9.304e-04  Data: 0.024 (0.053)
Train: 51 [ 150/1251 ( 12%)]  Loss: 4.301 (4.47)  Time: 0.169s, 6064.86/s  (0.202s, 5069.27/s)  LR: 9.304e-04  Data: 0.029 (0.044)
Train: 51 [ 200/1251 ( 16%)]  Loss: 4.472 (4.47)  Time: 0.166s, 6163.69/s  (0.202s, 5069.03/s)  LR: 9.304e-04  Data: 0.020 (0.040)
Train: 51 [ 250/1251 ( 20%)]  Loss: 4.309 (4.44)  Time: 0.164s, 6235.61/s  (0.198s, 5171.07/s)  LR: 9.304e-04  Data: 0.031 (0.037)
Train: 51 [ 300/1251 ( 24%)]  Loss: 4.591 (4.46)  Time: 0.167s, 6142.89/s  (0.197s, 5191.94/s)  LR: 9.304e-04  Data: 0.036 (0.036)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.616 (4.48)  Time: 0.199s, 5133.95/s  (0.196s, 5221.22/s)  LR: 9.304e-04  Data: 0.020 (0.035)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.628 (4.50)  Time: 0.162s, 6330.00/s  (0.197s, 5207.58/s)  LR: 9.304e-04  Data: 0.030 (0.034)
Train: 51 [ 450/1251 ( 36%)]  Loss: 4.768 (4.52)  Time: 0.180s, 5692.31/s  (0.196s, 5237.33/s)  LR: 9.304e-04  Data: 0.031 (0.033)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.861 (4.56)  Time: 0.168s, 6089.36/s  (0.195s, 5246.65/s)  LR: 9.304e-04  Data: 0.030 (0.033)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.270 (4.53)  Time: 0.157s, 6533.63/s  (0.195s, 5255.99/s)  LR: 9.304e-04  Data: 0.032 (0.033)
Train: 51 [ 600/1251 ( 48%)]  Loss: 4.269 (4.51)  Time: 0.196s, 5223.78/s  (0.195s, 5243.97/s)  LR: 9.304e-04  Data: 0.020 (0.032)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.430 (4.51)  Time: 0.176s, 5808.05/s  (0.195s, 5245.35/s)  LR: 9.304e-04  Data: 0.022 (0.032)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.565 (4.51)  Time: 0.213s, 4809.59/s  (0.195s, 5255.64/s)  LR: 9.304e-04  Data: 0.035 (0.032)
Train: 51 [ 750/1251 ( 60%)]  Loss: 4.530 (4.51)  Time: 0.163s, 6274.87/s  (0.194s, 5277.50/s)  LR: 9.304e-04  Data: 0.030 (0.032)
Train: 51 [ 800/1251 ( 64%)]  Loss: 4.370 (4.50)  Time: 0.175s, 5866.05/s  (0.195s, 5257.73/s)  LR: 9.304e-04  Data: 0.031 (0.032)
Train: 51 [ 850/1251 ( 68%)]  Loss: 4.282 (4.49)  Time: 0.164s, 6245.48/s  (0.194s, 5268.49/s)  LR: 9.304e-04  Data: 0.034 (0.031)
Train: 51 [ 900/1251 ( 72%)]  Loss: 4.639 (4.50)  Time: 0.167s, 6138.05/s  (0.194s, 5279.49/s)  LR: 9.304e-04  Data: 0.035 (0.031)
Train: 51 [ 950/1251 ( 76%)]  Loss: 4.527 (4.50)  Time: 0.515s, 1988.29/s  (0.195s, 5256.57/s)  LR: 9.304e-04  Data: 0.035 (0.031)
Train: 51 [1000/1251 ( 80%)]  Loss: 4.480 (4.50)  Time: 0.228s, 4489.98/s  (0.195s, 5253.70/s)  LR: 9.304e-04  Data: 0.029 (0.031)
Train: 51 [1050/1251 ( 84%)]  Loss: 4.509 (4.50)  Time: 0.163s, 6288.72/s  (0.195s, 5255.52/s)  LR: 9.304e-04  Data: 0.027 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 51 [1100/1251 ( 88%)]  Loss: 4.364 (4.49)  Time: 0.190s, 5384.20/s  (0.195s, 5254.19/s)  LR: 9.304e-04  Data: 0.033 (0.031)
Train: 51 [1150/1251 ( 92%)]  Loss: 4.302 (4.49)  Time: 0.353s, 2899.26/s  (0.195s, 5248.97/s)  LR: 9.304e-04  Data: 0.028 (0.030)
Train: 51 [1200/1251 ( 96%)]  Loss: 4.920 (4.50)  Time: 0.169s, 6076.24/s  (0.195s, 5241.68/s)  LR: 9.304e-04  Data: 0.034 (0.030)
Train: 51 [1250/1251 (100%)]  Loss: 4.536 (4.50)  Time: 0.114s, 9012.23/s  (0.195s, 5257.39/s)  LR: 9.304e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.893 (1.893)  Loss:  1.2947 (1.2947)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3579 (2.0442)  Acc@1: 76.8868 (59.0820)  Acc@5: 91.3915 (82.3620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)

Train: 52 [   0/1251 (  0%)]  Loss: 4.512 (4.51)  Time: 1.693s,  604.97/s  (1.693s,  604.97/s)  LR: 9.278e-04  Data: 1.565 (1.565)
Train: 52 [  50/1251 (  4%)]  Loss: 4.229 (4.37)  Time: 0.177s, 5781.45/s  (0.220s, 4655.34/s)  LR: 9.278e-04  Data: 0.032 (0.067)
Train: 52 [ 100/1251 (  8%)]  Loss: 4.178 (4.31)  Time: 0.166s, 6185.07/s  (0.210s, 4870.14/s)  LR: 9.278e-04  Data: 0.024 (0.049)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.601 (4.38)  Time: 0.165s, 6206.60/s  (0.203s, 5055.42/s)  LR: 9.278e-04  Data: 0.024 (0.043)
Train: 52 [ 200/1251 ( 16%)]  Loss: 4.209 (4.35)  Time: 0.165s, 6192.10/s  (0.199s, 5150.79/s)  LR: 9.278e-04  Data: 0.028 (0.039)
Train: 52 [ 250/1251 ( 20%)]  Loss: 4.533 (4.38)  Time: 0.166s, 6151.86/s  (0.198s, 5181.44/s)  LR: 9.278e-04  Data: 0.025 (0.037)
Train: 52 [ 300/1251 ( 24%)]  Loss: 4.305 (4.37)  Time: 0.171s, 5984.65/s  (0.196s, 5228.32/s)  LR: 9.278e-04  Data: 0.027 (0.036)
Train: 52 [ 350/1251 ( 28%)]  Loss: 4.679 (4.41)  Time: 0.190s, 5393.42/s  (0.195s, 5244.54/s)  LR: 9.278e-04  Data: 0.020 (0.034)
Train: 52 [ 400/1251 ( 32%)]  Loss: 4.499 (4.42)  Time: 0.183s, 5587.42/s  (0.195s, 5261.51/s)  LR: 9.278e-04  Data: 0.035 (0.035)
Train: 52 [ 450/1251 ( 36%)]  Loss: 4.117 (4.39)  Time: 0.199s, 5135.67/s  (0.194s, 5287.07/s)  LR: 9.278e-04  Data: 0.028 (0.035)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.441 (4.39)  Time: 0.186s, 5510.37/s  (0.194s, 5274.82/s)  LR: 9.278e-04  Data: 0.038 (0.035)
Train: 52 [ 550/1251 ( 44%)]  Loss: 5.022 (4.44)  Time: 0.157s, 6525.71/s  (0.193s, 5299.30/s)  LR: 9.278e-04  Data: 0.027 (0.034)
Train: 52 [ 600/1251 ( 48%)]  Loss: 4.734 (4.47)  Time: 0.210s, 4876.49/s  (0.193s, 5294.55/s)  LR: 9.278e-04  Data: 0.024 (0.034)
Train: 52 [ 650/1251 ( 52%)]  Loss: 4.718 (4.48)  Time: 0.187s, 5467.28/s  (0.193s, 5292.96/s)  LR: 9.278e-04  Data: 0.025 (0.033)
Train: 52 [ 700/1251 ( 56%)]  Loss: 4.624 (4.49)  Time: 0.162s, 6310.12/s  (0.193s, 5310.31/s)  LR: 9.278e-04  Data: 0.030 (0.033)
Train: 52 [ 750/1251 ( 60%)]  Loss: 4.082 (4.47)  Time: 0.174s, 5882.64/s  (0.193s, 5294.51/s)  LR: 9.278e-04  Data: 0.024 (0.032)
Train: 52 [ 800/1251 ( 64%)]  Loss: 4.523 (4.47)  Time: 0.179s, 5733.86/s  (0.194s, 5278.67/s)  LR: 9.278e-04  Data: 0.027 (0.032)
Train: 52 [ 850/1251 ( 68%)]  Loss: 4.157 (4.45)  Time: 0.192s, 5332.32/s  (0.194s, 5281.63/s)  LR: 9.278e-04  Data: 0.037 (0.032)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.631 (4.46)  Time: 0.171s, 5988.28/s  (0.194s, 5288.98/s)  LR: 9.278e-04  Data: 0.023 (0.032)
Train: 52 [ 950/1251 ( 76%)]  Loss: 4.363 (4.46)  Time: 0.169s, 6048.96/s  (0.194s, 5282.76/s)  LR: 9.278e-04  Data: 0.023 (0.031)
Train: 52 [1000/1251 ( 80%)]  Loss: 4.621 (4.47)  Time: 0.180s, 5699.99/s  (0.194s, 5281.89/s)  LR: 9.278e-04  Data: 0.031 (0.031)
Train: 52 [1050/1251 ( 84%)]  Loss: 4.306 (4.46)  Time: 0.176s, 5813.97/s  (0.194s, 5278.57/s)  LR: 9.278e-04  Data: 0.027 (0.031)
Train: 52 [1100/1251 ( 88%)]  Loss: 4.486 (4.46)  Time: 0.176s, 5825.09/s  (0.194s, 5266.17/s)  LR: 9.278e-04  Data: 0.025 (0.031)
Train: 52 [1150/1251 ( 92%)]  Loss: 4.067 (4.44)  Time: 0.151s, 6798.69/s  (0.194s, 5271.89/s)  LR: 9.278e-04  Data: 0.027 (0.031)
Train: 52 [1200/1251 ( 96%)]  Loss: 4.292 (4.44)  Time: 0.169s, 6070.06/s  (0.194s, 5267.19/s)  LR: 9.278e-04  Data: 0.028 (0.031)
Train: 52 [1250/1251 (100%)]  Loss: 4.512 (4.44)  Time: 0.116s, 8840.15/s  (0.194s, 5279.13/s)  LR: 9.278e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.756 (1.756)  Loss:  1.3323 (1.3323)  Acc@1: 75.9766 (75.9766)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.212)  Loss:  1.4428 (1.9896)  Acc@1: 75.3538 (59.0560)  Acc@5: 88.2076 (82.2220)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-52.pth.tar', 59.05600002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)

Train: 53 [   0/1251 (  0%)]  Loss: 4.625 (4.62)  Time: 1.963s,  521.74/s  (1.963s,  521.74/s)  LR: 9.250e-04  Data: 1.832 (1.832)
Train: 53 [  50/1251 (  4%)]  Loss: 4.467 (4.55)  Time: 0.163s, 6270.62/s  (0.224s, 4581.02/s)  LR: 9.250e-04  Data: 0.026 (0.077)
Train: 53 [ 100/1251 (  8%)]  Loss: 4.079 (4.39)  Time: 0.164s, 6238.63/s  (0.207s, 4949.93/s)  LR: 9.250e-04  Data: 0.022 (0.057)
Train: 53 [ 150/1251 ( 12%)]  Loss: 4.303 (4.37)  Time: 0.155s, 6615.63/s  (0.203s, 5050.61/s)  LR: 9.250e-04  Data: 0.030 (0.048)
Train: 53 [ 200/1251 ( 16%)]  Loss: 4.681 (4.43)  Time: 0.185s, 5528.60/s  (0.199s, 5155.57/s)  LR: 9.250e-04  Data: 0.031 (0.043)
Train: 53 [ 250/1251 ( 20%)]  Loss: 4.464 (4.44)  Time: 0.183s, 5595.78/s  (0.197s, 5188.33/s)  LR: 9.250e-04  Data: 0.020 (0.044)
Train: 53 [ 300/1251 ( 24%)]  Loss: 4.578 (4.46)  Time: 0.182s, 5613.99/s  (0.196s, 5226.64/s)  LR: 9.250e-04  Data: 0.020 (0.044)
Train: 53 [ 350/1251 ( 28%)]  Loss: 4.551 (4.47)  Time: 0.203s, 5032.36/s  (0.195s, 5259.01/s)  LR: 9.250e-04  Data: 0.031 (0.042)
Train: 53 [ 400/1251 ( 32%)]  Loss: 4.660 (4.49)  Time: 0.168s, 6100.02/s  (0.194s, 5285.79/s)  LR: 9.250e-04  Data: 0.028 (0.041)
Train: 53 [ 450/1251 ( 36%)]  Loss: 4.606 (4.50)  Time: 0.312s, 3280.61/s  (0.194s, 5283.82/s)  LR: 9.250e-04  Data: 0.029 (0.040)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.290 (4.48)  Time: 0.164s, 6237.31/s  (0.193s, 5297.22/s)  LR: 9.250e-04  Data: 0.026 (0.039)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.531 (4.49)  Time: 0.183s, 5600.29/s  (0.193s, 5295.05/s)  LR: 9.250e-04  Data: 0.023 (0.040)
Train: 53 [ 600/1251 ( 48%)]  Loss: 4.600 (4.50)  Time: 0.169s, 6044.00/s  (0.194s, 5290.82/s)  LR: 9.250e-04  Data: 0.020 (0.041)
Train: 53 [ 650/1251 ( 52%)]  Loss: 4.356 (4.49)  Time: 0.178s, 5744.96/s  (0.194s, 5291.19/s)  LR: 9.250e-04  Data: 0.026 (0.041)
Train: 53 [ 700/1251 ( 56%)]  Loss: 4.729 (4.50)  Time: 0.188s, 5442.08/s  (0.193s, 5294.08/s)  LR: 9.250e-04  Data: 0.029 (0.041)
Train: 53 [ 750/1251 ( 60%)]  Loss: 4.135 (4.48)  Time: 0.167s, 6122.58/s  (0.194s, 5290.11/s)  LR: 9.250e-04  Data: 0.028 (0.040)
Train: 53 [ 800/1251 ( 64%)]  Loss: 4.404 (4.47)  Time: 0.343s, 2986.00/s  (0.194s, 5290.79/s)  LR: 9.250e-04  Data: 0.029 (0.039)
Train: 53 [ 850/1251 ( 68%)]  Loss: 4.379 (4.47)  Time: 0.155s, 6590.41/s  (0.194s, 5289.94/s)  LR: 9.250e-04  Data: 0.028 (0.039)
Train: 53 [ 900/1251 ( 72%)]  Loss: 4.475 (4.47)  Time: 0.173s, 5919.49/s  (0.193s, 5294.51/s)  LR: 9.250e-04  Data: 0.028 (0.038)
Train: 53 [ 950/1251 ( 76%)]  Loss: 4.759 (4.48)  Time: 0.171s, 6000.07/s  (0.194s, 5288.72/s)  LR: 9.250e-04  Data: 0.034 (0.037)
Train: 53 [1000/1251 ( 80%)]  Loss: 4.470 (4.48)  Time: 0.302s, 3386.64/s  (0.194s, 5286.58/s)  LR: 9.250e-04  Data: 0.025 (0.037)
Train: 53 [1050/1251 ( 84%)]  Loss: 4.081 (4.46)  Time: 0.177s, 5784.02/s  (0.194s, 5271.97/s)  LR: 9.250e-04  Data: 0.037 (0.037)
Train: 53 [1100/1251 ( 88%)]  Loss: 4.390 (4.46)  Time: 0.172s, 5951.64/s  (0.194s, 5274.86/s)  LR: 9.250e-04  Data: 0.031 (0.036)
Train: 53 [1150/1251 ( 92%)]  Loss: 4.307 (4.46)  Time: 0.182s, 5614.89/s  (0.194s, 5270.91/s)  LR: 9.250e-04  Data: 0.024 (0.036)
Train: 53 [1200/1251 ( 96%)]  Loss: 4.484 (4.46)  Time: 0.513s, 1995.52/s  (0.194s, 5273.89/s)  LR: 9.250e-04  Data: 0.025 (0.036)
Train: 53 [1250/1251 (100%)]  Loss: 4.520 (4.46)  Time: 0.113s, 9023.61/s  (0.194s, 5290.57/s)  LR: 9.250e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.765 (1.765)  Loss:  1.5498 (1.5498)  Acc@1: 73.4375 (73.4375)  Acc@5: 89.2578 (89.2578)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4994 (2.1486)  Acc@1: 77.1226 (58.0660)  Acc@5: 90.2123 (81.6680)
Train: 54 [   0/1251 (  0%)]  Loss: 4.149 (4.15)  Time: 1.665s,  615.17/s  (1.665s,  615.17/s)  LR: 9.222e-04  Data: 1.541 (1.541)
Train: 54 [  50/1251 (  4%)]  Loss: 4.526 (4.34)  Time: 0.180s, 5680.76/s  (0.225s, 4547.48/s)  LR: 9.222e-04  Data: 0.026 (0.057)
Train: 54 [ 100/1251 (  8%)]  Loss: 4.714 (4.46)  Time: 0.187s, 5469.70/s  (0.208s, 4916.22/s)  LR: 9.222e-04  Data: 0.031 (0.043)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.186 (4.39)  Time: 0.171s, 5986.18/s  (0.203s, 5050.71/s)  LR: 9.222e-04  Data: 0.028 (0.038)
Train: 54 [ 200/1251 ( 16%)]  Loss: 4.823 (4.48)  Time: 0.256s, 4006.14/s  (0.198s, 5164.80/s)  LR: 9.222e-04  Data: 0.027 (0.036)
Train: 54 [ 250/1251 ( 20%)]  Loss: 4.623 (4.50)  Time: 0.172s, 5937.07/s  (0.197s, 5193.53/s)  LR: 9.222e-04  Data: 0.024 (0.034)
Train: 54 [ 300/1251 ( 24%)]  Loss: 4.613 (4.52)  Time: 0.167s, 6119.79/s  (0.195s, 5249.42/s)  LR: 9.222e-04  Data: 0.030 (0.034)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.370 (4.50)  Time: 0.168s, 6109.66/s  (0.195s, 5263.81/s)  LR: 9.222e-04  Data: 0.027 (0.033)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.602 (4.51)  Time: 0.314s, 3262.81/s  (0.194s, 5271.97/s)  LR: 9.222e-04  Data: 0.038 (0.032)
Train: 54 [ 450/1251 ( 36%)]  Loss: 4.016 (4.46)  Time: 0.220s, 4664.62/s  (0.193s, 5311.85/s)  LR: 9.222e-04  Data: 0.026 (0.032)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.542 (4.47)  Time: 0.162s, 6314.59/s  (0.193s, 5303.48/s)  LR: 9.222e-04  Data: 0.026 (0.031)
Train: 54 [ 550/1251 ( 44%)]  Loss: 4.590 (4.48)  Time: 0.166s, 6176.95/s  (0.193s, 5315.73/s)  LR: 9.222e-04  Data: 0.036 (0.031)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.722 (4.50)  Time: 0.165s, 6217.80/s  (0.193s, 5311.89/s)  LR: 9.222e-04  Data: 0.040 (0.031)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.797 (4.52)  Time: 0.364s, 2811.59/s  (0.193s, 5300.16/s)  LR: 9.222e-04  Data: 0.025 (0.031)
Train: 54 [ 700/1251 ( 56%)]  Loss: 4.138 (4.49)  Time: 0.165s, 6217.55/s  (0.193s, 5308.37/s)  LR: 9.222e-04  Data: 0.031 (0.030)
Train: 54 [ 750/1251 ( 60%)]  Loss: 4.185 (4.47)  Time: 0.193s, 5311.90/s  (0.193s, 5312.51/s)  LR: 9.222e-04  Data: 0.026 (0.030)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.246 (4.46)  Time: 0.179s, 5720.07/s  (0.193s, 5311.88/s)  LR: 9.222e-04  Data: 0.022 (0.030)
Train: 54 [ 850/1251 ( 68%)]  Loss: 4.619 (4.47)  Time: 0.169s, 6055.21/s  (0.193s, 5313.15/s)  LR: 9.222e-04  Data: 0.021 (0.030)
Train: 54 [ 900/1251 ( 72%)]  Loss: 4.228 (4.46)  Time: 0.179s, 5724.38/s  (0.193s, 5317.03/s)  LR: 9.222e-04  Data: 0.022 (0.030)
Train: 54 [ 950/1251 ( 76%)]  Loss: 4.425 (4.46)  Time: 0.172s, 5953.12/s  (0.193s, 5313.42/s)  LR: 9.222e-04  Data: 0.025 (0.030)
Train: 54 [1000/1251 ( 80%)]  Loss: 4.363 (4.45)  Time: 0.180s, 5698.65/s  (0.193s, 5310.23/s)  LR: 9.222e-04  Data: 0.027 (0.029)
Train: 54 [1050/1251 ( 84%)]  Loss: 4.657 (4.46)  Time: 0.159s, 6460.38/s  (0.193s, 5308.93/s)  LR: 9.222e-04  Data: 0.023 (0.029)
Train: 54 [1100/1251 ( 88%)]  Loss: 4.668 (4.47)  Time: 0.167s, 6148.43/s  (0.193s, 5297.77/s)  LR: 9.222e-04  Data: 0.032 (0.029)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.420 (4.47)  Time: 0.159s, 6445.62/s  (0.193s, 5302.91/s)  LR: 9.222e-04  Data: 0.029 (0.029)
Train: 54 [1200/1251 ( 96%)]  Loss: 4.865 (4.48)  Time: 0.176s, 5826.49/s  (0.193s, 5301.18/s)  LR: 9.222e-04  Data: 0.021 (0.030)
Train: 54 [1250/1251 (100%)]  Loss: 4.094 (4.47)  Time: 0.113s, 9034.54/s  (0.193s, 5303.81/s)  LR: 9.222e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.713 (1.713)  Loss:  1.3381 (1.3381)  Acc@1: 75.3906 (75.3906)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3156 (1.9465)  Acc@1: 73.9387 (59.9220)  Acc@5: 90.5660 (82.8840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-52.pth.tar', 59.05600002441406)

Train: 55 [   0/1251 (  0%)]  Loss: 4.412 (4.41)  Time: 1.623s,  630.90/s  (1.623s,  630.90/s)  LR: 9.194e-04  Data: 1.473 (1.473)
Train: 55 [  50/1251 (  4%)]  Loss: 4.321 (4.37)  Time: 0.225s, 4552.33/s  (0.218s, 4702.42/s)  LR: 9.194e-04  Data: 0.100 (0.069)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 55 [ 100/1251 (  8%)]  Loss: 4.432 (4.39)  Time: 0.180s, 5675.58/s  (0.203s, 5037.49/s)  LR: 9.194e-04  Data: 0.020 (0.053)
Train: 55 [ 150/1251 ( 12%)]  Loss: 4.302 (4.37)  Time: 0.153s, 6706.09/s  (0.199s, 5152.32/s)  LR: 9.194e-04  Data: 0.025 (0.046)
Train: 55 [ 200/1251 ( 16%)]  Loss: 4.357 (4.36)  Time: 0.160s, 6382.12/s  (0.198s, 5175.07/s)  LR: 9.194e-04  Data: 0.032 (0.042)
Train: 55 [ 250/1251 ( 20%)]  Loss: 4.629 (4.41)  Time: 0.171s, 5974.22/s  (0.196s, 5230.21/s)  LR: 9.194e-04  Data: 0.042 (0.042)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.309 (4.39)  Time: 0.176s, 5828.32/s  (0.194s, 5284.55/s)  LR: 9.194e-04  Data: 0.028 (0.042)
Train: 55 [ 350/1251 ( 28%)]  Loss: 4.588 (4.42)  Time: 0.186s, 5507.81/s  (0.193s, 5293.07/s)  LR: 9.194e-04  Data: 0.028 (0.042)
Train: 55 [ 400/1251 ( 32%)]  Loss: 4.509 (4.43)  Time: 0.172s, 5953.87/s  (0.193s, 5299.38/s)  LR: 9.194e-04  Data: 0.031 (0.041)
Train: 55 [ 450/1251 ( 36%)]  Loss: 4.753 (4.46)  Time: 0.160s, 6395.12/s  (0.192s, 5326.28/s)  LR: 9.194e-04  Data: 0.030 (0.039)
Train: 55 [ 500/1251 ( 40%)]  Loss: 4.904 (4.50)  Time: 0.176s, 5821.52/s  (0.192s, 5325.33/s)  LR: 9.194e-04  Data: 0.027 (0.038)
Train: 55 [ 550/1251 ( 44%)]  Loss: 4.235 (4.48)  Time: 0.164s, 6227.89/s  (0.193s, 5312.47/s)  LR: 9.194e-04  Data: 0.025 (0.037)
Train: 55 [ 600/1251 ( 48%)]  Loss: 4.506 (4.48)  Time: 0.193s, 5293.68/s  (0.193s, 5312.64/s)  LR: 9.194e-04  Data: 0.023 (0.036)
Train: 55 [ 650/1251 ( 52%)]  Loss: 4.143 (4.46)  Time: 0.167s, 6118.62/s  (0.192s, 5333.74/s)  LR: 9.194e-04  Data: 0.033 (0.036)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.369 (4.45)  Time: 0.163s, 6289.56/s  (0.192s, 5329.09/s)  LR: 9.194e-04  Data: 0.023 (0.035)
Train: 55 [ 750/1251 ( 60%)]  Loss: 4.401 (4.45)  Time: 0.190s, 5381.28/s  (0.192s, 5325.91/s)  LR: 9.194e-04  Data: 0.025 (0.035)
Train: 55 [ 800/1251 ( 64%)]  Loss: 4.728 (4.46)  Time: 0.179s, 5708.86/s  (0.193s, 5319.28/s)  LR: 9.194e-04  Data: 0.034 (0.034)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.882 (4.43)  Time: 0.175s, 5866.26/s  (0.193s, 5314.88/s)  LR: 9.194e-04  Data: 0.030 (0.034)
Train: 55 [ 900/1251 ( 72%)]  Loss: 4.202 (4.42)  Time: 0.178s, 5757.31/s  (0.193s, 5313.65/s)  LR: 9.194e-04  Data: 0.028 (0.034)
Train: 55 [ 950/1251 ( 76%)]  Loss: 4.631 (4.43)  Time: 0.163s, 6278.82/s  (0.193s, 5319.14/s)  LR: 9.194e-04  Data: 0.033 (0.033)
Train: 55 [1000/1251 ( 80%)]  Loss: 4.563 (4.44)  Time: 0.181s, 5654.75/s  (0.193s, 5306.93/s)  LR: 9.194e-04  Data: 0.029 (0.033)
Train: 55 [1050/1251 ( 84%)]  Loss: 4.272 (4.43)  Time: 0.161s, 6366.82/s  (0.193s, 5307.35/s)  LR: 9.194e-04  Data: 0.026 (0.033)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.751 (4.44)  Time: 0.167s, 6146.75/s  (0.193s, 5300.41/s)  LR: 9.194e-04  Data: 0.024 (0.033)
Train: 55 [1150/1251 ( 92%)]  Loss: 4.580 (4.45)  Time: 0.237s, 4314.56/s  (0.193s, 5302.70/s)  LR: 9.194e-04  Data: 0.032 (0.032)
Train: 55 [1200/1251 ( 96%)]  Loss: 4.178 (4.44)  Time: 0.161s, 6378.37/s  (0.193s, 5295.59/s)  LR: 9.194e-04  Data: 0.021 (0.032)
Train: 55 [1250/1251 (100%)]  Loss: 4.679 (4.45)  Time: 0.113s, 9036.65/s  (0.193s, 5309.71/s)  LR: 9.194e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.809 (1.809)  Loss:  1.3933 (1.3933)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3494 (2.0297)  Acc@1: 76.5330 (59.9680)  Acc@5: 91.5094 (83.0360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)

Train: 56 [   0/1251 (  0%)]  Loss: 4.435 (4.44)  Time: 1.776s,  576.51/s  (1.776s,  576.51/s)  LR: 9.165e-04  Data: 1.640 (1.640)
Train: 56 [  50/1251 (  4%)]  Loss: 4.901 (4.67)  Time: 0.153s, 6682.69/s  (0.220s, 4645.95/s)  LR: 9.165e-04  Data: 0.022 (0.073)
Train: 56 [ 100/1251 (  8%)]  Loss: 4.515 (4.62)  Time: 0.175s, 5859.60/s  (0.204s, 5007.48/s)  LR: 9.165e-04  Data: 0.026 (0.057)
Train: 56 [ 150/1251 ( 12%)]  Loss: 4.404 (4.56)  Time: 0.170s, 6037.31/s  (0.200s, 5114.44/s)  LR: 9.165e-04  Data: 0.029 (0.048)
Train: 56 [ 200/1251 ( 16%)]  Loss: 4.621 (4.58)  Time: 0.466s, 2197.81/s  (0.200s, 5119.00/s)  LR: 9.165e-04  Data: 0.021 (0.043)
Train: 56 [ 250/1251 ( 20%)]  Loss: 4.331 (4.53)  Time: 0.156s, 6549.08/s  (0.198s, 5165.86/s)  LR: 9.165e-04  Data: 0.026 (0.040)
Train: 56 [ 300/1251 ( 24%)]  Loss: 4.520 (4.53)  Time: 0.172s, 5966.84/s  (0.196s, 5227.17/s)  LR: 9.165e-04  Data: 0.025 (0.038)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.300 (4.50)  Time: 0.159s, 6458.58/s  (0.194s, 5266.06/s)  LR: 9.165e-04  Data: 0.024 (0.037)
Train: 56 [ 400/1251 ( 32%)]  Loss: 4.595 (4.51)  Time: 0.501s, 2043.56/s  (0.195s, 5252.36/s)  LR: 9.165e-04  Data: 0.020 (0.036)
Train: 56 [ 450/1251 ( 36%)]  Loss: 4.296 (4.49)  Time: 0.166s, 6170.05/s  (0.194s, 5285.65/s)  LR: 9.165e-04  Data: 0.034 (0.035)
Train: 56 [ 500/1251 ( 40%)]  Loss: 4.267 (4.47)  Time: 0.242s, 4239.73/s  (0.194s, 5283.71/s)  LR: 9.165e-04  Data: 0.026 (0.034)
Train: 56 [ 550/1251 ( 44%)]  Loss: 4.627 (4.48)  Time: 0.174s, 5883.98/s  (0.194s, 5289.54/s)  LR: 9.165e-04  Data: 0.026 (0.033)
Train: 56 [ 600/1251 ( 48%)]  Loss: 4.550 (4.49)  Time: 0.158s, 6462.13/s  (0.193s, 5307.47/s)  LR: 9.165e-04  Data: 0.025 (0.033)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.466 (4.49)  Time: 0.182s, 5632.63/s  (0.193s, 5299.98/s)  LR: 9.165e-04  Data: 0.021 (0.033)
Train: 56 [ 700/1251 ( 56%)]  Loss: 4.531 (4.49)  Time: 0.170s, 6022.25/s  (0.193s, 5300.03/s)  LR: 9.165e-04  Data: 0.024 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 56 [ 750/1251 ( 60%)]  Loss: 4.195 (4.47)  Time: 0.168s, 6082.64/s  (0.193s, 5295.86/s)  LR: 9.165e-04  Data: 0.026 (0.032)
Train: 56 [ 800/1251 ( 64%)]  Loss: 4.583 (4.48)  Time: 0.434s, 2357.64/s  (0.193s, 5295.76/s)  LR: 9.165e-04  Data: 0.025 (0.032)
Train: 56 [ 850/1251 ( 68%)]  Loss: 4.700 (4.49)  Time: 0.164s, 6225.93/s  (0.193s, 5302.42/s)  LR: 9.165e-04  Data: 0.033 (0.032)
Train: 56 [ 900/1251 ( 72%)]  Loss: 4.503 (4.49)  Time: 0.160s, 6402.04/s  (0.193s, 5307.53/s)  LR: 9.165e-04  Data: 0.029 (0.031)
Train: 56 [ 950/1251 ( 76%)]  Loss: 4.651 (4.50)  Time: 0.161s, 6357.31/s  (0.193s, 5304.27/s)  LR: 9.165e-04  Data: 0.031 (0.031)
Train: 56 [1000/1251 ( 80%)]  Loss: 4.200 (4.49)  Time: 0.182s, 5631.98/s  (0.193s, 5292.39/s)  LR: 9.165e-04  Data: 0.020 (0.031)
Train: 56 [1050/1251 ( 84%)]  Loss: 4.324 (4.48)  Time: 0.167s, 6117.48/s  (0.194s, 5291.30/s)  LR: 9.165e-04  Data: 0.025 (0.031)
Train: 56 [1100/1251 ( 88%)]  Loss: 4.284 (4.47)  Time: 0.190s, 5379.42/s  (0.194s, 5289.66/s)  LR: 9.165e-04  Data: 0.023 (0.031)
Train: 56 [1150/1251 ( 92%)]  Loss: 4.081 (4.45)  Time: 0.181s, 5645.93/s  (0.194s, 5277.83/s)  LR: 9.165e-04  Data: 0.032 (0.031)
Train: 56 [1200/1251 ( 96%)]  Loss: 4.430 (4.45)  Time: 0.162s, 6332.58/s  (0.194s, 5270.78/s)  LR: 9.165e-04  Data: 0.027 (0.032)
Train: 56 [1250/1251 (100%)]  Loss: 5.002 (4.47)  Time: 0.113s, 9033.04/s  (0.194s, 5287.94/s)  LR: 9.165e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.862 (1.862)  Loss:  1.2177 (1.2177)  Acc@1: 76.1719 (76.1719)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3770 (1.9930)  Acc@1: 76.4151 (59.6840)  Acc@5: 89.7406 (82.9200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)

Train: 57 [   0/1251 (  0%)]  Loss: 4.500 (4.50)  Time: 1.705s,  600.53/s  (1.705s,  600.53/s)  LR: 9.136e-04  Data: 1.549 (1.549)
Train: 57 [  50/1251 (  4%)]  Loss: 4.361 (4.43)  Time: 0.167s, 6134.84/s  (0.220s, 4645.90/s)  LR: 9.136e-04  Data: 0.025 (0.059)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.519 (4.46)  Time: 0.173s, 5929.74/s  (0.206s, 4978.93/s)  LR: 9.136e-04  Data: 0.033 (0.044)
Train: 57 [ 150/1251 ( 12%)]  Loss: 4.220 (4.40)  Time: 0.184s, 5553.79/s  (0.201s, 5082.03/s)  LR: 9.136e-04  Data: 0.027 (0.038)
Train: 57 [ 200/1251 ( 16%)]  Loss: 4.125 (4.35)  Time: 0.187s, 5484.38/s  (0.197s, 5203.85/s)  LR: 9.136e-04  Data: 0.031 (0.036)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.927 (4.28)  Time: 0.178s, 5761.29/s  (0.196s, 5213.24/s)  LR: 9.136e-04  Data: 0.030 (0.034)
Train: 57 [ 300/1251 ( 24%)]  Loss: 4.279 (4.28)  Time: 0.170s, 6037.99/s  (0.195s, 5254.02/s)  LR: 9.136e-04  Data: 0.023 (0.033)
Train: 57 [ 350/1251 ( 28%)]  Loss: 4.452 (4.30)  Time: 0.166s, 6164.46/s  (0.193s, 5310.18/s)  LR: 9.136e-04  Data: 0.032 (0.032)
Train: 57 [ 400/1251 ( 32%)]  Loss: 4.571 (4.33)  Time: 0.182s, 5641.85/s  (0.193s, 5295.69/s)  LR: 9.136e-04  Data: 0.034 (0.032)
Train: 57 [ 450/1251 ( 36%)]  Loss: 4.393 (4.33)  Time: 0.174s, 5880.98/s  (0.193s, 5311.46/s)  LR: 9.136e-04  Data: 0.027 (0.032)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.357 (4.34)  Time: 0.176s, 5819.87/s  (0.192s, 5323.21/s)  LR: 9.136e-04  Data: 0.030 (0.031)
Train: 57 [ 550/1251 ( 44%)]  Loss: 4.128 (4.32)  Time: 0.186s, 5508.41/s  (0.192s, 5324.76/s)  LR: 9.136e-04  Data: 0.036 (0.031)
Train: 57 [ 600/1251 ( 48%)]  Loss: 4.236 (4.31)  Time: 0.170s, 6006.77/s  (0.192s, 5322.52/s)  LR: 9.136e-04  Data: 0.031 (0.031)
Train: 57 [ 650/1251 ( 52%)]  Loss: 4.532 (4.33)  Time: 0.195s, 5253.98/s  (0.192s, 5319.54/s)  LR: 9.136e-04  Data: 0.031 (0.031)
Train: 57 [ 700/1251 ( 56%)]  Loss: 4.591 (4.35)  Time: 0.166s, 6170.98/s  (0.193s, 5315.39/s)  LR: 9.136e-04  Data: 0.026 (0.030)
Train: 57 [ 750/1251 ( 60%)]  Loss: 4.400 (4.35)  Time: 0.176s, 5819.21/s  (0.193s, 5318.15/s)  LR: 9.136e-04  Data: 0.029 (0.030)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.438 (4.35)  Time: 0.178s, 5739.78/s  (0.192s, 5325.89/s)  LR: 9.136e-04  Data: 0.033 (0.030)
Train: 57 [ 850/1251 ( 68%)]  Loss: 4.203 (4.35)  Time: 0.182s, 5637.32/s  (0.192s, 5326.23/s)  LR: 9.136e-04  Data: 0.026 (0.030)
Train: 57 [ 900/1251 ( 72%)]  Loss: 4.243 (4.34)  Time: 0.194s, 5289.33/s  (0.192s, 5323.30/s)  LR: 9.136e-04  Data: 0.029 (0.030)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.284 (4.34)  Time: 0.263s, 3896.90/s  (0.193s, 5311.98/s)  LR: 9.136e-04  Data: 0.023 (0.030)
Train: 57 [1000/1251 ( 80%)]  Loss: 4.362 (4.34)  Time: 0.181s, 5642.12/s  (0.193s, 5305.03/s)  LR: 9.136e-04  Data: 0.023 (0.030)
Train: 57 [1050/1251 ( 84%)]  Loss: 4.249 (4.34)  Time: 0.196s, 5228.95/s  (0.193s, 5307.27/s)  LR: 9.136e-04  Data: 0.026 (0.029)
Train: 57 [1100/1251 ( 88%)]  Loss: 4.466 (4.34)  Time: 0.177s, 5798.12/s  (0.193s, 5315.25/s)  LR: 9.136e-04  Data: 0.025 (0.029)
Train: 57 [1150/1251 ( 92%)]  Loss: 4.275 (4.34)  Time: 0.163s, 6300.65/s  (0.193s, 5314.73/s)  LR: 9.136e-04  Data: 0.027 (0.029)
Train: 57 [1200/1251 ( 96%)]  Loss: 4.438 (4.34)  Time: 0.165s, 6212.53/s  (0.193s, 5310.30/s)  LR: 9.136e-04  Data: 0.026 (0.029)
Train: 57 [1250/1251 (100%)]  Loss: 4.395 (4.34)  Time: 0.113s, 9035.74/s  (0.192s, 5323.44/s)  LR: 9.136e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  1.3002 (1.3002)  Acc@1: 78.7109 (78.7109)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2831 (1.9662)  Acc@1: 77.7123 (60.4860)  Acc@5: 91.5094 (83.0260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)

Train: 58 [   0/1251 (  0%)]  Loss: 4.437 (4.44)  Time: 1.751s,  584.70/s  (1.751s,  584.70/s)  LR: 9.107e-04  Data: 1.622 (1.622)
Train: 58 [  50/1251 (  4%)]  Loss: 4.657 (4.55)  Time: 0.178s, 5744.22/s  (0.226s, 4521.62/s)  LR: 9.107e-04  Data: 0.024 (0.081)
Train: 58 [ 100/1251 (  8%)]  Loss: 4.242 (4.45)  Time: 0.188s, 5444.88/s  (0.213s, 4816.63/s)  LR: 9.107e-04  Data: 0.029 (0.068)
Train: 58 [ 150/1251 ( 12%)]  Loss: 4.644 (4.50)  Time: 0.191s, 5352.51/s  (0.203s, 5040.36/s)  LR: 9.107e-04  Data: 0.020 (0.058)
Train: 58 [ 200/1251 ( 16%)]  Loss: 4.487 (4.49)  Time: 0.162s, 6321.62/s  (0.199s, 5134.36/s)  LR: 9.107e-04  Data: 0.028 (0.054)
Train: 58 [ 250/1251 ( 20%)]  Loss: 4.811 (4.55)  Time: 0.182s, 5638.28/s  (0.197s, 5192.06/s)  LR: 9.107e-04  Data: 0.031 (0.052)
Train: 58 [ 300/1251 ( 24%)]  Loss: 4.622 (4.56)  Time: 0.197s, 5189.28/s  (0.196s, 5233.24/s)  LR: 9.107e-04  Data: 0.025 (0.051)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.646 (4.57)  Time: 0.160s, 6382.53/s  (0.194s, 5272.70/s)  LR: 9.107e-04  Data: 0.026 (0.049)
Train: 58 [ 400/1251 ( 32%)]  Loss: 4.259 (4.53)  Time: 0.180s, 5692.85/s  (0.195s, 5262.38/s)  LR: 9.107e-04  Data: 0.024 (0.048)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 58 [ 450/1251 ( 36%)]  Loss: 4.642 (4.54)  Time: 0.364s, 2814.87/s  (0.194s, 5274.94/s)  LR: 9.107e-04  Data: 0.025 (0.046)
Train: 58 [ 500/1251 ( 40%)]  Loss: 4.608 (4.55)  Time: 0.165s, 6202.84/s  (0.194s, 5274.82/s)  LR: 9.107e-04  Data: 0.028 (0.044)
Train: 58 [ 550/1251 ( 44%)]  Loss: 4.712 (4.56)  Time: 0.164s, 6235.80/s  (0.193s, 5297.24/s)  LR: 9.107e-04  Data: 0.032 (0.043)
Train: 58 [ 600/1251 ( 48%)]  Loss: 4.351 (4.55)  Time: 0.196s, 5223.14/s  (0.194s, 5277.51/s)  LR: 9.107e-04  Data: 0.036 (0.041)
Train: 58 [ 650/1251 ( 52%)]  Loss: 4.415 (4.54)  Time: 0.181s, 5672.36/s  (0.194s, 5285.87/s)  LR: 9.107e-04  Data: 0.038 (0.040)
Train: 58 [ 700/1251 ( 56%)]  Loss: 4.225 (4.52)  Time: 0.193s, 5314.24/s  (0.193s, 5292.62/s)  LR: 9.107e-04  Data: 0.025 (0.040)
Train: 58 [ 750/1251 ( 60%)]  Loss: 4.472 (4.51)  Time: 0.168s, 6103.93/s  (0.194s, 5283.44/s)  LR: 9.107e-04  Data: 0.025 (0.039)
Train: 58 [ 800/1251 ( 64%)]  Loss: 4.499 (4.51)  Time: 0.163s, 6277.19/s  (0.193s, 5298.60/s)  LR: 9.107e-04  Data: 0.032 (0.038)
Train: 58 [ 850/1251 ( 68%)]  Loss: 4.568 (4.52)  Time: 0.244s, 4188.89/s  (0.194s, 5286.18/s)  LR: 9.107e-04  Data: 0.029 (0.037)
Train: 58 [ 900/1251 ( 72%)]  Loss: 4.598 (4.52)  Time: 0.181s, 5649.61/s  (0.194s, 5288.11/s)  LR: 9.107e-04  Data: 0.038 (0.037)
Train: 58 [ 950/1251 ( 76%)]  Loss: 4.464 (4.52)  Time: 0.179s, 5733.61/s  (0.194s, 5290.99/s)  LR: 9.107e-04  Data: 0.034 (0.037)
Train: 58 [1000/1251 ( 80%)]  Loss: 4.882 (4.54)  Time: 0.180s, 5703.42/s  (0.193s, 5292.76/s)  LR: 9.107e-04  Data: 0.027 (0.036)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.064 (4.51)  Time: 0.204s, 5031.59/s  (0.194s, 5281.70/s)  LR: 9.107e-04  Data: 0.026 (0.036)
Train: 58 [1100/1251 ( 88%)]  Loss: 4.689 (4.52)  Time: 0.190s, 5390.79/s  (0.194s, 5275.69/s)  LR: 9.107e-04  Data: 0.026 (0.035)
Train: 58 [1150/1251 ( 92%)]  Loss: 4.337 (4.51)  Time: 0.169s, 6075.04/s  (0.194s, 5269.65/s)  LR: 9.107e-04  Data: 0.030 (0.035)
Train: 58 [1200/1251 ( 96%)]  Loss: 4.294 (4.51)  Time: 0.175s, 5859.20/s  (0.194s, 5267.27/s)  LR: 9.107e-04  Data: 0.019 (0.035)
Train: 58 [1250/1251 (100%)]  Loss: 4.536 (4.51)  Time: 0.116s, 8806.61/s  (0.194s, 5288.24/s)  LR: 9.107e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.3208 (1.3208)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3406 (1.9672)  Acc@1: 76.4151 (60.3960)  Acc@5: 90.5660 (83.0440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)

Train: 59 [   0/1251 (  0%)]  Loss: 4.164 (4.16)  Time: 1.750s,  585.06/s  (1.750s,  585.06/s)  LR: 9.077e-04  Data: 1.628 (1.628)
Train: 59 [  50/1251 (  4%)]  Loss: 4.561 (4.36)  Time: 0.162s, 6313.29/s  (0.222s, 4617.02/s)  LR: 9.077e-04  Data: 0.027 (0.073)
Train: 59 [ 100/1251 (  8%)]  Loss: 4.521 (4.42)  Time: 0.167s, 6125.10/s  (0.208s, 4912.72/s)  LR: 9.077e-04  Data: 0.025 (0.051)
Train: 59 [ 150/1251 ( 12%)]  Loss: 4.534 (4.45)  Time: 0.158s, 6486.70/s  (0.202s, 5062.70/s)  LR: 9.077e-04  Data: 0.027 (0.043)
Train: 59 [ 200/1251 ( 16%)]  Loss: 4.492 (4.45)  Time: 0.202s, 5060.31/s  (0.200s, 5120.65/s)  LR: 9.077e-04  Data: 0.031 (0.039)
Train: 59 [ 250/1251 ( 20%)]  Loss: 4.632 (4.48)  Time: 0.187s, 5467.83/s  (0.197s, 5199.36/s)  LR: 9.077e-04  Data: 0.029 (0.037)
Train: 59 [ 300/1251 ( 24%)]  Loss: 4.508 (4.49)  Time: 0.182s, 5614.91/s  (0.195s, 5239.50/s)  LR: 9.077e-04  Data: 0.028 (0.035)
Train: 59 [ 350/1251 ( 28%)]  Loss: 4.383 (4.47)  Time: 0.182s, 5639.66/s  (0.193s, 5293.41/s)  LR: 9.077e-04  Data: 0.024 (0.034)
Train: 59 [ 400/1251 ( 32%)]  Loss: 4.220 (4.45)  Time: 0.164s, 6234.57/s  (0.193s, 5307.54/s)  LR: 9.077e-04  Data: 0.025 (0.034)
Train: 59 [ 450/1251 ( 36%)]  Loss: 4.457 (4.45)  Time: 0.202s, 5067.29/s  (0.193s, 5309.59/s)  LR: 9.077e-04  Data: 0.023 (0.035)
Train: 59 [ 500/1251 ( 40%)]  Loss: 4.340 (4.44)  Time: 0.170s, 6013.02/s  (0.192s, 5327.72/s)  LR: 9.077e-04  Data: 0.029 (0.036)
Train: 59 [ 550/1251 ( 44%)]  Loss: 4.385 (4.43)  Time: 0.190s, 5384.27/s  (0.193s, 5295.39/s)  LR: 9.077e-04  Data: 0.031 (0.036)
Train: 59 [ 600/1251 ( 48%)]  Loss: 4.601 (4.45)  Time: 0.161s, 6350.69/s  (0.193s, 5295.20/s)  LR: 9.077e-04  Data: 0.022 (0.036)
Train: 59 [ 650/1251 ( 52%)]  Loss: 4.473 (4.45)  Time: 0.172s, 5945.32/s  (0.193s, 5292.38/s)  LR: 9.077e-04  Data: 0.025 (0.037)
Train: 59 [ 700/1251 ( 56%)]  Loss: 4.328 (4.44)  Time: 0.199s, 5144.26/s  (0.193s, 5305.26/s)  LR: 9.077e-04  Data: 0.028 (0.037)
Train: 59 [ 750/1251 ( 60%)]  Loss: 4.518 (4.44)  Time: 0.173s, 5932.41/s  (0.193s, 5292.60/s)  LR: 9.077e-04  Data: 0.027 (0.038)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.733 (4.46)  Time: 0.162s, 6334.09/s  (0.193s, 5302.41/s)  LR: 9.077e-04  Data: 0.020 (0.038)
Train: 59 [ 850/1251 ( 68%)]  Loss: 4.041 (4.44)  Time: 0.180s, 5697.40/s  (0.193s, 5305.61/s)  LR: 9.077e-04  Data: 0.030 (0.039)
Train: 59 [ 900/1251 ( 72%)]  Loss: 4.668 (4.45)  Time: 0.228s, 4497.65/s  (0.193s, 5296.58/s)  LR: 9.077e-04  Data: 0.027 (0.039)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.472 (4.45)  Time: 0.172s, 5960.84/s  (0.194s, 5286.52/s)  LR: 9.077e-04  Data: 0.028 (0.040)
Train: 59 [1000/1251 ( 80%)]  Loss: 4.530 (4.46)  Time: 0.177s, 5781.28/s  (0.193s, 5293.88/s)  LR: 9.077e-04  Data: 0.026 (0.040)
Train: 59 [1050/1251 ( 84%)]  Loss: 4.331 (4.45)  Time: 0.185s, 5547.96/s  (0.193s, 5301.97/s)  LR: 9.077e-04  Data: 0.020 (0.040)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.613 (4.46)  Time: 0.173s, 5910.55/s  (0.193s, 5296.33/s)  LR: 9.077e-04  Data: 0.030 (0.041)
Train: 59 [1150/1251 ( 92%)]  Loss: 4.050 (4.44)  Time: 0.157s, 6505.67/s  (0.193s, 5297.44/s)  LR: 9.077e-04  Data: 0.035 (0.041)
Train: 59 [1200/1251 ( 96%)]  Loss: 4.485 (4.44)  Time: 0.165s, 6213.49/s  (0.193s, 5299.52/s)  LR: 9.077e-04  Data: 0.021 (0.040)
Train: 59 [1250/1251 (100%)]  Loss: 4.595 (4.45)  Time: 0.114s, 8998.97/s  (0.193s, 5308.28/s)  LR: 9.077e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.845 (1.845)  Loss:  1.3268 (1.3268)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.3009 (1.9934)  Acc@1: 78.6557 (60.8040)  Acc@5: 92.0991 (83.4660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)

Train: 60 [   0/1251 (  0%)]  Loss: 4.430 (4.43)  Time: 1.717s,  596.25/s  (1.717s,  596.25/s)  LR: 9.046e-04  Data: 1.598 (1.598)
Train: 60 [  50/1251 (  4%)]  Loss: 4.607 (4.52)  Time: 0.171s, 5978.28/s  (0.222s, 4605.14/s)  LR: 9.046e-04  Data: 0.025 (0.077)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.305 (4.45)  Time: 0.155s, 6605.44/s  (0.207s, 4956.62/s)  LR: 9.046e-04  Data: 0.030 (0.054)
Train: 60 [ 150/1251 ( 12%)]  Loss: 4.530 (4.47)  Time: 0.187s, 5466.13/s  (0.201s, 5096.54/s)  LR: 9.046e-04  Data: 0.021 (0.048)
Train: 60 [ 200/1251 ( 16%)]  Loss: 4.580 (4.49)  Time: 0.151s, 6784.46/s  (0.198s, 5165.64/s)  LR: 9.046e-04  Data: 0.023 (0.043)
Train: 60 [ 250/1251 ( 20%)]  Loss: 4.344 (4.47)  Time: 0.185s, 5520.37/s  (0.199s, 5137.54/s)  LR: 9.046e-04  Data: 0.027 (0.043)
Train: 60 [ 300/1251 ( 24%)]  Loss: 4.255 (4.44)  Time: 0.155s, 6592.63/s  (0.197s, 5195.65/s)  LR: 9.046e-04  Data: 0.026 (0.040)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.719 (4.47)  Time: 0.238s, 4303.07/s  (0.195s, 5257.12/s)  LR: 9.046e-04  Data: 0.028 (0.038)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.377 (4.46)  Time: 0.169s, 6041.82/s  (0.194s, 5266.00/s)  LR: 9.046e-04  Data: 0.025 (0.037)
Train: 60 [ 450/1251 ( 36%)]  Loss: 4.320 (4.45)  Time: 0.161s, 6355.85/s  (0.195s, 5259.59/s)  LR: 9.046e-04  Data: 0.024 (0.036)
Train: 60 [ 500/1251 ( 40%)]  Loss: 4.256 (4.43)  Time: 0.188s, 5454.46/s  (0.194s, 5275.85/s)  LR: 9.046e-04  Data: 0.023 (0.035)
Train: 60 [ 550/1251 ( 44%)]  Loss: 4.824 (4.46)  Time: 0.393s, 2603.48/s  (0.195s, 5263.51/s)  LR: 9.046e-04  Data: 0.026 (0.035)
Train: 60 [ 600/1251 ( 48%)]  Loss: 4.677 (4.48)  Time: 0.194s, 5272.18/s  (0.194s, 5285.33/s)  LR: 9.046e-04  Data: 0.021 (0.034)
Train: 60 [ 650/1251 ( 52%)]  Loss: 4.668 (4.49)  Time: 0.176s, 5830.90/s  (0.194s, 5290.46/s)  LR: 9.046e-04  Data: 0.019 (0.034)
Train: 60 [ 700/1251 ( 56%)]  Loss: 4.244 (4.48)  Time: 0.173s, 5926.19/s  (0.193s, 5294.48/s)  LR: 9.046e-04  Data: 0.039 (0.033)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.530 (4.48)  Time: 0.300s, 3417.94/s  (0.193s, 5296.89/s)  LR: 9.046e-04  Data: 0.025 (0.033)
Train: 60 [ 800/1251 ( 64%)]  Loss: 4.638 (4.49)  Time: 0.161s, 6355.01/s  (0.193s, 5299.37/s)  LR: 9.046e-04  Data: 0.035 (0.033)
Train: 60 [ 850/1251 ( 68%)]  Loss: 4.726 (4.50)  Time: 0.155s, 6607.62/s  (0.193s, 5296.26/s)  LR: 9.046e-04  Data: 0.027 (0.032)
Train: 60 [ 900/1251 ( 72%)]  Loss: 4.483 (4.50)  Time: 0.197s, 5207.93/s  (0.193s, 5297.53/s)  LR: 9.046e-04  Data: 0.029 (0.032)
Train: 60 [ 950/1251 ( 76%)]  Loss: 4.769 (4.51)  Time: 0.172s, 5936.88/s  (0.193s, 5296.72/s)  LR: 9.046e-04  Data: 0.025 (0.032)
Train: 60 [1000/1251 ( 80%)]  Loss: 4.703 (4.52)  Time: 0.185s, 5527.69/s  (0.193s, 5295.99/s)  LR: 9.046e-04  Data: 0.036 (0.032)
Train: 60 [1050/1251 ( 84%)]  Loss: 4.585 (4.53)  Time: 0.161s, 6353.76/s  (0.194s, 5284.59/s)  LR: 9.046e-04  Data: 0.027 (0.032)
Train: 60 [1100/1251 ( 88%)]  Loss: 4.819 (4.54)  Time: 0.162s, 6338.65/s  (0.194s, 5286.45/s)  LR: 9.046e-04  Data: 0.026 (0.031)
Train: 60 [1150/1251 ( 92%)]  Loss: 4.665 (4.54)  Time: 0.186s, 5506.91/s  (0.194s, 5284.17/s)  LR: 9.046e-04  Data: 0.027 (0.031)
Train: 60 [1200/1251 ( 96%)]  Loss: 4.487 (4.54)  Time: 0.155s, 6594.32/s  (0.194s, 5267.30/s)  LR: 9.046e-04  Data: 0.029 (0.031)
Train: 60 [1250/1251 (100%)]  Loss: 4.404 (4.54)  Time: 0.114s, 9021.43/s  (0.194s, 5283.57/s)  LR: 9.046e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.2521 (1.2521)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1636 (1.9180)  Acc@1: 77.3585 (60.7140)  Acc@5: 91.9811 (83.4800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)

Train: 61 [   0/1251 (  0%)]  Loss: 4.232 (4.23)  Time: 1.930s,  530.57/s  (1.930s,  530.57/s)  LR: 9.015e-04  Data: 1.815 (1.815)
Train: 61 [  50/1251 (  4%)]  Loss: 4.429 (4.33)  Time: 0.160s, 6385.07/s  (0.228s, 4481.40/s)  LR: 9.015e-04  Data: 0.030 (0.066)
Train: 61 [ 100/1251 (  8%)]  Loss: 4.138 (4.27)  Time: 0.177s, 5769.84/s  (0.211s, 4854.12/s)  LR: 9.015e-04  Data: 0.024 (0.048)
Train: 61 [ 150/1251 ( 12%)]  Loss: 4.550 (4.34)  Time: 0.172s, 5952.70/s  (0.206s, 4967.03/s)  LR: 9.015e-04  Data: 0.019 (0.041)
Train: 61 [ 200/1251 ( 16%)]  Loss: 4.316 (4.33)  Time: 0.159s, 6435.88/s  (0.202s, 5071.90/s)  LR: 9.015e-04  Data: 0.027 (0.038)
Train: 61 [ 250/1251 ( 20%)]  Loss: 4.487 (4.36)  Time: 0.179s, 5734.15/s  (0.200s, 5130.73/s)  LR: 9.015e-04  Data: 0.033 (0.036)
Train: 61 [ 300/1251 ( 24%)]  Loss: 4.477 (4.38)  Time: 0.168s, 6101.00/s  (0.198s, 5165.05/s)  LR: 9.015e-04  Data: 0.032 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 61 [ 350/1251 ( 28%)]  Loss: 4.824 (4.43)  Time: 0.160s, 6402.58/s  (0.197s, 5206.11/s)  LR: 9.015e-04  Data: 0.036 (0.034)
Train: 61 [ 400/1251 ( 32%)]  Loss: 4.469 (4.44)  Time: 0.176s, 5802.51/s  (0.196s, 5236.36/s)  LR: 9.015e-04  Data: 0.025 (0.034)
Train: 61 [ 450/1251 ( 36%)]  Loss: 4.482 (4.44)  Time: 0.169s, 6044.57/s  (0.195s, 5240.18/s)  LR: 9.015e-04  Data: 0.032 (0.033)
Train: 61 [ 500/1251 ( 40%)]  Loss: 4.363 (4.43)  Time: 0.185s, 5547.99/s  (0.195s, 5261.84/s)  LR: 9.015e-04  Data: 0.030 (0.032)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.596 (4.45)  Time: 0.170s, 6040.59/s  (0.194s, 5264.95/s)  LR: 9.015e-04  Data: 0.024 (0.032)
Train: 61 [ 600/1251 ( 48%)]  Loss: 4.228 (4.43)  Time: 0.169s, 6054.72/s  (0.195s, 5261.04/s)  LR: 9.015e-04  Data: 0.027 (0.032)
Train: 61 [ 650/1251 ( 52%)]  Loss: 4.274 (4.42)  Time: 0.169s, 6059.00/s  (0.194s, 5279.10/s)  LR: 9.015e-04  Data: 0.037 (0.032)
Train: 61 [ 700/1251 ( 56%)]  Loss: 4.761 (4.44)  Time: 0.161s, 6348.28/s  (0.194s, 5283.80/s)  LR: 9.015e-04  Data: 0.022 (0.033)
Train: 61 [ 750/1251 ( 60%)]  Loss: 4.263 (4.43)  Time: 0.189s, 5407.32/s  (0.194s, 5270.92/s)  LR: 9.015e-04  Data: 0.026 (0.034)
Train: 61 [ 800/1251 ( 64%)]  Loss: 4.167 (4.42)  Time: 0.325s, 3148.98/s  (0.194s, 5278.01/s)  LR: 9.015e-04  Data: 0.187 (0.034)
Train: 61 [ 850/1251 ( 68%)]  Loss: 4.270 (4.41)  Time: 0.230s, 4449.45/s  (0.194s, 5278.57/s)  LR: 9.015e-04  Data: 0.092 (0.035)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.449 (4.41)  Time: 0.185s, 5531.69/s  (0.194s, 5272.92/s)  LR: 9.015e-04  Data: 0.027 (0.035)
Train: 61 [ 950/1251 ( 76%)]  Loss: 4.235 (4.40)  Time: 0.165s, 6214.79/s  (0.194s, 5279.12/s)  LR: 9.015e-04  Data: 0.023 (0.035)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.738 (4.42)  Time: 0.160s, 6392.40/s  (0.194s, 5267.99/s)  LR: 9.015e-04  Data: 0.028 (0.036)
Train: 61 [1050/1251 ( 84%)]  Loss: 4.313 (4.41)  Time: 0.159s, 6455.22/s  (0.194s, 5270.24/s)  LR: 9.015e-04  Data: 0.032 (0.036)
Train: 61 [1100/1251 ( 88%)]  Loss: 4.691 (4.42)  Time: 0.168s, 6078.64/s  (0.194s, 5279.54/s)  LR: 9.015e-04  Data: 0.023 (0.036)
Train: 61 [1150/1251 ( 92%)]  Loss: 4.409 (4.42)  Time: 0.189s, 5428.22/s  (0.194s, 5265.13/s)  LR: 9.015e-04  Data: 0.028 (0.036)
Train: 61 [1200/1251 ( 96%)]  Loss: 4.333 (4.42)  Time: 0.189s, 5431.57/s  (0.195s, 5259.88/s)  LR: 9.015e-04  Data: 0.029 (0.036)
Train: 61 [1250/1251 (100%)]  Loss: 4.575 (4.43)  Time: 0.113s, 9028.96/s  (0.194s, 5282.01/s)  LR: 9.015e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.889 (1.889)  Loss:  1.2800 (1.2800)  Acc@1: 77.1484 (77.1484)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3598 (2.0005)  Acc@1: 77.5943 (60.7060)  Acc@5: 91.7453 (83.3820)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)

Train: 62 [   0/1251 (  0%)]  Loss: 4.030 (4.03)  Time: 1.754s,  583.66/s  (1.754s,  583.66/s)  LR: 8.984e-04  Data: 1.575 (1.575)
Train: 62 [  50/1251 (  4%)]  Loss: 4.364 (4.20)  Time: 0.179s, 5712.98/s  (0.220s, 4649.14/s)  LR: 8.984e-04  Data: 0.049 (0.067)
Train: 62 [ 100/1251 (  8%)]  Loss: 4.479 (4.29)  Time: 0.179s, 5735.48/s  (0.208s, 4929.21/s)  LR: 8.984e-04  Data: 0.025 (0.049)
Train: 62 [ 150/1251 ( 12%)]  Loss: 4.450 (4.33)  Time: 0.181s, 5665.73/s  (0.201s, 5085.97/s)  LR: 8.984e-04  Data: 0.057 (0.042)
Train: 62 [ 200/1251 ( 16%)]  Loss: 4.313 (4.33)  Time: 0.186s, 5510.57/s  (0.198s, 5183.61/s)  LR: 8.984e-04  Data: 0.028 (0.038)
Train: 62 [ 250/1251 ( 20%)]  Loss: 4.272 (4.32)  Time: 0.164s, 6258.91/s  (0.196s, 5220.67/s)  LR: 8.984e-04  Data: 0.027 (0.040)
Train: 62 [ 300/1251 ( 24%)]  Loss: 4.217 (4.30)  Time: 0.171s, 5983.06/s  (0.195s, 5261.35/s)  LR: 8.984e-04  Data: 0.033 (0.040)
Train: 62 [ 350/1251 ( 28%)]  Loss: 4.592 (4.34)  Time: 0.163s, 6288.03/s  (0.194s, 5269.82/s)  LR: 8.984e-04  Data: 0.026 (0.041)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.596 (4.37)  Time: 0.183s, 5602.35/s  (0.195s, 5261.68/s)  LR: 8.984e-04  Data: 0.026 (0.042)
Train: 62 [ 450/1251 ( 36%)]  Loss: 4.241 (4.36)  Time: 0.174s, 5889.10/s  (0.194s, 5281.09/s)  LR: 8.984e-04  Data: 0.028 (0.041)
Train: 62 [ 500/1251 ( 40%)]  Loss: 4.259 (4.35)  Time: 0.161s, 6349.43/s  (0.193s, 5296.10/s)  LR: 8.984e-04  Data: 0.026 (0.041)
Train: 62 [ 550/1251 ( 44%)]  Loss: 4.912 (4.39)  Time: 0.163s, 6296.70/s  (0.193s, 5312.18/s)  LR: 8.984e-04  Data: 0.025 (0.041)
Train: 62 [ 600/1251 ( 48%)]  Loss: 4.149 (4.38)  Time: 0.203s, 5045.69/s  (0.193s, 5298.21/s)  LR: 8.984e-04  Data: 0.031 (0.042)
Train: 62 [ 650/1251 ( 52%)]  Loss: 4.791 (4.40)  Time: 0.172s, 5968.06/s  (0.193s, 5307.78/s)  LR: 8.984e-04  Data: 0.029 (0.042)
Train: 62 [ 700/1251 ( 56%)]  Loss: 4.571 (4.42)  Time: 0.169s, 6048.23/s  (0.193s, 5292.68/s)  LR: 8.984e-04  Data: 0.025 (0.042)
Train: 62 [ 750/1251 ( 60%)]  Loss: 4.210 (4.40)  Time: 0.177s, 5772.22/s  (0.193s, 5315.07/s)  LR: 8.984e-04  Data: 0.026 (0.042)
Train: 62 [ 800/1251 ( 64%)]  Loss: 4.664 (4.42)  Time: 0.165s, 6220.05/s  (0.193s, 5306.84/s)  LR: 8.984e-04  Data: 0.027 (0.042)
Train: 62 [ 850/1251 ( 68%)]  Loss: 4.714 (4.43)  Time: 0.156s, 6549.41/s  (0.193s, 5316.78/s)  LR: 8.984e-04  Data: 0.021 (0.041)
Train: 62 [ 900/1251 ( 72%)]  Loss: 4.907 (4.46)  Time: 0.165s, 6187.37/s  (0.193s, 5311.17/s)  LR: 8.984e-04  Data: 0.031 (0.041)
Train: 62 [ 950/1251 ( 76%)]  Loss: 4.199 (4.45)  Time: 0.175s, 5862.34/s  (0.193s, 5311.03/s)  LR: 8.984e-04  Data: 0.029 (0.040)
Train: 62 [1000/1251 ( 80%)]  Loss: 4.419 (4.45)  Time: 0.149s, 6861.59/s  (0.193s, 5302.30/s)  LR: 8.984e-04  Data: 0.023 (0.039)
Train: 62 [1050/1251 ( 84%)]  Loss: 4.732 (4.46)  Time: 0.329s, 3109.31/s  (0.193s, 5299.72/s)  LR: 8.984e-04  Data: 0.029 (0.039)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.637 (4.47)  Time: 0.152s, 6726.97/s  (0.193s, 5298.83/s)  LR: 8.984e-04  Data: 0.026 (0.038)
Train: 62 [1150/1251 ( 92%)]  Loss: 4.413 (4.46)  Time: 0.176s, 5802.77/s  (0.193s, 5292.36/s)  LR: 8.984e-04  Data: 0.025 (0.038)
Train: 62 [1200/1251 ( 96%)]  Loss: 4.187 (4.45)  Time: 0.149s, 6886.71/s  (0.194s, 5291.44/s)  LR: 8.984e-04  Data: 0.026 (0.037)
Train: 62 [1250/1251 (100%)]  Loss: 4.169 (4.44)  Time: 0.114s, 8987.71/s  (0.193s, 5301.45/s)  LR: 8.984e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.775 (1.775)  Loss:  1.2710 (1.2710)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1971 (1.8785)  Acc@1: 78.0660 (61.5760)  Acc@5: 91.6274 (83.7420)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)

Train: 63 [   0/1251 (  0%)]  Loss: 3.935 (3.93)  Time: 1.627s,  629.46/s  (1.627s,  629.46/s)  LR: 8.952e-04  Data: 1.502 (1.502)
Train: 63 [  50/1251 (  4%)]  Loss: 4.151 (4.04)  Time: 0.172s, 5954.72/s  (0.226s, 4531.28/s)  LR: 8.952e-04  Data: 0.031 (0.073)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.363 (4.15)  Time: 0.163s, 6300.03/s  (0.207s, 4956.96/s)  LR: 8.952e-04  Data: 0.024 (0.059)
Train: 63 [ 150/1251 ( 12%)]  Loss: 4.252 (4.18)  Time: 0.166s, 6170.59/s  (0.198s, 5171.56/s)  LR: 8.952e-04  Data: 0.027 (0.049)
Train: 63 [ 200/1251 ( 16%)]  Loss: 4.164 (4.17)  Time: 0.167s, 6132.65/s  (0.196s, 5213.55/s)  LR: 8.952e-04  Data: 0.026 (0.046)
Train: 63 [ 250/1251 ( 20%)]  Loss: 4.333 (4.20)  Time: 0.166s, 6173.37/s  (0.195s, 5257.49/s)  LR: 8.952e-04  Data: 0.029 (0.042)
Train: 63 [ 300/1251 ( 24%)]  Loss: 4.257 (4.21)  Time: 0.169s, 6046.58/s  (0.194s, 5279.35/s)  LR: 8.952e-04  Data: 0.031 (0.040)
Train: 63 [ 350/1251 ( 28%)]  Loss: 4.374 (4.23)  Time: 0.178s, 5749.32/s  (0.193s, 5305.13/s)  LR: 8.952e-04  Data: 0.026 (0.038)
Train: 63 [ 400/1251 ( 32%)]  Loss: 4.439 (4.25)  Time: 0.169s, 6046.47/s  (0.193s, 5314.32/s)  LR: 8.952e-04  Data: 0.030 (0.037)
Train: 63 [ 450/1251 ( 36%)]  Loss: 4.451 (4.27)  Time: 0.174s, 5895.06/s  (0.193s, 5312.87/s)  LR: 8.952e-04  Data: 0.032 (0.036)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.660 (4.31)  Time: 0.157s, 6527.51/s  (0.192s, 5327.61/s)  LR: 8.952e-04  Data: 0.024 (0.035)
Train: 63 [ 550/1251 ( 44%)]  Loss: 4.542 (4.33)  Time: 0.160s, 6393.03/s  (0.193s, 5318.66/s)  LR: 8.952e-04  Data: 0.025 (0.035)
Train: 63 [ 600/1251 ( 48%)]  Loss: 4.353 (4.33)  Time: 0.186s, 5509.28/s  (0.192s, 5326.75/s)  LR: 8.952e-04  Data: 0.030 (0.034)
Train: 63 [ 650/1251 ( 52%)]  Loss: 4.398 (4.33)  Time: 0.179s, 5711.73/s  (0.192s, 5327.94/s)  LR: 8.952e-04  Data: 0.020 (0.034)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.868 (4.30)  Time: 0.158s, 6462.60/s  (0.192s, 5336.94/s)  LR: 8.952e-04  Data: 0.023 (0.033)
Train: 63 [ 750/1251 ( 60%)]  Loss: 4.182 (4.30)  Time: 0.159s, 6443.38/s  (0.192s, 5332.33/s)  LR: 8.952e-04  Data: 0.032 (0.034)
Train: 63 [ 800/1251 ( 64%)]  Loss: 4.229 (4.29)  Time: 0.170s, 6015.55/s  (0.192s, 5334.54/s)  LR: 8.952e-04  Data: 0.028 (0.033)
Train: 63 [ 850/1251 ( 68%)]  Loss: 4.657 (4.31)  Time: 0.194s, 5290.66/s  (0.192s, 5336.80/s)  LR: 8.952e-04  Data: 0.023 (0.034)
Train: 63 [ 900/1251 ( 72%)]  Loss: 4.552 (4.32)  Time: 0.161s, 6367.55/s  (0.192s, 5335.76/s)  LR: 8.952e-04  Data: 0.026 (0.033)
Train: 63 [ 950/1251 ( 76%)]  Loss: 4.295 (4.32)  Time: 0.174s, 5877.37/s  (0.192s, 5336.57/s)  LR: 8.952e-04  Data: 0.024 (0.034)
Train: 63 [1000/1251 ( 80%)]  Loss: 4.464 (4.33)  Time: 0.329s, 3114.84/s  (0.192s, 5330.27/s)  LR: 8.952e-04  Data: 0.044 (0.034)
Train: 63 [1050/1251 ( 84%)]  Loss: 4.258 (4.33)  Time: 0.182s, 5624.81/s  (0.192s, 5324.84/s)  LR: 8.952e-04  Data: 0.024 (0.034)
Train: 63 [1100/1251 ( 88%)]  Loss: 4.498 (4.33)  Time: 0.171s, 5998.62/s  (0.192s, 5320.81/s)  LR: 8.952e-04  Data: 0.027 (0.035)
Train: 63 [1150/1251 ( 92%)]  Loss: 4.322 (4.33)  Time: 0.157s, 6532.11/s  (0.193s, 5319.28/s)  LR: 8.952e-04  Data: 0.021 (0.036)
Train: 63 [1200/1251 ( 96%)]  Loss: 4.308 (4.33)  Time: 0.177s, 5786.17/s  (0.193s, 5316.63/s)  LR: 8.952e-04  Data: 0.022 (0.036)
Train: 63 [1250/1251 (100%)]  Loss: 4.270 (4.33)  Time: 0.114s, 9011.49/s  (0.192s, 5330.60/s)  LR: 8.952e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.853 (1.853)  Loss:  1.2530 (1.2530)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.2788 (1.9649)  Acc@1: 75.9434 (60.5320)  Acc@5: 90.6840 (83.0520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)

Train: 64 [   0/1251 (  0%)]  Loss: 4.596 (4.60)  Time: 1.894s,  540.68/s  (1.894s,  540.68/s)  LR: 8.920e-04  Data: 1.769 (1.769)
Train: 64 [  50/1251 (  4%)]  Loss: 4.577 (4.59)  Time: 0.190s, 5380.18/s  (0.232s, 4420.99/s)  LR: 8.920e-04  Data: 0.023 (0.078)
Train: 64 [ 100/1251 (  8%)]  Loss: 4.439 (4.54)  Time: 0.178s, 5752.65/s  (0.209s, 4890.63/s)  LR: 8.920e-04  Data: 0.024 (0.058)
Train: 64 [ 150/1251 ( 12%)]  Loss: 4.261 (4.47)  Time: 0.167s, 6144.51/s  (0.201s, 5102.06/s)  LR: 8.920e-04  Data: 0.034 (0.052)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.363 (4.45)  Time: 0.185s, 5542.43/s  (0.198s, 5167.03/s)  LR: 8.920e-04  Data: 0.049 (0.047)
Train: 64 [ 250/1251 ( 20%)]  Loss: 4.865 (4.52)  Time: 0.178s, 5758.17/s  (0.197s, 5195.62/s)  LR: 8.920e-04  Data: 0.021 (0.044)
Train: 64 [ 300/1251 ( 24%)]  Loss: 4.490 (4.51)  Time: 0.173s, 5912.62/s  (0.195s, 5251.20/s)  LR: 8.920e-04  Data: 0.025 (0.042)
Train: 64 [ 350/1251 ( 28%)]  Loss: 4.363 (4.49)  Time: 0.177s, 5786.50/s  (0.199s, 5144.25/s)  LR: 8.920e-04  Data: 0.030 (0.042)
Train: 64 [ 400/1251 ( 32%)]  Loss: 4.749 (4.52)  Time: 0.179s, 5728.49/s  (0.197s, 5185.51/s)  LR: 8.920e-04  Data: 0.028 (0.040)
Train: 64 [ 450/1251 ( 36%)]  Loss: 4.728 (4.54)  Time: 0.187s, 5480.30/s  (0.196s, 5216.53/s)  LR: 8.920e-04  Data: 0.039 (0.039)
Train: 64 [ 500/1251 ( 40%)]  Loss: 4.612 (4.55)  Time: 0.175s, 5845.62/s  (0.196s, 5237.10/s)  LR: 8.920e-04  Data: 0.032 (0.038)
Train: 64 [ 550/1251 ( 44%)]  Loss: 4.403 (4.54)  Time: 0.183s, 5593.25/s  (0.195s, 5260.83/s)  LR: 8.920e-04  Data: 0.032 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 64 [ 600/1251 ( 48%)]  Loss: 4.492 (4.53)  Time: 0.351s, 2918.06/s  (0.195s, 5253.84/s)  LR: 8.920e-04  Data: 0.019 (0.036)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.092 (4.50)  Time: 0.176s, 5808.54/s  (0.195s, 5261.02/s)  LR: 8.920e-04  Data: 0.026 (0.036)
Train: 64 [ 700/1251 ( 56%)]  Loss: 4.837 (4.52)  Time: 0.192s, 5344.76/s  (0.194s, 5266.39/s)  LR: 8.920e-04  Data: 0.025 (0.035)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.540 (4.53)  Time: 0.177s, 5783.70/s  (0.194s, 5276.67/s)  LR: 8.920e-04  Data: 0.032 (0.035)
Train: 64 [ 800/1251 ( 64%)]  Loss: 4.542 (4.53)  Time: 0.157s, 6517.73/s  (0.194s, 5275.09/s)  LR: 8.920e-04  Data: 0.029 (0.035)
Train: 64 [ 850/1251 ( 68%)]  Loss: 3.829 (4.49)  Time: 0.205s, 5002.44/s  (0.194s, 5275.10/s)  LR: 8.920e-04  Data: 0.025 (0.034)
Train: 64 [ 900/1251 ( 72%)]  Loss: 4.458 (4.49)  Time: 0.181s, 5659.55/s  (0.194s, 5279.02/s)  LR: 8.920e-04  Data: 0.019 (0.034)
Train: 64 [ 950/1251 ( 76%)]  Loss: 4.617 (4.49)  Time: 0.156s, 6578.53/s  (0.194s, 5285.73/s)  LR: 8.920e-04  Data: 0.035 (0.034)
Train: 64 [1000/1251 ( 80%)]  Loss: 4.089 (4.47)  Time: 0.166s, 6180.74/s  (0.194s, 5284.52/s)  LR: 8.920e-04  Data: 0.022 (0.035)
Train: 64 [1050/1251 ( 84%)]  Loss: 4.623 (4.48)  Time: 0.156s, 6568.85/s  (0.194s, 5284.60/s)  LR: 8.920e-04  Data: 0.031 (0.035)
Train: 64 [1100/1251 ( 88%)]  Loss: 4.608 (4.49)  Time: 0.182s, 5620.90/s  (0.194s, 5278.88/s)  LR: 8.920e-04  Data: 0.024 (0.036)
Train: 64 [1150/1251 ( 92%)]  Loss: 4.642 (4.49)  Time: 0.219s, 4682.05/s  (0.194s, 5276.45/s)  LR: 8.920e-04  Data: 0.095 (0.036)
Train: 64 [1200/1251 ( 96%)]  Loss: 4.415 (4.49)  Time: 0.156s, 6565.37/s  (0.194s, 5279.52/s)  LR: 8.920e-04  Data: 0.031 (0.036)
Train: 64 [1250/1251 (100%)]  Loss: 3.924 (4.47)  Time: 0.113s, 9091.03/s  (0.193s, 5292.12/s)  LR: 8.920e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.842 (1.842)  Loss:  1.2874 (1.2874)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3505 (1.9579)  Acc@1: 77.3585 (60.2460)  Acc@5: 91.7453 (82.9140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-64.pth.tar', 60.245999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)

Train: 65 [   0/1251 (  0%)]  Loss: 4.412 (4.41)  Time: 1.815s,  564.16/s  (1.815s,  564.16/s)  LR: 8.887e-04  Data: 1.684 (1.684)
Train: 65 [  50/1251 (  4%)]  Loss: 4.520 (4.47)  Time: 0.297s, 3453.41/s  (0.222s, 4608.90/s)  LR: 8.887e-04  Data: 0.038 (0.064)
Train: 65 [ 100/1251 (  8%)]  Loss: 4.410 (4.45)  Time: 0.195s, 5239.73/s  (0.209s, 4905.53/s)  LR: 8.887e-04  Data: 0.022 (0.047)
Train: 65 [ 150/1251 ( 12%)]  Loss: 4.634 (4.49)  Time: 0.173s, 5926.83/s  (0.199s, 5157.33/s)  LR: 8.887e-04  Data: 0.025 (0.040)
Train: 65 [ 200/1251 ( 16%)]  Loss: 4.613 (4.52)  Time: 0.256s, 3999.86/s  (0.197s, 5185.36/s)  LR: 8.887e-04  Data: 0.033 (0.041)
Train: 65 [ 250/1251 ( 20%)]  Loss: 4.788 (4.56)  Time: 0.164s, 6250.94/s  (0.196s, 5221.36/s)  LR: 8.887e-04  Data: 0.029 (0.041)
Train: 65 [ 300/1251 ( 24%)]  Loss: 4.427 (4.54)  Time: 0.200s, 5130.69/s  (0.195s, 5252.82/s)  LR: 8.887e-04  Data: 0.024 (0.042)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.439 (4.53)  Time: 0.171s, 5974.01/s  (0.195s, 5260.04/s)  LR: 8.887e-04  Data: 0.019 (0.043)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.266 (4.50)  Time: 0.158s, 6499.42/s  (0.194s, 5282.49/s)  LR: 8.887e-04  Data: 0.024 (0.043)
Train: 65 [ 450/1251 ( 36%)]  Loss: 4.275 (4.48)  Time: 0.160s, 6392.05/s  (0.193s, 5307.56/s)  LR: 8.887e-04  Data: 0.028 (0.042)
Train: 65 [ 500/1251 ( 40%)]  Loss: 4.392 (4.47)  Time: 0.186s, 5501.15/s  (0.193s, 5301.98/s)  LR: 8.887e-04  Data: 0.026 (0.042)
Train: 65 [ 550/1251 ( 44%)]  Loss: 4.460 (4.47)  Time: 0.176s, 5802.49/s  (0.193s, 5305.62/s)  LR: 8.887e-04  Data: 0.028 (0.043)
Train: 65 [ 600/1251 ( 48%)]  Loss: 4.285 (4.46)  Time: 0.171s, 5979.85/s  (0.193s, 5315.06/s)  LR: 8.887e-04  Data: 0.023 (0.043)
Train: 65 [ 650/1251 ( 52%)]  Loss: 4.032 (4.43)  Time: 0.191s, 5372.50/s  (0.193s, 5316.88/s)  LR: 8.887e-04  Data: 0.022 (0.042)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 65 [ 700/1251 ( 56%)]  Loss: 4.294 (4.42)  Time: 0.171s, 5996.87/s  (0.192s, 5324.53/s)  LR: 8.887e-04  Data: 0.022 (0.041)
Train: 65 [ 750/1251 ( 60%)]  Loss: 4.172 (4.40)  Time: 0.164s, 6248.80/s  (0.193s, 5312.53/s)  LR: 8.887e-04  Data: 0.019 (0.040)
Train: 65 [ 800/1251 ( 64%)]  Loss: 4.213 (4.39)  Time: 0.279s, 3667.19/s  (0.193s, 5310.38/s)  LR: 8.887e-04  Data: 0.026 (0.039)
Train: 65 [ 850/1251 ( 68%)]  Loss: 4.569 (4.40)  Time: 0.185s, 5541.72/s  (0.193s, 5310.04/s)  LR: 8.887e-04  Data: 0.039 (0.039)
Train: 65 [ 900/1251 ( 72%)]  Loss: 4.386 (4.40)  Time: 0.160s, 6416.49/s  (0.193s, 5313.34/s)  LR: 8.887e-04  Data: 0.027 (0.038)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.333 (4.40)  Time: 0.176s, 5811.62/s  (0.193s, 5307.59/s)  LR: 8.887e-04  Data: 0.028 (0.037)
Train: 65 [1000/1251 ( 80%)]  Loss: 4.305 (4.39)  Time: 0.244s, 4198.47/s  (0.193s, 5311.37/s)  LR: 8.887e-04  Data: 0.025 (0.037)
Train: 65 [1050/1251 ( 84%)]  Loss: 4.370 (4.39)  Time: 0.189s, 5431.66/s  (0.193s, 5314.28/s)  LR: 8.887e-04  Data: 0.027 (0.036)
Train: 65 [1100/1251 ( 88%)]  Loss: 4.644 (4.40)  Time: 0.176s, 5816.78/s  (0.193s, 5301.14/s)  LR: 8.887e-04  Data: 0.030 (0.036)
Train: 65 [1150/1251 ( 92%)]  Loss: 4.417 (4.40)  Time: 0.178s, 5748.37/s  (0.193s, 5303.66/s)  LR: 8.887e-04  Data: 0.031 (0.036)
Train: 65 [1200/1251 ( 96%)]  Loss: 4.629 (4.41)  Time: 0.197s, 5193.15/s  (0.193s, 5299.97/s)  LR: 8.887e-04  Data: 0.030 (0.036)
Train: 65 [1250/1251 (100%)]  Loss: 4.701 (4.42)  Time: 0.114s, 9019.82/s  (0.193s, 5313.49/s)  LR: 8.887e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  1.2910 (1.2910)  Acc@1: 75.7812 (75.7812)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2445 (1.9490)  Acc@1: 76.0613 (61.1340)  Acc@5: 92.0991 (83.8600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-64.pth.tar', 60.245999990234374)

Train: 66 [   0/1251 (  0%)]  Loss: 4.427 (4.43)  Time: 1.683s,  608.56/s  (1.683s,  608.56/s)  LR: 8.854e-04  Data: 1.569 (1.569)
Train: 66 [  50/1251 (  4%)]  Loss: 4.411 (4.42)  Time: 0.175s, 5836.50/s  (0.222s, 4607.42/s)  LR: 8.854e-04  Data: 0.025 (0.069)
Train: 66 [ 100/1251 (  8%)]  Loss: 4.400 (4.41)  Time: 0.170s, 6030.40/s  (0.208s, 4920.63/s)  LR: 8.854e-04  Data: 0.025 (0.050)
Train: 66 [ 150/1251 ( 12%)]  Loss: 4.658 (4.47)  Time: 0.181s, 5653.93/s  (0.201s, 5097.15/s)  LR: 8.854e-04  Data: 0.019 (0.043)
Train: 66 [ 200/1251 ( 16%)]  Loss: 4.664 (4.51)  Time: 0.168s, 6113.34/s  (0.198s, 5180.81/s)  LR: 8.854e-04  Data: 0.026 (0.042)
Train: 66 [ 250/1251 ( 20%)]  Loss: 4.341 (4.48)  Time: 0.186s, 5494.69/s  (0.197s, 5184.94/s)  LR: 8.854e-04  Data: 0.026 (0.040)
Train: 66 [ 300/1251 ( 24%)]  Loss: 4.155 (4.44)  Time: 0.185s, 5549.32/s  (0.196s, 5226.68/s)  LR: 8.854e-04  Data: 0.025 (0.040)
Train: 66 [ 350/1251 ( 28%)]  Loss: 4.419 (4.43)  Time: 0.164s, 6245.53/s  (0.194s, 5268.54/s)  LR: 8.854e-04  Data: 0.030 (0.040)
Train: 66 [ 400/1251 ( 32%)]  Loss: 4.463 (4.44)  Time: 0.346s, 2962.21/s  (0.194s, 5279.63/s)  LR: 8.854e-04  Data: 0.223 (0.041)
Train: 66 [ 450/1251 ( 36%)]  Loss: 4.724 (4.47)  Time: 0.197s, 5186.49/s  (0.194s, 5278.51/s)  LR: 8.854e-04  Data: 0.028 (0.041)
Train: 66 [ 500/1251 ( 40%)]  Loss: 4.713 (4.49)  Time: 0.160s, 6416.68/s  (0.193s, 5295.36/s)  LR: 8.854e-04  Data: 0.021 (0.041)
Train: 66 [ 550/1251 ( 44%)]  Loss: 4.312 (4.47)  Time: 0.197s, 5204.34/s  (0.193s, 5299.18/s)  LR: 8.854e-04  Data: 0.020 (0.041)
Train: 66 [ 600/1251 ( 48%)]  Loss: 4.238 (4.46)  Time: 0.330s, 3106.88/s  (0.193s, 5302.25/s)  LR: 8.854e-04  Data: 0.197 (0.040)
Train: 66 [ 650/1251 ( 52%)]  Loss: 4.375 (4.45)  Time: 0.165s, 6214.92/s  (0.193s, 5292.07/s)  LR: 8.854e-04  Data: 0.030 (0.041)
Train: 66 [ 700/1251 ( 56%)]  Loss: 4.455 (4.45)  Time: 0.166s, 6153.35/s  (0.193s, 5301.42/s)  LR: 8.854e-04  Data: 0.025 (0.041)
Train: 66 [ 750/1251 ( 60%)]  Loss: 4.435 (4.45)  Time: 0.184s, 5577.53/s  (0.193s, 5315.66/s)  LR: 8.854e-04  Data: 0.029 (0.041)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.710 (4.46)  Time: 0.241s, 4244.51/s  (0.193s, 5312.95/s)  LR: 8.854e-04  Data: 0.108 (0.041)
Train: 66 [ 850/1251 ( 68%)]  Loss: 4.632 (4.47)  Time: 0.162s, 6317.27/s  (0.193s, 5309.31/s)  LR: 8.854e-04  Data: 0.026 (0.041)
Train: 66 [ 900/1251 ( 72%)]  Loss: 4.280 (4.46)  Time: 0.193s, 5312.41/s  (0.193s, 5299.12/s)  LR: 8.854e-04  Data: 0.035 (0.041)
Train: 66 [ 950/1251 ( 76%)]  Loss: 4.419 (4.46)  Time: 0.429s, 2385.08/s  (0.193s, 5299.68/s)  LR: 8.854e-04  Data: 0.024 (0.041)
Train: 66 [1000/1251 ( 80%)]  Loss: 3.975 (4.44)  Time: 0.172s, 5960.77/s  (0.193s, 5295.95/s)  LR: 8.854e-04  Data: 0.042 (0.040)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.283 (4.43)  Time: 0.177s, 5796.83/s  (0.193s, 5296.97/s)  LR: 8.854e-04  Data: 0.028 (0.040)
Train: 66 [1100/1251 ( 88%)]  Loss: 4.564 (4.44)  Time: 0.174s, 5870.30/s  (0.193s, 5296.25/s)  LR: 8.854e-04  Data: 0.026 (0.040)
Train: 66 [1150/1251 ( 92%)]  Loss: 4.275 (4.43)  Time: 0.270s, 3791.59/s  (0.193s, 5292.40/s)  LR: 8.854e-04  Data: 0.034 (0.039)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.180 (4.42)  Time: 0.239s, 4285.40/s  (0.193s, 5295.58/s)  LR: 8.854e-04  Data: 0.032 (0.039)
Train: 66 [1250/1251 (100%)]  Loss: 4.858 (4.44)  Time: 0.114s, 8985.17/s  (0.193s, 5308.21/s)  LR: 8.854e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.883 (1.883)  Loss:  1.2578 (1.2578)  Acc@1: 75.7812 (75.7812)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.3612 (1.9722)  Acc@1: 76.7689 (60.6980)  Acc@5: 89.5047 (83.0500)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)

Train: 67 [   0/1251 (  0%)]  Loss: 4.375 (4.38)  Time: 1.722s,  594.55/s  (1.722s,  594.55/s)  LR: 8.820e-04  Data: 1.592 (1.592)
Train: 67 [  50/1251 (  4%)]  Loss: 4.508 (4.44)  Time: 0.173s, 5904.31/s  (0.221s, 4625.79/s)  LR: 8.820e-04  Data: 0.026 (0.069)
Train: 67 [ 100/1251 (  8%)]  Loss: 4.520 (4.47)  Time: 0.188s, 5454.89/s  (0.206s, 4969.36/s)  LR: 8.820e-04  Data: 0.026 (0.049)
Train: 67 [ 150/1251 ( 12%)]  Loss: 4.096 (4.37)  Time: 0.205s, 5000.42/s  (0.198s, 5174.52/s)  LR: 8.820e-04  Data: 0.022 (0.042)
Train: 67 [ 200/1251 ( 16%)]  Loss: 4.089 (4.32)  Time: 0.162s, 6329.93/s  (0.198s, 5179.04/s)  LR: 8.820e-04  Data: 0.025 (0.044)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.707 (4.38)  Time: 0.169s, 6065.42/s  (0.195s, 5242.11/s)  LR: 8.820e-04  Data: 0.026 (0.043)
Train: 67 [ 300/1251 ( 24%)]  Loss: 4.216 (4.36)  Time: 0.168s, 6077.53/s  (0.195s, 5249.73/s)  LR: 8.820e-04  Data: 0.027 (0.042)
Train: 67 [ 350/1251 ( 28%)]  Loss: 4.526 (4.38)  Time: 0.184s, 5562.25/s  (0.195s, 5254.54/s)  LR: 8.820e-04  Data: 0.032 (0.040)
Train: 67 [ 400/1251 ( 32%)]  Loss: 4.240 (4.36)  Time: 0.160s, 6405.02/s  (0.194s, 5267.28/s)  LR: 8.820e-04  Data: 0.030 (0.038)
Train: 67 [ 450/1251 ( 36%)]  Loss: 4.057 (4.33)  Time: 0.157s, 6521.15/s  (0.194s, 5277.62/s)  LR: 8.820e-04  Data: 0.031 (0.037)
Train: 67 [ 500/1251 ( 40%)]  Loss: 4.768 (4.37)  Time: 0.186s, 5491.38/s  (0.193s, 5303.60/s)  LR: 8.820e-04  Data: 0.027 (0.036)
Train: 67 [ 550/1251 ( 44%)]  Loss: 4.380 (4.37)  Time: 0.147s, 6986.79/s  (0.193s, 5292.96/s)  LR: 8.820e-04  Data: 0.027 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 67 [ 600/1251 ( 48%)]  Loss: 4.558 (4.39)  Time: 0.155s, 6586.15/s  (0.193s, 5307.65/s)  LR: 8.820e-04  Data: 0.035 (0.035)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.960 (4.36)  Time: 0.186s, 5518.98/s  (0.193s, 5303.20/s)  LR: 8.820e-04  Data: 0.022 (0.034)
Train: 67 [ 700/1251 ( 56%)]  Loss: 4.527 (4.37)  Time: 0.160s, 6403.14/s  (0.194s, 5291.93/s)  LR: 8.820e-04  Data: 0.029 (0.034)
Train: 67 [ 750/1251 ( 60%)]  Loss: 4.418 (4.37)  Time: 0.183s, 5600.92/s  (0.194s, 5288.08/s)  LR: 8.820e-04  Data: 0.023 (0.033)
Train: 67 [ 800/1251 ( 64%)]  Loss: 4.828 (4.40)  Time: 0.158s, 6497.10/s  (0.193s, 5301.36/s)  LR: 8.820e-04  Data: 0.036 (0.033)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.504 (4.40)  Time: 0.197s, 5195.17/s  (0.193s, 5299.89/s)  LR: 8.820e-04  Data: 0.027 (0.033)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.380 (4.40)  Time: 0.178s, 5742.59/s  (0.193s, 5295.79/s)  LR: 8.820e-04  Data: 0.031 (0.033)
Train: 67 [ 950/1251 ( 76%)]  Loss: 4.462 (4.41)  Time: 0.178s, 5753.70/s  (0.193s, 5292.77/s)  LR: 8.820e-04  Data: 0.023 (0.032)
Train: 67 [1000/1251 ( 80%)]  Loss: 4.553 (4.41)  Time: 0.170s, 6006.92/s  (0.193s, 5301.91/s)  LR: 8.820e-04  Data: 0.032 (0.032)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.552 (4.42)  Time: 0.163s, 6294.01/s  (0.193s, 5306.05/s)  LR: 8.820e-04  Data: 0.027 (0.032)
Train: 67 [1100/1251 ( 88%)]  Loss: 4.355 (4.42)  Time: 0.167s, 6128.91/s  (0.193s, 5299.65/s)  LR: 8.820e-04  Data: 0.029 (0.032)
Train: 67 [1150/1251 ( 92%)]  Loss: 4.477 (4.42)  Time: 0.179s, 5723.08/s  (0.193s, 5295.14/s)  LR: 8.820e-04  Data: 0.042 (0.032)
Train: 67 [1200/1251 ( 96%)]  Loss: 4.537 (4.42)  Time: 0.178s, 5762.13/s  (0.193s, 5292.14/s)  LR: 8.820e-04  Data: 0.029 (0.031)
Train: 67 [1250/1251 (100%)]  Loss: 4.766 (4.44)  Time: 0.113s, 9052.29/s  (0.193s, 5300.05/s)  LR: 8.820e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.847 (1.847)  Loss:  1.2376 (1.2376)  Acc@1: 78.3203 (78.3203)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3623 (1.8988)  Acc@1: 77.4764 (61.7560)  Acc@5: 91.7453 (84.0040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)

Train: 68 [   0/1251 (  0%)]  Loss: 4.336 (4.34)  Time: 1.736s,  589.86/s  (1.736s,  589.86/s)  LR: 8.786e-04  Data: 1.615 (1.615)
Train: 68 [  50/1251 (  4%)]  Loss: 4.634 (4.48)  Time: 0.163s, 6295.34/s  (0.225s, 4544.23/s)  LR: 8.786e-04  Data: 0.029 (0.074)
Train: 68 [ 100/1251 (  8%)]  Loss: 4.197 (4.39)  Time: 0.154s, 6654.69/s  (0.204s, 5014.73/s)  LR: 8.786e-04  Data: 0.035 (0.051)
Train: 68 [ 150/1251 ( 12%)]  Loss: 4.660 (4.46)  Time: 0.174s, 5896.71/s  (0.200s, 5113.32/s)  LR: 8.786e-04  Data: 0.024 (0.044)
Train: 68 [ 200/1251 ( 16%)]  Loss: 4.544 (4.47)  Time: 0.175s, 5853.01/s  (0.197s, 5185.70/s)  LR: 8.786e-04  Data: 0.025 (0.040)
Train: 68 [ 250/1251 ( 20%)]  Loss: 4.610 (4.50)  Time: 0.180s, 5690.09/s  (0.196s, 5234.02/s)  LR: 8.786e-04  Data: 0.034 (0.038)
Train: 68 [ 300/1251 ( 24%)]  Loss: 4.727 (4.53)  Time: 0.177s, 5770.74/s  (0.194s, 5265.41/s)  LR: 8.786e-04  Data: 0.024 (0.036)
Train: 68 [ 350/1251 ( 28%)]  Loss: 4.274 (4.50)  Time: 0.186s, 5511.55/s  (0.193s, 5299.69/s)  LR: 8.786e-04  Data: 0.023 (0.035)
Train: 68 [ 400/1251 ( 32%)]  Loss: 4.411 (4.49)  Time: 0.166s, 6150.20/s  (0.193s, 5304.66/s)  LR: 8.786e-04  Data: 0.028 (0.034)
Train: 68 [ 450/1251 ( 36%)]  Loss: 4.482 (4.49)  Time: 0.174s, 5887.71/s  (0.192s, 5321.25/s)  LR: 8.786e-04  Data: 0.024 (0.035)
Train: 68 [ 500/1251 ( 40%)]  Loss: 4.621 (4.50)  Time: 0.182s, 5631.19/s  (0.193s, 5315.09/s)  LR: 8.786e-04  Data: 0.033 (0.037)
Train: 68 [ 550/1251 ( 44%)]  Loss: 4.392 (4.49)  Time: 0.206s, 4961.49/s  (0.193s, 5316.28/s)  LR: 8.786e-04  Data: 0.031 (0.038)
Train: 68 [ 600/1251 ( 48%)]  Loss: 4.424 (4.49)  Time: 0.165s, 6187.88/s  (0.192s, 5327.79/s)  LR: 8.786e-04  Data: 0.025 (0.038)
Train: 68 [ 650/1251 ( 52%)]  Loss: 4.353 (4.48)  Time: 0.173s, 5926.52/s  (0.193s, 5316.66/s)  LR: 8.786e-04  Data: 0.024 (0.040)
Train: 68 [ 700/1251 ( 56%)]  Loss: 4.753 (4.49)  Time: 0.157s, 6516.67/s  (0.193s, 5316.68/s)  LR: 8.786e-04  Data: 0.027 (0.040)
Train: 68 [ 750/1251 ( 60%)]  Loss: 4.239 (4.48)  Time: 0.172s, 5961.24/s  (0.192s, 5321.69/s)  LR: 8.786e-04  Data: 0.031 (0.040)
Train: 68 [ 800/1251 ( 64%)]  Loss: 4.288 (4.47)  Time: 0.168s, 6094.69/s  (0.192s, 5325.39/s)  LR: 8.786e-04  Data: 0.025 (0.040)
Train: 68 [ 850/1251 ( 68%)]  Loss: 4.415 (4.46)  Time: 0.156s, 6543.54/s  (0.192s, 5335.46/s)  LR: 8.786e-04  Data: 0.030 (0.039)
Train: 68 [ 900/1251 ( 72%)]  Loss: 4.497 (4.47)  Time: 0.171s, 5972.26/s  (0.192s, 5331.99/s)  LR: 8.786e-04  Data: 0.026 (0.039)
Train: 68 [ 950/1251 ( 76%)]  Loss: 4.649 (4.48)  Time: 0.183s, 5602.80/s  (0.192s, 5331.38/s)  LR: 8.786e-04  Data: 0.021 (0.038)
Train: 68 [1000/1251 ( 80%)]  Loss: 4.217 (4.46)  Time: 0.167s, 6121.52/s  (0.192s, 5326.60/s)  LR: 8.786e-04  Data: 0.025 (0.038)
Train: 68 [1050/1251 ( 84%)]  Loss: 4.674 (4.47)  Time: 0.184s, 5557.12/s  (0.192s, 5319.68/s)  LR: 8.786e-04  Data: 0.026 (0.037)
Train: 68 [1100/1251 ( 88%)]  Loss: 4.099 (4.46)  Time: 0.166s, 6159.96/s  (0.193s, 5315.13/s)  LR: 8.786e-04  Data: 0.022 (0.037)
Train: 68 [1150/1251 ( 92%)]  Loss: 4.564 (4.46)  Time: 0.158s, 6477.28/s  (0.193s, 5311.05/s)  LR: 8.786e-04  Data: 0.023 (0.036)
Train: 68 [1200/1251 ( 96%)]  Loss: 4.366 (4.46)  Time: 0.343s, 2984.09/s  (0.193s, 5310.20/s)  LR: 8.786e-04  Data: 0.025 (0.036)
Train: 68 [1250/1251 (100%)]  Loss: 4.633 (4.46)  Time: 0.114s, 9009.77/s  (0.192s, 5322.60/s)  LR: 8.786e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.832 (1.832)  Loss:  1.2730 (1.2730)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3389 (1.9335)  Acc@1: 77.0047 (61.2480)  Acc@5: 91.0377 (83.7520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)

Train: 69 [   0/1251 (  0%)]  Loss: 4.539 (4.54)  Time: 1.779s,  575.58/s  (1.779s,  575.58/s)  LR: 8.752e-04  Data: 1.631 (1.631)
Train: 69 [  50/1251 (  4%)]  Loss: 4.402 (4.47)  Time: 0.200s, 5131.53/s  (0.223s, 4586.94/s)  LR: 8.752e-04  Data: 0.020 (0.079)
Train: 69 [ 100/1251 (  8%)]  Loss: 4.571 (4.50)  Time: 0.163s, 6292.49/s  (0.208s, 4923.26/s)  LR: 8.752e-04  Data: 0.021 (0.062)
Train: 69 [ 150/1251 ( 12%)]  Loss: 4.220 (4.43)  Time: 0.182s, 5618.71/s  (0.199s, 5136.15/s)  LR: 8.752e-04  Data: 0.023 (0.054)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.441 (4.43)  Time: 0.180s, 5678.00/s  (0.198s, 5174.28/s)  LR: 8.752e-04  Data: 0.029 (0.050)
Train: 69 [ 250/1251 ( 20%)]  Loss: 4.156 (4.39)  Time: 0.159s, 6448.81/s  (0.197s, 5202.30/s)  LR: 8.752e-04  Data: 0.028 (0.048)
Train: 69 [ 300/1251 ( 24%)]  Loss: 4.616 (4.42)  Time: 0.162s, 6330.00/s  (0.195s, 5240.87/s)  LR: 8.752e-04  Data: 0.022 (0.044)
Train: 69 [ 350/1251 ( 28%)]  Loss: 4.711 (4.46)  Time: 0.166s, 6178.93/s  (0.194s, 5278.74/s)  LR: 8.752e-04  Data: 0.026 (0.042)
Train: 69 [ 400/1251 ( 32%)]  Loss: 4.572 (4.47)  Time: 0.164s, 6240.54/s  (0.193s, 5292.64/s)  LR: 8.752e-04  Data: 0.030 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 69 [ 450/1251 ( 36%)]  Loss: 4.849 (4.51)  Time: 0.229s, 4479.22/s  (0.193s, 5297.88/s)  LR: 8.752e-04  Data: 0.027 (0.039)
Train: 69 [ 500/1251 ( 40%)]  Loss: 4.596 (4.52)  Time: 0.157s, 6532.17/s  (0.193s, 5317.74/s)  LR: 8.752e-04  Data: 0.031 (0.038)
Train: 69 [ 550/1251 ( 44%)]  Loss: 4.057 (4.48)  Time: 0.175s, 5858.65/s  (0.192s, 5344.49/s)  LR: 8.752e-04  Data: 0.024 (0.037)
Train: 69 [ 600/1251 ( 48%)]  Loss: 4.577 (4.49)  Time: 0.177s, 5791.72/s  (0.192s, 5337.40/s)  LR: 8.752e-04  Data: 0.025 (0.036)
Train: 69 [ 650/1251 ( 52%)]  Loss: 4.314 (4.47)  Time: 0.181s, 5646.66/s  (0.192s, 5325.65/s)  LR: 8.752e-04  Data: 0.023 (0.036)
Train: 69 [ 700/1251 ( 56%)]  Loss: 4.210 (4.46)  Time: 0.185s, 5529.04/s  (0.192s, 5332.88/s)  LR: 8.752e-04  Data: 0.026 (0.035)
Train: 69 [ 750/1251 ( 60%)]  Loss: 4.514 (4.46)  Time: 0.207s, 4958.34/s  (0.192s, 5330.07/s)  LR: 8.752e-04  Data: 0.029 (0.034)
Train: 69 [ 800/1251 ( 64%)]  Loss: 4.381 (4.45)  Time: 0.149s, 6875.91/s  (0.192s, 5327.06/s)  LR: 8.752e-04  Data: 0.024 (0.034)
Train: 69 [ 850/1251 ( 68%)]  Loss: 4.468 (4.46)  Time: 0.191s, 5360.16/s  (0.193s, 5313.92/s)  LR: 8.752e-04  Data: 0.032 (0.034)
Train: 69 [ 900/1251 ( 72%)]  Loss: 4.641 (4.47)  Time: 0.200s, 5109.62/s  (0.193s, 5309.32/s)  LR: 8.752e-04  Data: 0.024 (0.033)
Train: 69 [ 950/1251 ( 76%)]  Loss: 4.421 (4.46)  Time: 0.185s, 5538.19/s  (0.193s, 5307.61/s)  LR: 8.752e-04  Data: 0.025 (0.033)
Train: 69 [1000/1251 ( 80%)]  Loss: 4.178 (4.45)  Time: 0.184s, 5568.81/s  (0.193s, 5315.96/s)  LR: 8.752e-04  Data: 0.026 (0.033)
Train: 69 [1050/1251 ( 84%)]  Loss: 4.347 (4.44)  Time: 0.183s, 5594.54/s  (0.193s, 5313.67/s)  LR: 8.752e-04  Data: 0.024 (0.033)
Train: 69 [1100/1251 ( 88%)]  Loss: 4.345 (4.44)  Time: 0.178s, 5747.86/s  (0.193s, 5314.85/s)  LR: 8.752e-04  Data: 0.031 (0.032)
Train: 69 [1150/1251 ( 92%)]  Loss: 4.321 (4.44)  Time: 0.223s, 4602.16/s  (0.193s, 5313.55/s)  LR: 8.752e-04  Data: 0.030 (0.032)
Train: 69 [1200/1251 ( 96%)]  Loss: 4.670 (4.44)  Time: 0.192s, 5339.70/s  (0.193s, 5303.96/s)  LR: 8.752e-04  Data: 0.021 (0.032)
Train: 69 [1250/1251 (100%)]  Loss: 4.588 (4.45)  Time: 0.114s, 9007.49/s  (0.193s, 5318.36/s)  LR: 8.752e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.819 (1.819)  Loss:  1.2460 (1.2460)  Acc@1: 78.2227 (78.2227)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3057 (2.0157)  Acc@1: 77.3585 (61.1180)  Acc@5: 91.1557 (83.7680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)

Train: 70 [   0/1251 (  0%)]  Loss: 4.433 (4.43)  Time: 1.773s,  577.47/s  (1.773s,  577.47/s)  LR: 8.717e-04  Data: 1.644 (1.644)
Train: 70 [  50/1251 (  4%)]  Loss: 4.282 (4.36)  Time: 0.164s, 6256.61/s  (0.222s, 4617.00/s)  LR: 8.717e-04  Data: 0.030 (0.068)
Train: 70 [ 100/1251 (  8%)]  Loss: 4.138 (4.28)  Time: 0.164s, 6229.58/s  (0.204s, 5015.71/s)  LR: 8.717e-04  Data: 0.024 (0.048)
Train: 70 [ 150/1251 ( 12%)]  Loss: 4.048 (4.23)  Time: 0.166s, 6154.99/s  (0.200s, 5121.07/s)  LR: 8.717e-04  Data: 0.025 (0.042)
Train: 70 [ 200/1251 ( 16%)]  Loss: 4.373 (4.25)  Time: 0.167s, 6124.79/s  (0.199s, 5134.35/s)  LR: 8.717e-04  Data: 0.028 (0.038)
Train: 70 [ 250/1251 ( 20%)]  Loss: 4.280 (4.26)  Time: 0.182s, 5632.94/s  (0.196s, 5220.25/s)  LR: 8.717e-04  Data: 0.026 (0.036)
Train: 70 [ 300/1251 ( 24%)]  Loss: 4.569 (4.30)  Time: 0.196s, 5217.09/s  (0.195s, 5244.33/s)  LR: 8.717e-04  Data: 0.036 (0.035)
Train: 70 [ 350/1251 ( 28%)]  Loss: 4.208 (4.29)  Time: 0.182s, 5637.22/s  (0.194s, 5268.66/s)  LR: 8.717e-04  Data: 0.026 (0.034)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.070 (4.27)  Time: 0.181s, 5668.49/s  (0.193s, 5304.63/s)  LR: 8.717e-04  Data: 0.022 (0.035)
Train: 70 [ 450/1251 ( 36%)]  Loss: 4.197 (4.26)  Time: 0.175s, 5841.70/s  (0.193s, 5302.71/s)  LR: 8.717e-04  Data: 0.031 (0.037)
Train: 70 [ 500/1251 ( 40%)]  Loss: 4.213 (4.26)  Time: 0.276s, 3710.23/s  (0.193s, 5298.90/s)  LR: 8.717e-04  Data: 0.137 (0.038)
Train: 70 [ 550/1251 ( 44%)]  Loss: 4.413 (4.27)  Time: 0.165s, 6208.12/s  (0.193s, 5298.03/s)  LR: 8.717e-04  Data: 0.028 (0.038)
Train: 70 [ 600/1251 ( 48%)]  Loss: 4.721 (4.30)  Time: 0.185s, 5548.01/s  (0.193s, 5310.30/s)  LR: 8.717e-04  Data: 0.026 (0.037)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.185 (4.30)  Time: 0.154s, 6670.81/s  (0.192s, 5320.04/s)  LR: 8.717e-04  Data: 0.025 (0.038)
Train: 70 [ 700/1251 ( 56%)]  Loss: 4.699 (4.32)  Time: 0.156s, 6575.33/s  (0.193s, 5311.47/s)  LR: 8.717e-04  Data: 0.033 (0.039)
Train: 70 [ 750/1251 ( 60%)]  Loss: 4.557 (4.34)  Time: 0.182s, 5626.94/s  (0.193s, 5314.49/s)  LR: 8.717e-04  Data: 0.022 (0.039)
Train: 70 [ 800/1251 ( 64%)]  Loss: 4.555 (4.35)  Time: 0.178s, 5747.57/s  (0.193s, 5316.91/s)  LR: 8.717e-04  Data: 0.033 (0.040)
Train: 70 [ 850/1251 ( 68%)]  Loss: 4.210 (4.34)  Time: 0.168s, 6101.68/s  (0.193s, 5313.55/s)  LR: 8.717e-04  Data: 0.026 (0.040)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.388 (4.34)  Time: 0.490s, 2091.12/s  (0.192s, 5319.52/s)  LR: 8.717e-04  Data: 0.365 (0.040)
Train: 70 [ 950/1251 ( 76%)]  Loss: 4.793 (4.37)  Time: 0.163s, 6298.16/s  (0.192s, 5322.16/s)  LR: 8.717e-04  Data: 0.027 (0.040)
Train: 70 [1000/1251 ( 80%)]  Loss: 4.355 (4.37)  Time: 0.181s, 5661.75/s  (0.193s, 5312.45/s)  LR: 8.717e-04  Data: 0.022 (0.040)
Train: 70 [1050/1251 ( 84%)]  Loss: 4.485 (4.37)  Time: 0.163s, 6263.29/s  (0.192s, 5319.74/s)  LR: 8.717e-04  Data: 0.028 (0.039)
Train: 70 [1100/1251 ( 88%)]  Loss: 4.404 (4.37)  Time: 0.168s, 6088.05/s  (0.193s, 5319.41/s)  LR: 8.717e-04  Data: 0.033 (0.039)
Train: 70 [1150/1251 ( 92%)]  Loss: 4.568 (4.38)  Time: 0.172s, 5953.68/s  (0.193s, 5308.21/s)  LR: 8.717e-04  Data: 0.028 (0.039)
Train: 70 [1200/1251 ( 96%)]  Loss: 4.354 (4.38)  Time: 0.172s, 5937.05/s  (0.193s, 5308.11/s)  LR: 8.717e-04  Data: 0.032 (0.038)
Train: 70 [1250/1251 (100%)]  Loss: 4.394 (4.38)  Time: 0.113s, 9087.38/s  (0.193s, 5317.99/s)  LR: 8.717e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.3614 (1.3614)  Acc@1: 79.1016 (79.1016)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.4231 (2.0018)  Acc@1: 78.0660 (61.3380)  Acc@5: 91.3915 (83.9460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)

Train: 71 [   0/1251 (  0%)]  Loss: 4.094 (4.09)  Time: 1.720s,  595.49/s  (1.720s,  595.49/s)  LR: 8.682e-04  Data: 1.590 (1.590)
Train: 71 [  50/1251 (  4%)]  Loss: 4.483 (4.29)  Time: 0.157s, 6534.71/s  (0.219s, 4678.61/s)  LR: 8.682e-04  Data: 0.035 (0.063)
Train: 71 [ 100/1251 (  8%)]  Loss: 4.670 (4.42)  Time: 0.158s, 6492.46/s  (0.205s, 4985.99/s)  LR: 8.682e-04  Data: 0.028 (0.053)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 71 [ 150/1251 ( 12%)]  Loss: 4.433 (4.42)  Time: 0.156s, 6579.29/s  (0.200s, 5119.24/s)  LR: 8.682e-04  Data: 0.029 (0.048)
Train: 71 [ 200/1251 ( 16%)]  Loss: 4.272 (4.39)  Time: 0.180s, 5686.21/s  (0.197s, 5186.39/s)  LR: 8.682e-04  Data: 0.020 (0.047)
Train: 71 [ 250/1251 ( 20%)]  Loss: 4.567 (4.42)  Time: 0.193s, 5302.68/s  (0.197s, 5209.23/s)  LR: 8.682e-04  Data: 0.049 (0.046)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.948 (4.50)  Time: 0.178s, 5739.81/s  (0.196s, 5226.53/s)  LR: 8.682e-04  Data: 0.030 (0.046)
Train: 71 [ 350/1251 ( 28%)]  Loss: 4.476 (4.49)  Time: 0.175s, 5865.14/s  (0.195s, 5252.54/s)  LR: 8.682e-04  Data: 0.029 (0.046)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.307 (4.47)  Time: 0.167s, 6120.96/s  (0.194s, 5282.53/s)  LR: 8.682e-04  Data: 0.030 (0.045)
Train: 71 [ 450/1251 ( 36%)]  Loss: 4.426 (4.47)  Time: 0.348s, 2945.99/s  (0.193s, 5293.71/s)  LR: 8.682e-04  Data: 0.219 (0.045)
Train: 71 [ 500/1251 ( 40%)]  Loss: 4.403 (4.46)  Time: 0.169s, 6047.72/s  (0.193s, 5308.93/s)  LR: 8.682e-04  Data: 0.025 (0.045)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.977 (4.42)  Time: 0.158s, 6471.61/s  (0.192s, 5329.48/s)  LR: 8.682e-04  Data: 0.030 (0.045)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.987 (4.39)  Time: 0.168s, 6098.23/s  (0.192s, 5334.78/s)  LR: 8.682e-04  Data: 0.024 (0.045)
Train: 71 [ 650/1251 ( 52%)]  Loss: 4.616 (4.40)  Time: 0.169s, 6075.23/s  (0.192s, 5336.35/s)  LR: 8.682e-04  Data: 0.037 (0.045)
Train: 71 [ 700/1251 ( 56%)]  Loss: 4.208 (4.39)  Time: 0.170s, 6034.71/s  (0.192s, 5339.25/s)  LR: 8.682e-04  Data: 0.039 (0.045)
Train: 71 [ 750/1251 ( 60%)]  Loss: 4.351 (4.39)  Time: 0.226s, 4539.56/s  (0.192s, 5335.11/s)  LR: 8.682e-04  Data: 0.073 (0.044)
Train: 71 [ 800/1251 ( 64%)]  Loss: 4.703 (4.41)  Time: 0.166s, 6184.86/s  (0.192s, 5337.63/s)  LR: 8.682e-04  Data: 0.027 (0.043)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.269 (4.40)  Time: 0.157s, 6525.97/s  (0.192s, 5325.24/s)  LR: 8.682e-04  Data: 0.040 (0.042)
Train: 71 [ 900/1251 ( 72%)]  Loss: 4.812 (4.42)  Time: 0.181s, 5668.10/s  (0.192s, 5331.22/s)  LR: 8.682e-04  Data: 0.029 (0.041)
Train: 71 [ 950/1251 ( 76%)]  Loss: 4.429 (4.42)  Time: 0.168s, 6079.95/s  (0.192s, 5327.23/s)  LR: 8.682e-04  Data: 0.024 (0.041)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.891 (4.40)  Time: 0.173s, 5906.78/s  (0.192s, 5327.77/s)  LR: 8.682e-04  Data: 0.034 (0.040)
Train: 71 [1050/1251 ( 84%)]  Loss: 4.519 (4.40)  Time: 0.171s, 5971.34/s  (0.192s, 5322.64/s)  LR: 8.682e-04  Data: 0.040 (0.040)
Train: 71 [1100/1251 ( 88%)]  Loss: 4.604 (4.41)  Time: 0.195s, 5261.15/s  (0.192s, 5322.21/s)  LR: 8.682e-04  Data: 0.022 (0.039)
Train: 71 [1150/1251 ( 92%)]  Loss: 4.780 (4.43)  Time: 0.172s, 5964.14/s  (0.193s, 5317.04/s)  LR: 8.682e-04  Data: 0.029 (0.040)
Train: 71 [1200/1251 ( 96%)]  Loss: 3.891 (4.40)  Time: 0.309s, 3314.18/s  (0.193s, 5313.78/s)  LR: 8.682e-04  Data: 0.181 (0.040)
Train: 71 [1250/1251 (100%)]  Loss: 4.746 (4.42)  Time: 0.116s, 8844.97/s  (0.192s, 5323.12/s)  LR: 8.682e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.761 (1.761)  Loss:  1.4394 (1.4394)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3808 (1.9792)  Acc@1: 77.4764 (61.0000)  Acc@5: 91.2736 (83.7780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)

Train: 72 [   0/1251 (  0%)]  Loss: 4.657 (4.66)  Time: 1.773s,  577.49/s  (1.773s,  577.49/s)  LR: 8.646e-04  Data: 1.656 (1.656)
Train: 72 [  50/1251 (  4%)]  Loss: 4.276 (4.47)  Time: 0.148s, 6923.55/s  (0.225s, 4552.03/s)  LR: 8.646e-04  Data: 0.023 (0.081)
Train: 72 [ 100/1251 (  8%)]  Loss: 4.746 (4.56)  Time: 0.164s, 6241.33/s  (0.211s, 4863.61/s)  LR: 8.646e-04  Data: 0.027 (0.066)
Train: 72 [ 150/1251 ( 12%)]  Loss: 4.098 (4.44)  Time: 0.197s, 5206.39/s  (0.203s, 5053.73/s)  LR: 8.646e-04  Data: 0.029 (0.057)
Train: 72 [ 200/1251 ( 16%)]  Loss: 4.543 (4.46)  Time: 0.189s, 5431.79/s  (0.198s, 5180.91/s)  LR: 8.646e-04  Data: 0.031 (0.051)
Train: 72 [ 250/1251 ( 20%)]  Loss: 4.526 (4.47)  Time: 0.160s, 6393.77/s  (0.196s, 5228.17/s)  LR: 8.646e-04  Data: 0.027 (0.047)
Train: 72 [ 300/1251 ( 24%)]  Loss: 4.345 (4.46)  Time: 0.171s, 5981.43/s  (0.195s, 5258.85/s)  LR: 8.646e-04  Data: 0.030 (0.046)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.356 (4.44)  Time: 0.171s, 6002.91/s  (0.194s, 5275.72/s)  LR: 8.646e-04  Data: 0.027 (0.046)
Train: 72 [ 400/1251 ( 32%)]  Loss: 4.684 (4.47)  Time: 0.169s, 6063.19/s  (0.193s, 5313.18/s)  LR: 8.646e-04  Data: 0.027 (0.045)
Train: 72 [ 450/1251 ( 36%)]  Loss: 4.378 (4.46)  Time: 0.178s, 5759.58/s  (0.193s, 5307.22/s)  LR: 8.646e-04  Data: 0.025 (0.045)
Train: 72 [ 500/1251 ( 40%)]  Loss: 4.577 (4.47)  Time: 0.163s, 6274.17/s  (0.193s, 5319.47/s)  LR: 8.646e-04  Data: 0.031 (0.044)
Train: 72 [ 550/1251 ( 44%)]  Loss: 4.482 (4.47)  Time: 0.170s, 6029.11/s  (0.193s, 5315.42/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 600/1251 ( 48%)]  Loss: 4.017 (4.44)  Time: 0.164s, 6226.21/s  (0.192s, 5333.19/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 650/1251 ( 52%)]  Loss: 4.030 (4.41)  Time: 0.182s, 5613.54/s  (0.192s, 5330.50/s)  LR: 8.646e-04  Data: 0.022 (0.044)
Train: 72 [ 700/1251 ( 56%)]  Loss: 4.461 (4.41)  Time: 0.160s, 6413.84/s  (0.192s, 5338.15/s)  LR: 8.646e-04  Data: 0.023 (0.044)
Train: 72 [ 750/1251 ( 60%)]  Loss: 4.234 (4.40)  Time: 0.170s, 6026.68/s  (0.192s, 5344.29/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 800/1251 ( 64%)]  Loss: 4.641 (4.41)  Time: 0.189s, 5431.40/s  (0.192s, 5338.96/s)  LR: 8.646e-04  Data: 0.026 (0.044)
Train: 72 [ 850/1251 ( 68%)]  Loss: 4.670 (4.43)  Time: 0.176s, 5822.75/s  (0.192s, 5342.37/s)  LR: 8.646e-04  Data: 0.027 (0.043)
Train: 72 [ 900/1251 ( 72%)]  Loss: 4.901 (4.45)  Time: 0.174s, 5876.92/s  (0.192s, 5339.61/s)  LR: 8.646e-04  Data: 0.026 (0.042)
Train: 72 [ 950/1251 ( 76%)]  Loss: 4.485 (4.46)  Time: 0.173s, 5924.53/s  (0.192s, 5331.27/s)  LR: 8.646e-04  Data: 0.046 (0.042)
Train: 72 [1000/1251 ( 80%)]  Loss: 4.510 (4.46)  Time: 0.189s, 5413.51/s  (0.192s, 5329.66/s)  LR: 8.646e-04  Data: 0.046 (0.041)
Train: 72 [1050/1251 ( 84%)]  Loss: 4.431 (4.46)  Time: 0.177s, 5783.91/s  (0.192s, 5321.38/s)  LR: 8.646e-04  Data: 0.019 (0.040)
Train: 72 [1100/1251 ( 88%)]  Loss: 4.456 (4.46)  Time: 0.181s, 5664.93/s  (0.192s, 5323.04/s)  LR: 8.646e-04  Data: 0.031 (0.040)
Train: 72 [1150/1251 ( 92%)]  Loss: 4.385 (4.45)  Time: 0.186s, 5503.56/s  (0.193s, 5313.52/s)  LR: 8.646e-04  Data: 0.023 (0.040)
Train: 72 [1200/1251 ( 96%)]  Loss: 4.396 (4.45)  Time: 0.172s, 5942.62/s  (0.193s, 5300.38/s)  LR: 8.646e-04  Data: 0.021 (0.040)
Train: 72 [1250/1251 (100%)]  Loss: 4.615 (4.46)  Time: 0.114s, 8990.67/s  (0.193s, 5314.07/s)  LR: 8.646e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.730 (1.730)  Loss:  1.1955 (1.1955)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.3375 (1.9334)  Acc@1: 75.3538 (61.1400)  Acc@5: 91.1557 (83.6540)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)

Train: 73 [   0/1251 (  0%)]  Loss: 4.266 (4.27)  Time: 1.929s,  530.75/s  (1.929s,  530.75/s)  LR: 8.610e-04  Data: 1.805 (1.805)
Train: 73 [  50/1251 (  4%)]  Loss: 3.903 (4.08)  Time: 0.173s, 5920.28/s  (0.223s, 4584.88/s)  LR: 8.610e-04  Data: 0.030 (0.074)
Train: 73 [ 100/1251 (  8%)]  Loss: 4.164 (4.11)  Time: 0.184s, 5560.22/s  (0.209s, 4907.91/s)  LR: 8.610e-04  Data: 0.032 (0.052)
Train: 73 [ 150/1251 ( 12%)]  Loss: 4.201 (4.13)  Time: 0.164s, 6233.52/s  (0.201s, 5090.40/s)  LR: 8.610e-04  Data: 0.029 (0.046)
Train: 73 [ 200/1251 ( 16%)]  Loss: 4.376 (4.18)  Time: 0.225s, 4559.30/s  (0.199s, 5135.20/s)  LR: 8.610e-04  Data: 0.026 (0.042)
Train: 73 [ 250/1251 ( 20%)]  Loss: 4.952 (4.31)  Time: 0.171s, 5988.85/s  (0.197s, 5203.77/s)  LR: 8.610e-04  Data: 0.035 (0.040)
Train: 73 [ 300/1251 ( 24%)]  Loss: 4.307 (4.31)  Time: 0.224s, 4562.34/s  (0.194s, 5273.74/s)  LR: 8.610e-04  Data: 0.028 (0.038)
Train: 73 [ 350/1251 ( 28%)]  Loss: 4.289 (4.31)  Time: 0.169s, 6057.58/s  (0.195s, 5258.90/s)  LR: 8.610e-04  Data: 0.023 (0.036)
Train: 73 [ 400/1251 ( 32%)]  Loss: 4.479 (4.33)  Time: 0.290s, 3525.97/s  (0.194s, 5280.75/s)  LR: 8.610e-04  Data: 0.030 (0.035)
Train: 73 [ 450/1251 ( 36%)]  Loss: 4.479 (4.34)  Time: 0.159s, 6451.85/s  (0.194s, 5285.00/s)  LR: 8.610e-04  Data: 0.024 (0.036)
Train: 73 [ 500/1251 ( 40%)]  Loss: 4.308 (4.34)  Time: 0.175s, 5841.50/s  (0.193s, 5310.71/s)  LR: 8.610e-04  Data: 0.026 (0.036)
Train: 73 [ 550/1251 ( 44%)]  Loss: 4.270 (4.33)  Time: 0.201s, 5100.15/s  (0.193s, 5318.48/s)  LR: 8.610e-04  Data: 0.028 (0.036)
Train: 73 [ 600/1251 ( 48%)]  Loss: 4.668 (4.36)  Time: 0.176s, 5819.90/s  (0.193s, 5319.42/s)  LR: 8.610e-04  Data: 0.033 (0.035)
Train: 73 [ 650/1251 ( 52%)]  Loss: 4.340 (4.36)  Time: 0.155s, 6623.45/s  (0.193s, 5295.03/s)  LR: 8.610e-04  Data: 0.028 (0.034)
Train: 73 [ 700/1251 ( 56%)]  Loss: 4.786 (4.39)  Time: 0.160s, 6397.02/s  (0.193s, 5299.15/s)  LR: 8.610e-04  Data: 0.028 (0.034)
Train: 73 [ 750/1251 ( 60%)]  Loss: 4.661 (4.40)  Time: 0.196s, 5230.15/s  (0.193s, 5302.39/s)  LR: 8.610e-04  Data: 0.026 (0.034)
Train: 73 [ 800/1251 ( 64%)]  Loss: 4.501 (4.41)  Time: 0.186s, 5499.11/s  (0.193s, 5306.54/s)  LR: 8.610e-04  Data: 0.030 (0.033)
Train: 73 [ 850/1251 ( 68%)]  Loss: 4.171 (4.40)  Time: 0.180s, 5678.10/s  (0.192s, 5327.11/s)  LR: 8.610e-04  Data: 0.028 (0.033)
Train: 73 [ 900/1251 ( 72%)]  Loss: 4.319 (4.39)  Time: 0.453s, 2261.16/s  (0.193s, 5312.23/s)  LR: 8.610e-04  Data: 0.326 (0.033)
Train: 73 [ 950/1251 ( 76%)]  Loss: 4.602 (4.40)  Time: 0.183s, 5585.06/s  (0.193s, 5306.42/s)  LR: 8.610e-04  Data: 0.020 (0.035)
Train: 73 [1000/1251 ( 80%)]  Loss: 4.366 (4.40)  Time: 0.165s, 6221.79/s  (0.193s, 5307.36/s)  LR: 8.610e-04  Data: 0.025 (0.035)
Train: 73 [1050/1251 ( 84%)]  Loss: 4.268 (4.39)  Time: 0.313s, 3273.58/s  (0.193s, 5307.86/s)  LR: 8.610e-04  Data: 0.028 (0.035)
Train: 73 [1100/1251 ( 88%)]  Loss: 4.824 (4.41)  Time: 0.161s, 6377.71/s  (0.193s, 5305.71/s)  LR: 8.610e-04  Data: 0.033 (0.035)
Train: 73 [1150/1251 ( 92%)]  Loss: 4.740 (4.43)  Time: 0.166s, 6172.14/s  (0.193s, 5302.94/s)  LR: 8.610e-04  Data: 0.027 (0.035)
Train: 73 [1200/1251 ( 96%)]  Loss: 4.497 (4.43)  Time: 0.174s, 5883.28/s  (0.193s, 5294.47/s)  LR: 8.610e-04  Data: 0.027 (0.034)
Train: 73 [1250/1251 (100%)]  Loss: 4.472 (4.43)  Time: 0.113s, 9028.96/s  (0.193s, 5311.93/s)  LR: 8.610e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1868 (1.1868)  Acc@1: 79.0039 (79.0039)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2417 (1.9178)  Acc@1: 77.5943 (61.2540)  Acc@5: 91.9811 (83.9340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)

Train: 74 [   0/1251 (  0%)]  Loss: 4.501 (4.50)  Time: 1.723s,  594.14/s  (1.723s,  594.14/s)  LR: 8.574e-04  Data: 1.600 (1.600)
Train: 74 [  50/1251 (  4%)]  Loss: 4.466 (4.48)  Time: 0.165s, 6203.51/s  (0.218s, 4691.43/s)  LR: 8.574e-04  Data: 0.025 (0.069)
Train: 74 [ 100/1251 (  8%)]  Loss: 4.644 (4.54)  Time: 0.179s, 5731.11/s  (0.208s, 4914.09/s)  LR: 8.574e-04  Data: 0.026 (0.049)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.586 (4.55)  Time: 0.192s, 5334.33/s  (0.201s, 5091.46/s)  LR: 8.574e-04  Data: 0.025 (0.042)
Train: 74 [ 200/1251 ( 16%)]  Loss: 4.194 (4.48)  Time: 0.184s, 5566.10/s  (0.199s, 5139.19/s)  LR: 8.574e-04  Data: 0.024 (0.038)
Train: 74 [ 250/1251 ( 20%)]  Loss: 4.516 (4.48)  Time: 0.179s, 5723.91/s  (0.196s, 5211.53/s)  LR: 8.574e-04  Data: 0.028 (0.036)
Train: 74 [ 300/1251 ( 24%)]  Loss: 4.303 (4.46)  Time: 0.166s, 6155.89/s  (0.194s, 5273.83/s)  LR: 8.574e-04  Data: 0.027 (0.035)
Train: 74 [ 350/1251 ( 28%)]  Loss: 4.430 (4.45)  Time: 0.164s, 6251.45/s  (0.194s, 5274.71/s)  LR: 8.574e-04  Data: 0.029 (0.035)
Train: 74 [ 400/1251 ( 32%)]  Loss: 4.275 (4.43)  Time: 0.175s, 5836.89/s  (0.194s, 5270.88/s)  LR: 8.574e-04  Data: 0.019 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 74 [ 450/1251 ( 36%)]  Loss: 4.145 (4.41)  Time: 0.195s, 5244.72/s  (0.194s, 5266.62/s)  LR: 8.574e-04  Data: 0.030 (0.037)
Train: 74 [ 500/1251 ( 40%)]  Loss: 4.542 (4.42)  Time: 0.152s, 6749.34/s  (0.193s, 5295.07/s)  LR: 8.574e-04  Data: 0.021 (0.037)
Train: 74 [ 550/1251 ( 44%)]  Loss: 4.193 (4.40)  Time: 0.154s, 6658.22/s  (0.193s, 5301.86/s)  LR: 8.574e-04  Data: 0.023 (0.038)
Train: 74 [ 600/1251 ( 48%)]  Loss: 4.487 (4.41)  Time: 0.166s, 6153.72/s  (0.193s, 5297.17/s)  LR: 8.574e-04  Data: 0.034 (0.039)
Train: 74 [ 650/1251 ( 52%)]  Loss: 4.162 (4.39)  Time: 0.163s, 6266.18/s  (0.193s, 5315.85/s)  LR: 8.574e-04  Data: 0.031 (0.038)
Train: 74 [ 700/1251 ( 56%)]  Loss: 4.628 (4.40)  Time: 0.163s, 6287.42/s  (0.192s, 5321.27/s)  LR: 8.574e-04  Data: 0.029 (0.038)
Train: 74 [ 750/1251 ( 60%)]  Loss: 4.276 (4.40)  Time: 0.175s, 5844.59/s  (0.193s, 5315.14/s)  LR: 8.574e-04  Data: 0.023 (0.039)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.853 (4.42)  Time: 0.166s, 6167.62/s  (0.193s, 5305.45/s)  LR: 8.574e-04  Data: 0.034 (0.040)
Train: 74 [ 850/1251 ( 68%)]  Loss: 4.687 (4.44)  Time: 0.194s, 5290.30/s  (0.193s, 5306.16/s)  LR: 8.574e-04  Data: 0.025 (0.040)
Train: 74 [ 900/1251 ( 72%)]  Loss: 4.518 (4.44)  Time: 0.182s, 5640.58/s  (0.193s, 5315.88/s)  LR: 8.574e-04  Data: 0.024 (0.040)
Train: 74 [ 950/1251 ( 76%)]  Loss: 4.170 (4.43)  Time: 0.331s, 3091.33/s  (0.193s, 5307.79/s)  LR: 8.574e-04  Data: 0.022 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 74 [1000/1251 ( 80%)]  Loss: 4.819 (4.45)  Time: 0.166s, 6163.55/s  (0.193s, 5304.85/s)  LR: 8.574e-04  Data: 0.030 (0.039)
Train: 74 [1050/1251 ( 84%)]  Loss: 4.144 (4.43)  Time: 0.166s, 6159.61/s  (0.193s, 5299.89/s)  LR: 8.574e-04  Data: 0.023 (0.039)
Train: 74 [1100/1251 ( 88%)]  Loss: 4.183 (4.42)  Time: 0.179s, 5730.48/s  (0.193s, 5310.74/s)  LR: 8.574e-04  Data: 0.025 (0.038)
Train: 74 [1150/1251 ( 92%)]  Loss: 4.121 (4.41)  Time: 0.250s, 4093.77/s  (0.193s, 5307.28/s)  LR: 8.574e-04  Data: 0.026 (0.038)
Train: 74 [1200/1251 ( 96%)]  Loss: 4.412 (4.41)  Time: 0.297s, 3442.83/s  (0.193s, 5294.19/s)  LR: 8.574e-04  Data: 0.026 (0.037)
Train: 74 [1250/1251 (100%)]  Loss: 4.433 (4.41)  Time: 0.113s, 9046.34/s  (0.193s, 5303.61/s)  LR: 8.574e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.807 (1.807)  Loss:  1.2259 (1.2259)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.2322 (1.9228)  Acc@1: 77.4764 (61.1300)  Acc@5: 90.9198 (83.7240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)

Train: 75 [   0/1251 (  0%)]  Loss: 4.019 (4.02)  Time: 1.709s,  599.12/s  (1.709s,  599.12/s)  LR: 8.537e-04  Data: 1.586 (1.586)
Train: 75 [  50/1251 (  4%)]  Loss: 4.502 (4.26)  Time: 0.155s, 6595.08/s  (0.227s, 4507.08/s)  LR: 8.537e-04  Data: 0.028 (0.086)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.488 (4.34)  Time: 0.169s, 6043.02/s  (0.209s, 4899.77/s)  LR: 8.537e-04  Data: 0.031 (0.066)
Train: 75 [ 150/1251 ( 12%)]  Loss: 4.606 (4.40)  Time: 0.163s, 6273.30/s  (0.203s, 5056.76/s)  LR: 8.537e-04  Data: 0.026 (0.058)
Train: 75 [ 200/1251 ( 16%)]  Loss: 4.341 (4.39)  Time: 0.164s, 6249.75/s  (0.201s, 5106.96/s)  LR: 8.537e-04  Data: 0.026 (0.055)
Train: 75 [ 250/1251 ( 20%)]  Loss: 4.503 (4.41)  Time: 0.178s, 5750.56/s  (0.199s, 5155.69/s)  LR: 8.537e-04  Data: 0.027 (0.054)
Train: 75 [ 300/1251 ( 24%)]  Loss: 4.508 (4.42)  Time: 0.170s, 6025.24/s  (0.197s, 5194.48/s)  LR: 8.537e-04  Data: 0.026 (0.052)
Train: 75 [ 350/1251 ( 28%)]  Loss: 4.700 (4.46)  Time: 0.187s, 5485.79/s  (0.196s, 5219.03/s)  LR: 8.537e-04  Data: 0.025 (0.051)
Train: 75 [ 400/1251 ( 32%)]  Loss: 4.471 (4.46)  Time: 0.170s, 6023.61/s  (0.196s, 5236.08/s)  LR: 8.537e-04  Data: 0.031 (0.050)
Train: 75 [ 450/1251 ( 36%)]  Loss: 4.379 (4.45)  Time: 0.177s, 5784.19/s  (0.195s, 5254.99/s)  LR: 8.537e-04  Data: 0.025 (0.050)
Train: 75 [ 500/1251 ( 40%)]  Loss: 4.270 (4.44)  Time: 0.155s, 6610.21/s  (0.194s, 5271.02/s)  LR: 8.537e-04  Data: 0.030 (0.048)
Train: 75 [ 550/1251 ( 44%)]  Loss: 4.364 (4.43)  Time: 0.199s, 5136.01/s  (0.194s, 5279.58/s)  LR: 8.537e-04  Data: 0.022 (0.047)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.529 (4.44)  Time: 0.166s, 6171.88/s  (0.194s, 5279.87/s)  LR: 8.537e-04  Data: 0.036 (0.046)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.997 (4.41)  Time: 0.170s, 6010.05/s  (0.194s, 5286.18/s)  LR: 8.537e-04  Data: 0.024 (0.044)
Train: 75 [ 700/1251 ( 56%)]  Loss: 4.244 (4.39)  Time: 0.194s, 5284.72/s  (0.194s, 5269.04/s)  LR: 8.537e-04  Data: 0.040 (0.043)
Train: 75 [ 750/1251 ( 60%)]  Loss: 4.279 (4.39)  Time: 0.189s, 5410.91/s  (0.194s, 5281.73/s)  LR: 8.537e-04  Data: 0.027 (0.042)
Train: 75 [ 800/1251 ( 64%)]  Loss: 4.902 (4.42)  Time: 0.191s, 5363.51/s  (0.194s, 5275.60/s)  LR: 8.537e-04  Data: 0.034 (0.043)
Train: 75 [ 850/1251 ( 68%)]  Loss: 4.498 (4.42)  Time: 0.164s, 6230.48/s  (0.194s, 5276.58/s)  LR: 8.537e-04  Data: 0.024 (0.042)
Train: 75 [ 900/1251 ( 72%)]  Loss: 4.426 (4.42)  Time: 0.173s, 5922.44/s  (0.194s, 5277.18/s)  LR: 8.537e-04  Data: 0.029 (0.042)
Train: 75 [ 950/1251 ( 76%)]  Loss: 4.219 (4.41)  Time: 0.174s, 5871.51/s  (0.194s, 5274.18/s)  LR: 8.537e-04  Data: 0.030 (0.042)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.326 (4.41)  Time: 0.168s, 6087.96/s  (0.194s, 5272.14/s)  LR: 8.537e-04  Data: 0.036 (0.042)
Train: 75 [1050/1251 ( 84%)]  Loss: 4.510 (4.41)  Time: 0.166s, 6159.79/s  (0.194s, 5272.16/s)  LR: 8.537e-04  Data: 0.024 (0.041)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.844 (4.39)  Time: 0.188s, 5447.36/s  (0.194s, 5270.10/s)  LR: 8.537e-04  Data: 0.025 (0.041)
Train: 75 [1150/1251 ( 92%)]  Loss: 4.229 (4.38)  Time: 0.172s, 5960.84/s  (0.194s, 5279.55/s)  LR: 8.537e-04  Data: 0.029 (0.041)
Train: 75 [1200/1251 ( 96%)]  Loss: 4.526 (4.39)  Time: 0.159s, 6425.79/s  (0.194s, 5269.47/s)  LR: 8.537e-04  Data: 0.038 (0.041)
Train: 75 [1250/1251 (100%)]  Loss: 4.220 (4.38)  Time: 0.113s, 9023.15/s  (0.194s, 5280.60/s)  LR: 8.537e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.790 (1.790)  Loss:  1.2838 (1.2838)  Acc@1: 76.8555 (76.8555)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2998 (1.9152)  Acc@1: 76.1792 (61.2080)  Acc@5: 91.0377 (83.7140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)

Train: 76 [   0/1251 (  0%)]  Loss: 4.361 (4.36)  Time: 1.692s,  605.36/s  (1.692s,  605.36/s)  LR: 8.500e-04  Data: 1.561 (1.561)
Train: 76 [  50/1251 (  4%)]  Loss: 4.315 (4.34)  Time: 0.174s, 5870.22/s  (0.221s, 4637.94/s)  LR: 8.500e-04  Data: 0.035 (0.068)
Train: 76 [ 100/1251 (  8%)]  Loss: 4.414 (4.36)  Time: 0.172s, 5947.36/s  (0.207s, 4935.89/s)  LR: 8.500e-04  Data: 0.033 (0.051)
Train: 76 [ 150/1251 ( 12%)]  Loss: 4.291 (4.35)  Time: 0.154s, 6647.23/s  (0.200s, 5132.44/s)  LR: 8.500e-04  Data: 0.023 (0.046)
Train: 76 [ 200/1251 ( 16%)]  Loss: 4.166 (4.31)  Time: 0.186s, 5502.17/s  (0.196s, 5230.54/s)  LR: 8.500e-04  Data: 0.026 (0.042)
Train: 76 [ 250/1251 ( 20%)]  Loss: 4.854 (4.40)  Time: 0.177s, 5778.37/s  (0.195s, 5259.58/s)  LR: 8.500e-04  Data: 0.030 (0.039)
Train: 76 [ 300/1251 ( 24%)]  Loss: 4.474 (4.41)  Time: 0.154s, 6644.11/s  (0.194s, 5284.56/s)  LR: 8.500e-04  Data: 0.020 (0.037)
Train: 76 [ 350/1251 ( 28%)]  Loss: 4.438 (4.41)  Time: 0.161s, 6348.18/s  (0.194s, 5291.77/s)  LR: 8.500e-04  Data: 0.025 (0.037)
Train: 76 [ 400/1251 ( 32%)]  Loss: 4.330 (4.40)  Time: 0.293s, 3490.11/s  (0.193s, 5314.53/s)  LR: 8.500e-04  Data: 0.023 (0.037)
Train: 76 [ 450/1251 ( 36%)]  Loss: 4.535 (4.42)  Time: 0.185s, 5531.46/s  (0.193s, 5305.80/s)  LR: 8.500e-04  Data: 0.038 (0.036)
Train: 76 [ 500/1251 ( 40%)]  Loss: 4.500 (4.43)  Time: 0.174s, 5881.27/s  (0.193s, 5309.64/s)  LR: 8.500e-04  Data: 0.019 (0.035)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.740 (4.45)  Time: 0.155s, 6597.03/s  (0.192s, 5323.70/s)  LR: 8.500e-04  Data: 0.021 (0.036)
Train: 76 [ 600/1251 ( 48%)]  Loss: 4.377 (4.45)  Time: 0.177s, 5783.89/s  (0.192s, 5328.30/s)  LR: 8.500e-04  Data: 0.039 (0.036)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.971 (4.41)  Time: 0.176s, 5834.73/s  (0.192s, 5330.87/s)  LR: 8.500e-04  Data: 0.036 (0.035)
Train: 76 [ 700/1251 ( 56%)]  Loss: 4.416 (4.41)  Time: 0.165s, 6220.89/s  (0.192s, 5331.79/s)  LR: 8.500e-04  Data: 0.020 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 76 [ 750/1251 ( 60%)]  Loss: 4.288 (4.40)  Time: 0.178s, 5739.14/s  (0.192s, 5331.70/s)  LR: 8.500e-04  Data: 0.032 (0.034)
Train: 76 [ 800/1251 ( 64%)]  Loss: 4.225 (4.39)  Time: 0.179s, 5709.02/s  (0.192s, 5334.16/s)  LR: 8.500e-04  Data: 0.025 (0.034)
Train: 76 [ 850/1251 ( 68%)]  Loss: 4.315 (4.39)  Time: 0.174s, 5890.00/s  (0.192s, 5332.11/s)  LR: 8.500e-04  Data: 0.027 (0.033)
Train: 76 [ 900/1251 ( 72%)]  Loss: 4.271 (4.38)  Time: 0.173s, 5907.62/s  (0.192s, 5322.73/s)  LR: 8.500e-04  Data: 0.029 (0.033)
Train: 76 [ 950/1251 ( 76%)]  Loss: 4.068 (4.37)  Time: 0.180s, 5703.05/s  (0.193s, 5319.13/s)  LR: 8.500e-04  Data: 0.032 (0.033)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.682 (4.38)  Time: 0.186s, 5514.77/s  (0.192s, 5329.93/s)  LR: 8.500e-04  Data: 0.034 (0.032)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.999 (4.37)  Time: 0.186s, 5504.70/s  (0.192s, 5330.78/s)  LR: 8.500e-04  Data: 0.032 (0.032)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.036 (4.35)  Time: 0.342s, 2992.30/s  (0.192s, 5326.51/s)  LR: 8.500e-04  Data: 0.023 (0.032)
Train: 76 [1150/1251 ( 92%)]  Loss: 4.496 (4.36)  Time: 0.159s, 6438.46/s  (0.192s, 5325.10/s)  LR: 8.500e-04  Data: 0.022 (0.032)
Train: 76 [1200/1251 ( 96%)]  Loss: 4.053 (4.34)  Time: 0.163s, 6281.82/s  (0.192s, 5321.94/s)  LR: 8.500e-04  Data: 0.031 (0.032)
Train: 76 [1250/1251 (100%)]  Loss: 4.494 (4.35)  Time: 0.113s, 9084.63/s  (0.192s, 5329.98/s)  LR: 8.500e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.826 (1.826)  Loss:  1.1483 (1.1483)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3435 (1.9415)  Acc@1: 79.4811 (61.9800)  Acc@5: 92.0991 (84.3780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)

Train: 77 [   0/1251 (  0%)]  Loss: 4.260 (4.26)  Time: 1.744s,  587.28/s  (1.744s,  587.28/s)  LR: 8.462e-04  Data: 1.618 (1.618)
Train: 77 [  50/1251 (  4%)]  Loss: 4.803 (4.53)  Time: 0.172s, 5950.37/s  (0.221s, 4636.32/s)  LR: 8.462e-04  Data: 0.029 (0.078)
Train: 77 [ 100/1251 (  8%)]  Loss: 4.300 (4.45)  Time: 0.189s, 5429.01/s  (0.203s, 5039.80/s)  LR: 8.462e-04  Data: 0.028 (0.059)
Train: 77 [ 150/1251 ( 12%)]  Loss: 4.575 (4.48)  Time: 0.175s, 5847.41/s  (0.198s, 5172.34/s)  LR: 8.462e-04  Data: 0.035 (0.052)
Train: 77 [ 200/1251 ( 16%)]  Loss: 4.252 (4.44)  Time: 0.159s, 6457.99/s  (0.196s, 5220.17/s)  LR: 8.462e-04  Data: 0.027 (0.047)
Train: 77 [ 250/1251 ( 20%)]  Loss: 4.246 (4.41)  Time: 0.177s, 5801.13/s  (0.196s, 5235.30/s)  LR: 8.462e-04  Data: 0.034 (0.046)
Train: 77 [ 300/1251 ( 24%)]  Loss: 4.405 (4.41)  Time: 0.159s, 6435.09/s  (0.194s, 5275.66/s)  LR: 8.462e-04  Data: 0.025 (0.046)
Train: 77 [ 350/1251 ( 28%)]  Loss: 4.387 (4.40)  Time: 0.167s, 6133.73/s  (0.194s, 5285.37/s)  LR: 8.462e-04  Data: 0.024 (0.046)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.989 (4.36)  Time: 0.177s, 5800.29/s  (0.193s, 5296.86/s)  LR: 8.462e-04  Data: 0.025 (0.046)
Train: 77 [ 450/1251 ( 36%)]  Loss: 4.303 (4.35)  Time: 0.193s, 5292.33/s  (0.193s, 5301.80/s)  LR: 8.462e-04  Data: 0.022 (0.044)
Train: 77 [ 500/1251 ( 40%)]  Loss: 4.203 (4.34)  Time: 0.187s, 5472.31/s  (0.192s, 5325.64/s)  LR: 8.462e-04  Data: 0.029 (0.043)
Train: 77 [ 550/1251 ( 44%)]  Loss: 4.129 (4.32)  Time: 0.174s, 5890.59/s  (0.192s, 5328.50/s)  LR: 8.462e-04  Data: 0.026 (0.041)
Train: 77 [ 600/1251 ( 48%)]  Loss: 4.368 (4.32)  Time: 0.299s, 3421.16/s  (0.192s, 5322.66/s)  LR: 8.462e-04  Data: 0.024 (0.040)
Train: 77 [ 650/1251 ( 52%)]  Loss: 4.336 (4.33)  Time: 0.183s, 5606.85/s  (0.192s, 5341.12/s)  LR: 8.462e-04  Data: 0.037 (0.039)
Train: 77 [ 700/1251 ( 56%)]  Loss: 4.413 (4.33)  Time: 0.175s, 5863.89/s  (0.191s, 5349.74/s)  LR: 8.462e-04  Data: 0.027 (0.039)
Train: 77 [ 750/1251 ( 60%)]  Loss: 4.348 (4.33)  Time: 0.172s, 5963.95/s  (0.192s, 5336.08/s)  LR: 8.462e-04  Data: 0.026 (0.040)
Train: 77 [ 800/1251 ( 64%)]  Loss: 4.439 (4.34)  Time: 0.191s, 5370.31/s  (0.192s, 5319.63/s)  LR: 8.462e-04  Data: 0.024 (0.040)
Train: 77 [ 850/1251 ( 68%)]  Loss: 4.450 (4.34)  Time: 0.360s, 2846.55/s  (0.192s, 5321.21/s)  LR: 8.462e-04  Data: 0.022 (0.040)
Train: 77 [ 900/1251 ( 72%)]  Loss: 4.585 (4.36)  Time: 0.165s, 6197.86/s  (0.192s, 5329.16/s)  LR: 8.462e-04  Data: 0.025 (0.040)
Train: 77 [ 950/1251 ( 76%)]  Loss: 4.428 (4.36)  Time: 0.202s, 5065.54/s  (0.192s, 5327.76/s)  LR: 8.462e-04  Data: 0.025 (0.040)
Train: 77 [1000/1251 ( 80%)]  Loss: 4.567 (4.37)  Time: 0.201s, 5101.07/s  (0.192s, 5328.92/s)  LR: 8.462e-04  Data: 0.022 (0.040)
Train: 77 [1050/1251 ( 84%)]  Loss: 4.474 (4.38)  Time: 0.187s, 5486.67/s  (0.192s, 5333.22/s)  LR: 8.462e-04  Data: 0.029 (0.040)
Train: 77 [1100/1251 ( 88%)]  Loss: 4.447 (4.38)  Time: 0.170s, 6025.83/s  (0.192s, 5325.66/s)  LR: 8.462e-04  Data: 0.020 (0.039)
Train: 77 [1150/1251 ( 92%)]  Loss: 4.010 (4.36)  Time: 0.179s, 5707.63/s  (0.192s, 5325.26/s)  LR: 8.462e-04  Data: 0.021 (0.039)
Train: 77 [1200/1251 ( 96%)]  Loss: 4.130 (4.35)  Time: 0.153s, 6674.18/s  (0.193s, 5318.22/s)  LR: 8.462e-04  Data: 0.022 (0.039)
Train: 77 [1250/1251 (100%)]  Loss: 4.375 (4.35)  Time: 0.112s, 9103.25/s  (0.192s, 5329.70/s)  LR: 8.462e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.923 (1.923)  Loss:  1.2415 (1.2415)  Acc@1: 77.9297 (77.9297)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1628 (1.8729)  Acc@1: 80.0708 (62.1400)  Acc@5: 93.6321 (84.5740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)

Train: 78 [   0/1251 (  0%)]  Loss: 4.462 (4.46)  Time: 1.772s,  577.84/s  (1.772s,  577.84/s)  LR: 8.424e-04  Data: 1.645 (1.645)
Train: 78 [  50/1251 (  4%)]  Loss: 4.421 (4.44)  Time: 0.168s, 6096.05/s  (0.219s, 4674.91/s)  LR: 8.424e-04  Data: 0.025 (0.062)
Train: 78 [ 100/1251 (  8%)]  Loss: 4.386 (4.42)  Time: 0.211s, 4845.69/s  (0.203s, 5045.43/s)  LR: 8.424e-04  Data: 0.033 (0.045)
Train: 78 [ 150/1251 ( 12%)]  Loss: 4.857 (4.53)  Time: 0.154s, 6639.93/s  (0.199s, 5144.61/s)  LR: 8.424e-04  Data: 0.022 (0.039)
Train: 78 [ 200/1251 ( 16%)]  Loss: 4.079 (4.44)  Time: 0.195s, 5250.89/s  (0.198s, 5171.22/s)  LR: 8.424e-04  Data: 0.038 (0.036)
Train: 78 [ 250/1251 ( 20%)]  Loss: 4.348 (4.43)  Time: 0.171s, 5991.42/s  (0.196s, 5234.33/s)  LR: 8.424e-04  Data: 0.021 (0.035)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.795 (4.48)  Time: 0.201s, 5099.08/s  (0.195s, 5243.56/s)  LR: 8.424e-04  Data: 0.031 (0.034)
Train: 78 [ 350/1251 ( 28%)]  Loss: 4.252 (4.45)  Time: 0.199s, 5152.08/s  (0.194s, 5275.78/s)  LR: 8.424e-04  Data: 0.024 (0.033)
Train: 78 [ 400/1251 ( 32%)]  Loss: 4.540 (4.46)  Time: 0.163s, 6298.16/s  (0.193s, 5308.10/s)  LR: 8.424e-04  Data: 0.024 (0.032)
Train: 78 [ 450/1251 ( 36%)]  Loss: 4.427 (4.46)  Time: 0.181s, 5651.14/s  (0.193s, 5313.16/s)  LR: 8.424e-04  Data: 0.027 (0.032)
Train: 78 [ 500/1251 ( 40%)]  Loss: 4.299 (4.44)  Time: 0.166s, 6181.06/s  (0.192s, 5332.33/s)  LR: 8.424e-04  Data: 0.023 (0.031)
Train: 78 [ 550/1251 ( 44%)]  Loss: 4.196 (4.42)  Time: 0.185s, 5534.30/s  (0.192s, 5344.37/s)  LR: 8.424e-04  Data: 0.027 (0.031)
Train: 78 [ 600/1251 ( 48%)]  Loss: 4.655 (4.44)  Time: 0.168s, 6089.67/s  (0.192s, 5333.62/s)  LR: 8.424e-04  Data: 0.029 (0.031)
Train: 78 [ 650/1251 ( 52%)]  Loss: 4.288 (4.43)  Time: 0.162s, 6317.88/s  (0.192s, 5328.08/s)  LR: 8.424e-04  Data: 0.023 (0.030)
Train: 78 [ 700/1251 ( 56%)]  Loss: 4.490 (4.43)  Time: 0.164s, 6230.69/s  (0.192s, 5331.37/s)  LR: 8.424e-04  Data: 0.032 (0.030)
Train: 78 [ 750/1251 ( 60%)]  Loss: 4.421 (4.43)  Time: 0.336s, 3045.30/s  (0.192s, 5331.97/s)  LR: 8.424e-04  Data: 0.028 (0.030)
Train: 78 [ 800/1251 ( 64%)]  Loss: 4.352 (4.43)  Time: 0.159s, 6453.58/s  (0.192s, 5332.32/s)  LR: 8.424e-04  Data: 0.031 (0.030)
Train: 78 [ 850/1251 ( 68%)]  Loss: 4.180 (4.41)  Time: 0.202s, 5081.59/s  (0.192s, 5326.97/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [ 900/1251 ( 72%)]  Loss: 3.983 (4.39)  Time: 0.156s, 6567.38/s  (0.192s, 5322.00/s)  LR: 8.424e-04  Data: 0.032 (0.030)
Train: 78 [ 950/1251 ( 76%)]  Loss: 4.959 (4.42)  Time: 0.401s, 2555.16/s  (0.193s, 5308.37/s)  LR: 8.424e-04  Data: 0.042 (0.030)
Train: 78 [1000/1251 ( 80%)]  Loss: 4.562 (4.43)  Time: 0.178s, 5763.45/s  (0.193s, 5311.67/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [1050/1251 ( 84%)]  Loss: 4.538 (4.43)  Time: 0.170s, 6035.67/s  (0.193s, 5305.73/s)  LR: 8.424e-04  Data: 0.033 (0.030)
Train: 78 [1100/1251 ( 88%)]  Loss: 4.346 (4.43)  Time: 0.163s, 6269.02/s  (0.193s, 5300.42/s)  LR: 8.424e-04  Data: 0.035 (0.030)
Train: 78 [1150/1251 ( 92%)]  Loss: 4.633 (4.44)  Time: 0.174s, 5879.05/s  (0.193s, 5301.17/s)  LR: 8.424e-04  Data: 0.030 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 78 [1200/1251 ( 96%)]  Loss: 4.285 (4.43)  Time: 0.622s, 1646.70/s  (0.193s, 5294.40/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [1250/1251 (100%)]  Loss: 4.521 (4.43)  Time: 0.113s, 9031.56/s  (0.193s, 5317.43/s)  LR: 8.424e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1446 (1.1446)  Acc@1: 79.1016 (79.1016)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.2895 (1.9020)  Acc@1: 78.3019 (62.3100)  Acc@5: 91.0377 (84.3400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)

Train: 79 [   0/1251 (  0%)]  Loss: 4.349 (4.35)  Time: 1.666s,  614.50/s  (1.666s,  614.50/s)  LR: 8.386e-04  Data: 1.528 (1.528)
Train: 79 [  50/1251 (  4%)]  Loss: 4.179 (4.26)  Time: 0.170s, 6006.37/s  (0.218s, 4696.47/s)  LR: 8.386e-04  Data: 0.028 (0.063)
Train: 79 [ 100/1251 (  8%)]  Loss: 4.122 (4.22)  Time: 0.179s, 5719.48/s  (0.207s, 4957.48/s)  LR: 8.386e-04  Data: 0.026 (0.046)
Train: 79 [ 150/1251 ( 12%)]  Loss: 4.039 (4.17)  Time: 0.176s, 5819.34/s  (0.199s, 5137.82/s)  LR: 8.386e-04  Data: 0.026 (0.040)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.127 (4.16)  Time: 0.186s, 5513.45/s  (0.197s, 5194.19/s)  LR: 8.386e-04  Data: 0.028 (0.037)
Train: 79 [ 250/1251 ( 20%)]  Loss: 4.004 (4.14)  Time: 0.170s, 6008.59/s  (0.195s, 5247.63/s)  LR: 8.386e-04  Data: 0.025 (0.036)
Train: 79 [ 300/1251 ( 24%)]  Loss: 4.501 (4.19)  Time: 0.168s, 6113.14/s  (0.195s, 5255.16/s)  LR: 8.386e-04  Data: 0.024 (0.035)
Train: 79 [ 350/1251 ( 28%)]  Loss: 4.555 (4.23)  Time: 0.161s, 6374.20/s  (0.194s, 5268.33/s)  LR: 8.386e-04  Data: 0.024 (0.034)
Train: 79 [ 400/1251 ( 32%)]  Loss: 4.397 (4.25)  Time: 0.153s, 6701.21/s  (0.193s, 5297.55/s)  LR: 8.386e-04  Data: 0.023 (0.033)
Train: 79 [ 450/1251 ( 36%)]  Loss: 4.459 (4.27)  Time: 0.165s, 6207.76/s  (0.193s, 5301.34/s)  LR: 8.386e-04  Data: 0.030 (0.033)
Train: 79 [ 500/1251 ( 40%)]  Loss: 4.312 (4.28)  Time: 0.160s, 6407.15/s  (0.192s, 5324.14/s)  LR: 8.386e-04  Data: 0.034 (0.032)
Train: 79 [ 550/1251 ( 44%)]  Loss: 4.280 (4.28)  Time: 0.187s, 5477.74/s  (0.192s, 5336.46/s)  LR: 8.386e-04  Data: 0.027 (0.032)
Train: 79 [ 600/1251 ( 48%)]  Loss: 4.537 (4.30)  Time: 0.314s, 3259.67/s  (0.192s, 5333.44/s)  LR: 8.386e-04  Data: 0.041 (0.032)
Train: 79 [ 650/1251 ( 52%)]  Loss: 4.294 (4.30)  Time: 0.163s, 6271.88/s  (0.192s, 5333.11/s)  LR: 8.386e-04  Data: 0.024 (0.031)
Train: 79 [ 700/1251 ( 56%)]  Loss: 4.397 (4.30)  Time: 0.169s, 6045.00/s  (0.192s, 5334.97/s)  LR: 8.386e-04  Data: 0.023 (0.032)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.903 (4.28)  Time: 0.185s, 5540.24/s  (0.192s, 5336.83/s)  LR: 8.386e-04  Data: 0.027 (0.033)
Train: 79 [ 800/1251 ( 64%)]  Loss: 4.421 (4.29)  Time: 0.180s, 5685.44/s  (0.192s, 5335.40/s)  LR: 8.386e-04  Data: 0.039 (0.034)
Train: 79 [ 850/1251 ( 68%)]  Loss: 4.003 (4.27)  Time: 0.166s, 6175.44/s  (0.192s, 5334.41/s)  LR: 8.386e-04  Data: 0.028 (0.033)
Train: 79 [ 900/1251 ( 72%)]  Loss: 4.474 (4.28)  Time: 0.190s, 5398.55/s  (0.192s, 5329.41/s)  LR: 8.386e-04  Data: 0.023 (0.033)
Train: 79 [ 950/1251 ( 76%)]  Loss: 4.449 (4.29)  Time: 0.185s, 5521.65/s  (0.192s, 5329.45/s)  LR: 8.386e-04  Data: 0.033 (0.033)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.951 (4.27)  Time: 0.188s, 5449.58/s  (0.192s, 5334.79/s)  LR: 8.386e-04  Data: 0.029 (0.033)
Train: 79 [1050/1251 ( 84%)]  Loss: 4.516 (4.28)  Time: 0.204s, 5007.89/s  (0.192s, 5336.17/s)  LR: 8.386e-04  Data: 0.023 (0.032)
Train: 79 [1100/1251 ( 88%)]  Loss: 4.365 (4.29)  Time: 0.171s, 5996.10/s  (0.192s, 5324.64/s)  LR: 8.386e-04  Data: 0.036 (0.032)
Train: 79 [1150/1251 ( 92%)]  Loss: 4.376 (4.29)  Time: 0.160s, 6388.58/s  (0.192s, 5320.42/s)  LR: 8.386e-04  Data: 0.030 (0.032)
Train: 79 [1200/1251 ( 96%)]  Loss: 4.116 (4.29)  Time: 0.177s, 5787.26/s  (0.193s, 5317.02/s)  LR: 8.386e-04  Data: 0.022 (0.032)
Train: 79 [1250/1251 (100%)]  Loss: 4.485 (4.29)  Time: 0.114s, 8996.03/s  (0.192s, 5331.14/s)  LR: 8.386e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.775 (1.775)  Loss:  1.1775 (1.1775)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.2608 (1.9369)  Acc@1: 78.3019 (61.9840)  Acc@5: 91.6274 (84.3460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)

Train: 80 [   0/1251 (  0%)]  Loss: 4.409 (4.41)  Time: 1.746s,  586.63/s  (1.746s,  586.63/s)  LR: 8.347e-04  Data: 1.618 (1.618)
Train: 80 [  50/1251 (  4%)]  Loss: 4.304 (4.36)  Time: 0.183s, 5591.55/s  (0.220s, 4655.10/s)  LR: 8.347e-04  Data: 0.020 (0.066)
Train: 80 [ 100/1251 (  8%)]  Loss: 4.615 (4.44)  Time: 0.167s, 6116.87/s  (0.208s, 4933.12/s)  LR: 8.347e-04  Data: 0.029 (0.048)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.253 (4.40)  Time: 0.160s, 6398.10/s  (0.201s, 5098.85/s)  LR: 8.347e-04  Data: 0.026 (0.041)
Train: 80 [ 200/1251 ( 16%)]  Loss: 4.176 (4.35)  Time: 0.177s, 5798.83/s  (0.199s, 5152.98/s)  LR: 8.347e-04  Data: 0.026 (0.038)
Train: 80 [ 250/1251 ( 20%)]  Loss: 4.671 (4.40)  Time: 0.196s, 5218.13/s  (0.196s, 5213.65/s)  LR: 8.347e-04  Data: 0.023 (0.036)
Train: 80 [ 300/1251 ( 24%)]  Loss: 4.359 (4.40)  Time: 0.159s, 6458.06/s  (0.195s, 5247.08/s)  LR: 8.347e-04  Data: 0.028 (0.035)
Train: 80 [ 350/1251 ( 28%)]  Loss: 4.197 (4.37)  Time: 0.164s, 6242.79/s  (0.193s, 5298.21/s)  LR: 8.347e-04  Data: 0.022 (0.035)
Train: 80 [ 400/1251 ( 32%)]  Loss: 4.351 (4.37)  Time: 0.416s, 2460.18/s  (0.194s, 5280.87/s)  LR: 8.347e-04  Data: 0.022 (0.034)
Train: 80 [ 450/1251 ( 36%)]  Loss: 4.109 (4.34)  Time: 0.156s, 6569.88/s  (0.193s, 5304.23/s)  LR: 8.347e-04  Data: 0.027 (0.033)
Train: 80 [ 500/1251 ( 40%)]  Loss: 4.558 (4.36)  Time: 0.153s, 6686.70/s  (0.192s, 5326.81/s)  LR: 8.347e-04  Data: 0.031 (0.033)
Train: 80 [ 550/1251 ( 44%)]  Loss: 4.316 (4.36)  Time: 0.176s, 5801.91/s  (0.193s, 5317.45/s)  LR: 8.347e-04  Data: 0.028 (0.033)
Train: 80 [ 600/1251 ( 48%)]  Loss: 4.487 (4.37)  Time: 0.174s, 5890.79/s  (0.193s, 5315.75/s)  LR: 8.347e-04  Data: 0.033 (0.032)
Train: 80 [ 650/1251 ( 52%)]  Loss: 4.334 (4.37)  Time: 0.167s, 6124.38/s  (0.192s, 5331.54/s)  LR: 8.347e-04  Data: 0.024 (0.032)
Train: 80 [ 700/1251 ( 56%)]  Loss: 4.557 (4.38)  Time: 0.171s, 6000.26/s  (0.192s, 5339.32/s)  LR: 8.347e-04  Data: 0.024 (0.032)
Train: 80 [ 750/1251 ( 60%)]  Loss: 4.272 (4.37)  Time: 0.197s, 5188.22/s  (0.192s, 5336.03/s)  LR: 8.347e-04  Data: 0.067 (0.033)
Train: 80 [ 800/1251 ( 64%)]  Loss: 4.041 (4.35)  Time: 0.205s, 4998.37/s  (0.192s, 5327.30/s)  LR: 8.347e-04  Data: 0.020 (0.035)
Train: 80 [ 850/1251 ( 68%)]  Loss: 4.575 (4.37)  Time: 0.175s, 5864.58/s  (0.192s, 5336.25/s)  LR: 8.347e-04  Data: 0.027 (0.035)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.289 (4.36)  Time: 0.168s, 6098.42/s  (0.192s, 5340.31/s)  LR: 8.347e-04  Data: 0.026 (0.036)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.930 (4.34)  Time: 0.319s, 3212.97/s  (0.192s, 5336.84/s)  LR: 8.347e-04  Data: 0.124 (0.036)
Train: 80 [1000/1251 ( 80%)]  Loss: 4.306 (4.34)  Time: 0.166s, 6162.74/s  (0.192s, 5330.77/s)  LR: 8.347e-04  Data: 0.022 (0.036)
Train: 80 [1050/1251 ( 84%)]  Loss: 4.476 (4.34)  Time: 0.163s, 6294.76/s  (0.192s, 5331.15/s)  LR: 8.347e-04  Data: 0.027 (0.036)
Train: 80 [1100/1251 ( 88%)]  Loss: 4.328 (4.34)  Time: 0.158s, 6494.15/s  (0.192s, 5324.26/s)  LR: 8.347e-04  Data: 0.026 (0.037)
Train: 80 [1150/1251 ( 92%)]  Loss: 4.163 (4.34)  Time: 0.554s, 1849.84/s  (0.193s, 5313.61/s)  LR: 8.347e-04  Data: 0.037 (0.036)
Train: 80 [1200/1251 ( 96%)]  Loss: 4.550 (4.35)  Time: 0.172s, 5952.72/s  (0.193s, 5307.72/s)  LR: 8.347e-04  Data: 0.023 (0.036)
Train: 80 [1250/1251 (100%)]  Loss: 4.118 (4.34)  Time: 0.114s, 8963.22/s  (0.192s, 5330.02/s)  LR: 8.347e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.850 (1.850)  Loss:  1.1093 (1.1093)  Acc@1: 79.1016 (79.1016)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1664 (1.8430)  Acc@1: 77.4764 (62.6020)  Acc@5: 92.2170 (84.7180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)

Train: 81 [   0/1251 (  0%)]  Loss: 4.351 (4.35)  Time: 1.727s,  592.78/s  (1.727s,  592.78/s)  LR: 8.308e-04  Data: 1.577 (1.577)
Train: 81 [  50/1251 (  4%)]  Loss: 4.338 (4.34)  Time: 0.186s, 5492.81/s  (0.225s, 4560.60/s)  LR: 8.308e-04  Data: 0.020 (0.070)
Train: 81 [ 100/1251 (  8%)]  Loss: 4.459 (4.38)  Time: 0.145s, 7042.30/s  (0.205s, 5000.57/s)  LR: 8.308e-04  Data: 0.027 (0.050)
Train: 81 [ 150/1251 ( 12%)]  Loss: 4.044 (4.30)  Time: 0.174s, 5886.24/s  (0.202s, 5068.37/s)  LR: 8.308e-04  Data: 0.027 (0.043)
Train: 81 [ 200/1251 ( 16%)]  Loss: 4.273 (4.29)  Time: 0.179s, 5718.97/s  (0.199s, 5143.03/s)  LR: 8.308e-04  Data: 0.029 (0.039)
Train: 81 [ 250/1251 ( 20%)]  Loss: 4.423 (4.31)  Time: 0.177s, 5791.18/s  (0.196s, 5222.49/s)  LR: 8.308e-04  Data: 0.030 (0.037)
Train: 81 [ 300/1251 ( 24%)]  Loss: 4.257 (4.31)  Time: 0.172s, 5936.51/s  (0.195s, 5249.41/s)  LR: 8.308e-04  Data: 0.025 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 81 [ 350/1251 ( 28%)]  Loss: 4.040 (4.27)  Time: 0.165s, 6204.00/s  (0.194s, 5266.81/s)  LR: 8.308e-04  Data: 0.023 (0.034)
Train: 81 [ 400/1251 ( 32%)]  Loss: 4.630 (4.31)  Time: 0.177s, 5800.75/s  (0.194s, 5276.17/s)  LR: 8.308e-04  Data: 0.028 (0.034)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.952 (4.28)  Time: 0.170s, 6024.90/s  (0.193s, 5301.00/s)  LR: 8.308e-04  Data: 0.036 (0.033)
Train: 81 [ 500/1251 ( 40%)]  Loss: 4.210 (4.27)  Time: 0.160s, 6414.75/s  (0.193s, 5315.38/s)  LR: 8.308e-04  Data: 0.025 (0.033)
Train: 81 [ 550/1251 ( 44%)]  Loss: 4.822 (4.32)  Time: 0.167s, 6144.36/s  (0.193s, 5314.50/s)  LR: 8.308e-04  Data: 0.031 (0.032)
Train: 81 [ 600/1251 ( 48%)]  Loss: 4.256 (4.31)  Time: 0.163s, 6295.13/s  (0.193s, 5312.95/s)  LR: 8.308e-04  Data: 0.026 (0.032)
Train: 81 [ 650/1251 ( 52%)]  Loss: 4.470 (4.32)  Time: 0.162s, 6339.20/s  (0.192s, 5327.12/s)  LR: 8.308e-04  Data: 0.023 (0.032)
Train: 81 [ 700/1251 ( 56%)]  Loss: 4.466 (4.33)  Time: 0.155s, 6624.24/s  (0.192s, 5332.30/s)  LR: 8.308e-04  Data: 0.028 (0.031)
Train: 81 [ 750/1251 ( 60%)]  Loss: 4.448 (4.34)  Time: 0.202s, 5059.61/s  (0.192s, 5323.37/s)  LR: 8.308e-04  Data: 0.035 (0.031)
Train: 81 [ 800/1251 ( 64%)]  Loss: 4.386 (4.34)  Time: 0.164s, 6243.84/s  (0.192s, 5328.90/s)  LR: 8.308e-04  Data: 0.026 (0.031)
Train: 81 [ 850/1251 ( 68%)]  Loss: 4.346 (4.34)  Time: 0.174s, 5882.36/s  (0.192s, 5338.04/s)  LR: 8.308e-04  Data: 0.026 (0.031)
Train: 81 [ 900/1251 ( 72%)]  Loss: 4.264 (4.34)  Time: 0.214s, 4787.83/s  (0.192s, 5339.33/s)  LR: 8.308e-04  Data: 0.021 (0.031)
Train: 81 [ 950/1251 ( 76%)]  Loss: 4.127 (4.33)  Time: 0.292s, 3502.53/s  (0.192s, 5336.54/s)  LR: 8.308e-04  Data: 0.026 (0.030)
Train: 81 [1000/1251 ( 80%)]  Loss: 4.651 (4.34)  Time: 0.369s, 2773.59/s  (0.192s, 5336.87/s)  LR: 8.308e-04  Data: 0.027 (0.030)
Train: 81 [1050/1251 ( 84%)]  Loss: 4.270 (4.34)  Time: 0.152s, 6716.25/s  (0.192s, 5344.33/s)  LR: 8.308e-04  Data: 0.025 (0.030)
Train: 81 [1100/1251 ( 88%)]  Loss: 4.172 (4.33)  Time: 0.179s, 5725.78/s  (0.192s, 5341.45/s)  LR: 8.308e-04  Data: 0.019 (0.030)
Train: 81 [1150/1251 ( 92%)]  Loss: 4.375 (4.33)  Time: 0.520s, 1970.33/s  (0.192s, 5332.61/s)  LR: 8.308e-04  Data: 0.018 (0.030)
Train: 81 [1200/1251 ( 96%)]  Loss: 4.268 (4.33)  Time: 0.167s, 6127.45/s  (0.192s, 5325.30/s)  LR: 8.308e-04  Data: 0.025 (0.030)
Train: 81 [1250/1251 (100%)]  Loss: 4.198 (4.33)  Time: 0.114s, 9020.03/s  (0.192s, 5342.47/s)  LR: 8.308e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.875 (1.875)  Loss:  1.3802 (1.3802)  Acc@1: 76.0742 (76.0742)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.1955 (1.9297)  Acc@1: 78.7736 (62.1680)  Acc@5: 91.9811 (84.5600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)

Train: 82 [   0/1251 (  0%)]  Loss: 4.277 (4.28)  Time: 1.728s,  592.73/s  (1.728s,  592.73/s)  LR: 8.269e-04  Data: 1.596 (1.596)
Train: 82 [  50/1251 (  4%)]  Loss: 4.348 (4.31)  Time: 0.167s, 6113.55/s  (0.223s, 4585.79/s)  LR: 8.269e-04  Data: 0.026 (0.080)
Train: 82 [ 100/1251 (  8%)]  Loss: 4.004 (4.21)  Time: 0.172s, 5943.64/s  (0.205s, 4994.07/s)  LR: 8.269e-04  Data: 0.031 (0.061)
Train: 82 [ 150/1251 ( 12%)]  Loss: 4.290 (4.23)  Time: 0.160s, 6400.54/s  (0.199s, 5146.42/s)  LR: 8.269e-04  Data: 0.028 (0.053)
Train: 82 [ 200/1251 ( 16%)]  Loss: 4.121 (4.21)  Time: 0.187s, 5461.74/s  (0.197s, 5209.86/s)  LR: 8.269e-04  Data: 0.023 (0.047)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.378 (4.24)  Time: 0.176s, 5819.90/s  (0.195s, 5247.26/s)  LR: 8.269e-04  Data: 0.029 (0.045)
Train: 82 [ 300/1251 ( 24%)]  Loss: 4.451 (4.27)  Time: 0.168s, 6092.78/s  (0.194s, 5272.41/s)  LR: 8.269e-04  Data: 0.032 (0.043)
Train: 82 [ 350/1251 ( 28%)]  Loss: 4.842 (4.34)  Time: 0.177s, 5800.82/s  (0.193s, 5311.63/s)  LR: 8.269e-04  Data: 0.026 (0.041)
Train: 82 [ 400/1251 ( 32%)]  Loss: 4.508 (4.36)  Time: 0.170s, 6015.65/s  (0.193s, 5312.52/s)  LR: 8.269e-04  Data: 0.020 (0.039)
Train: 82 [ 450/1251 ( 36%)]  Loss: 4.426 (4.36)  Time: 0.179s, 5708.55/s  (0.193s, 5295.47/s)  LR: 8.269e-04  Data: 0.056 (0.038)
Train: 82 [ 500/1251 ( 40%)]  Loss: 4.393 (4.37)  Time: 0.259s, 3949.82/s  (0.193s, 5313.71/s)  LR: 8.269e-04  Data: 0.028 (0.037)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.437 (4.37)  Time: 0.186s, 5500.54/s  (0.192s, 5326.33/s)  LR: 8.269e-04  Data: 0.024 (0.037)
Train: 82 [ 600/1251 ( 48%)]  Loss: 4.475 (4.38)  Time: 0.162s, 6337.72/s  (0.192s, 5331.95/s)  LR: 8.269e-04  Data: 0.030 (0.036)
Train: 82 [ 650/1251 ( 52%)]  Loss: 4.283 (4.37)  Time: 0.155s, 6618.31/s  (0.193s, 5315.48/s)  LR: 8.269e-04  Data: 0.029 (0.035)
Train: 82 [ 700/1251 ( 56%)]  Loss: 4.780 (4.40)  Time: 0.159s, 6421.69/s  (0.192s, 5321.94/s)  LR: 8.269e-04  Data: 0.024 (0.035)
Train: 82 [ 750/1251 ( 60%)]  Loss: 4.531 (4.41)  Time: 0.168s, 6098.15/s  (0.192s, 5332.35/s)  LR: 8.269e-04  Data: 0.021 (0.034)
Train: 82 [ 800/1251 ( 64%)]  Loss: 4.121 (4.39)  Time: 0.168s, 6091.25/s  (0.192s, 5337.84/s)  LR: 8.269e-04  Data: 0.020 (0.034)
Train: 82 [ 850/1251 ( 68%)]  Loss: 4.198 (4.38)  Time: 0.177s, 5773.48/s  (0.192s, 5329.04/s)  LR: 8.269e-04  Data: 0.027 (0.034)
Train: 82 [ 900/1251 ( 72%)]  Loss: 4.329 (4.38)  Time: 0.184s, 5576.49/s  (0.192s, 5327.15/s)  LR: 8.269e-04  Data: 0.024 (0.033)
Train: 82 [ 950/1251 ( 76%)]  Loss: 4.173 (4.37)  Time: 0.176s, 5805.33/s  (0.192s, 5325.98/s)  LR: 8.269e-04  Data: 0.027 (0.033)
Train: 82 [1000/1251 ( 80%)]  Loss: 4.480 (4.37)  Time: 0.174s, 5885.83/s  (0.192s, 5321.58/s)  LR: 8.269e-04  Data: 0.031 (0.033)
Train: 82 [1050/1251 ( 84%)]  Loss: 4.503 (4.38)  Time: 0.195s, 5249.50/s  (0.193s, 5316.77/s)  LR: 8.269e-04  Data: 0.024 (0.033)
Train: 82 [1100/1251 ( 88%)]  Loss: 4.589 (4.39)  Time: 0.175s, 5841.09/s  (0.193s, 5311.14/s)  LR: 8.269e-04  Data: 0.028 (0.032)
Train: 82 [1150/1251 ( 92%)]  Loss: 4.197 (4.38)  Time: 0.168s, 6078.19/s  (0.193s, 5312.60/s)  LR: 8.269e-04  Data: 0.021 (0.032)
Train: 82 [1200/1251 ( 96%)]  Loss: 4.623 (4.39)  Time: 0.183s, 5589.75/s  (0.193s, 5312.78/s)  LR: 8.269e-04  Data: 0.034 (0.032)
Train: 82 [1250/1251 (100%)]  Loss: 4.450 (4.39)  Time: 0.114s, 9021.75/s  (0.192s, 5322.37/s)  LR: 8.269e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.818 (1.818)  Loss:  1.0422 (1.0422)  Acc@1: 78.6133 (78.6133)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2565 (1.8634)  Acc@1: 76.8868 (61.6560)  Acc@5: 90.9198 (84.1380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)

Train: 83 [   0/1251 (  0%)]  Loss: 4.106 (4.11)  Time: 1.846s,  554.81/s  (1.846s,  554.81/s)  LR: 8.229e-04  Data: 1.716 (1.716)
Train: 83 [  50/1251 (  4%)]  Loss: 4.325 (4.22)  Time: 0.162s, 6337.32/s  (0.223s, 4592.96/s)  LR: 8.229e-04  Data: 0.029 (0.074)
Train: 83 [ 100/1251 (  8%)]  Loss: 4.207 (4.21)  Time: 0.182s, 5620.77/s  (0.209s, 4908.25/s)  LR: 8.229e-04  Data: 0.021 (0.060)
Train: 83 [ 150/1251 ( 12%)]  Loss: 4.015 (4.16)  Time: 0.189s, 5414.49/s  (0.199s, 5141.80/s)  LR: 8.229e-04  Data: 0.029 (0.052)
Train: 83 [ 200/1251 ( 16%)]  Loss: 4.036 (4.14)  Time: 0.171s, 5996.55/s  (0.197s, 5203.00/s)  LR: 8.229e-04  Data: 0.030 (0.048)
Train: 83 [ 250/1251 ( 20%)]  Loss: 4.474 (4.19)  Time: 0.179s, 5728.96/s  (0.196s, 5213.54/s)  LR: 8.229e-04  Data: 0.033 (0.044)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 83 [ 300/1251 ( 24%)]  Loss: 4.354 (4.22)  Time: 0.182s, 5632.17/s  (0.195s, 5240.40/s)  LR: 8.229e-04  Data: 0.028 (0.042)
Train: 83 [ 350/1251 ( 28%)]  Loss: 4.571 (4.26)  Time: 0.169s, 6043.28/s  (0.194s, 5270.59/s)  LR: 8.229e-04  Data: 0.020 (0.040)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.884 (4.22)  Time: 0.161s, 6359.37/s  (0.194s, 5278.62/s)  LR: 8.229e-04  Data: 0.025 (0.038)
Train: 83 [ 450/1251 ( 36%)]  Loss: 4.416 (4.24)  Time: 0.213s, 4797.92/s  (0.194s, 5284.58/s)  LR: 8.229e-04  Data: 0.027 (0.037)
Train: 83 [ 500/1251 ( 40%)]  Loss: 4.628 (4.27)  Time: 0.164s, 6260.08/s  (0.193s, 5297.98/s)  LR: 8.229e-04  Data: 0.028 (0.036)
Train: 83 [ 550/1251 ( 44%)]  Loss: 3.984 (4.25)  Time: 0.179s, 5707.47/s  (0.194s, 5287.87/s)  LR: 8.229e-04  Data: 0.027 (0.036)
Train: 83 [ 600/1251 ( 48%)]  Loss: 4.659 (4.28)  Time: 0.179s, 5726.48/s  (0.194s, 5291.30/s)  LR: 8.229e-04  Data: 0.021 (0.035)
Train: 83 [ 650/1251 ( 52%)]  Loss: 4.272 (4.28)  Time: 0.171s, 5971.36/s  (0.193s, 5295.08/s)  LR: 8.229e-04  Data: 0.024 (0.035)
Train: 83 [ 700/1251 ( 56%)]  Loss: 4.715 (4.31)  Time: 0.191s, 5355.29/s  (0.193s, 5306.72/s)  LR: 8.229e-04  Data: 0.029 (0.034)
Train: 83 [ 750/1251 ( 60%)]  Loss: 4.445 (4.32)  Time: 0.188s, 5442.81/s  (0.193s, 5303.97/s)  LR: 8.229e-04  Data: 0.026 (0.034)
Train: 83 [ 800/1251 ( 64%)]  Loss: 4.478 (4.33)  Time: 0.159s, 6445.23/s  (0.193s, 5300.41/s)  LR: 8.229e-04  Data: 0.025 (0.034)
Train: 83 [ 850/1251 ( 68%)]  Loss: 4.411 (4.33)  Time: 0.181s, 5656.95/s  (0.193s, 5313.47/s)  LR: 8.229e-04  Data: 0.025 (0.033)
Train: 83 [ 900/1251 ( 72%)]  Loss: 4.035 (4.32)  Time: 0.174s, 5874.83/s  (0.193s, 5302.38/s)  LR: 8.229e-04  Data: 0.031 (0.033)
Train: 83 [ 950/1251 ( 76%)]  Loss: 4.147 (4.31)  Time: 0.261s, 3929.68/s  (0.193s, 5311.91/s)  LR: 8.229e-04  Data: 0.034 (0.033)
Train: 83 [1000/1251 ( 80%)]  Loss: 4.109 (4.30)  Time: 0.156s, 6559.35/s  (0.193s, 5315.01/s)  LR: 8.229e-04  Data: 0.023 (0.033)
Train: 83 [1050/1251 ( 84%)]  Loss: 4.544 (4.31)  Time: 0.160s, 6398.25/s  (0.193s, 5312.69/s)  LR: 8.229e-04  Data: 0.031 (0.032)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.879 (4.29)  Time: 0.439s, 2330.60/s  (0.193s, 5305.39/s)  LR: 8.229e-04  Data: 0.020 (0.032)
Train: 83 [1150/1251 ( 92%)]  Loss: 4.176 (4.29)  Time: 0.165s, 6209.91/s  (0.193s, 5307.92/s)  LR: 8.229e-04  Data: 0.026 (0.032)
Train: 83 [1200/1251 ( 96%)]  Loss: 4.644 (4.30)  Time: 0.303s, 3381.49/s  (0.193s, 5305.52/s)  LR: 8.229e-04  Data: 0.023 (0.032)
Train: 83 [1250/1251 (100%)]  Loss: 4.272 (4.30)  Time: 0.113s, 9026.42/s  (0.192s, 5323.50/s)  LR: 8.229e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.761 (1.761)  Loss:  1.3063 (1.3063)  Acc@1: 78.4180 (78.4180)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2702 (1.9176)  Acc@1: 78.0660 (62.1360)  Acc@5: 91.1557 (84.4280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)

Train: 84 [   0/1251 (  0%)]  Loss: 4.202 (4.20)  Time: 1.813s,  564.86/s  (1.813s,  564.86/s)  LR: 8.189e-04  Data: 1.688 (1.688)
Train: 84 [  50/1251 (  4%)]  Loss: 4.281 (4.24)  Time: 0.174s, 5872.47/s  (0.217s, 4711.09/s)  LR: 8.189e-04  Data: 0.033 (0.071)
Train: 84 [ 100/1251 (  8%)]  Loss: 4.104 (4.20)  Time: 0.170s, 6029.18/s  (0.204s, 5023.32/s)  LR: 8.189e-04  Data: 0.024 (0.050)
Train: 84 [ 150/1251 ( 12%)]  Loss: 4.508 (4.27)  Time: 0.168s, 6095.05/s  (0.199s, 5146.40/s)  LR: 8.189e-04  Data: 0.028 (0.043)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.858 (4.19)  Time: 0.179s, 5708.98/s  (0.196s, 5234.50/s)  LR: 8.189e-04  Data: 0.031 (0.040)
Train: 84 [ 250/1251 ( 20%)]  Loss: 4.315 (4.21)  Time: 0.215s, 4771.34/s  (0.195s, 5245.55/s)  LR: 8.189e-04  Data: 0.034 (0.038)
Train: 84 [ 300/1251 ( 24%)]  Loss: 4.404 (4.24)  Time: 0.163s, 6278.17/s  (0.194s, 5290.82/s)  LR: 8.189e-04  Data: 0.029 (0.036)
Train: 84 [ 350/1251 ( 28%)]  Loss: 4.443 (4.26)  Time: 0.194s, 5282.29/s  (0.194s, 5268.87/s)  LR: 8.189e-04  Data: 0.021 (0.035)
Train: 84 [ 400/1251 ( 32%)]  Loss: 4.039 (4.24)  Time: 0.358s, 2857.17/s  (0.194s, 5285.45/s)  LR: 8.189e-04  Data: 0.026 (0.034)
Train: 84 [ 450/1251 ( 36%)]  Loss: 4.275 (4.24)  Time: 0.190s, 5403.05/s  (0.194s, 5287.10/s)  LR: 8.189e-04  Data: 0.044 (0.033)
Train: 84 [ 500/1251 ( 40%)]  Loss: 4.362 (4.25)  Time: 0.158s, 6465.09/s  (0.193s, 5297.85/s)  LR: 8.189e-04  Data: 0.031 (0.033)
Train: 84 [ 550/1251 ( 44%)]  Loss: 4.225 (4.25)  Time: 0.167s, 6130.38/s  (0.193s, 5315.85/s)  LR: 8.189e-04  Data: 0.026 (0.032)
Train: 84 [ 600/1251 ( 48%)]  Loss: 4.593 (4.28)  Time: 0.162s, 6302.24/s  (0.193s, 5317.99/s)  LR: 8.189e-04  Data: 0.028 (0.032)
Train: 84 [ 650/1251 ( 52%)]  Loss: 4.524 (4.30)  Time: 0.177s, 5788.01/s  (0.192s, 5320.19/s)  LR: 8.189e-04  Data: 0.037 (0.032)
Train: 84 [ 700/1251 ( 56%)]  Loss: 4.273 (4.29)  Time: 0.255s, 4019.78/s  (0.192s, 5322.62/s)  LR: 8.189e-04  Data: 0.032 (0.032)
Train: 84 [ 750/1251 ( 60%)]  Loss: 4.416 (4.30)  Time: 0.227s, 4507.86/s  (0.192s, 5328.55/s)  LR: 8.189e-04  Data: 0.022 (0.032)
Train: 84 [ 800/1251 ( 64%)]  Loss: 4.693 (4.32)  Time: 0.178s, 5744.23/s  (0.193s, 5306.18/s)  LR: 8.189e-04  Data: 0.034 (0.033)
Train: 84 [ 850/1251 ( 68%)]  Loss: 4.034 (4.31)  Time: 0.183s, 5595.85/s  (0.193s, 5303.19/s)  LR: 8.189e-04  Data: 0.053 (0.034)
Train: 84 [ 900/1251 ( 72%)]  Loss: 4.353 (4.31)  Time: 0.169s, 6044.26/s  (0.193s, 5305.07/s)  LR: 8.189e-04  Data: 0.036 (0.034)
Train: 84 [ 950/1251 ( 76%)]  Loss: 4.539 (4.32)  Time: 0.185s, 5543.52/s  (0.193s, 5316.68/s)  LR: 8.189e-04  Data: 0.028 (0.034)
Train: 84 [1000/1251 ( 80%)]  Loss: 4.490 (4.33)  Time: 0.217s, 4729.78/s  (0.193s, 5305.89/s)  LR: 8.189e-04  Data: 0.024 (0.035)
Train: 84 [1050/1251 ( 84%)]  Loss: 4.176 (4.32)  Time: 0.275s, 3720.43/s  (0.193s, 5305.31/s)  LR: 8.189e-04  Data: 0.153 (0.036)
Train: 84 [1100/1251 ( 88%)]  Loss: 4.468 (4.33)  Time: 0.155s, 6610.32/s  (0.193s, 5307.07/s)  LR: 8.189e-04  Data: 0.027 (0.036)
Train: 84 [1150/1251 ( 92%)]  Loss: 4.517 (4.34)  Time: 0.183s, 5594.32/s  (0.193s, 5311.38/s)  LR: 8.189e-04  Data: 0.027 (0.036)
Train: 84 [1200/1251 ( 96%)]  Loss: 4.469 (4.34)  Time: 0.158s, 6478.15/s  (0.193s, 5297.12/s)  LR: 8.189e-04  Data: 0.032 (0.036)
Train: 84 [1250/1251 (100%)]  Loss: 4.353 (4.34)  Time: 0.113s, 9045.24/s  (0.193s, 5311.16/s)  LR: 8.189e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.852 (1.852)  Loss:  1.2175 (1.2175)  Acc@1: 77.3438 (77.3438)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.2846 (1.8833)  Acc@1: 76.6509 (61.4180)  Acc@5: 91.7453 (83.8520)
Train: 85 [   0/1251 (  0%)]  Loss: 4.479 (4.48)  Time: 1.901s,  538.54/s  (1.901s,  538.54/s)  LR: 8.148e-04  Data: 1.760 (1.760)
Train: 85 [  50/1251 (  4%)]  Loss: 4.489 (4.48)  Time: 0.184s, 5556.30/s  (0.221s, 4634.18/s)  LR: 8.148e-04  Data: 0.019 (0.070)
Train: 85 [ 100/1251 (  8%)]  Loss: 4.444 (4.47)  Time: 0.178s, 5745.18/s  (0.208s, 4927.03/s)  LR: 8.148e-04  Data: 0.028 (0.049)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 85 [ 150/1251 ( 12%)]  Loss: 4.346 (4.44)  Time: 0.166s, 6155.96/s  (0.200s, 5127.96/s)  LR: 8.148e-04  Data: 0.025 (0.042)
Train: 85 [ 200/1251 ( 16%)]  Loss: 4.038 (4.36)  Time: 0.357s, 2866.61/s  (0.198s, 5174.26/s)  LR: 8.148e-04  Data: 0.031 (0.039)
Train: 85 [ 250/1251 ( 20%)]  Loss: 4.902 (4.45)  Time: 0.165s, 6213.45/s  (0.196s, 5211.49/s)  LR: 8.148e-04  Data: 0.030 (0.037)
Train: 85 [ 300/1251 ( 24%)]  Loss: 4.454 (4.45)  Time: 0.153s, 6686.32/s  (0.194s, 5278.31/s)  LR: 8.148e-04  Data: 0.021 (0.035)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.996 (4.39)  Time: 0.169s, 6074.23/s  (0.194s, 5284.26/s)  LR: 8.148e-04  Data: 0.023 (0.034)
Train: 85 [ 400/1251 ( 32%)]  Loss: 4.143 (4.37)  Time: 0.180s, 5700.74/s  (0.194s, 5265.46/s)  LR: 8.148e-04  Data: 0.030 (0.033)
Train: 85 [ 450/1251 ( 36%)]  Loss: 4.182 (4.35)  Time: 0.176s, 5806.76/s  (0.193s, 5294.93/s)  LR: 8.148e-04  Data: 0.029 (0.033)
Train: 85 [ 500/1251 ( 40%)]  Loss: 4.726 (4.38)  Time: 0.163s, 6292.87/s  (0.193s, 5312.11/s)  LR: 8.148e-04  Data: 0.026 (0.032)
Train: 85 [ 550/1251 ( 44%)]  Loss: 4.039 (4.35)  Time: 0.197s, 5197.92/s  (0.192s, 5321.50/s)  LR: 8.148e-04  Data: 0.042 (0.032)
Train: 85 [ 600/1251 ( 48%)]  Loss: 4.262 (4.35)  Time: 0.163s, 6277.09/s  (0.193s, 5316.18/s)  LR: 8.148e-04  Data: 0.022 (0.032)
Train: 85 [ 650/1251 ( 52%)]  Loss: 4.287 (4.34)  Time: 0.158s, 6466.43/s  (0.192s, 5319.90/s)  LR: 8.148e-04  Data: 0.032 (0.031)
Train: 85 [ 700/1251 ( 56%)]  Loss: 4.446 (4.35)  Time: 0.189s, 5421.83/s  (0.193s, 5315.76/s)  LR: 8.148e-04  Data: 0.025 (0.031)
Train: 85 [ 750/1251 ( 60%)]  Loss: 4.284 (4.34)  Time: 0.170s, 6012.63/s  (0.192s, 5325.32/s)  LR: 8.148e-04  Data: 0.029 (0.031)
Train: 85 [ 800/1251 ( 64%)]  Loss: 4.477 (4.35)  Time: 0.157s, 6515.62/s  (0.193s, 5312.96/s)  LR: 8.148e-04  Data: 0.027 (0.031)
Train: 85 [ 850/1251 ( 68%)]  Loss: 4.502 (4.36)  Time: 0.155s, 6585.94/s  (0.193s, 5310.77/s)  LR: 8.148e-04  Data: 0.026 (0.030)
Train: 85 [ 900/1251 ( 72%)]  Loss: 4.353 (4.36)  Time: 0.164s, 6237.68/s  (0.193s, 5314.03/s)  LR: 8.148e-04  Data: 0.035 (0.030)
Train: 85 [ 950/1251 ( 76%)]  Loss: 4.277 (4.36)  Time: 0.168s, 6111.58/s  (0.193s, 5313.86/s)  LR: 8.148e-04  Data: 0.020 (0.030)
Train: 85 [1000/1251 ( 80%)]  Loss: 4.229 (4.35)  Time: 0.171s, 6001.43/s  (0.193s, 5312.66/s)  LR: 8.148e-04  Data: 0.027 (0.030)
Train: 85 [1050/1251 ( 84%)]  Loss: 4.597 (4.36)  Time: 0.178s, 5760.80/s  (0.193s, 5312.36/s)  LR: 8.148e-04  Data: 0.023 (0.030)
Train: 85 [1100/1251 ( 88%)]  Loss: 4.497 (4.37)  Time: 0.187s, 5477.01/s  (0.193s, 5312.16/s)  LR: 8.148e-04  Data: 0.031 (0.030)
Train: 85 [1150/1251 ( 92%)]  Loss: 4.390 (4.37)  Time: 0.167s, 6117.63/s  (0.193s, 5305.97/s)  LR: 8.148e-04  Data: 0.032 (0.030)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.494 (4.37)  Time: 0.162s, 6305.93/s  (0.193s, 5303.99/s)  LR: 8.148e-04  Data: 0.023 (0.030)
Train: 85 [1250/1251 (100%)]  Loss: 4.298 (4.37)  Time: 0.114s, 9004.99/s  (0.193s, 5311.53/s)  LR: 8.148e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.767 (1.767)  Loss:  1.2436 (1.2436)  Acc@1: 78.5156 (78.5156)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2642 (1.9088)  Acc@1: 77.9481 (62.7100)  Acc@5: 92.2170 (84.7340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)

Train: 86 [   0/1251 (  0%)]  Loss: 4.256 (4.26)  Time: 1.759s,  582.27/s  (1.759s,  582.27/s)  LR: 8.108e-04  Data: 1.636 (1.636)
Train: 86 [  50/1251 (  4%)]  Loss: 4.178 (4.22)  Time: 0.181s, 5646.11/s  (0.225s, 4558.40/s)  LR: 8.108e-04  Data: 0.026 (0.074)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.932 (4.12)  Time: 0.191s, 5361.44/s  (0.206s, 4982.57/s)  LR: 8.108e-04  Data: 0.024 (0.058)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.705 (4.27)  Time: 0.169s, 6046.59/s  (0.197s, 5195.25/s)  LR: 8.108e-04  Data: 0.024 (0.050)
Train: 86 [ 200/1251 ( 16%)]  Loss: 4.244 (4.26)  Time: 0.251s, 4077.86/s  (0.196s, 5225.19/s)  LR: 8.108e-04  Data: 0.029 (0.047)
Train: 86 [ 250/1251 ( 20%)]  Loss: 4.517 (4.31)  Time: 0.165s, 6205.76/s  (0.195s, 5250.22/s)  LR: 8.108e-04  Data: 0.020 (0.043)
Train: 86 [ 300/1251 ( 24%)]  Loss: 4.178 (4.29)  Time: 0.177s, 5770.42/s  (0.195s, 5254.06/s)  LR: 8.108e-04  Data: 0.031 (0.041)
Train: 86 [ 350/1251 ( 28%)]  Loss: 4.599 (4.33)  Time: 0.148s, 6931.26/s  (0.194s, 5286.38/s)  LR: 8.108e-04  Data: 0.029 (0.039)
Train: 86 [ 400/1251 ( 32%)]  Loss: 4.178 (4.31)  Time: 0.160s, 6392.87/s  (0.193s, 5310.04/s)  LR: 8.108e-04  Data: 0.029 (0.038)
Train: 86 [ 450/1251 ( 36%)]  Loss: 4.569 (4.34)  Time: 0.173s, 5920.00/s  (0.192s, 5343.66/s)  LR: 8.108e-04  Data: 0.038 (0.037)
Train: 86 [ 500/1251 ( 40%)]  Loss: 4.559 (4.36)  Time: 0.243s, 4214.43/s  (0.192s, 5338.04/s)  LR: 8.108e-04  Data: 0.021 (0.037)
Train: 86 [ 550/1251 ( 44%)]  Loss: 4.524 (4.37)  Time: 0.181s, 5660.90/s  (0.192s, 5331.05/s)  LR: 8.108e-04  Data: 0.023 (0.036)
Train: 86 [ 600/1251 ( 48%)]  Loss: 4.356 (4.37)  Time: 0.169s, 6071.35/s  (0.192s, 5338.81/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [ 650/1251 ( 52%)]  Loss: 4.634 (4.39)  Time: 0.153s, 6674.67/s  (0.192s, 5346.09/s)  LR: 8.108e-04  Data: 0.024 (0.036)
Train: 86 [ 700/1251 ( 56%)]  Loss: 4.720 (4.41)  Time: 0.176s, 5815.39/s  (0.192s, 5339.75/s)  LR: 8.108e-04  Data: 0.026 (0.036)
Train: 86 [ 750/1251 ( 60%)]  Loss: 4.459 (4.41)  Time: 0.167s, 6130.89/s  (0.192s, 5346.09/s)  LR: 8.108e-04  Data: 0.025 (0.036)
Train: 86 [ 800/1251 ( 64%)]  Loss: 4.685 (4.43)  Time: 0.186s, 5496.52/s  (0.192s, 5340.73/s)  LR: 8.108e-04  Data: 0.020 (0.037)
Train: 86 [ 850/1251 ( 68%)]  Loss: 4.617 (4.44)  Time: 0.202s, 5071.14/s  (0.192s, 5340.20/s)  LR: 8.108e-04  Data: 0.019 (0.037)
Train: 86 [ 900/1251 ( 72%)]  Loss: 4.360 (4.44)  Time: 0.153s, 6703.04/s  (0.192s, 5342.84/s)  LR: 8.108e-04  Data: 0.026 (0.037)
Train: 86 [ 950/1251 ( 76%)]  Loss: 4.337 (4.43)  Time: 0.177s, 5799.48/s  (0.192s, 5344.85/s)  LR: 8.108e-04  Data: 0.033 (0.037)
Train: 86 [1000/1251 ( 80%)]  Loss: 4.214 (4.42)  Time: 0.174s, 5875.94/s  (0.192s, 5342.47/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [1050/1251 ( 84%)]  Loss: 4.653 (4.43)  Time: 0.159s, 6424.05/s  (0.192s, 5337.13/s)  LR: 8.108e-04  Data: 0.025 (0.036)
Train: 86 [1100/1251 ( 88%)]  Loss: 4.389 (4.43)  Time: 0.239s, 4290.94/s  (0.192s, 5331.15/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [1150/1251 ( 92%)]  Loss: 4.489 (4.43)  Time: 0.171s, 5971.96/s  (0.192s, 5333.73/s)  LR: 8.108e-04  Data: 0.029 (0.035)
Train: 86 [1200/1251 ( 96%)]  Loss: 4.278 (4.43)  Time: 0.160s, 6400.49/s  (0.192s, 5331.63/s)  LR: 8.108e-04  Data: 0.023 (0.035)
Train: 86 [1250/1251 (100%)]  Loss: 4.312 (4.42)  Time: 0.114s, 8982.99/s  (0.192s, 5343.89/s)  LR: 8.108e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.935 (1.935)  Loss:  1.2340 (1.2340)  Acc@1: 78.7109 (78.7109)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2591 (1.8772)  Acc@1: 79.4811 (62.5340)  Acc@5: 92.3349 (84.6000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)

Train: 87 [   0/1251 (  0%)]  Loss: 3.969 (3.97)  Time: 1.688s,  606.74/s  (1.688s,  606.74/s)  LR: 8.066e-04  Data: 1.544 (1.544)
Train: 87 [  50/1251 (  4%)]  Loss: 4.387 (4.18)  Time: 0.171s, 6004.10/s  (0.225s, 4548.60/s)  LR: 8.066e-04  Data: 0.024 (0.077)
Train: 87 [ 100/1251 (  8%)]  Loss: 4.474 (4.28)  Time: 0.178s, 5756.01/s  (0.208s, 4921.86/s)  LR: 8.066e-04  Data: 0.033 (0.062)
Train: 87 [ 150/1251 ( 12%)]  Loss: 4.484 (4.33)  Time: 0.174s, 5870.83/s  (0.200s, 5121.84/s)  LR: 8.066e-04  Data: 0.019 (0.054)
Train: 87 [ 200/1251 ( 16%)]  Loss: 4.364 (4.34)  Time: 0.181s, 5649.21/s  (0.198s, 5162.48/s)  LR: 8.066e-04  Data: 0.034 (0.053)
Train: 87 [ 250/1251 ( 20%)]  Loss: 4.394 (4.35)  Time: 0.176s, 5816.68/s  (0.196s, 5228.13/s)  LR: 8.066e-04  Data: 0.030 (0.050)
Train: 87 [ 300/1251 ( 24%)]  Loss: 4.562 (4.38)  Time: 0.181s, 5667.21/s  (0.196s, 5233.45/s)  LR: 8.066e-04  Data: 0.025 (0.047)
Train: 87 [ 350/1251 ( 28%)]  Loss: 4.142 (4.35)  Time: 0.173s, 5906.76/s  (0.195s, 5258.63/s)  LR: 8.066e-04  Data: 0.027 (0.044)
Train: 87 [ 400/1251 ( 32%)]  Loss: 4.414 (4.35)  Time: 0.167s, 6118.51/s  (0.195s, 5254.78/s)  LR: 8.066e-04  Data: 0.025 (0.042)
Train: 87 [ 450/1251 ( 36%)]  Loss: 4.756 (4.39)  Time: 0.168s, 6092.77/s  (0.194s, 5285.38/s)  LR: 8.066e-04  Data: 0.032 (0.040)
Train: 87 [ 500/1251 ( 40%)]  Loss: 4.500 (4.40)  Time: 0.173s, 5935.98/s  (0.194s, 5283.16/s)  LR: 8.066e-04  Data: 0.024 (0.039)
Train: 87 [ 550/1251 ( 44%)]  Loss: 4.377 (4.40)  Time: 0.177s, 5794.42/s  (0.193s, 5293.47/s)  LR: 8.066e-04  Data: 0.029 (0.038)
Train: 87 [ 600/1251 ( 48%)]  Loss: 4.598 (4.42)  Time: 0.181s, 5669.33/s  (0.193s, 5292.95/s)  LR: 8.066e-04  Data: 0.023 (0.037)
Train: 87 [ 650/1251 ( 52%)]  Loss: 4.031 (4.39)  Time: 0.170s, 6040.68/s  (0.193s, 5300.03/s)  LR: 8.066e-04  Data: 0.028 (0.036)
Train: 87 [ 700/1251 ( 56%)]  Loss: 4.295 (4.38)  Time: 0.173s, 5913.33/s  (0.193s, 5296.96/s)  LR: 8.066e-04  Data: 0.024 (0.036)
Train: 87 [ 750/1251 ( 60%)]  Loss: 4.631 (4.40)  Time: 0.162s, 6309.27/s  (0.193s, 5302.42/s)  LR: 8.066e-04  Data: 0.035 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 87 [ 800/1251 ( 64%)]  Loss: 4.025 (4.38)  Time: 0.161s, 6372.19/s  (0.193s, 5317.35/s)  LR: 8.066e-04  Data: 0.031 (0.035)
Train: 87 [ 850/1251 ( 68%)]  Loss: 4.589 (4.39)  Time: 0.328s, 3119.46/s  (0.193s, 5308.36/s)  LR: 8.066e-04  Data: 0.032 (0.035)
Train: 87 [ 900/1251 ( 72%)]  Loss: 4.617 (4.40)  Time: 0.198s, 5169.58/s  (0.193s, 5308.43/s)  LR: 8.066e-04  Data: 0.026 (0.034)
Train: 87 [ 950/1251 ( 76%)]  Loss: 4.360 (4.40)  Time: 0.158s, 6475.09/s  (0.193s, 5308.06/s)  LR: 8.066e-04  Data: 0.028 (0.034)
Train: 87 [1000/1251 ( 80%)]  Loss: 4.104 (4.38)  Time: 0.168s, 6079.67/s  (0.193s, 5300.74/s)  LR: 8.066e-04  Data: 0.026 (0.034)
Train: 87 [1050/1251 ( 84%)]  Loss: 4.099 (4.37)  Time: 0.320s, 3198.08/s  (0.193s, 5307.19/s)  LR: 8.066e-04  Data: 0.036 (0.033)
Train: 87 [1100/1251 ( 88%)]  Loss: 4.214 (4.36)  Time: 0.173s, 5907.08/s  (0.193s, 5299.51/s)  LR: 8.066e-04  Data: 0.020 (0.033)
Train: 87 [1150/1251 ( 92%)]  Loss: 4.097 (4.35)  Time: 0.176s, 5826.04/s  (0.193s, 5299.62/s)  LR: 8.066e-04  Data: 0.020 (0.033)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.327 (4.35)  Time: 0.314s, 3263.73/s  (0.193s, 5299.11/s)  LR: 8.066e-04  Data: 0.030 (0.033)
Train: 87 [1250/1251 (100%)]  Loss: 4.437 (4.36)  Time: 0.114s, 8999.78/s  (0.193s, 5314.22/s)  LR: 8.066e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.814 (1.814)  Loss:  1.1774 (1.1774)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.3291 (1.8957)  Acc@1: 78.1840 (62.9940)  Acc@5: 92.6887 (84.9180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)

Train: 88 [   0/1251 (  0%)]  Loss: 4.428 (4.43)  Time: 1.792s,  571.36/s  (1.792s,  571.36/s)  LR: 8.025e-04  Data: 1.660 (1.660)
Train: 88 [  50/1251 (  4%)]  Loss: 4.414 (4.42)  Time: 0.177s, 5799.52/s  (0.229s, 4478.76/s)  LR: 8.025e-04  Data: 0.025 (0.068)
Train: 88 [ 100/1251 (  8%)]  Loss: 4.535 (4.46)  Time: 0.173s, 5903.60/s  (0.209s, 4905.81/s)  LR: 8.025e-04  Data: 0.031 (0.048)
Train: 88 [ 150/1251 ( 12%)]  Loss: 4.665 (4.51)  Time: 0.176s, 5817.22/s  (0.203s, 5051.24/s)  LR: 8.025e-04  Data: 0.033 (0.042)
Train: 88 [ 200/1251 ( 16%)]  Loss: 4.592 (4.53)  Time: 0.162s, 6306.45/s  (0.199s, 5140.25/s)  LR: 8.025e-04  Data: 0.020 (0.040)
Train: 88 [ 250/1251 ( 20%)]  Loss: 4.185 (4.47)  Time: 0.164s, 6226.47/s  (0.197s, 5186.84/s)  LR: 8.025e-04  Data: 0.030 (0.041)
Train: 88 [ 300/1251 ( 24%)]  Loss: 4.261 (4.44)  Time: 0.189s, 5411.79/s  (0.195s, 5238.57/s)  LR: 8.025e-04  Data: 0.024 (0.041)
Train: 88 [ 350/1251 ( 28%)]  Loss: 3.981 (4.38)  Time: 0.181s, 5663.90/s  (0.196s, 5236.76/s)  LR: 8.025e-04  Data: 0.025 (0.042)
Train: 88 [ 400/1251 ( 32%)]  Loss: 4.137 (4.36)  Time: 0.161s, 6353.77/s  (0.194s, 5273.42/s)  LR: 8.025e-04  Data: 0.020 (0.042)
Train: 88 [ 450/1251 ( 36%)]  Loss: 4.575 (4.38)  Time: 0.169s, 6062.27/s  (0.194s, 5286.69/s)  LR: 8.025e-04  Data: 0.027 (0.042)
Train: 88 [ 500/1251 ( 40%)]  Loss: 4.242 (4.37)  Time: 0.166s, 6176.89/s  (0.193s, 5313.11/s)  LR: 8.025e-04  Data: 0.031 (0.042)
Train: 88 [ 550/1251 ( 44%)]  Loss: 4.184 (4.35)  Time: 0.195s, 5259.72/s  (0.193s, 5312.15/s)  LR: 8.025e-04  Data: 0.023 (0.042)
Train: 88 [ 600/1251 ( 48%)]  Loss: 4.567 (4.37)  Time: 0.169s, 6060.62/s  (0.193s, 5296.87/s)  LR: 8.025e-04  Data: 0.026 (0.043)
Train: 88 [ 650/1251 ( 52%)]  Loss: 4.425 (4.37)  Time: 0.178s, 5740.19/s  (0.193s, 5301.73/s)  LR: 8.025e-04  Data: 0.026 (0.043)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.992 (4.35)  Time: 0.198s, 5169.98/s  (0.193s, 5308.85/s)  LR: 8.025e-04  Data: 0.024 (0.043)
Train: 88 [ 750/1251 ( 60%)]  Loss: 4.626 (4.36)  Time: 0.177s, 5795.67/s  (0.193s, 5306.56/s)  LR: 8.025e-04  Data: 0.025 (0.043)
Train: 88 [ 800/1251 ( 64%)]  Loss: 4.449 (4.37)  Time: 0.174s, 5878.78/s  (0.193s, 5305.73/s)  LR: 8.025e-04  Data: 0.023 (0.043)
Train: 88 [ 850/1251 ( 68%)]  Loss: 4.588 (4.38)  Time: 0.170s, 6023.39/s  (0.193s, 5308.80/s)  LR: 8.025e-04  Data: 0.021 (0.044)
Train: 88 [ 900/1251 ( 72%)]  Loss: 4.533 (4.39)  Time: 0.162s, 6321.33/s  (0.193s, 5310.80/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [ 950/1251 ( 76%)]  Loss: 4.392 (4.39)  Time: 0.172s, 5943.27/s  (0.193s, 5306.71/s)  LR: 8.025e-04  Data: 0.027 (0.044)
Train: 88 [1000/1251 ( 80%)]  Loss: 4.364 (4.39)  Time: 0.166s, 6182.68/s  (0.193s, 5306.56/s)  LR: 8.025e-04  Data: 0.025 (0.044)
Train: 88 [1050/1251 ( 84%)]  Loss: 4.399 (4.39)  Time: 0.177s, 5769.42/s  (0.193s, 5311.38/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [1100/1251 ( 88%)]  Loss: 4.250 (4.38)  Time: 0.178s, 5767.38/s  (0.193s, 5310.55/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.788 (4.36)  Time: 0.259s, 3946.52/s  (0.193s, 5306.64/s)  LR: 8.025e-04  Data: 0.023 (0.044)
Train: 88 [1200/1251 ( 96%)]  Loss: 4.005 (4.34)  Time: 0.186s, 5505.93/s  (0.193s, 5301.83/s)  LR: 8.025e-04  Data: 0.027 (0.044)
Train: 88 [1250/1251 (100%)]  Loss: 4.430 (4.35)  Time: 0.113s, 9031.88/s  (0.193s, 5318.42/s)  LR: 8.025e-04  Data: 0.000 (0.043)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.863 (1.863)  Loss:  1.1548 (1.1548)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2239 (1.8664)  Acc@1: 76.8868 (62.5100)  Acc@5: 91.6274 (84.6060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)

Train: 89 [   0/1251 (  0%)]  Loss: 4.337 (4.34)  Time: 1.858s,  551.22/s  (1.858s,  551.22/s)  LR: 7.983e-04  Data: 1.713 (1.713)
Train: 89 [  50/1251 (  4%)]  Loss: 4.313 (4.32)  Time: 0.166s, 6164.43/s  (0.223s, 4595.49/s)  LR: 7.983e-04  Data: 0.030 (0.076)
Train: 89 [ 100/1251 (  8%)]  Loss: 4.390 (4.35)  Time: 0.157s, 6534.34/s  (0.208s, 4922.15/s)  LR: 7.983e-04  Data: 0.027 (0.052)
Train: 89 [ 150/1251 ( 12%)]  Loss: 4.636 (4.42)  Time: 0.161s, 6353.14/s  (0.199s, 5150.69/s)  LR: 7.983e-04  Data: 0.033 (0.044)
Train: 89 [ 200/1251 ( 16%)]  Loss: 4.183 (4.37)  Time: 0.168s, 6107.30/s  (0.197s, 5202.74/s)  LR: 7.983e-04  Data: 0.025 (0.040)
Train: 89 [ 250/1251 ( 20%)]  Loss: 4.364 (4.37)  Time: 0.168s, 6092.42/s  (0.196s, 5236.45/s)  LR: 7.983e-04  Data: 0.034 (0.038)
Train: 89 [ 300/1251 ( 24%)]  Loss: 4.388 (4.37)  Time: 0.181s, 5652.80/s  (0.195s, 5260.51/s)  LR: 7.983e-04  Data: 0.028 (0.036)
Train: 89 [ 350/1251 ( 28%)]  Loss: 4.671 (4.41)  Time: 0.175s, 5851.42/s  (0.193s, 5297.68/s)  LR: 7.983e-04  Data: 0.030 (0.035)
Train: 89 [ 400/1251 ( 32%)]  Loss: 4.415 (4.41)  Time: 0.186s, 5494.44/s  (0.193s, 5305.11/s)  LR: 7.983e-04  Data: 0.026 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 89 [ 450/1251 ( 36%)]  Loss: 4.201 (4.39)  Time: 0.187s, 5480.27/s  (0.193s, 5311.50/s)  LR: 7.983e-04  Data: 0.034 (0.034)
Train: 89 [ 500/1251 ( 40%)]  Loss: 4.199 (4.37)  Time: 0.158s, 6478.54/s  (0.192s, 5328.19/s)  LR: 7.983e-04  Data: 0.026 (0.033)
Train: 89 [ 550/1251 ( 44%)]  Loss: 4.503 (4.38)  Time: 0.158s, 6463.45/s  (0.193s, 5317.43/s)  LR: 7.983e-04  Data: 0.029 (0.033)
Train: 89 [ 600/1251 ( 48%)]  Loss: 4.352 (4.38)  Time: 0.189s, 5426.74/s  (0.192s, 5339.55/s)  LR: 7.983e-04  Data: 0.029 (0.032)
Train: 89 [ 650/1251 ( 52%)]  Loss: 4.753 (4.41)  Time: 0.182s, 5621.06/s  (0.193s, 5319.27/s)  LR: 7.983e-04  Data: 0.024 (0.032)
Train: 89 [ 700/1251 ( 56%)]  Loss: 4.051 (4.38)  Time: 0.165s, 6190.74/s  (0.193s, 5317.60/s)  LR: 7.983e-04  Data: 0.028 (0.032)
Train: 89 [ 750/1251 ( 60%)]  Loss: 4.581 (4.40)  Time: 0.527s, 1944.49/s  (0.193s, 5308.62/s)  LR: 7.983e-04  Data: 0.026 (0.031)
Train: 89 [ 800/1251 ( 64%)]  Loss: 4.327 (4.39)  Time: 0.171s, 5981.90/s  (0.192s, 5320.91/s)  LR: 7.983e-04  Data: 0.025 (0.031)
Train: 89 [ 850/1251 ( 68%)]  Loss: 4.138 (4.38)  Time: 0.150s, 6833.06/s  (0.192s, 5324.07/s)  LR: 7.983e-04  Data: 0.024 (0.031)
Train: 89 [ 900/1251 ( 72%)]  Loss: 4.090 (4.36)  Time: 0.181s, 5650.72/s  (0.192s, 5322.04/s)  LR: 7.983e-04  Data: 0.026 (0.031)
Train: 89 [ 950/1251 ( 76%)]  Loss: 4.191 (4.35)  Time: 0.163s, 6288.18/s  (0.192s, 5321.36/s)  LR: 7.983e-04  Data: 0.030 (0.031)
Train: 89 [1000/1251 ( 80%)]  Loss: 3.914 (4.33)  Time: 0.165s, 6193.81/s  (0.192s, 5319.57/s)  LR: 7.983e-04  Data: 0.025 (0.030)
Train: 89 [1050/1251 ( 84%)]  Loss: 4.245 (4.33)  Time: 0.178s, 5762.99/s  (0.193s, 5311.18/s)  LR: 7.983e-04  Data: 0.024 (0.030)
Train: 89 [1100/1251 ( 88%)]  Loss: 4.319 (4.33)  Time: 0.171s, 5993.61/s  (0.193s, 5315.66/s)  LR: 7.983e-04  Data: 0.023 (0.030)
Train: 89 [1150/1251 ( 92%)]  Loss: 4.568 (4.34)  Time: 0.151s, 6759.91/s  (0.193s, 5308.86/s)  LR: 7.983e-04  Data: 0.027 (0.030)
Train: 89 [1200/1251 ( 96%)]  Loss: 4.087 (4.33)  Time: 0.155s, 6605.03/s  (0.193s, 5308.92/s)  LR: 7.983e-04  Data: 0.027 (0.030)
Train: 89 [1250/1251 (100%)]  Loss: 4.176 (4.32)  Time: 0.114s, 9014.78/s  (0.192s, 5324.74/s)  LR: 7.983e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.784 (1.784)  Loss:  1.2485 (1.2485)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.3580 (1.9880)  Acc@1: 77.7123 (61.8520)  Acc@5: 91.2736 (84.3260)
Train: 90 [   0/1251 (  0%)]  Loss: 4.020 (4.02)  Time: 1.798s,  569.53/s  (1.798s,  569.53/s)  LR: 7.941e-04  Data: 1.670 (1.670)
Train: 90 [  50/1251 (  4%)]  Loss: 4.368 (4.19)  Time: 0.192s, 5324.28/s  (0.221s, 4626.18/s)  LR: 7.941e-04  Data: 0.029 (0.064)
Train: 90 [ 100/1251 (  8%)]  Loss: 4.167 (4.18)  Time: 0.180s, 5687.16/s  (0.204s, 5017.09/s)  LR: 7.941e-04  Data: 0.025 (0.047)
Train: 90 [ 150/1251 ( 12%)]  Loss: 4.041 (4.15)  Time: 0.167s, 6149.40/s  (0.199s, 5145.91/s)  LR: 7.941e-04  Data: 0.026 (0.042)
Train: 90 [ 200/1251 ( 16%)]  Loss: 4.406 (4.20)  Time: 0.181s, 5665.50/s  (0.197s, 5210.59/s)  LR: 7.941e-04  Data: 0.031 (0.039)
Train: 90 [ 250/1251 ( 20%)]  Loss: 4.438 (4.24)  Time: 0.192s, 5341.42/s  (0.195s, 5238.68/s)  LR: 7.941e-04  Data: 0.027 (0.037)
Train: 90 [ 300/1251 ( 24%)]  Loss: 4.314 (4.25)  Time: 0.182s, 5622.11/s  (0.195s, 5261.28/s)  LR: 7.941e-04  Data: 0.025 (0.036)
Train: 90 [ 350/1251 ( 28%)]  Loss: 4.828 (4.32)  Time: 0.168s, 6099.61/s  (0.194s, 5275.09/s)  LR: 7.941e-04  Data: 0.029 (0.036)
Train: 90 [ 400/1251 ( 32%)]  Loss: 4.283 (4.32)  Time: 0.200s, 5107.71/s  (0.194s, 5285.04/s)  LR: 7.941e-04  Data: 0.025 (0.035)
Train: 90 [ 450/1251 ( 36%)]  Loss: 4.231 (4.31)  Time: 0.178s, 5762.78/s  (0.193s, 5305.10/s)  LR: 7.941e-04  Data: 0.041 (0.034)
Train: 90 [ 500/1251 ( 40%)]  Loss: 4.357 (4.31)  Time: 0.185s, 5543.06/s  (0.193s, 5314.81/s)  LR: 7.941e-04  Data: 0.029 (0.035)
Train: 90 [ 550/1251 ( 44%)]  Loss: 4.486 (4.33)  Time: 0.195s, 5251.59/s  (0.192s, 5335.80/s)  LR: 7.941e-04  Data: 0.023 (0.035)
Train: 90 [ 600/1251 ( 48%)]  Loss: 4.326 (4.33)  Time: 0.199s, 5148.32/s  (0.193s, 5315.28/s)  LR: 7.941e-04  Data: 0.029 (0.037)
Train: 90 [ 650/1251 ( 52%)]  Loss: 4.812 (4.36)  Time: 0.180s, 5678.73/s  (0.192s, 5336.39/s)  LR: 7.941e-04  Data: 0.053 (0.037)
Train: 90 [ 700/1251 ( 56%)]  Loss: 4.301 (4.36)  Time: 0.167s, 6132.42/s  (0.192s, 5339.86/s)  LR: 7.941e-04  Data: 0.020 (0.038)
Train: 90 [ 750/1251 ( 60%)]  Loss: 4.404 (4.36)  Time: 0.174s, 5871.56/s  (0.192s, 5345.46/s)  LR: 7.941e-04  Data: 0.030 (0.038)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.335 (4.36)  Time: 0.352s, 2908.33/s  (0.192s, 5337.49/s)  LR: 7.941e-04  Data: 0.029 (0.037)
Train: 90 [ 850/1251 ( 68%)]  Loss: 4.611 (4.37)  Time: 0.164s, 6227.64/s  (0.192s, 5333.51/s)  LR: 7.941e-04  Data: 0.036 (0.036)
Train: 90 [ 900/1251 ( 72%)]  Loss: 4.458 (4.38)  Time: 0.188s, 5460.79/s  (0.192s, 5336.56/s)  LR: 7.941e-04  Data: 0.026 (0.036)
Train: 90 [ 950/1251 ( 76%)]  Loss: 4.534 (4.39)  Time: 0.185s, 5529.59/s  (0.192s, 5334.58/s)  LR: 7.941e-04  Data: 0.029 (0.035)
Train: 90 [1000/1251 ( 80%)]  Loss: 4.323 (4.38)  Time: 0.175s, 5849.95/s  (0.192s, 5337.91/s)  LR: 7.941e-04  Data: 0.032 (0.035)
Train: 90 [1050/1251 ( 84%)]  Loss: 4.207 (4.37)  Time: 0.695s, 1474.28/s  (0.193s, 5316.52/s)  LR: 7.941e-04  Data: 0.038 (0.035)
Train: 90 [1100/1251 ( 88%)]  Loss: 4.582 (4.38)  Time: 0.180s, 5677.38/s  (0.192s, 5322.25/s)  LR: 7.941e-04  Data: 0.025 (0.034)
Train: 90 [1150/1251 ( 92%)]  Loss: 4.449 (4.39)  Time: 0.170s, 6020.07/s  (0.193s, 5314.65/s)  LR: 7.941e-04  Data: 0.033 (0.034)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.974 (4.37)  Time: 0.173s, 5907.01/s  (0.193s, 5316.27/s)  LR: 7.941e-04  Data: 0.036 (0.034)
Train: 90 [1250/1251 (100%)]  Loss: 4.207 (4.36)  Time: 0.114s, 9015.31/s  (0.192s, 5331.57/s)  LR: 7.941e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.0878 (1.0878)  Acc@1: 82.2266 (82.2266)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1970 (1.8853)  Acc@1: 77.8302 (62.5980)  Acc@5: 92.2170 (84.5400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)

Train: 91 [   0/1251 (  0%)]  Loss: 4.323 (4.32)  Time: 1.636s,  626.05/s  (1.636s,  626.05/s)  LR: 7.899e-04  Data: 1.504 (1.504)
Train: 91 [  50/1251 (  4%)]  Loss: 4.235 (4.28)  Time: 0.181s, 5646.61/s  (0.223s, 4582.04/s)  LR: 7.899e-04  Data: 0.033 (0.071)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 91 [ 100/1251 (  8%)]  Loss: 4.438 (4.33)  Time: 0.185s, 5535.25/s  (0.206s, 4972.38/s)  LR: 7.899e-04  Data: 0.023 (0.050)
Train: 91 [ 150/1251 ( 12%)]  Loss: 4.466 (4.37)  Time: 0.158s, 6489.76/s  (0.200s, 5128.16/s)  LR: 7.899e-04  Data: 0.030 (0.042)
Train: 91 [ 200/1251 ( 16%)]  Loss: 4.504 (4.39)  Time: 0.267s, 3834.30/s  (0.197s, 5187.21/s)  LR: 7.899e-04  Data: 0.022 (0.039)
Train: 91 [ 250/1251 ( 20%)]  Loss: 4.461 (4.40)  Time: 0.169s, 6060.26/s  (0.196s, 5216.16/s)  LR: 7.899e-04  Data: 0.026 (0.037)
Train: 91 [ 300/1251 ( 24%)]  Loss: 4.128 (4.36)  Time: 0.173s, 5908.94/s  (0.195s, 5256.48/s)  LR: 7.899e-04  Data: 0.032 (0.035)
Train: 91 [ 350/1251 ( 28%)]  Loss: 4.446 (4.38)  Time: 0.167s, 6119.05/s  (0.195s, 5243.12/s)  LR: 7.899e-04  Data: 0.050 (0.035)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.965 (4.33)  Time: 0.161s, 6359.76/s  (0.194s, 5267.13/s)  LR: 7.899e-04  Data: 0.025 (0.034)
Train: 91 [ 450/1251 ( 36%)]  Loss: 4.490 (4.35)  Time: 0.166s, 6152.65/s  (0.194s, 5284.09/s)  LR: 7.899e-04  Data: 0.026 (0.033)
Train: 91 [ 500/1251 ( 40%)]  Loss: 4.189 (4.33)  Time: 0.175s, 5851.02/s  (0.194s, 5283.12/s)  LR: 7.899e-04  Data: 0.027 (0.033)
Train: 91 [ 550/1251 ( 44%)]  Loss: 4.698 (4.36)  Time: 0.158s, 6488.96/s  (0.194s, 5282.17/s)  LR: 7.899e-04  Data: 0.032 (0.032)
Train: 91 [ 600/1251 ( 48%)]  Loss: 4.382 (4.36)  Time: 0.162s, 6304.19/s  (0.194s, 5286.70/s)  LR: 7.899e-04  Data: 0.033 (0.032)
Train: 91 [ 650/1251 ( 52%)]  Loss: 4.736 (4.39)  Time: 0.165s, 6187.35/s  (0.193s, 5298.31/s)  LR: 7.899e-04  Data: 0.025 (0.032)
Train: 91 [ 700/1251 ( 56%)]  Loss: 4.357 (4.39)  Time: 0.173s, 5910.43/s  (0.193s, 5293.23/s)  LR: 7.899e-04  Data: 0.023 (0.032)
Train: 91 [ 750/1251 ( 60%)]  Loss: 4.319 (4.38)  Time: 0.178s, 5750.59/s  (0.193s, 5293.79/s)  LR: 7.899e-04  Data: 0.029 (0.032)
Train: 91 [ 800/1251 ( 64%)]  Loss: 4.654 (4.40)  Time: 0.161s, 6356.44/s  (0.194s, 5287.01/s)  LR: 7.899e-04  Data: 0.025 (0.031)
Train: 91 [ 850/1251 ( 68%)]  Loss: 4.434 (4.40)  Time: 0.154s, 6658.49/s  (0.193s, 5293.95/s)  LR: 7.899e-04  Data: 0.030 (0.031)
Train: 91 [ 900/1251 ( 72%)]  Loss: 4.418 (4.40)  Time: 0.173s, 5903.35/s  (0.193s, 5292.59/s)  LR: 7.899e-04  Data: 0.034 (0.031)
Train: 91 [ 950/1251 ( 76%)]  Loss: 4.320 (4.40)  Time: 0.176s, 5816.88/s  (0.194s, 5289.37/s)  LR: 7.899e-04  Data: 0.024 (0.031)
Train: 91 [1000/1251 ( 80%)]  Loss: 4.680 (4.41)  Time: 0.169s, 6073.35/s  (0.194s, 5289.63/s)  LR: 7.899e-04  Data: 0.036 (0.031)
Train: 91 [1050/1251 ( 84%)]  Loss: 4.394 (4.41)  Time: 0.171s, 5985.21/s  (0.194s, 5283.87/s)  LR: 7.899e-04  Data: 0.038 (0.031)
Train: 91 [1100/1251 ( 88%)]  Loss: 4.363 (4.41)  Time: 0.179s, 5723.77/s  (0.194s, 5280.95/s)  LR: 7.899e-04  Data: 0.039 (0.031)
Train: 91 [1150/1251 ( 92%)]  Loss: 4.291 (4.40)  Time: 0.152s, 6716.05/s  (0.194s, 5284.84/s)  LR: 7.899e-04  Data: 0.025 (0.030)
Train: 91 [1200/1251 ( 96%)]  Loss: 4.279 (4.40)  Time: 0.186s, 5505.53/s  (0.194s, 5268.06/s)  LR: 7.899e-04  Data: 0.023 (0.030)
Train: 91 [1250/1251 (100%)]  Loss: 4.306 (4.40)  Time: 0.114s, 9012.38/s  (0.194s, 5288.19/s)  LR: 7.899e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.860 (1.860)  Loss:  1.3280 (1.3280)  Acc@1: 78.2227 (78.2227)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3228 (1.9772)  Acc@1: 77.8302 (62.0100)  Acc@5: 92.3349 (84.1880)
Train: 92 [   0/1251 (  0%)]  Loss: 4.632 (4.63)  Time: 1.911s,  535.75/s  (1.911s,  535.75/s)  LR: 7.856e-04  Data: 1.691 (1.691)
Train: 92 [  50/1251 (  4%)]  Loss: 4.581 (4.61)  Time: 0.166s, 6185.03/s  (0.225s, 4559.66/s)  LR: 7.856e-04  Data: 0.027 (0.072)
Train: 92 [ 100/1251 (  8%)]  Loss: 4.347 (4.52)  Time: 0.157s, 6521.09/s  (0.207s, 4942.97/s)  LR: 7.856e-04  Data: 0.026 (0.053)
Train: 92 [ 150/1251 ( 12%)]  Loss: 4.303 (4.47)  Time: 0.166s, 6171.29/s  (0.203s, 5046.69/s)  LR: 7.856e-04  Data: 0.025 (0.052)
Train: 92 [ 200/1251 ( 16%)]  Loss: 4.268 (4.43)  Time: 0.259s, 3961.14/s  (0.199s, 5136.87/s)  LR: 7.856e-04  Data: 0.137 (0.048)
Train: 92 [ 250/1251 ( 20%)]  Loss: 4.242 (4.40)  Time: 0.168s, 6083.64/s  (0.196s, 5224.29/s)  LR: 7.856e-04  Data: 0.027 (0.047)
Train: 92 [ 300/1251 ( 24%)]  Loss: 4.250 (4.37)  Time: 0.190s, 5393.25/s  (0.194s, 5269.14/s)  LR: 7.856e-04  Data: 0.025 (0.045)
Train: 92 [ 350/1251 ( 28%)]  Loss: 4.277 (4.36)  Time: 0.158s, 6486.83/s  (0.193s, 5292.99/s)  LR: 7.856e-04  Data: 0.029 (0.045)
Train: 92 [ 400/1251 ( 32%)]  Loss: 4.613 (4.39)  Time: 0.374s, 2738.35/s  (0.194s, 5286.99/s)  LR: 7.856e-04  Data: 0.250 (0.046)
Train: 92 [ 450/1251 ( 36%)]  Loss: 4.225 (4.37)  Time: 0.180s, 5682.55/s  (0.194s, 5283.65/s)  LR: 7.856e-04  Data: 0.023 (0.046)
Train: 92 [ 500/1251 ( 40%)]  Loss: 4.232 (4.36)  Time: 0.164s, 6239.51/s  (0.193s, 5314.33/s)  LR: 7.856e-04  Data: 0.026 (0.045)
Train: 92 [ 550/1251 ( 44%)]  Loss: 4.096 (4.34)  Time: 0.162s, 6336.98/s  (0.192s, 5321.31/s)  LR: 7.856e-04  Data: 0.021 (0.045)
Train: 92 [ 600/1251 ( 48%)]  Loss: 4.619 (4.36)  Time: 0.325s, 3155.22/s  (0.192s, 5323.22/s)  LR: 7.856e-04  Data: 0.025 (0.043)
Train: 92 [ 650/1251 ( 52%)]  Loss: 4.579 (4.38)  Time: 0.199s, 5142.11/s  (0.192s, 5323.98/s)  LR: 7.856e-04  Data: 0.029 (0.042)
Train: 92 [ 700/1251 ( 56%)]  Loss: 4.535 (4.39)  Time: 0.163s, 6268.99/s  (0.192s, 5327.67/s)  LR: 7.856e-04  Data: 0.030 (0.041)
Train: 92 [ 750/1251 ( 60%)]  Loss: 4.332 (4.38)  Time: 0.179s, 5717.95/s  (0.192s, 5328.16/s)  LR: 7.856e-04  Data: 0.050 (0.040)
Train: 92 [ 800/1251 ( 64%)]  Loss: 4.174 (4.37)  Time: 0.358s, 2858.36/s  (0.192s, 5325.20/s)  LR: 7.856e-04  Data: 0.025 (0.040)
Train: 92 [ 850/1251 ( 68%)]  Loss: 4.316 (4.37)  Time: 0.179s, 5720.81/s  (0.192s, 5327.28/s)  LR: 7.856e-04  Data: 0.023 (0.039)
Train: 92 [ 900/1251 ( 72%)]  Loss: 4.396 (4.37)  Time: 0.156s, 6559.62/s  (0.192s, 5327.43/s)  LR: 7.856e-04  Data: 0.029 (0.038)
Train: 92 [ 950/1251 ( 76%)]  Loss: 4.202 (4.36)  Time: 0.196s, 5218.17/s  (0.192s, 5332.78/s)  LR: 7.856e-04  Data: 0.033 (0.038)
Train: 92 [1000/1251 ( 80%)]  Loss: 4.199 (4.35)  Time: 0.160s, 6407.82/s  (0.192s, 5334.77/s)  LR: 7.856e-04  Data: 0.025 (0.037)
Train: 92 [1050/1251 ( 84%)]  Loss: 4.361 (4.35)  Time: 0.170s, 6026.25/s  (0.192s, 5331.51/s)  LR: 7.856e-04  Data: 0.032 (0.037)
Train: 92 [1100/1251 ( 88%)]  Loss: 4.313 (4.35)  Time: 0.317s, 3235.22/s  (0.192s, 5326.59/s)  LR: 7.856e-04  Data: 0.022 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 92 [1150/1251 ( 92%)]  Loss: 4.142 (4.34)  Time: 0.185s, 5521.83/s  (0.192s, 5329.68/s)  LR: 7.856e-04  Data: 0.019 (0.036)
Train: 92 [1200/1251 ( 96%)]  Loss: 4.151 (4.34)  Time: 0.191s, 5365.85/s  (0.192s, 5323.76/s)  LR: 7.856e-04  Data: 0.031 (0.036)
Train: 92 [1250/1251 (100%)]  Loss: 4.099 (4.33)  Time: 0.114s, 9015.94/s  (0.192s, 5332.82/s)  LR: 7.856e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.793 (1.793)  Loss:  1.1249 (1.1249)  Acc@1: 79.7852 (79.7852)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1974 (1.8533)  Acc@1: 79.3632 (62.7140)  Acc@5: 93.6321 (84.9240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)

Train: 93 [   0/1251 (  0%)]  Loss: 4.298 (4.30)  Time: 1.752s,  584.32/s  (1.752s,  584.32/s)  LR: 7.813e-04  Data: 1.620 (1.620)
Train: 93 [  50/1251 (  4%)]  Loss: 4.482 (4.39)  Time: 0.158s, 6491.58/s  (0.218s, 4690.99/s)  LR: 7.813e-04  Data: 0.028 (0.073)
Train: 93 [ 100/1251 (  8%)]  Loss: 3.663 (4.15)  Time: 0.178s, 5744.99/s  (0.205s, 4994.41/s)  LR: 7.813e-04  Data: 0.026 (0.057)
Train: 93 [ 150/1251 ( 12%)]  Loss: 4.661 (4.28)  Time: 0.167s, 6125.17/s  (0.201s, 5092.12/s)  LR: 7.813e-04  Data: 0.028 (0.053)
Train: 93 [ 200/1251 ( 16%)]  Loss: 4.468 (4.31)  Time: 0.175s, 5839.32/s  (0.199s, 5157.71/s)  LR: 7.813e-04  Data: 0.026 (0.051)
Train: 93 [ 250/1251 ( 20%)]  Loss: 3.984 (4.26)  Time: 0.174s, 5885.47/s  (0.196s, 5223.06/s)  LR: 7.813e-04  Data: 0.028 (0.048)
Train: 93 [ 300/1251 ( 24%)]  Loss: 4.355 (4.27)  Time: 0.173s, 5910.68/s  (0.194s, 5271.87/s)  LR: 7.813e-04  Data: 0.023 (0.047)
Train: 93 [ 350/1251 ( 28%)]  Loss: 4.804 (4.34)  Time: 0.182s, 5627.20/s  (0.194s, 5268.73/s)  LR: 7.813e-04  Data: 0.025 (0.046)
Train: 93 [ 400/1251 ( 32%)]  Loss: 4.419 (4.35)  Time: 0.151s, 6776.16/s  (0.193s, 5296.28/s)  LR: 7.813e-04  Data: 0.025 (0.044)
Train: 93 [ 450/1251 ( 36%)]  Loss: 3.726 (4.29)  Time: 0.178s, 5737.88/s  (0.194s, 5287.90/s)  LR: 7.813e-04  Data: 0.024 (0.042)
Train: 93 [ 500/1251 ( 40%)]  Loss: 4.529 (4.31)  Time: 0.150s, 6811.46/s  (0.194s, 5277.18/s)  LR: 7.813e-04  Data: 0.025 (0.041)
Train: 93 [ 550/1251 ( 44%)]  Loss: 4.435 (4.32)  Time: 0.173s, 5903.04/s  (0.193s, 5302.80/s)  LR: 7.813e-04  Data: 0.029 (0.040)
Train: 93 [ 600/1251 ( 48%)]  Loss: 4.533 (4.34)  Time: 0.170s, 6030.41/s  (0.193s, 5309.49/s)  LR: 7.813e-04  Data: 0.019 (0.039)
Train: 93 [ 650/1251 ( 52%)]  Loss: 4.225 (4.33)  Time: 0.179s, 5732.01/s  (0.192s, 5323.28/s)  LR: 7.813e-04  Data: 0.022 (0.038)
Train: 93 [ 700/1251 ( 56%)]  Loss: 4.531 (4.34)  Time: 0.179s, 5718.90/s  (0.193s, 5314.33/s)  LR: 7.813e-04  Data: 0.029 (0.037)
Train: 93 [ 750/1251 ( 60%)]  Loss: 4.579 (4.36)  Time: 0.164s, 6228.53/s  (0.193s, 5311.76/s)  LR: 7.813e-04  Data: 0.025 (0.038)
Train: 93 [ 800/1251 ( 64%)]  Loss: 4.525 (4.37)  Time: 0.177s, 5777.95/s  (0.192s, 5322.99/s)  LR: 7.813e-04  Data: 0.023 (0.038)
Train: 93 [ 850/1251 ( 68%)]  Loss: 4.563 (4.38)  Time: 0.191s, 5365.15/s  (0.193s, 5316.27/s)  LR: 7.813e-04  Data: 0.026 (0.039)
Train: 93 [ 900/1251 ( 72%)]  Loss: 4.185 (4.37)  Time: 0.166s, 6183.89/s  (0.193s, 5312.80/s)  LR: 7.813e-04  Data: 0.020 (0.039)
Train: 93 [ 950/1251 ( 76%)]  Loss: 4.577 (4.38)  Time: 0.177s, 5801.26/s  (0.193s, 5308.31/s)  LR: 7.813e-04  Data: 0.023 (0.038)
Train: 93 [1000/1251 ( 80%)]  Loss: 4.502 (4.38)  Time: 0.226s, 4528.64/s  (0.193s, 5309.98/s)  LR: 7.813e-04  Data: 0.023 (0.038)
Train: 93 [1050/1251 ( 84%)]  Loss: 4.293 (4.38)  Time: 0.154s, 6647.07/s  (0.193s, 5303.20/s)  LR: 7.813e-04  Data: 0.028 (0.037)
Train: 93 [1100/1251 ( 88%)]  Loss: 4.411 (4.38)  Time: 0.146s, 6995.31/s  (0.193s, 5305.71/s)  LR: 7.813e-04  Data: 0.020 (0.037)
Train: 93 [1150/1251 ( 92%)]  Loss: 4.615 (4.39)  Time: 0.223s, 4588.01/s  (0.195s, 5254.03/s)  LR: 7.813e-04  Data: 0.027 (0.037)
Train: 93 [1200/1251 ( 96%)]  Loss: 4.252 (4.38)  Time: 0.178s, 5759.17/s  (0.195s, 5254.69/s)  LR: 7.813e-04  Data: 0.039 (0.036)
Train: 93 [1250/1251 (100%)]  Loss: 4.886 (4.40)  Time: 0.114s, 8983.05/s  (0.194s, 5266.87/s)  LR: 7.813e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.902 (1.902)  Loss:  1.2497 (1.2497)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1973 (1.8543)  Acc@1: 78.7736 (62.6200)  Acc@5: 93.0424 (84.7880)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)

Train: 94 [   0/1251 (  0%)]  Loss: 4.372 (4.37)  Time: 1.811s,  565.38/s  (1.811s,  565.38/s)  LR: 7.769e-04  Data: 1.689 (1.689)
Train: 94 [  50/1251 (  4%)]  Loss: 4.400 (4.39)  Time: 0.186s, 5499.21/s  (0.225s, 4543.25/s)  LR: 7.769e-04  Data: 0.027 (0.067)
Train: 94 [ 100/1251 (  8%)]  Loss: 4.038 (4.27)  Time: 0.174s, 5888.88/s  (0.206s, 4981.65/s)  LR: 7.769e-04  Data: 0.023 (0.050)
Train: 94 [ 150/1251 ( 12%)]  Loss: 4.194 (4.25)  Time: 0.176s, 5814.03/s  (0.200s, 5130.69/s)  LR: 7.769e-04  Data: 0.024 (0.044)
Train: 94 [ 200/1251 ( 16%)]  Loss: 4.238 (4.25)  Time: 0.173s, 5912.33/s  (0.198s, 5174.22/s)  LR: 7.769e-04  Data: 0.023 (0.040)
Train: 94 [ 250/1251 ( 20%)]  Loss: 4.511 (4.29)  Time: 0.172s, 5964.49/s  (0.197s, 5206.64/s)  LR: 7.769e-04  Data: 0.025 (0.037)
Train: 94 [ 300/1251 ( 24%)]  Loss: 3.804 (4.22)  Time: 0.166s, 6157.36/s  (0.195s, 5252.98/s)  LR: 7.769e-04  Data: 0.021 (0.036)
Train: 94 [ 350/1251 ( 28%)]  Loss: 3.810 (4.17)  Time: 0.160s, 6418.17/s  (0.195s, 5261.05/s)  LR: 7.769e-04  Data: 0.025 (0.035)
Train: 94 [ 400/1251 ( 32%)]  Loss: 4.574 (4.22)  Time: 0.172s, 5942.22/s  (0.195s, 5261.51/s)  LR: 7.769e-04  Data: 0.027 (0.034)
Train: 94 [ 450/1251 ( 36%)]  Loss: 4.374 (4.23)  Time: 0.183s, 5585.85/s  (0.193s, 5294.47/s)  LR: 7.769e-04  Data: 0.026 (0.034)
Train: 94 [ 500/1251 ( 40%)]  Loss: 4.477 (4.25)  Time: 0.158s, 6491.47/s  (0.193s, 5306.85/s)  LR: 7.769e-04  Data: 0.030 (0.033)
Train: 94 [ 550/1251 ( 44%)]  Loss: 4.375 (4.26)  Time: 0.175s, 5847.30/s  (0.193s, 5313.51/s)  LR: 7.769e-04  Data: 0.023 (0.033)
Train: 94 [ 600/1251 ( 48%)]  Loss: 4.364 (4.27)  Time: 0.166s, 6160.32/s  (0.193s, 5305.96/s)  LR: 7.769e-04  Data: 0.025 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 94 [ 650/1251 ( 52%)]  Loss: 4.666 (4.30)  Time: 0.168s, 6103.29/s  (0.193s, 5301.77/s)  LR: 7.769e-04  Data: 0.036 (0.032)
Train: 94 [ 700/1251 ( 56%)]  Loss: 4.534 (4.32)  Time: 0.178s, 5760.08/s  (0.193s, 5318.32/s)  LR: 7.769e-04  Data: 0.028 (0.032)
Train: 94 [ 750/1251 ( 60%)]  Loss: 4.283 (4.31)  Time: 0.162s, 6330.67/s  (0.193s, 5317.98/s)  LR: 7.769e-04  Data: 0.027 (0.032)
Train: 94 [ 800/1251 ( 64%)]  Loss: 4.367 (4.32)  Time: 0.160s, 6413.33/s  (0.193s, 5316.93/s)  LR: 7.769e-04  Data: 0.024 (0.031)
Train: 94 [ 850/1251 ( 68%)]  Loss: 3.976 (4.30)  Time: 0.178s, 5751.35/s  (0.193s, 5297.65/s)  LR: 7.769e-04  Data: 0.026 (0.031)
Train: 94 [ 900/1251 ( 72%)]  Loss: 4.194 (4.29)  Time: 0.189s, 5405.66/s  (0.192s, 5321.64/s)  LR: 7.769e-04  Data: 0.028 (0.031)
Train: 94 [ 950/1251 ( 76%)]  Loss: 4.604 (4.31)  Time: 0.176s, 5817.11/s  (0.193s, 5309.12/s)  LR: 7.769e-04  Data: 0.026 (0.031)
Train: 94 [1000/1251 ( 80%)]  Loss: 4.359 (4.31)  Time: 0.166s, 6164.93/s  (0.193s, 5303.48/s)  LR: 7.769e-04  Data: 0.030 (0.031)
Train: 94 [1050/1251 ( 84%)]  Loss: 4.110 (4.30)  Time: 0.191s, 5350.74/s  (0.193s, 5298.28/s)  LR: 7.769e-04  Data: 0.032 (0.030)
Train: 94 [1100/1251 ( 88%)]  Loss: 4.407 (4.31)  Time: 0.164s, 6249.50/s  (0.193s, 5297.57/s)  LR: 7.769e-04  Data: 0.031 (0.030)
Train: 94 [1150/1251 ( 92%)]  Loss: 3.932 (4.29)  Time: 0.178s, 5746.53/s  (0.193s, 5292.92/s)  LR: 7.769e-04  Data: 0.027 (0.030)
Train: 94 [1200/1251 ( 96%)]  Loss: 4.239 (4.29)  Time: 0.165s, 6192.29/s  (0.193s, 5293.12/s)  LR: 7.769e-04  Data: 0.035 (0.030)
Train: 94 [1250/1251 (100%)]  Loss: 4.558 (4.30)  Time: 0.113s, 9062.78/s  (0.193s, 5303.56/s)  LR: 7.769e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.816 (1.816)  Loss:  1.3213 (1.3213)  Acc@1: 78.3203 (78.3203)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2692 (1.9439)  Acc@1: 77.4764 (62.4520)  Acc@5: 92.4528 (84.8020)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-94.pth.tar', 62.45200004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)

Train: 95 [   0/1251 (  0%)]  Loss: 3.978 (3.98)  Time: 1.715s,  596.99/s  (1.715s,  596.99/s)  LR: 7.725e-04  Data: 1.588 (1.588)
Train: 95 [  50/1251 (  4%)]  Loss: 4.329 (4.15)  Time: 0.182s, 5619.83/s  (0.220s, 4658.66/s)  LR: 7.725e-04  Data: 0.021 (0.072)
Train: 95 [ 100/1251 (  8%)]  Loss: 4.590 (4.30)  Time: 0.160s, 6408.52/s  (0.208s, 4931.50/s)  LR: 7.725e-04  Data: 0.021 (0.060)
Train: 95 [ 150/1251 ( 12%)]  Loss: 3.961 (4.21)  Time: 0.186s, 5514.48/s  (0.202s, 5057.71/s)  LR: 7.725e-04  Data: 0.028 (0.049)
Train: 95 [ 200/1251 ( 16%)]  Loss: 4.554 (4.28)  Time: 0.163s, 6284.76/s  (0.199s, 5154.09/s)  LR: 7.725e-04  Data: 0.030 (0.044)
Train: 95 [ 250/1251 ( 20%)]  Loss: 4.242 (4.28)  Time: 0.254s, 4024.55/s  (0.197s, 5200.88/s)  LR: 7.725e-04  Data: 0.031 (0.041)
Train: 95 [ 300/1251 ( 24%)]  Loss: 4.211 (4.27)  Time: 0.192s, 5346.98/s  (0.195s, 5239.84/s)  LR: 7.725e-04  Data: 0.027 (0.039)
Train: 95 [ 350/1251 ( 28%)]  Loss: 4.062 (4.24)  Time: 0.192s, 5324.50/s  (0.195s, 5254.24/s)  LR: 7.725e-04  Data: 0.026 (0.037)
Train: 95 [ 400/1251 ( 32%)]  Loss: 4.406 (4.26)  Time: 0.166s, 6175.15/s  (0.194s, 5269.50/s)  LR: 7.725e-04  Data: 0.030 (0.036)
Train: 95 [ 450/1251 ( 36%)]  Loss: 4.647 (4.30)  Time: 0.790s, 1295.52/s  (0.196s, 5227.93/s)  LR: 7.725e-04  Data: 0.028 (0.035)
Train: 95 [ 500/1251 ( 40%)]  Loss: 4.176 (4.29)  Time: 0.191s, 5366.53/s  (0.195s, 5244.98/s)  LR: 7.725e-04  Data: 0.027 (0.035)
Train: 95 [ 550/1251 ( 44%)]  Loss: 4.672 (4.32)  Time: 0.162s, 6302.21/s  (0.194s, 5281.02/s)  LR: 7.725e-04  Data: 0.024 (0.034)
Train: 95 [ 600/1251 ( 48%)]  Loss: 4.334 (4.32)  Time: 0.180s, 5681.81/s  (0.194s, 5286.70/s)  LR: 7.725e-04  Data: 0.021 (0.034)
Train: 95 [ 650/1251 ( 52%)]  Loss: 4.258 (4.32)  Time: 0.369s, 2774.60/s  (0.194s, 5288.27/s)  LR: 7.725e-04  Data: 0.031 (0.033)
Train: 95 [ 700/1251 ( 56%)]  Loss: 4.269 (4.31)  Time: 0.171s, 5976.26/s  (0.194s, 5290.02/s)  LR: 7.725e-04  Data: 0.029 (0.033)
Train: 95 [ 750/1251 ( 60%)]  Loss: 4.245 (4.31)  Time: 0.176s, 5807.31/s  (0.194s, 5291.25/s)  LR: 7.725e-04  Data: 0.033 (0.032)
Train: 95 [ 800/1251 ( 64%)]  Loss: 4.488 (4.32)  Time: 0.162s, 6320.95/s  (0.194s, 5288.57/s)  LR: 7.725e-04  Data: 0.023 (0.032)
Train: 95 [ 850/1251 ( 68%)]  Loss: 4.429 (4.33)  Time: 0.312s, 3280.10/s  (0.193s, 5301.47/s)  LR: 7.725e-04  Data: 0.028 (0.032)
Train: 95 [ 900/1251 ( 72%)]  Loss: 4.070 (4.31)  Time: 0.171s, 5982.36/s  (0.194s, 5288.57/s)  LR: 7.725e-04  Data: 0.023 (0.032)
Train: 95 [ 950/1251 ( 76%)]  Loss: 4.677 (4.33)  Time: 0.169s, 6054.85/s  (0.194s, 5281.66/s)  LR: 7.725e-04  Data: 0.022 (0.032)
Train: 95 [1000/1251 ( 80%)]  Loss: 4.235 (4.33)  Time: 0.150s, 6810.16/s  (0.193s, 5294.70/s)  LR: 7.725e-04  Data: 0.025 (0.031)
Train: 95 [1050/1251 ( 84%)]  Loss: 4.575 (4.34)  Time: 0.163s, 6278.73/s  (0.194s, 5291.74/s)  LR: 7.725e-04  Data: 0.027 (0.031)
Train: 95 [1100/1251 ( 88%)]  Loss: 4.144 (4.33)  Time: 0.164s, 6239.70/s  (0.193s, 5292.19/s)  LR: 7.725e-04  Data: 0.025 (0.031)
Train: 95 [1150/1251 ( 92%)]  Loss: 4.302 (4.33)  Time: 0.292s, 3501.10/s  (0.194s, 5285.61/s)  LR: 7.725e-04  Data: 0.026 (0.032)
Train: 95 [1200/1251 ( 96%)]  Loss: 4.458 (4.33)  Time: 0.168s, 6110.74/s  (0.194s, 5284.25/s)  LR: 7.725e-04  Data: 0.021 (0.032)
Train: 95 [1250/1251 (100%)]  Loss: 4.252 (4.33)  Time: 0.113s, 9037.93/s  (0.193s, 5299.12/s)  LR: 7.725e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.795 (1.795)  Loss:  1.2557 (1.2557)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2827 (1.9055)  Acc@1: 79.0094 (62.9960)  Acc@5: 92.5708 (84.9900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-94.pth.tar', 62.45200004150391)

Train: 96 [   0/1251 (  0%)]  Loss: 4.133 (4.13)  Time: 1.801s,  568.44/s  (1.801s,  568.44/s)  LR: 7.681e-04  Data: 1.674 (1.674)
Train: 96 [  50/1251 (  4%)]  Loss: 4.359 (4.25)  Time: 0.169s, 6059.63/s  (0.222s, 4611.90/s)  LR: 7.681e-04  Data: 0.021 (0.068)
Train: 96 [ 100/1251 (  8%)]  Loss: 4.645 (4.38)  Time: 0.179s, 5720.80/s  (0.205s, 5000.80/s)  LR: 7.681e-04  Data: 0.027 (0.048)
Train: 96 [ 150/1251 ( 12%)]  Loss: 4.112 (4.31)  Time: 0.189s, 5427.65/s  (0.199s, 5150.61/s)  LR: 7.681e-04  Data: 0.024 (0.042)
Train: 96 [ 200/1251 ( 16%)]  Loss: 4.272 (4.30)  Time: 0.186s, 5516.78/s  (0.197s, 5186.85/s)  LR: 7.681e-04  Data: 0.029 (0.039)
Train: 96 [ 250/1251 ( 20%)]  Loss: 4.098 (4.27)  Time: 0.161s, 6378.56/s  (0.197s, 5200.40/s)  LR: 7.681e-04  Data: 0.026 (0.037)
Train: 96 [ 300/1251 ( 24%)]  Loss: 4.321 (4.28)  Time: 0.187s, 5475.93/s  (0.195s, 5239.98/s)  LR: 7.681e-04  Data: 0.023 (0.035)
Train: 96 [ 350/1251 ( 28%)]  Loss: 4.366 (4.29)  Time: 0.171s, 5989.97/s  (0.195s, 5254.50/s)  LR: 7.681e-04  Data: 0.026 (0.034)
Train: 96 [ 400/1251 ( 32%)]  Loss: 4.325 (4.29)  Time: 0.388s, 2637.79/s  (0.196s, 5236.10/s)  LR: 7.681e-04  Data: 0.031 (0.034)
Train: 96 [ 450/1251 ( 36%)]  Loss: 4.514 (4.31)  Time: 0.176s, 5803.45/s  (0.195s, 5258.19/s)  LR: 7.681e-04  Data: 0.030 (0.034)
Train: 96 [ 500/1251 ( 40%)]  Loss: 4.320 (4.32)  Time: 0.181s, 5643.72/s  (0.194s, 5288.18/s)  LR: 7.681e-04  Data: 0.021 (0.033)
Train: 96 [ 550/1251 ( 44%)]  Loss: 4.124 (4.30)  Time: 0.166s, 6161.25/s  (0.194s, 5283.06/s)  LR: 7.681e-04  Data: 0.034 (0.033)
Train: 96 [ 600/1251 ( 48%)]  Loss: 4.433 (4.31)  Time: 0.282s, 3630.74/s  (0.193s, 5296.33/s)  LR: 7.681e-04  Data: 0.033 (0.032)
Train: 96 [ 650/1251 ( 52%)]  Loss: 4.579 (4.33)  Time: 0.190s, 5375.73/s  (0.193s, 5305.89/s)  LR: 7.681e-04  Data: 0.029 (0.032)
Train: 96 [ 700/1251 ( 56%)]  Loss: 4.423 (4.33)  Time: 0.176s, 5827.99/s  (0.193s, 5296.74/s)  LR: 7.681e-04  Data: 0.024 (0.032)
Train: 96 [ 750/1251 ( 60%)]  Loss: 4.488 (4.34)  Time: 0.167s, 6128.84/s  (0.193s, 5313.28/s)  LR: 7.681e-04  Data: 0.029 (0.032)
Train: 96 [ 800/1251 ( 64%)]  Loss: 4.453 (4.35)  Time: 0.348s, 2942.70/s  (0.193s, 5312.85/s)  LR: 7.681e-04  Data: 0.027 (0.031)
Train: 96 [ 850/1251 ( 68%)]  Loss: 4.530 (4.36)  Time: 0.180s, 5689.08/s  (0.192s, 5319.87/s)  LR: 7.681e-04  Data: 0.027 (0.031)
Train: 96 [ 900/1251 ( 72%)]  Loss: 4.470 (4.37)  Time: 0.167s, 6145.42/s  (0.193s, 5315.70/s)  LR: 7.681e-04  Data: 0.029 (0.031)
Train: 96 [ 950/1251 ( 76%)]  Loss: 4.444 (4.37)  Time: 0.167s, 6134.14/s  (0.193s, 5317.43/s)  LR: 7.681e-04  Data: 0.034 (0.031)
Train: 96 [1000/1251 ( 80%)]  Loss: 4.174 (4.36)  Time: 0.169s, 6049.57/s  (0.193s, 5317.36/s)  LR: 7.681e-04  Data: 0.030 (0.031)
Train: 96 [1050/1251 ( 84%)]  Loss: 4.422 (4.36)  Time: 0.175s, 5843.42/s  (0.193s, 5310.69/s)  LR: 7.681e-04  Data: 0.028 (0.031)
Train: 96 [1100/1251 ( 88%)]  Loss: 4.228 (4.36)  Time: 0.224s, 4571.02/s  (0.193s, 5311.47/s)  LR: 7.681e-04  Data: 0.032 (0.031)
Train: 96 [1150/1251 ( 92%)]  Loss: 4.462 (4.36)  Time: 0.183s, 5593.82/s  (0.193s, 5309.80/s)  LR: 7.681e-04  Data: 0.031 (0.030)
Train: 96 [1200/1251 ( 96%)]  Loss: 4.467 (4.37)  Time: 0.178s, 5740.00/s  (0.193s, 5305.48/s)  LR: 7.681e-04  Data: 0.020 (0.030)
Train: 96 [1250/1251 (100%)]  Loss: 4.158 (4.36)  Time: 0.113s, 9022.17/s  (0.193s, 5319.10/s)  LR: 7.681e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  1.2409 (1.2409)  Acc@1: 78.2227 (78.2227)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.3482 (1.9240)  Acc@1: 77.7123 (62.4740)  Acc@5: 91.8632 (84.7840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-96.pth.tar', 62.47400001464844)

Train: 97 [   0/1251 (  0%)]  Loss: 4.182 (4.18)  Time: 1.795s,  570.48/s  (1.795s,  570.48/s)  LR: 7.637e-04  Data: 1.484 (1.484)
Train: 97 [  50/1251 (  4%)]  Loss: 4.465 (4.32)  Time: 0.166s, 6151.35/s  (0.221s, 4641.32/s)  LR: 7.637e-04  Data: 0.026 (0.068)
Train: 97 [ 100/1251 (  8%)]  Loss: 4.544 (4.40)  Time: 0.169s, 6045.44/s  (0.207s, 4941.18/s)  LR: 7.637e-04  Data: 0.030 (0.048)
Train: 97 [ 150/1251 ( 12%)]  Loss: 4.589 (4.45)  Time: 0.169s, 6071.99/s  (0.202s, 5072.01/s)  LR: 7.637e-04  Data: 0.034 (0.042)
Train: 97 [ 200/1251 ( 16%)]  Loss: 4.374 (4.43)  Time: 0.175s, 5859.10/s  (0.198s, 5159.57/s)  LR: 7.637e-04  Data: 0.026 (0.039)
Train: 97 [ 250/1251 ( 20%)]  Loss: 4.368 (4.42)  Time: 0.197s, 5185.87/s  (0.196s, 5223.92/s)  LR: 7.637e-04  Data: 0.026 (0.036)
Train: 97 [ 300/1251 ( 24%)]  Loss: 4.440 (4.42)  Time: 0.163s, 6301.06/s  (0.195s, 5244.09/s)  LR: 7.637e-04  Data: 0.026 (0.035)
Train: 97 [ 350/1251 ( 28%)]  Loss: 4.171 (4.39)  Time: 0.164s, 6239.08/s  (0.195s, 5255.74/s)  LR: 7.637e-04  Data: 0.024 (0.034)
Train: 97 [ 400/1251 ( 32%)]  Loss: 4.078 (4.36)  Time: 0.166s, 6163.39/s  (0.194s, 5289.18/s)  LR: 7.637e-04  Data: 0.032 (0.033)
Train: 97 [ 450/1251 ( 36%)]  Loss: 4.341 (4.36)  Time: 0.148s, 6911.58/s  (0.193s, 5294.62/s)  LR: 7.637e-04  Data: 0.027 (0.033)
Train: 97 [ 500/1251 ( 40%)]  Loss: 4.567 (4.37)  Time: 0.177s, 5797.10/s  (0.193s, 5306.41/s)  LR: 7.637e-04  Data: 0.027 (0.032)
Train: 97 [ 550/1251 ( 44%)]  Loss: 4.634 (4.40)  Time: 0.166s, 6159.21/s  (0.193s, 5301.91/s)  LR: 7.637e-04  Data: 0.031 (0.032)
Train: 97 [ 600/1251 ( 48%)]  Loss: 4.287 (4.39)  Time: 0.182s, 5634.96/s  (0.193s, 5299.34/s)  LR: 7.637e-04  Data: 0.034 (0.031)
Train: 97 [ 650/1251 ( 52%)]  Loss: 4.400 (4.39)  Time: 0.179s, 5735.88/s  (0.193s, 5295.29/s)  LR: 7.637e-04  Data: 0.031 (0.031)
Train: 97 [ 700/1251 ( 56%)]  Loss: 4.592 (4.40)  Time: 0.165s, 6212.28/s  (0.193s, 5300.26/s)  LR: 7.637e-04  Data: 0.029 (0.031)
Train: 97 [ 750/1251 ( 60%)]  Loss: 3.861 (4.37)  Time: 0.177s, 5796.10/s  (0.193s, 5316.05/s)  LR: 7.637e-04  Data: 0.033 (0.031)
Train: 97 [ 800/1251 ( 64%)]  Loss: 4.769 (4.39)  Time: 0.198s, 5180.03/s  (0.193s, 5307.06/s)  LR: 7.637e-04  Data: 0.025 (0.031)
Train: 97 [ 850/1251 ( 68%)]  Loss: 4.475 (4.40)  Time: 0.168s, 6089.68/s  (0.194s, 5288.05/s)  LR: 7.637e-04  Data: 0.022 (0.030)
Train: 97 [ 900/1251 ( 72%)]  Loss: 4.328 (4.39)  Time: 0.194s, 5288.02/s  (0.194s, 5289.05/s)  LR: 7.637e-04  Data: 0.065 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 97 [ 950/1251 ( 76%)]  Loss: 4.439 (4.40)  Time: 0.154s, 6648.12/s  (0.193s, 5303.67/s)  LR: 7.637e-04  Data: 0.028 (0.030)
Train: 97 [1000/1251 ( 80%)]  Loss: 4.109 (4.38)  Time: 0.181s, 5665.89/s  (0.193s, 5301.24/s)  LR: 7.637e-04  Data: 0.027 (0.030)
Train: 97 [1050/1251 ( 84%)]  Loss: 4.067 (4.37)  Time: 0.176s, 5824.97/s  (0.193s, 5309.21/s)  LR: 7.637e-04  Data: 0.027 (0.030)
Train: 97 [1100/1251 ( 88%)]  Loss: 4.372 (4.37)  Time: 0.168s, 6092.54/s  (0.193s, 5299.46/s)  LR: 7.637e-04  Data: 0.028 (0.030)
Train: 97 [1150/1251 ( 92%)]  Loss: 4.517 (4.37)  Time: 0.181s, 5659.98/s  (0.193s, 5295.53/s)  LR: 7.637e-04  Data: 0.024 (0.030)
Train: 97 [1200/1251 ( 96%)]  Loss: 4.346 (4.37)  Time: 0.190s, 5401.34/s  (0.194s, 5288.13/s)  LR: 7.637e-04  Data: 0.025 (0.030)
Train: 97 [1250/1251 (100%)]  Loss: 4.329 (4.37)  Time: 0.139s, 7376.86/s  (0.193s, 5302.80/s)  LR: 7.637e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.788 (1.788)  Loss:  1.2318 (1.2318)  Acc@1: 79.2969 (79.2969)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.2447 (1.8785)  Acc@1: 78.7736 (62.9940)  Acc@5: 93.2783 (84.9500)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)

Train: 98 [   0/1251 (  0%)]  Loss: 4.586 (4.59)  Time: 1.899s,  539.11/s  (1.899s,  539.11/s)  LR: 7.593e-04  Data: 1.778 (1.778)
Train: 98 [  50/1251 (  4%)]  Loss: 4.793 (4.69)  Time: 0.167s, 6121.60/s  (0.228s, 4497.98/s)  LR: 7.593e-04  Data: 0.029 (0.085)
Train: 98 [ 100/1251 (  8%)]  Loss: 4.355 (4.58)  Time: 0.166s, 6183.35/s  (0.208s, 4919.76/s)  LR: 7.593e-04  Data: 0.020 (0.063)
Train: 98 [ 150/1251 ( 12%)]  Loss: 3.896 (4.41)  Time: 0.160s, 6391.62/s  (0.201s, 5097.54/s)  LR: 7.593e-04  Data: 0.026 (0.053)
Train: 98 [ 200/1251 ( 16%)]  Loss: 4.776 (4.48)  Time: 0.169s, 6075.56/s  (0.199s, 5144.15/s)  LR: 7.593e-04  Data: 0.036 (0.050)
Train: 98 [ 250/1251 ( 20%)]  Loss: 4.094 (4.42)  Time: 0.187s, 5470.54/s  (0.195s, 5239.49/s)  LR: 7.593e-04  Data: 0.024 (0.046)
Train: 98 [ 300/1251 ( 24%)]  Loss: 4.511 (4.43)  Time: 0.171s, 5980.49/s  (0.194s, 5283.42/s)  LR: 7.593e-04  Data: 0.024 (0.044)
Train: 98 [ 350/1251 ( 28%)]  Loss: 4.435 (4.43)  Time: 0.182s, 5617.47/s  (0.194s, 5289.17/s)  LR: 7.593e-04  Data: 0.032 (0.042)
Train: 98 [ 400/1251 ( 32%)]  Loss: 4.433 (4.43)  Time: 0.185s, 5521.60/s  (0.193s, 5314.59/s)  LR: 7.593e-04  Data: 0.024 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 98 [ 450/1251 ( 36%)]  Loss: 4.324 (4.42)  Time: 0.415s, 2467.30/s  (0.193s, 5302.98/s)  LR: 7.593e-04  Data: 0.020 (0.038)
Train: 98 [ 500/1251 ( 40%)]  Loss: 4.724 (4.45)  Time: 0.162s, 6337.78/s  (0.193s, 5311.32/s)  LR: 7.593e-04  Data: 0.024 (0.038)
Train: 98 [ 550/1251 ( 44%)]  Loss: 4.285 (4.43)  Time: 0.183s, 5610.33/s  (0.192s, 5320.57/s)  LR: 7.593e-04  Data: 0.025 (0.037)
Train: 98 [ 600/1251 ( 48%)]  Loss: 4.153 (4.41)  Time: 0.475s, 2157.35/s  (0.192s, 5327.57/s)  LR: 7.593e-04  Data: 0.019 (0.036)
Train: 98 [ 650/1251 ( 52%)]  Loss: 4.141 (4.39)  Time: 0.170s, 6032.95/s  (0.193s, 5319.40/s)  LR: 7.593e-04  Data: 0.028 (0.036)
Train: 98 [ 700/1251 ( 56%)]  Loss: 4.045 (4.37)  Time: 0.180s, 5679.76/s  (0.192s, 5322.49/s)  LR: 7.593e-04  Data: 0.023 (0.035)
Train: 98 [ 750/1251 ( 60%)]  Loss: 4.106 (4.35)  Time: 0.166s, 6151.72/s  (0.192s, 5327.85/s)  LR: 7.593e-04  Data: 0.028 (0.035)
Train: 98 [ 800/1251 ( 64%)]  Loss: 4.139 (4.34)  Time: 0.192s, 5326.34/s  (0.192s, 5336.55/s)  LR: 7.593e-04  Data: 0.027 (0.034)
Train: 98 [ 850/1251 ( 68%)]  Loss: 4.414 (4.35)  Time: 0.193s, 5294.60/s  (0.192s, 5324.44/s)  LR: 7.593e-04  Data: 0.021 (0.034)
Train: 98 [ 900/1251 ( 72%)]  Loss: 4.702 (4.36)  Time: 0.166s, 6161.94/s  (0.192s, 5326.15/s)  LR: 7.593e-04  Data: 0.028 (0.034)
Train: 98 [ 950/1251 ( 76%)]  Loss: 4.282 (4.36)  Time: 0.154s, 6645.84/s  (0.192s, 5321.32/s)  LR: 7.593e-04  Data: 0.027 (0.033)
Train: 98 [1000/1251 ( 80%)]  Loss: 4.534 (4.37)  Time: 0.174s, 5897.53/s  (0.193s, 5316.03/s)  LR: 7.593e-04  Data: 0.023 (0.033)
Train: 98 [1050/1251 ( 84%)]  Loss: 4.368 (4.37)  Time: 0.192s, 5321.03/s  (0.193s, 5319.15/s)  LR: 7.593e-04  Data: 0.029 (0.033)
Train: 98 [1100/1251 ( 88%)]  Loss: 4.539 (4.38)  Time: 0.402s, 2548.16/s  (0.193s, 5315.77/s)  LR: 7.593e-04  Data: 0.022 (0.033)
Train: 98 [1150/1251 ( 92%)]  Loss: 4.258 (4.37)  Time: 0.162s, 6316.44/s  (0.193s, 5316.05/s)  LR: 7.593e-04  Data: 0.027 (0.032)
Train: 98 [1200/1251 ( 96%)]  Loss: 3.833 (4.35)  Time: 0.149s, 6849.86/s  (0.193s, 5304.67/s)  LR: 7.593e-04  Data: 0.025 (0.032)
Train: 98 [1250/1251 (100%)]  Loss: 4.179 (4.34)  Time: 0.114s, 9014.29/s  (0.192s, 5327.27/s)  LR: 7.593e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.840 (1.840)  Loss:  1.1708 (1.1708)  Acc@1: 77.7344 (77.7344)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.3117 (1.8980)  Acc@1: 78.1840 (62.4940)  Acc@5: 93.0424 (84.7900)
Train: 99 [   0/1251 (  0%)]  Loss: 4.385 (4.39)  Time: 1.836s,  557.81/s  (1.836s,  557.81/s)  LR: 7.548e-04  Data: 1.706 (1.706)
Train: 99 [  50/1251 (  4%)]  Loss: 4.061 (4.22)  Time: 0.173s, 5915.26/s  (0.219s, 4675.76/s)  LR: 7.548e-04  Data: 0.024 (0.070)
Train: 99 [ 100/1251 (  8%)]  Loss: 4.287 (4.24)  Time: 0.161s, 6359.81/s  (0.207s, 4956.64/s)  LR: 7.548e-04  Data: 0.020 (0.058)
Train: 99 [ 150/1251 ( 12%)]  Loss: 4.645 (4.34)  Time: 0.161s, 6345.77/s  (0.199s, 5151.66/s)  LR: 7.548e-04  Data: 0.028 (0.050)
Train: 99 [ 200/1251 ( 16%)]  Loss: 4.301 (4.34)  Time: 0.239s, 4286.48/s  (0.199s, 5155.88/s)  LR: 7.548e-04  Data: 0.029 (0.046)
Train: 99 [ 250/1251 ( 20%)]  Loss: 4.273 (4.33)  Time: 0.188s, 5443.61/s  (0.197s, 5201.91/s)  LR: 7.548e-04  Data: 0.027 (0.042)
Train: 99 [ 300/1251 ( 24%)]  Loss: 4.227 (4.31)  Time: 0.196s, 5216.67/s  (0.195s, 5249.19/s)  LR: 7.548e-04  Data: 0.024 (0.039)
Train: 99 [ 350/1251 ( 28%)]  Loss: 3.796 (4.25)  Time: 0.309s, 3313.94/s  (0.195s, 5245.43/s)  LR: 7.548e-04  Data: 0.031 (0.038)
Train: 99 [ 400/1251 ( 32%)]  Loss: 4.476 (4.27)  Time: 0.162s, 6308.78/s  (0.194s, 5280.14/s)  LR: 7.548e-04  Data: 0.025 (0.037)
Train: 99 [ 450/1251 ( 36%)]  Loss: 4.440 (4.29)  Time: 0.167s, 6114.26/s  (0.194s, 5279.70/s)  LR: 7.548e-04  Data: 0.028 (0.036)
Train: 99 [ 500/1251 ( 40%)]  Loss: 4.466 (4.31)  Time: 0.180s, 5678.69/s  (0.193s, 5293.25/s)  LR: 7.548e-04  Data: 0.027 (0.035)
Train: 99 [ 550/1251 ( 44%)]  Loss: 4.794 (4.35)  Time: 0.297s, 3452.85/s  (0.194s, 5285.60/s)  LR: 7.548e-04  Data: 0.025 (0.034)
Train: 99 [ 600/1251 ( 48%)]  Loss: 4.073 (4.32)  Time: 0.179s, 5725.55/s  (0.193s, 5298.23/s)  LR: 7.548e-04  Data: 0.029 (0.034)
Train: 99 [ 650/1251 ( 52%)]  Loss: 4.451 (4.33)  Time: 0.159s, 6437.75/s  (0.193s, 5306.93/s)  LR: 7.548e-04  Data: 0.029 (0.034)
Train: 99 [ 700/1251 ( 56%)]  Loss: 4.376 (4.34)  Time: 0.155s, 6617.83/s  (0.192s, 5323.72/s)  LR: 7.548e-04  Data: 0.028 (0.033)
Train: 99 [ 750/1251 ( 60%)]  Loss: 4.117 (4.32)  Time: 0.176s, 5818.38/s  (0.192s, 5332.42/s)  LR: 7.548e-04  Data: 0.038 (0.033)
Train: 99 [ 800/1251 ( 64%)]  Loss: 4.431 (4.33)  Time: 0.171s, 5985.27/s  (0.192s, 5326.98/s)  LR: 7.548e-04  Data: 0.026 (0.033)
Train: 99 [ 850/1251 ( 68%)]  Loss: 4.027 (4.31)  Time: 0.181s, 5665.48/s  (0.193s, 5311.17/s)  LR: 7.548e-04  Data: 0.029 (0.032)
Train: 99 [ 900/1251 ( 72%)]  Loss: 4.595 (4.33)  Time: 0.184s, 5577.40/s  (0.192s, 5322.49/s)  LR: 7.548e-04  Data: 0.026 (0.032)
Train: 99 [ 950/1251 ( 76%)]  Loss: 4.287 (4.33)  Time: 0.176s, 5828.18/s  (0.193s, 5319.00/s)  LR: 7.548e-04  Data: 0.033 (0.032)
Train: 99 [1000/1251 ( 80%)]  Loss: 3.804 (4.30)  Time: 0.173s, 5933.71/s  (0.192s, 5321.47/s)  LR: 7.548e-04  Data: 0.023 (0.032)
Train: 99 [1050/1251 ( 84%)]  Loss: 4.373 (4.30)  Time: 0.179s, 5710.94/s  (0.193s, 5309.39/s)  LR: 7.548e-04  Data: 0.028 (0.032)
Train: 99 [1100/1251 ( 88%)]  Loss: 4.448 (4.31)  Time: 0.190s, 5376.69/s  (0.193s, 5307.99/s)  LR: 7.548e-04  Data: 0.034 (0.031)
Train: 99 [1150/1251 ( 92%)]  Loss: 3.982 (4.30)  Time: 0.163s, 6271.50/s  (0.193s, 5309.66/s)  LR: 7.548e-04  Data: 0.028 (0.031)
Train: 99 [1200/1251 ( 96%)]  Loss: 4.256 (4.29)  Time: 0.188s, 5443.21/s  (0.193s, 5304.25/s)  LR: 7.548e-04  Data: 0.025 (0.031)
Train: 99 [1250/1251 (100%)]  Loss: 3.959 (4.28)  Time: 0.114s, 8995.17/s  (0.193s, 5314.92/s)  LR: 7.548e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.817 (1.817)  Loss:  1.3312 (1.3312)  Acc@1: 79.7852 (79.7852)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3181 (1.9700)  Acc@1: 78.6557 (62.4960)  Acc@5: 93.7500 (84.5600)
Train: 100 [   0/1251 (  0%)]  Loss: 4.239 (4.24)  Time: 1.790s,  572.14/s  (1.790s,  572.14/s)  LR: 7.503e-04  Data: 1.667 (1.667)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 100 [  50/1251 (  4%)]  Loss: 3.929 (4.08)  Time: 0.180s, 5693.52/s  (0.222s, 4615.94/s)  LR: 7.503e-04  Data: 0.026 (0.064)
Train: 100 [ 100/1251 (  8%)]  Loss: 4.616 (4.26)  Time: 0.166s, 6185.91/s  (0.206s, 4965.90/s)  LR: 7.503e-04  Data: 0.025 (0.046)
Train: 100 [ 150/1251 ( 12%)]  Loss: 4.211 (4.25)  Time: 0.168s, 6094.04/s  (0.200s, 5110.16/s)  LR: 7.503e-04  Data: 0.025 (0.040)
Train: 100 [ 200/1251 ( 16%)]  Loss: 4.010 (4.20)  Time: 0.343s, 2986.57/s  (0.198s, 5163.95/s)  LR: 7.503e-04  Data: 0.025 (0.037)
Train: 100 [ 250/1251 ( 20%)]  Loss: 4.272 (4.21)  Time: 0.167s, 6141.72/s  (0.196s, 5234.92/s)  LR: 7.503e-04  Data: 0.035 (0.035)
Train: 100 [ 300/1251 ( 24%)]  Loss: 3.975 (4.18)  Time: 0.169s, 6074.53/s  (0.194s, 5284.05/s)  LR: 7.503e-04  Data: 0.025 (0.034)
Train: 100 [ 350/1251 ( 28%)]  Loss: 4.205 (4.18)  Time: 0.173s, 5905.62/s  (0.194s, 5289.40/s)  LR: 7.503e-04  Data: 0.026 (0.033)
Train: 100 [ 400/1251 ( 32%)]  Loss: 4.294 (4.19)  Time: 0.180s, 5674.19/s  (0.193s, 5317.66/s)  LR: 7.503e-04  Data: 0.023 (0.032)
Train: 100 [ 450/1251 ( 36%)]  Loss: 4.104 (4.19)  Time: 0.171s, 5981.89/s  (0.193s, 5308.98/s)  LR: 7.503e-04  Data: 0.023 (0.032)
Train: 100 [ 500/1251 ( 40%)]  Loss: 4.356 (4.20)  Time: 0.179s, 5717.14/s  (0.193s, 5315.46/s)  LR: 7.503e-04  Data: 0.021 (0.032)
Train: 100 [ 550/1251 ( 44%)]  Loss: 4.521 (4.23)  Time: 0.179s, 5712.09/s  (0.192s, 5325.15/s)  LR: 7.503e-04  Data: 0.025 (0.031)
Train: 100 [ 600/1251 ( 48%)]  Loss: 4.165 (4.22)  Time: 0.176s, 5803.42/s  (0.193s, 5316.47/s)  LR: 7.503e-04  Data: 0.026 (0.031)
Train: 100 [ 650/1251 ( 52%)]  Loss: 4.313 (4.23)  Time: 0.229s, 4464.19/s  (0.192s, 5320.11/s)  LR: 7.503e-04  Data: 0.024 (0.031)
Train: 100 [ 700/1251 ( 56%)]  Loss: 3.976 (4.21)  Time: 0.190s, 5396.37/s  (0.192s, 5324.62/s)  LR: 7.503e-04  Data: 0.023 (0.031)
Train: 100 [ 750/1251 ( 60%)]  Loss: 4.448 (4.23)  Time: 0.165s, 6195.70/s  (0.192s, 5332.06/s)  LR: 7.503e-04  Data: 0.026 (0.031)
Train: 100 [ 800/1251 ( 64%)]  Loss: 4.404 (4.24)  Time: 0.161s, 6343.90/s  (0.193s, 5313.54/s)  LR: 7.503e-04  Data: 0.027 (0.031)
Train: 100 [ 850/1251 ( 68%)]  Loss: 4.625 (4.26)  Time: 0.180s, 5677.22/s  (0.193s, 5313.60/s)  LR: 7.503e-04  Data: 0.036 (0.030)
Train: 100 [ 900/1251 ( 72%)]  Loss: 4.443 (4.27)  Time: 0.166s, 6173.07/s  (0.193s, 5316.80/s)  LR: 7.503e-04  Data: 0.032 (0.030)
Train: 100 [ 950/1251 ( 76%)]  Loss: 4.471 (4.28)  Time: 0.168s, 6086.81/s  (0.193s, 5305.91/s)  LR: 7.503e-04  Data: 0.028 (0.030)
Train: 100 [1000/1251 ( 80%)]  Loss: 4.117 (4.27)  Time: 0.683s, 1499.44/s  (0.194s, 5291.16/s)  LR: 7.503e-04  Data: 0.028 (0.030)
Train: 100 [1050/1251 ( 84%)]  Loss: 4.585 (4.29)  Time: 0.169s, 6076.05/s  (0.193s, 5295.90/s)  LR: 7.503e-04  Data: 0.030 (0.030)
Train: 100 [1100/1251 ( 88%)]  Loss: 4.358 (4.29)  Time: 0.166s, 6158.78/s  (0.193s, 5298.49/s)  LR: 7.503e-04  Data: 0.029 (0.030)
Train: 100 [1150/1251 ( 92%)]  Loss: 4.632 (4.30)  Time: 0.158s, 6472.57/s  (0.193s, 5300.09/s)  LR: 7.503e-04  Data: 0.026 (0.030)
Train: 100 [1200/1251 ( 96%)]  Loss: 4.105 (4.29)  Time: 0.424s, 2412.94/s  (0.193s, 5296.60/s)  LR: 7.503e-04  Data: 0.034 (0.030)
Train: 100 [1250/1251 (100%)]  Loss: 4.488 (4.30)  Time: 0.113s, 9030.86/s  (0.193s, 5312.57/s)  LR: 7.503e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.820 (1.820)  Loss:  1.1879 (1.1879)  Acc@1: 80.5664 (80.5664)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.3871 (1.9179)  Acc@1: 77.1227 (62.4380)  Acc@5: 91.5094 (84.6260)
Train: 101 [   0/1251 (  0%)]  Loss: 4.079 (4.08)  Time: 1.763s,  580.97/s  (1.763s,  580.97/s)  LR: 7.457e-04  Data: 1.625 (1.625)
Train: 101 [  50/1251 (  4%)]  Loss: 4.492 (4.29)  Time: 0.166s, 6152.33/s  (0.228s, 4492.77/s)  LR: 7.457e-04  Data: 0.032 (0.070)
Train: 101 [ 100/1251 (  8%)]  Loss: 4.240 (4.27)  Time: 0.183s, 5593.28/s  (0.210s, 4867.85/s)  LR: 7.457e-04  Data: 0.027 (0.052)
Train: 101 [ 150/1251 ( 12%)]  Loss: 4.272 (4.27)  Time: 0.164s, 6249.08/s  (0.200s, 5131.80/s)  LR: 7.457e-04  Data: 0.034 (0.046)
Train: 101 [ 200/1251 ( 16%)]  Loss: 4.749 (4.37)  Time: 0.172s, 5946.85/s  (0.201s, 5103.27/s)  LR: 7.457e-04  Data: 0.025 (0.043)
Train: 101 [ 250/1251 ( 20%)]  Loss: 4.548 (4.40)  Time: 0.174s, 5875.08/s  (0.197s, 5186.31/s)  LR: 7.457e-04  Data: 0.027 (0.040)
Train: 101 [ 300/1251 ( 24%)]  Loss: 4.197 (4.37)  Time: 0.171s, 5978.05/s  (0.195s, 5239.72/s)  LR: 7.457e-04  Data: 0.023 (0.038)
Train: 101 [ 350/1251 ( 28%)]  Loss: 4.358 (4.37)  Time: 0.180s, 5690.72/s  (0.196s, 5237.57/s)  LR: 7.457e-04  Data: 0.027 (0.036)
Train: 101 [ 400/1251 ( 32%)]  Loss: 4.185 (4.35)  Time: 0.187s, 5463.94/s  (0.195s, 5259.56/s)  LR: 7.457e-04  Data: 0.022 (0.036)
Train: 101 [ 450/1251 ( 36%)]  Loss: 4.328 (4.34)  Time: 0.169s, 6056.19/s  (0.194s, 5282.05/s)  LR: 7.457e-04  Data: 0.032 (0.035)
Train: 101 [ 500/1251 ( 40%)]  Loss: 4.350 (4.35)  Time: 0.157s, 6523.94/s  (0.194s, 5272.85/s)  LR: 7.457e-04  Data: 0.026 (0.034)
Train: 101 [ 550/1251 ( 44%)]  Loss: 4.543 (4.36)  Time: 0.156s, 6565.68/s  (0.193s, 5294.74/s)  LR: 7.457e-04  Data: 0.029 (0.034)
Train: 101 [ 600/1251 ( 48%)]  Loss: 3.820 (4.32)  Time: 0.170s, 6024.11/s  (0.193s, 5294.41/s)  LR: 7.457e-04  Data: 0.029 (0.033)
Train: 101 [ 650/1251 ( 52%)]  Loss: 3.993 (4.30)  Time: 0.749s, 1366.74/s  (0.194s, 5268.32/s)  LR: 7.457e-04  Data: 0.021 (0.033)
Train: 101 [ 700/1251 ( 56%)]  Loss: 4.190 (4.29)  Time: 0.171s, 5991.49/s  (0.193s, 5299.92/s)  LR: 7.457e-04  Data: 0.027 (0.033)
Train: 101 [ 750/1251 ( 60%)]  Loss: 4.495 (4.30)  Time: 0.167s, 6134.06/s  (0.193s, 5298.85/s)  LR: 7.457e-04  Data: 0.039 (0.032)
Train: 101 [ 800/1251 ( 64%)]  Loss: 4.362 (4.31)  Time: 0.182s, 5621.60/s  (0.193s, 5293.74/s)  LR: 7.457e-04  Data: 0.044 (0.032)
Train: 101 [ 850/1251 ( 68%)]  Loss: 4.319 (4.31)  Time: 0.259s, 3950.34/s  (0.193s, 5308.63/s)  LR: 7.457e-04  Data: 0.024 (0.032)
Train: 101 [ 900/1251 ( 72%)]  Loss: 4.326 (4.31)  Time: 0.181s, 5670.68/s  (0.193s, 5311.20/s)  LR: 7.457e-04  Data: 0.035 (0.032)
Train: 101 [ 950/1251 ( 76%)]  Loss: 4.434 (4.31)  Time: 0.188s, 5448.12/s  (0.193s, 5310.15/s)  LR: 7.457e-04  Data: 0.022 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 101 [1000/1251 ( 80%)]  Loss: 4.314 (4.31)  Time: 0.157s, 6501.84/s  (0.193s, 5303.85/s)  LR: 7.457e-04  Data: 0.024 (0.031)
Train: 101 [1050/1251 ( 84%)]  Loss: 4.516 (4.32)  Time: 0.163s, 6291.33/s  (0.193s, 5301.98/s)  LR: 7.457e-04  Data: 0.031 (0.031)
Train: 101 [1100/1251 ( 88%)]  Loss: 4.170 (4.32)  Time: 0.175s, 5846.10/s  (0.193s, 5296.45/s)  LR: 7.457e-04  Data: 0.032 (0.031)
Train: 101 [1150/1251 ( 92%)]  Loss: 3.577 (4.29)  Time: 0.176s, 5815.42/s  (0.193s, 5293.57/s)  LR: 7.457e-04  Data: 0.026 (0.031)
Train: 101 [1200/1251 ( 96%)]  Loss: 4.396 (4.29)  Time: 0.156s, 6551.66/s  (0.193s, 5293.71/s)  LR: 7.457e-04  Data: 0.028 (0.031)
Train: 101 [1250/1251 (100%)]  Loss: 4.312 (4.29)  Time: 0.114s, 8983.30/s  (0.193s, 5304.57/s)  LR: 7.457e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.772 (1.772)  Loss:  1.1632 (1.1632)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.2268 (1.8743)  Acc@1: 77.5943 (62.4960)  Acc@5: 91.9811 (84.9220)
Train: 102 [   0/1251 (  0%)]  Loss: 4.255 (4.26)  Time: 1.661s,  616.62/s  (1.661s,  616.62/s)  LR: 7.411e-04  Data: 1.530 (1.530)
Train: 102 [  50/1251 (  4%)]  Loss: 4.278 (4.27)  Time: 0.174s, 5883.32/s  (0.221s, 4632.55/s)  LR: 7.411e-04  Data: 0.025 (0.066)
Train: 102 [ 100/1251 (  8%)]  Loss: 4.494 (4.34)  Time: 0.169s, 6074.07/s  (0.204s, 5025.74/s)  LR: 7.411e-04  Data: 0.035 (0.047)
Train: 102 [ 150/1251 ( 12%)]  Loss: 4.823 (4.46)  Time: 0.163s, 6299.56/s  (0.199s, 5157.46/s)  LR: 7.411e-04  Data: 0.032 (0.042)
Train: 102 [ 200/1251 ( 16%)]  Loss: 4.245 (4.42)  Time: 0.188s, 5458.62/s  (0.197s, 5193.90/s)  LR: 7.411e-04  Data: 0.029 (0.039)
Train: 102 [ 250/1251 ( 20%)]  Loss: 4.452 (4.42)  Time: 0.197s, 5210.88/s  (0.196s, 5227.69/s)  LR: 7.411e-04  Data: 0.023 (0.038)
Train: 102 [ 300/1251 ( 24%)]  Loss: 4.502 (4.44)  Time: 0.177s, 5798.64/s  (0.195s, 5262.59/s)  LR: 7.411e-04  Data: 0.025 (0.036)
Train: 102 [ 350/1251 ( 28%)]  Loss: 3.964 (4.38)  Time: 0.181s, 5647.77/s  (0.194s, 5290.05/s)  LR: 7.411e-04  Data: 0.022 (0.035)
Train: 102 [ 400/1251 ( 32%)]  Loss: 3.919 (4.33)  Time: 0.164s, 6226.65/s  (0.193s, 5308.44/s)  LR: 7.411e-04  Data: 0.030 (0.034)
Train: 102 [ 450/1251 ( 36%)]  Loss: 4.384 (4.33)  Time: 0.161s, 6351.29/s  (0.193s, 5293.03/s)  LR: 7.411e-04  Data: 0.027 (0.035)
Train: 102 [ 500/1251 ( 40%)]  Loss: 4.225 (4.32)  Time: 0.174s, 5878.07/s  (0.193s, 5318.19/s)  LR: 7.411e-04  Data: 0.022 (0.034)
Train: 102 [ 550/1251 ( 44%)]  Loss: 4.213 (4.31)  Time: 0.193s, 5308.96/s  (0.192s, 5326.76/s)  LR: 7.411e-04  Data: 0.027 (0.034)
Train: 102 [ 600/1251 ( 48%)]  Loss: 4.075 (4.29)  Time: 0.330s, 3101.69/s  (0.192s, 5321.63/s)  LR: 7.411e-04  Data: 0.026 (0.034)
Train: 102 [ 650/1251 ( 52%)]  Loss: 4.141 (4.28)  Time: 0.168s, 6085.63/s  (0.193s, 5309.66/s)  LR: 7.411e-04  Data: 0.030 (0.033)
Train: 102 [ 700/1251 ( 56%)]  Loss: 4.142 (4.27)  Time: 0.157s, 6539.70/s  (0.192s, 5330.03/s)  LR: 7.411e-04  Data: 0.031 (0.033)
Train: 102 [ 750/1251 ( 60%)]  Loss: 4.272 (4.27)  Time: 0.180s, 5685.20/s  (0.192s, 5330.79/s)  LR: 7.411e-04  Data: 0.027 (0.032)
Train: 102 [ 800/1251 ( 64%)]  Loss: 4.462 (4.29)  Time: 0.177s, 5788.28/s  (0.192s, 5342.92/s)  LR: 7.411e-04  Data: 0.019 (0.032)
Train: 102 [ 850/1251 ( 68%)]  Loss: 4.247 (4.28)  Time: 0.220s, 4654.75/s  (0.192s, 5335.82/s)  LR: 7.411e-04  Data: 0.033 (0.032)
Train: 102 [ 900/1251 ( 72%)]  Loss: 3.945 (4.27)  Time: 0.176s, 5822.87/s  (0.192s, 5332.50/s)  LR: 7.411e-04  Data: 0.021 (0.032)
Train: 102 [ 950/1251 ( 76%)]  Loss: 4.497 (4.28)  Time: 0.180s, 5690.24/s  (0.192s, 5330.78/s)  LR: 7.411e-04  Data: 0.026 (0.032)
Train: 102 [1000/1251 ( 80%)]  Loss: 4.447 (4.28)  Time: 0.166s, 6162.06/s  (0.192s, 5322.39/s)  LR: 7.411e-04  Data: 0.028 (0.032)
Train: 102 [1050/1251 ( 84%)]  Loss: 4.341 (4.29)  Time: 0.180s, 5695.22/s  (0.193s, 5318.51/s)  LR: 7.411e-04  Data: 0.027 (0.033)
Train: 102 [1100/1251 ( 88%)]  Loss: 4.335 (4.29)  Time: 0.175s, 5856.70/s  (0.193s, 5311.67/s)  LR: 7.411e-04  Data: 0.028 (0.034)
Train: 102 [1150/1251 ( 92%)]  Loss: 4.306 (4.29)  Time: 0.180s, 5689.70/s  (0.193s, 5308.55/s)  LR: 7.411e-04  Data: 0.025 (0.034)
Train: 102 [1200/1251 ( 96%)]  Loss: 4.273 (4.29)  Time: 0.163s, 6277.57/s  (0.193s, 5305.39/s)  LR: 7.411e-04  Data: 0.028 (0.035)
Train: 102 [1250/1251 (100%)]  Loss: 4.329 (4.29)  Time: 0.114s, 8998.73/s  (0.193s, 5312.56/s)  LR: 7.411e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.779 (1.779)  Loss:  1.1346 (1.1346)  Acc@1: 79.1016 (79.1016)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1965 (1.8351)  Acc@1: 79.9528 (63.1480)  Acc@5: 92.3349 (85.1400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)

Train: 103 [   0/1251 (  0%)]  Loss: 4.419 (4.42)  Time: 1.880s,  544.68/s  (1.880s,  544.68/s)  LR: 7.365e-04  Data: 1.749 (1.749)
Train: 103 [  50/1251 (  4%)]  Loss: 4.393 (4.41)  Time: 0.178s, 5765.52/s  (0.224s, 4580.39/s)  LR: 7.365e-04  Data: 0.019 (0.078)
Train: 103 [ 100/1251 (  8%)]  Loss: 4.035 (4.28)  Time: 0.178s, 5755.35/s  (0.207s, 4939.66/s)  LR: 7.365e-04  Data: 0.030 (0.052)
Train: 103 [ 150/1251 ( 12%)]  Loss: 4.523 (4.34)  Time: 0.191s, 5347.27/s  (0.201s, 5099.22/s)  LR: 7.365e-04  Data: 0.020 (0.044)
Train: 103 [ 200/1251 ( 16%)]  Loss: 4.292 (4.33)  Time: 0.173s, 5935.25/s  (0.200s, 5117.86/s)  LR: 7.365e-04  Data: 0.031 (0.040)
Train: 103 [ 250/1251 ( 20%)]  Loss: 4.432 (4.35)  Time: 0.162s, 6327.56/s  (0.198s, 5170.31/s)  LR: 7.365e-04  Data: 0.029 (0.037)
Train: 103 [ 300/1251 ( 24%)]  Loss: 3.992 (4.30)  Time: 0.182s, 5618.32/s  (0.194s, 5266.87/s)  LR: 7.365e-04  Data: 0.022 (0.036)
Train: 103 [ 350/1251 ( 28%)]  Loss: 4.850 (4.37)  Time: 0.166s, 6163.67/s  (0.195s, 5241.84/s)  LR: 7.365e-04  Data: 0.024 (0.035)
Train: 103 [ 400/1251 ( 32%)]  Loss: 4.081 (4.34)  Time: 0.155s, 6601.88/s  (0.193s, 5307.89/s)  LR: 7.365e-04  Data: 0.030 (0.034)
Train: 103 [ 450/1251 ( 36%)]  Loss: 4.407 (4.34)  Time: 0.185s, 5534.15/s  (0.192s, 5321.72/s)  LR: 7.365e-04  Data: 0.023 (0.033)
Train: 103 [ 500/1251 ( 40%)]  Loss: 4.316 (4.34)  Time: 0.171s, 5976.90/s  (0.192s, 5331.01/s)  LR: 7.365e-04  Data: 0.034 (0.033)
Train: 103 [ 550/1251 ( 44%)]  Loss: 4.409 (4.35)  Time: 0.194s, 5275.30/s  (0.192s, 5327.42/s)  LR: 7.365e-04  Data: 0.029 (0.032)
Train: 103 [ 600/1251 ( 48%)]  Loss: 4.268 (4.34)  Time: 0.162s, 6328.74/s  (0.193s, 5312.60/s)  LR: 7.365e-04  Data: 0.030 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 103 [ 650/1251 ( 52%)]  Loss: 4.462 (4.35)  Time: 0.176s, 5824.04/s  (0.192s, 5322.95/s)  LR: 7.365e-04  Data: 0.032 (0.032)
Train: 103 [ 700/1251 ( 56%)]  Loss: 4.436 (4.35)  Time: 0.170s, 6020.02/s  (0.192s, 5323.53/s)  LR: 7.365e-04  Data: 0.031 (0.032)
Train: 103 [ 750/1251 ( 60%)]  Loss: 4.550 (4.37)  Time: 0.162s, 6316.82/s  (0.193s, 5310.08/s)  LR: 7.365e-04  Data: 0.030 (0.031)
Train: 103 [ 800/1251 ( 64%)]  Loss: 4.210 (4.36)  Time: 0.184s, 5558.78/s  (0.193s, 5307.46/s)  LR: 7.365e-04  Data: 0.045 (0.031)
Train: 103 [ 850/1251 ( 68%)]  Loss: 4.177 (4.35)  Time: 0.188s, 5434.11/s  (0.193s, 5315.28/s)  LR: 7.365e-04  Data: 0.028 (0.031)
Train: 103 [ 900/1251 ( 72%)]  Loss: 4.571 (4.36)  Time: 0.183s, 5590.88/s  (0.192s, 5320.23/s)  LR: 7.365e-04  Data: 0.027 (0.031)
Train: 103 [ 950/1251 ( 76%)]  Loss: 4.574 (4.37)  Time: 0.179s, 5714.42/s  (0.193s, 5312.36/s)  LR: 7.365e-04  Data: 0.029 (0.031)
Train: 103 [1000/1251 ( 80%)]  Loss: 4.358 (4.37)  Time: 0.193s, 5310.71/s  (0.193s, 5300.47/s)  LR: 7.365e-04  Data: 0.029 (0.031)
Train: 103 [1050/1251 ( 84%)]  Loss: 4.547 (4.38)  Time: 0.175s, 5855.41/s  (0.193s, 5309.08/s)  LR: 7.365e-04  Data: 0.033 (0.031)
Train: 103 [1100/1251 ( 88%)]  Loss: 4.493 (4.38)  Time: 0.168s, 6083.72/s  (0.193s, 5303.57/s)  LR: 7.365e-04  Data: 0.033 (0.030)
Train: 103 [1150/1251 ( 92%)]  Loss: 4.240 (4.38)  Time: 0.160s, 6391.54/s  (0.193s, 5303.64/s)  LR: 7.365e-04  Data: 0.030 (0.030)
Train: 103 [1200/1251 ( 96%)]  Loss: 4.082 (4.36)  Time: 0.153s, 6676.91/s  (0.193s, 5303.55/s)  LR: 7.365e-04  Data: 0.023 (0.030)
Train: 103 [1250/1251 (100%)]  Loss: 4.574 (4.37)  Time: 0.113s, 9022.17/s  (0.193s, 5314.19/s)  LR: 7.365e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.864 (1.864)  Loss:  1.1686 (1.1686)  Acc@1: 78.4180 (78.4180)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.1598 (1.8761)  Acc@1: 80.6604 (62.9740)  Acc@5: 93.7500 (84.8820)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)

Train: 104 [   0/1251 (  0%)]  Loss: 4.393 (4.39)  Time: 1.731s,  591.41/s  (1.731s,  591.41/s)  LR: 7.319e-04  Data: 1.607 (1.607)
Train: 104 [  50/1251 (  4%)]  Loss: 4.414 (4.40)  Time: 0.168s, 6085.26/s  (0.228s, 4499.27/s)  LR: 7.319e-04  Data: 0.031 (0.085)
Train: 104 [ 100/1251 (  8%)]  Loss: 4.300 (4.37)  Time: 0.161s, 6341.03/s  (0.206s, 4968.21/s)  LR: 7.319e-04  Data: 0.027 (0.063)
Train: 104 [ 150/1251 ( 12%)]  Loss: 4.228 (4.33)  Time: 0.180s, 5689.48/s  (0.202s, 5076.86/s)  LR: 7.319e-04  Data: 0.020 (0.058)
Train: 104 [ 200/1251 ( 16%)]  Loss: 4.142 (4.30)  Time: 0.169s, 6046.75/s  (0.197s, 5206.28/s)  LR: 7.319e-04  Data: 0.025 (0.053)
Train: 104 [ 250/1251 ( 20%)]  Loss: 4.726 (4.37)  Time: 0.171s, 5973.59/s  (0.195s, 5255.81/s)  LR: 7.319e-04  Data: 0.033 (0.048)
Train: 104 [ 300/1251 ( 24%)]  Loss: 4.584 (4.40)  Time: 0.190s, 5383.55/s  (0.195s, 5261.72/s)  LR: 7.319e-04  Data: 0.022 (0.046)
Train: 104 [ 350/1251 ( 28%)]  Loss: 4.455 (4.41)  Time: 0.181s, 5663.34/s  (0.194s, 5283.86/s)  LR: 7.319e-04  Data: 0.034 (0.043)
Train: 104 [ 400/1251 ( 32%)]  Loss: 4.539 (4.42)  Time: 0.192s, 5342.82/s  (0.193s, 5302.84/s)  LR: 7.319e-04  Data: 0.025 (0.041)
Train: 104 [ 450/1251 ( 36%)]  Loss: 4.361 (4.41)  Time: 0.174s, 5897.74/s  (0.192s, 5320.19/s)  LR: 7.319e-04  Data: 0.024 (0.040)
Train: 104 [ 500/1251 ( 40%)]  Loss: 3.784 (4.36)  Time: 0.189s, 5427.62/s  (0.193s, 5307.23/s)  LR: 7.319e-04  Data: 0.042 (0.039)
Train: 104 [ 550/1251 ( 44%)]  Loss: 4.214 (4.34)  Time: 0.167s, 6116.82/s  (0.192s, 5323.99/s)  LR: 7.319e-04  Data: 0.037 (0.038)
Train: 104 [ 600/1251 ( 48%)]  Loss: 4.421 (4.35)  Time: 0.162s, 6326.54/s  (0.192s, 5337.29/s)  LR: 7.319e-04  Data: 0.025 (0.037)
Train: 104 [ 650/1251 ( 52%)]  Loss: 4.546 (4.36)  Time: 0.173s, 5906.34/s  (0.192s, 5321.60/s)  LR: 7.319e-04  Data: 0.026 (0.036)
Train: 104 [ 700/1251 ( 56%)]  Loss: 4.486 (4.37)  Time: 0.161s, 6359.97/s  (0.192s, 5327.93/s)  LR: 7.319e-04  Data: 0.028 (0.036)
Train: 104 [ 750/1251 ( 60%)]  Loss: 4.272 (4.37)  Time: 0.169s, 6064.21/s  (0.192s, 5329.84/s)  LR: 7.319e-04  Data: 0.028 (0.035)
Train: 104 [ 800/1251 ( 64%)]  Loss: 4.387 (4.37)  Time: 0.333s, 3074.84/s  (0.192s, 5324.82/s)  LR: 7.319e-04  Data: 0.026 (0.035)
Train: 104 [ 850/1251 ( 68%)]  Loss: 4.398 (4.37)  Time: 0.164s, 6230.83/s  (0.192s, 5331.10/s)  LR: 7.319e-04  Data: 0.028 (0.035)
Train: 104 [ 900/1251 ( 72%)]  Loss: 4.123 (4.36)  Time: 0.167s, 6124.08/s  (0.192s, 5333.42/s)  LR: 7.319e-04  Data: 0.023 (0.034)
Train: 104 [ 950/1251 ( 76%)]  Loss: 4.207 (4.35)  Time: 0.322s, 3185.02/s  (0.192s, 5327.70/s)  LR: 7.319e-04  Data: 0.026 (0.034)
Train: 104 [1000/1251 ( 80%)]  Loss: 3.850 (4.33)  Time: 0.161s, 6351.94/s  (0.192s, 5323.34/s)  LR: 7.319e-04  Data: 0.028 (0.034)
Train: 104 [1050/1251 ( 84%)]  Loss: 4.255 (4.32)  Time: 0.175s, 5838.65/s  (0.192s, 5323.63/s)  LR: 7.319e-04  Data: 0.027 (0.034)
Train: 104 [1100/1251 ( 88%)]  Loss: 4.206 (4.32)  Time: 0.160s, 6417.80/s  (0.193s, 5318.26/s)  LR: 7.319e-04  Data: 0.028 (0.033)
Train: 104 [1150/1251 ( 92%)]  Loss: 4.125 (4.31)  Time: 0.530s, 1933.53/s  (0.193s, 5315.76/s)  LR: 7.319e-04  Data: 0.026 (0.033)
Train: 104 [1200/1251 ( 96%)]  Loss: 4.266 (4.31)  Time: 0.188s, 5453.16/s  (0.193s, 5318.90/s)  LR: 7.319e-04  Data: 0.025 (0.033)
Train: 104 [1250/1251 (100%)]  Loss: 3.897 (4.29)  Time: 0.113s, 9022.66/s  (0.192s, 5329.97/s)  LR: 7.319e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.900 (1.900)  Loss:  1.1844 (1.1844)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1931 (1.8640)  Acc@1: 79.9528 (63.2840)  Acc@5: 93.9859 (85.3460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)

Train: 105 [   0/1251 (  0%)]  Loss: 3.885 (3.89)  Time: 1.826s,  560.87/s  (1.826s,  560.87/s)  LR: 7.273e-04  Data: 1.581 (1.581)
Train: 105 [  50/1251 (  4%)]  Loss: 4.149 (4.02)  Time: 0.168s, 6091.40/s  (0.225s, 4547.96/s)  LR: 7.273e-04  Data: 0.027 (0.079)
Train: 105 [ 100/1251 (  8%)]  Loss: 4.317 (4.12)  Time: 0.165s, 6216.34/s  (0.205s, 4987.14/s)  LR: 7.273e-04  Data: 0.028 (0.058)
Train: 105 [ 150/1251 ( 12%)]  Loss: 4.620 (4.24)  Time: 0.167s, 6142.43/s  (0.198s, 5166.44/s)  LR: 7.273e-04  Data: 0.034 (0.051)
Train: 105 [ 200/1251 ( 16%)]  Loss: 4.538 (4.30)  Time: 0.238s, 4294.03/s  (0.196s, 5221.52/s)  LR: 7.273e-04  Data: 0.028 (0.046)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 105 [ 250/1251 ( 20%)]  Loss: 4.451 (4.33)  Time: 0.158s, 6465.20/s  (0.196s, 5234.04/s)  LR: 7.273e-04  Data: 0.023 (0.043)
Train: 105 [ 300/1251 ( 24%)]  Loss: 4.320 (4.33)  Time: 0.171s, 5978.50/s  (0.195s, 5261.91/s)  LR: 7.273e-04  Data: 0.038 (0.040)
Train: 105 [ 350/1251 ( 28%)]  Loss: 4.312 (4.32)  Time: 0.163s, 6299.02/s  (0.194s, 5276.04/s)  LR: 7.273e-04  Data: 0.023 (0.039)
Train: 105 [ 400/1251 ( 32%)]  Loss: 4.242 (4.31)  Time: 0.181s, 5662.72/s  (0.193s, 5294.59/s)  LR: 7.273e-04  Data: 0.023 (0.039)
Train: 105 [ 450/1251 ( 36%)]  Loss: 4.126 (4.30)  Time: 0.153s, 6695.27/s  (0.194s, 5291.34/s)  LR: 7.273e-04  Data: 0.027 (0.038)
Train: 105 [ 500/1251 ( 40%)]  Loss: 4.204 (4.29)  Time: 0.170s, 6032.05/s  (0.192s, 5321.81/s)  LR: 7.273e-04  Data: 0.032 (0.037)
Train: 105 [ 550/1251 ( 44%)]  Loss: 3.873 (4.25)  Time: 0.179s, 5719.89/s  (0.193s, 5318.31/s)  LR: 7.273e-04  Data: 0.027 (0.036)
Train: 105 [ 600/1251 ( 48%)]  Loss: 4.594 (4.28)  Time: 0.166s, 6150.41/s  (0.192s, 5334.21/s)  LR: 7.273e-04  Data: 0.031 (0.035)
Train: 105 [ 650/1251 ( 52%)]  Loss: 4.242 (4.28)  Time: 0.161s, 6341.96/s  (0.192s, 5336.16/s)  LR: 7.273e-04  Data: 0.029 (0.036)
Train: 105 [ 700/1251 ( 56%)]  Loss: 4.399 (4.28)  Time: 0.180s, 5695.86/s  (0.192s, 5335.35/s)  LR: 7.273e-04  Data: 0.051 (0.036)
Train: 105 [ 750/1251 ( 60%)]  Loss: 3.938 (4.26)  Time: 0.162s, 6314.83/s  (0.192s, 5334.29/s)  LR: 7.273e-04  Data: 0.025 (0.035)
Train: 105 [ 800/1251 ( 64%)]  Loss: 4.334 (4.27)  Time: 0.168s, 6096.45/s  (0.192s, 5325.01/s)  LR: 7.273e-04  Data: 0.019 (0.035)
Train: 105 [ 850/1251 ( 68%)]  Loss: 4.504 (4.28)  Time: 0.170s, 6039.40/s  (0.192s, 5330.13/s)  LR: 7.273e-04  Data: 0.023 (0.034)
Train: 105 [ 900/1251 ( 72%)]  Loss: 3.870 (4.26)  Time: 0.160s, 6398.19/s  (0.192s, 5328.56/s)  LR: 7.273e-04  Data: 0.031 (0.034)
Train: 105 [ 950/1251 ( 76%)]  Loss: 4.530 (4.27)  Time: 0.170s, 6014.27/s  (0.192s, 5329.90/s)  LR: 7.273e-04  Data: 0.020 (0.034)
Train: 105 [1000/1251 ( 80%)]  Loss: 4.522 (4.28)  Time: 0.191s, 5370.81/s  (0.192s, 5331.82/s)  LR: 7.273e-04  Data: 0.029 (0.033)
Train: 105 [1050/1251 ( 84%)]  Loss: 4.287 (4.28)  Time: 0.150s, 6824.42/s  (0.192s, 5331.01/s)  LR: 7.273e-04  Data: 0.027 (0.033)
Train: 105 [1100/1251 ( 88%)]  Loss: 4.038 (4.27)  Time: 0.193s, 5303.32/s  (0.192s, 5328.23/s)  LR: 7.273e-04  Data: 0.031 (0.033)
Train: 105 [1150/1251 ( 92%)]  Loss: 4.359 (4.28)  Time: 0.159s, 6457.34/s  (0.192s, 5320.53/s)  LR: 7.273e-04  Data: 0.031 (0.033)
Train: 105 [1200/1251 ( 96%)]  Loss: 4.168 (4.27)  Time: 0.180s, 5691.42/s  (0.193s, 5316.32/s)  LR: 7.273e-04  Data: 0.029 (0.032)
Train: 105 [1250/1251 (100%)]  Loss: 4.243 (4.27)  Time: 0.114s, 9015.41/s  (0.192s, 5332.46/s)  LR: 7.273e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.829 (1.829)  Loss:  1.2723 (1.2723)  Acc@1: 77.5391 (77.5391)  Acc@5: 93.8477 (93.8477)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2611 (1.9124)  Acc@1: 77.1226 (61.3560)  Acc@5: 93.0424 (84.2000)
Train: 106 [   0/1251 (  0%)]  Loss: 4.688 (4.69)  Time: 2.105s,  486.39/s  (2.105s,  486.39/s)  LR: 7.226e-04  Data: 1.974 (1.974)
Train: 106 [  50/1251 (  4%)]  Loss: 4.317 (4.50)  Time: 0.165s, 6221.52/s  (0.233s, 4386.33/s)  LR: 7.226e-04  Data: 0.031 (0.090)
Train: 106 [ 100/1251 (  8%)]  Loss: 4.344 (4.45)  Time: 0.196s, 5235.36/s  (0.209s, 4908.83/s)  LR: 7.226e-04  Data: 0.024 (0.064)
Train: 106 [ 150/1251 ( 12%)]  Loss: 4.352 (4.43)  Time: 0.170s, 6026.03/s  (0.201s, 5087.70/s)  LR: 7.226e-04  Data: 0.027 (0.054)
Train: 106 [ 200/1251 ( 16%)]  Loss: 4.464 (4.43)  Time: 0.168s, 6101.09/s  (0.200s, 5123.19/s)  LR: 7.226e-04  Data: 0.028 (0.048)
Train: 106 [ 250/1251 ( 20%)]  Loss: 4.196 (4.39)  Time: 0.173s, 5935.39/s  (0.198s, 5175.71/s)  LR: 7.226e-04  Data: 0.031 (0.044)
Train: 106 [ 300/1251 ( 24%)]  Loss: 4.203 (4.37)  Time: 0.153s, 6689.99/s  (0.195s, 5238.85/s)  LR: 7.226e-04  Data: 0.024 (0.041)
Train: 106 [ 350/1251 ( 28%)]  Loss: 4.176 (4.34)  Time: 0.171s, 5996.71/s  (0.194s, 5284.24/s)  LR: 7.226e-04  Data: 0.029 (0.039)
Train: 106 [ 400/1251 ( 32%)]  Loss: 4.178 (4.32)  Time: 0.185s, 5544.82/s  (0.194s, 5291.61/s)  LR: 7.226e-04  Data: 0.028 (0.038)
Train: 106 [ 450/1251 ( 36%)]  Loss: 4.234 (4.32)  Time: 0.169s, 6046.64/s  (0.193s, 5304.55/s)  LR: 7.226e-04  Data: 0.025 (0.037)
Train: 106 [ 500/1251 ( 40%)]  Loss: 4.396 (4.32)  Time: 0.186s, 5510.71/s  (0.193s, 5304.38/s)  LR: 7.226e-04  Data: 0.028 (0.036)
Train: 106 [ 550/1251 ( 44%)]  Loss: 4.598 (4.35)  Time: 0.165s, 6214.04/s  (0.193s, 5312.88/s)  LR: 7.226e-04  Data: 0.025 (0.035)
Train: 106 [ 600/1251 ( 48%)]  Loss: 4.622 (4.37)  Time: 0.166s, 6172.88/s  (0.193s, 5311.10/s)  LR: 7.226e-04  Data: 0.031 (0.035)
Train: 106 [ 650/1251 ( 52%)]  Loss: 4.379 (4.37)  Time: 0.162s, 6311.28/s  (0.193s, 5313.65/s)  LR: 7.226e-04  Data: 0.024 (0.034)
Train: 106 [ 700/1251 ( 56%)]  Loss: 3.945 (4.34)  Time: 0.167s, 6147.77/s  (0.193s, 5315.65/s)  LR: 7.226e-04  Data: 0.030 (0.034)
Train: 106 [ 750/1251 ( 60%)]  Loss: 4.132 (4.33)  Time: 0.174s, 5890.46/s  (0.192s, 5320.15/s)  LR: 7.226e-04  Data: 0.032 (0.034)
Train: 106 [ 800/1251 ( 64%)]  Loss: 4.253 (4.32)  Time: 0.155s, 6592.47/s  (0.193s, 5317.22/s)  LR: 7.226e-04  Data: 0.029 (0.033)
Train: 106 [ 850/1251 ( 68%)]  Loss: 4.646 (4.34)  Time: 0.178s, 5745.17/s  (0.192s, 5320.19/s)  LR: 7.226e-04  Data: 0.030 (0.033)
Train: 106 [ 900/1251 ( 72%)]  Loss: 4.468 (4.35)  Time: 0.177s, 5775.46/s  (0.192s, 5326.58/s)  LR: 7.226e-04  Data: 0.029 (0.033)
Train: 106 [ 950/1251 ( 76%)]  Loss: 4.425 (4.35)  Time: 0.178s, 5765.13/s  (0.193s, 5318.66/s)  LR: 7.226e-04  Data: 0.023 (0.032)
Train: 106 [1000/1251 ( 80%)]  Loss: 4.598 (4.36)  Time: 0.188s, 5445.14/s  (0.193s, 5314.22/s)  LR: 7.226e-04  Data: 0.024 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Train: 106 [1050/1251 ( 84%)]  Loss: 3.816 (4.34)  Time: 0.391s, 2621.58/s  (0.193s, 5306.03/s)  LR: 7.226e-04  Data: 0.029 (0.032)

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 106 [1100/1251 ( 88%)]  Loss: 4.073 (4.33)  Time: 0.163s, 6291.74/s  (0.193s, 5307.65/s)  LR: 7.226e-04  Data: 0.028 (0.032)
Train: 106 [1150/1251 ( 92%)]  Loss: 4.303 (4.33)  Time: 0.184s, 5570.53/s  (0.193s, 5309.12/s)  LR: 7.226e-04  Data: 0.029 (0.032)
Train: 106 [1200/1251 ( 96%)]  Loss: 4.117 (4.32)  Time: 0.171s, 5979.13/s  (0.193s, 5304.52/s)  LR: 7.226e-04  Data: 0.027 (0.031)
Train: 106 [1250/1251 (100%)]  Loss: 4.259 (4.31)  Time: 0.114s, 8956.30/s  (0.192s, 5319.74/s)  LR: 7.226e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.1844 (1.1844)  Acc@1: 79.6875 (79.6875)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2452 (1.8583)  Acc@1: 78.8915 (63.2880)  Acc@5: 93.2783 (85.3240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-93.pth.tar', 62.619999958496095)

Train: 107 [   0/1251 (  0%)]  Loss: 4.534 (4.53)  Time: 1.872s,  547.01/s  (1.872s,  547.01/s)  LR: 7.179e-04  Data: 1.584 (1.584)
Train: 107 [  50/1251 (  4%)]  Loss: 4.244 (4.39)  Time: 0.156s, 6553.98/s  (0.228s, 4493.37/s)  LR: 7.179e-04  Data: 0.029 (0.076)
Train: 107 [ 100/1251 (  8%)]  Loss: 4.182 (4.32)  Time: 0.173s, 5912.32/s  (0.209s, 4902.82/s)  LR: 7.179e-04  Data: 0.020 (0.061)
Train: 107 [ 150/1251 ( 12%)]  Loss: 4.042 (4.25)  Time: 0.186s, 5509.45/s  (0.200s, 5121.22/s)  LR: 7.179e-04  Data: 0.026 (0.051)
Train: 107 [ 200/1251 ( 16%)]  Loss: 4.415 (4.28)  Time: 0.180s, 5694.01/s  (0.198s, 5163.25/s)  LR: 7.179e-04  Data: 0.026 (0.050)
Train: 107 [ 250/1251 ( 20%)]  Loss: 4.635 (4.34)  Time: 0.168s, 6093.80/s  (0.196s, 5222.90/s)  LR: 7.179e-04  Data: 0.028 (0.049)
Train: 107 [ 300/1251 ( 24%)]  Loss: 4.559 (4.37)  Time: 0.188s, 5456.91/s  (0.195s, 5245.26/s)  LR: 7.179e-04  Data: 0.027 (0.047)
Train: 107 [ 350/1251 ( 28%)]  Loss: 4.247 (4.36)  Time: 0.171s, 5977.32/s  (0.195s, 5254.66/s)  LR: 7.179e-04  Data: 0.029 (0.047)
Train: 107 [ 400/1251 ( 32%)]  Loss: 4.583 (4.38)  Time: 0.183s, 5602.93/s  (0.194s, 5277.46/s)  LR: 7.179e-04  Data: 0.032 (0.046)
Train: 107 [ 450/1251 ( 36%)]  Loss: 4.022 (4.35)  Time: 0.163s, 6288.41/s  (0.194s, 5275.11/s)  LR: 7.179e-04  Data: 0.034 (0.046)
Train: 107 [ 500/1251 ( 40%)]  Loss: 4.449 (4.36)  Time: 0.169s, 6058.14/s  (0.193s, 5301.47/s)  LR: 7.179e-04  Data: 0.029 (0.046)
Train: 107 [ 550/1251 ( 44%)]  Loss: 4.443 (4.36)  Time: 0.179s, 5716.68/s  (0.193s, 5306.09/s)  LR: 7.179e-04  Data: 0.033 (0.045)
Train: 107 [ 600/1251 ( 48%)]  Loss: 4.450 (4.37)  Time: 0.182s, 5628.15/s  (0.193s, 5315.72/s)  LR: 7.179e-04  Data: 0.024 (0.044)
Train: 107 [ 650/1251 ( 52%)]  Loss: 4.170 (4.36)  Time: 0.152s, 6747.93/s  (0.193s, 5299.70/s)  LR: 7.179e-04  Data: 0.026 (0.043)
Train: 107 [ 700/1251 ( 56%)]  Loss: 4.191 (4.34)  Time: 0.175s, 5859.20/s  (0.193s, 5311.06/s)  LR: 7.179e-04  Data: 0.034 (0.042)
Train: 107 [ 750/1251 ( 60%)]  Loss: 4.376 (4.35)  Time: 0.176s, 5812.29/s  (0.193s, 5300.83/s)  LR: 7.179e-04  Data: 0.023 (0.041)
Train: 107 [ 800/1251 ( 64%)]  Loss: 4.359 (4.35)  Time: 0.158s, 6474.66/s  (0.193s, 5304.42/s)  LR: 7.179e-04  Data: 0.025 (0.040)
Train: 107 [ 850/1251 ( 68%)]  Loss: 4.073 (4.33)  Time: 0.155s, 6595.67/s  (0.193s, 5299.75/s)  LR: 7.179e-04  Data: 0.024 (0.039)
Train: 107 [ 900/1251 ( 72%)]  Loss: 4.571 (4.34)  Time: 0.175s, 5857.49/s  (0.193s, 5303.55/s)  LR: 7.179e-04  Data: 0.032 (0.039)
Train: 107 [ 950/1251 ( 76%)]  Loss: 4.524 (4.35)  Time: 0.161s, 6366.24/s  (0.193s, 5307.69/s)  LR: 7.179e-04  Data: 0.021 (0.039)
Train: 107 [1000/1251 ( 80%)]  Loss: 4.095 (4.34)  Time: 0.187s, 5468.45/s  (0.193s, 5302.97/s)  LR: 7.179e-04  Data: 0.025 (0.039)
Train: 107 [1050/1251 ( 84%)]  Loss: 4.393 (4.34)  Time: 0.180s, 5676.13/s  (0.193s, 5307.89/s)  LR: 7.179e-04  Data: 0.032 (0.039)
Train: 107 [1100/1251 ( 88%)]  Loss: 4.271 (4.34)  Time: 0.181s, 5667.47/s  (0.193s, 5305.98/s)  LR: 7.179e-04  Data: 0.020 (0.039)
Train: 107 [1150/1251 ( 92%)]  Loss: 4.284 (4.34)  Time: 0.182s, 5618.25/s  (0.194s, 5290.92/s)  LR: 7.179e-04  Data: 0.024 (0.039)
Train: 107 [1200/1251 ( 96%)]  Loss: 4.412 (4.34)  Time: 0.195s, 5262.20/s  (0.194s, 5291.44/s)  LR: 7.179e-04  Data: 0.024 (0.040)
Train: 107 [1250/1251 (100%)]  Loss: 4.565 (4.35)  Time: 0.113s, 9022.98/s  (0.193s, 5303.06/s)  LR: 7.179e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.840 (1.840)  Loss:  1.2321 (1.2321)  Acc@1: 79.5898 (79.5898)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.3530 (1.8850)  Acc@1: 77.4764 (63.3140)  Acc@5: 92.4528 (85.3080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)

Train: 108 [   0/1251 (  0%)]  Loss: 4.025 (4.03)  Time: 1.818s,  563.21/s  (1.818s,  563.21/s)  LR: 7.132e-04  Data: 1.687 (1.687)
Train: 108 [  50/1251 (  4%)]  Loss: 4.236 (4.13)  Time: 0.174s, 5887.02/s  (0.224s, 4576.34/s)  LR: 7.132e-04  Data: 0.027 (0.078)
Train: 108 [ 100/1251 (  8%)]  Loss: 4.735 (4.33)  Time: 0.177s, 5800.26/s  (0.209s, 4890.31/s)  LR: 7.132e-04  Data: 0.026 (0.053)
Train: 108 [ 150/1251 ( 12%)]  Loss: 4.030 (4.26)  Time: 0.158s, 6482.23/s  (0.203s, 5043.10/s)  LR: 7.132e-04  Data: 0.025 (0.044)
Train: 108 [ 200/1251 ( 16%)]  Loss: 4.160 (4.24)  Time: 0.190s, 5383.85/s  (0.200s, 5124.37/s)  LR: 7.132e-04  Data: 0.026 (0.040)
Train: 108 [ 250/1251 ( 20%)]  Loss: 4.432 (4.27)  Time: 0.177s, 5769.73/s  (0.197s, 5205.26/s)  LR: 7.132e-04  Data: 0.025 (0.038)
Train: 108 [ 300/1251 ( 24%)]  Loss: 4.218 (4.26)  Time: 0.195s, 5259.36/s  (0.196s, 5222.46/s)  LR: 7.132e-04  Data: 0.020 (0.036)
Train: 108 [ 350/1251 ( 28%)]  Loss: 4.196 (4.25)  Time: 0.174s, 5898.33/s  (0.195s, 5254.60/s)  LR: 7.132e-04  Data: 0.025 (0.035)
Train: 108 [ 400/1251 ( 32%)]  Loss: 4.358 (4.27)  Time: 0.178s, 5738.18/s  (0.194s, 5269.25/s)  LR: 7.132e-04  Data: 0.027 (0.034)
Train: 108 [ 450/1251 ( 36%)]  Loss: 4.225 (4.26)  Time: 0.194s, 5266.70/s  (0.194s, 5286.90/s)  LR: 7.132e-04  Data: 0.038 (0.034)
Train: 108 [ 500/1251 ( 40%)]  Loss: 4.472 (4.28)  Time: 0.173s, 5921.61/s  (0.193s, 5304.70/s)  LR: 7.132e-04  Data: 0.028 (0.033)
Train: 108 [ 550/1251 ( 44%)]  Loss: 4.264 (4.28)  Time: 0.160s, 6402.08/s  (0.192s, 5321.06/s)  LR: 7.132e-04  Data: 0.034 (0.033)
Train: 108 [ 600/1251 ( 48%)]  Loss: 4.386 (4.29)  Time: 0.161s, 6342.01/s  (0.192s, 5329.91/s)  LR: 7.132e-04  Data: 0.024 (0.032)
Train: 108 [ 650/1251 ( 52%)]  Loss: 4.040 (4.27)  Time: 0.180s, 5702.94/s  (0.192s, 5321.70/s)  LR: 7.132e-04  Data: 0.022 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 108 [ 700/1251 ( 56%)]  Loss: 4.440 (4.28)  Time: 0.181s, 5658.40/s  (0.192s, 5325.93/s)  LR: 7.132e-04  Data: 0.026 (0.032)
Train: 108 [ 750/1251 ( 60%)]  Loss: 4.489 (4.29)  Time: 0.179s, 5723.66/s  (0.192s, 5322.87/s)  LR: 7.132e-04  Data: 0.025 (0.031)
Train: 108 [ 800/1251 ( 64%)]  Loss: 4.117 (4.28)  Time: 0.249s, 4112.78/s  (0.192s, 5329.74/s)  LR: 7.132e-04  Data: 0.026 (0.031)
Train: 108 [ 850/1251 ( 68%)]  Loss: 4.436 (4.29)  Time: 0.162s, 6333.98/s  (0.192s, 5332.41/s)  LR: 7.132e-04  Data: 0.030 (0.031)
Train: 108 [ 900/1251 ( 72%)]  Loss: 4.504 (4.30)  Time: 0.352s, 2909.77/s  (0.192s, 5328.13/s)  LR: 7.132e-04  Data: 0.028 (0.031)
Train: 108 [ 950/1251 ( 76%)]  Loss: 4.513 (4.31)  Time: 0.163s, 6292.15/s  (0.193s, 5318.80/s)  LR: 7.132e-04  Data: 0.035 (0.031)
Train: 108 [1000/1251 ( 80%)]  Loss: 4.219 (4.31)  Time: 0.351s, 2920.55/s  (0.192s, 5323.13/s)  LR: 7.132e-04  Data: 0.026 (0.031)
Train: 108 [1050/1251 ( 84%)]  Loss: 4.456 (4.32)  Time: 0.151s, 6801.44/s  (0.193s, 5311.68/s)  LR: 7.132e-04  Data: 0.024 (0.031)
Train: 108 [1100/1251 ( 88%)]  Loss: 4.419 (4.32)  Time: 0.241s, 4253.97/s  (0.193s, 5303.58/s)  LR: 7.132e-04  Data: 0.028 (0.030)
Train: 108 [1150/1251 ( 92%)]  Loss: 4.233 (4.32)  Time: 0.173s, 5922.30/s  (0.193s, 5297.79/s)  LR: 7.132e-04  Data: 0.024 (0.030)
Train: 108 [1200/1251 ( 96%)]  Loss: 4.436 (4.32)  Time: 0.161s, 6359.33/s  (0.193s, 5296.62/s)  LR: 7.132e-04  Data: 0.030 (0.030)
Train: 108 [1250/1251 (100%)]  Loss: 4.229 (4.32)  Time: 0.113s, 9089.99/s  (0.193s, 5314.46/s)  LR: 7.132e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.881 (1.881)  Loss:  1.0811 (1.0811)  Acc@1: 79.2969 (79.2969)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1932 (1.8572)  Acc@1: 79.3632 (63.1280)  Acc@5: 93.1604 (84.8600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-92.pth.tar', 62.71400008544922)

Train: 109 [   0/1251 (  0%)]  Loss: 4.313 (4.31)  Time: 1.825s,  560.95/s  (1.825s,  560.95/s)  LR: 7.084e-04  Data: 1.694 (1.694)
Train: 109 [  50/1251 (  4%)]  Loss: 4.546 (4.43)  Time: 0.170s, 6037.99/s  (0.223s, 4589.17/s)  LR: 7.084e-04  Data: 0.031 (0.082)
Train: 109 [ 100/1251 (  8%)]  Loss: 4.125 (4.33)  Time: 0.186s, 5511.53/s  (0.205s, 4983.83/s)  LR: 7.084e-04  Data: 0.027 (0.061)
Train: 109 [ 150/1251 ( 12%)]  Loss: 4.694 (4.42)  Time: 0.162s, 6324.08/s  (0.201s, 5092.77/s)  LR: 7.084e-04  Data: 0.028 (0.051)
Train: 109 [ 200/1251 ( 16%)]  Loss: 4.112 (4.36)  Time: 0.247s, 4144.09/s  (0.200s, 5129.08/s)  LR: 7.084e-04  Data: 0.125 (0.049)
Train: 109 [ 250/1251 ( 20%)]  Loss: 4.628 (4.40)  Time: 0.177s, 5800.65/s  (0.197s, 5196.02/s)  LR: 7.084e-04  Data: 0.023 (0.047)
Train: 109 [ 300/1251 ( 24%)]  Loss: 4.535 (4.42)  Time: 0.187s, 5484.84/s  (0.197s, 5199.42/s)  LR: 7.084e-04  Data: 0.028 (0.048)
Train: 109 [ 350/1251 ( 28%)]  Loss: 4.050 (4.38)  Time: 0.155s, 6614.50/s  (0.195s, 5241.09/s)  LR: 7.084e-04  Data: 0.025 (0.047)
Train: 109 [ 400/1251 ( 32%)]  Loss: 4.229 (4.36)  Time: 0.168s, 6111.25/s  (0.195s, 5241.43/s)  LR: 7.084e-04  Data: 0.033 (0.045)
Train: 109 [ 450/1251 ( 36%)]  Loss: 4.038 (4.33)  Time: 0.175s, 5853.74/s  (0.194s, 5266.68/s)  LR: 7.084e-04  Data: 0.021 (0.043)
Train: 109 [ 500/1251 ( 40%)]  Loss: 4.174 (4.31)  Time: 0.159s, 6449.43/s  (0.193s, 5295.42/s)  LR: 7.084e-04  Data: 0.023 (0.041)
Train: 109 [ 550/1251 ( 44%)]  Loss: 4.243 (4.31)  Time: 0.190s, 5391.93/s  (0.193s, 5302.34/s)  LR: 7.084e-04  Data: 0.041 (0.041)
Train: 109 [ 600/1251 ( 48%)]  Loss: 4.362 (4.31)  Time: 0.167s, 6122.03/s  (0.193s, 5300.95/s)  LR: 7.084e-04  Data: 0.026 (0.040)
Train: 109 [ 650/1251 ( 52%)]  Loss: 3.866 (4.28)  Time: 0.184s, 5551.41/s  (0.193s, 5311.13/s)  LR: 7.084e-04  Data: 0.025 (0.039)
Train: 109 [ 700/1251 ( 56%)]  Loss: 4.262 (4.28)  Time: 0.166s, 6186.34/s  (0.193s, 5303.18/s)  LR: 7.084e-04  Data: 0.032 (0.038)
Train: 109 [ 750/1251 ( 60%)]  Loss: 4.064 (4.27)  Time: 0.174s, 5868.32/s  (0.193s, 5309.43/s)  LR: 7.084e-04  Data: 0.042 (0.038)
Train: 109 [ 800/1251 ( 64%)]  Loss: 4.332 (4.27)  Time: 0.156s, 6568.54/s  (0.193s, 5309.29/s)  LR: 7.084e-04  Data: 0.028 (0.037)
Train: 109 [ 850/1251 ( 68%)]  Loss: 4.355 (4.27)  Time: 0.172s, 5953.01/s  (0.193s, 5304.02/s)  LR: 7.084e-04  Data: 0.027 (0.036)
Train: 109 [ 900/1251 ( 72%)]  Loss: 4.088 (4.26)  Time: 0.186s, 5515.70/s  (0.193s, 5294.14/s)  LR: 7.084e-04  Data: 0.061 (0.036)
Train: 109 [ 950/1251 ( 76%)]  Loss: 4.542 (4.28)  Time: 0.214s, 4780.50/s  (0.193s, 5296.67/s)  LR: 7.084e-04  Data: 0.028 (0.036)
Train: 109 [1000/1251 ( 80%)]  Loss: 4.503 (4.29)  Time: 0.148s, 6932.03/s  (0.193s, 5303.46/s)  LR: 7.084e-04  Data: 0.018 (0.035)
Train: 109 [1050/1251 ( 84%)]  Loss: 4.298 (4.29)  Time: 0.167s, 6119.53/s  (0.196s, 5234.50/s)  LR: 7.084e-04  Data: 0.029 (0.035)
Train: 109 [1100/1251 ( 88%)]  Loss: 4.262 (4.29)  Time: 0.157s, 6508.97/s  (0.195s, 5238.76/s)  LR: 7.084e-04  Data: 0.027 (0.035)
Train: 109 [1150/1251 ( 92%)]  Loss: 4.745 (4.31)  Time: 0.173s, 5932.28/s  (0.196s, 5237.59/s)  LR: 7.084e-04  Data: 0.021 (0.034)
Train: 109 [1200/1251 ( 96%)]  Loss: 4.251 (4.30)  Time: 0.164s, 6248.64/s  (0.196s, 5235.56/s)  LR: 7.084e-04  Data: 0.026 (0.034)
Train: 109 [1250/1251 (100%)]  Loss: 4.845 (4.33)  Time: 0.113s, 9036.37/s  (0.195s, 5248.82/s)  LR: 7.084e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.794 (1.794)  Loss:  1.1591 (1.1591)  Acc@1: 78.7109 (78.7109)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1878 (1.8706)  Acc@1: 80.1887 (62.8340)  Acc@5: 93.9858 (85.1080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-109.pth.tar', 62.83400005615234)

Train: 110 [   0/1251 (  0%)]  Loss: 4.143 (4.14)  Time: 1.630s,  628.34/s  (1.630s,  628.34/s)  LR: 7.037e-04  Data: 1.486 (1.486)
Train: 110 [  50/1251 (  4%)]  Loss: 4.240 (4.19)  Time: 0.172s, 5963.15/s  (0.226s, 4524.82/s)  LR: 7.037e-04  Data: 0.025 (0.067)
Train: 110 [ 100/1251 (  8%)]  Loss: 4.236 (4.21)  Time: 0.171s, 5992.73/s  (0.208s, 4921.53/s)  LR: 7.037e-04  Data: 0.022 (0.048)
Train: 110 [ 150/1251 ( 12%)]  Loss: 4.334 (4.24)  Time: 0.161s, 6352.49/s  (0.199s, 5144.40/s)  LR: 7.037e-04  Data: 0.026 (0.042)
Train: 110 [ 200/1251 ( 16%)]  Loss: 4.487 (4.29)  Time: 0.158s, 6474.27/s  (0.196s, 5231.16/s)  LR: 7.037e-04  Data: 0.030 (0.042)
Train: 110 [ 250/1251 ( 20%)]  Loss: 4.351 (4.30)  Time: 0.169s, 6058.14/s  (0.193s, 5306.46/s)  LR: 7.037e-04  Data: 0.030 (0.039)
Train: 110 [ 300/1251 ( 24%)]  Loss: 4.486 (4.33)  Time: 0.194s, 5288.03/s  (0.193s, 5301.84/s)  LR: 7.037e-04  Data: 0.020 (0.039)
Train: 110 [ 350/1251 ( 28%)]  Loss: 4.714 (4.37)  Time: 0.178s, 5757.42/s  (0.192s, 5342.46/s)  LR: 7.037e-04  Data: 0.024 (0.038)
Train: 110 [ 400/1251 ( 32%)]  Loss: 4.598 (4.40)  Time: 0.294s, 3478.05/s  (0.191s, 5347.63/s)  LR: 7.037e-04  Data: 0.025 (0.037)
Train: 110 [ 450/1251 ( 36%)]  Loss: 4.506 (4.41)  Time: 0.183s, 5588.28/s  (0.191s, 5351.68/s)  LR: 7.037e-04  Data: 0.043 (0.036)
Train: 110 [ 500/1251 ( 40%)]  Loss: 4.471 (4.42)  Time: 0.179s, 5708.91/s  (0.191s, 5353.06/s)  LR: 7.037e-04  Data: 0.039 (0.035)
Train: 110 [ 550/1251 ( 44%)]  Loss: 4.505 (4.42)  Time: 0.269s, 3801.47/s  (0.192s, 5346.73/s)  LR: 7.037e-04  Data: 0.028 (0.034)
Train: 110 [ 600/1251 ( 48%)]  Loss: 4.385 (4.42)  Time: 0.182s, 5636.16/s  (0.191s, 5355.07/s)  LR: 7.037e-04  Data: 0.038 (0.034)
Train: 110 [ 650/1251 ( 52%)]  Loss: 4.224 (4.41)  Time: 0.171s, 5987.04/s  (0.191s, 5354.68/s)  LR: 7.037e-04  Data: 0.028 (0.033)
Train: 110 [ 700/1251 ( 56%)]  Loss: 4.262 (4.40)  Time: 0.187s, 5468.57/s  (0.192s, 5346.35/s)  LR: 7.037e-04  Data: 0.031 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 110 [ 750/1251 ( 60%)]  Loss: 4.081 (4.38)  Time: 0.189s, 5421.95/s  (0.191s, 5347.34/s)  LR: 7.037e-04  Data: 0.031 (0.033)
Train: 110 [ 800/1251 ( 64%)]  Loss: 4.537 (4.39)  Time: 0.333s, 3078.44/s  (0.191s, 5355.29/s)  LR: 7.037e-04  Data: 0.020 (0.033)
Train: 110 [ 850/1251 ( 68%)]  Loss: 4.690 (4.40)  Time: 0.277s, 3696.22/s  (0.191s, 5356.24/s)  LR: 7.037e-04  Data: 0.019 (0.032)
Train: 110 [ 900/1251 ( 72%)]  Loss: 4.444 (4.40)  Time: 0.180s, 5703.42/s  (0.191s, 5360.19/s)  LR: 7.037e-04  Data: 0.031 (0.032)
Train: 110 [ 950/1251 ( 76%)]  Loss: 3.986 (4.38)  Time: 0.193s, 5297.99/s  (0.191s, 5354.03/s)  LR: 7.037e-04  Data: 0.036 (0.032)
Train: 110 [1000/1251 ( 80%)]  Loss: 4.270 (4.38)  Time: 0.161s, 6361.13/s  (0.191s, 5352.64/s)  LR: 7.037e-04  Data: 0.026 (0.032)
Train: 110 [1050/1251 ( 84%)]  Loss: 4.389 (4.38)  Time: 0.279s, 3675.59/s  (0.192s, 5344.27/s)  LR: 7.037e-04  Data: 0.024 (0.031)
Train: 110 [1100/1251 ( 88%)]  Loss: 4.229 (4.37)  Time: 0.179s, 5730.47/s  (0.192s, 5338.10/s)  LR: 7.037e-04  Data: 0.025 (0.031)
Train: 110 [1150/1251 ( 92%)]  Loss: 4.582 (4.38)  Time: 0.189s, 5410.68/s  (0.192s, 5344.83/s)  LR: 7.037e-04  Data: 0.038 (0.031)
Train: 110 [1200/1251 ( 96%)]  Loss: 3.921 (4.36)  Time: 0.152s, 6744.77/s  (0.192s, 5344.75/s)  LR: 7.037e-04  Data: 0.023 (0.031)
Train: 110 [1250/1251 (100%)]  Loss: 4.653 (4.37)  Time: 0.114s, 9007.09/s  (0.191s, 5353.66/s)  LR: 7.037e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.857 (1.857)  Loss:  1.2132 (1.2132)  Acc@1: 79.4922 (79.4922)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2740 (1.8962)  Acc@1: 78.5377 (62.7020)  Acc@5: 92.2170 (85.0060)
Train: 111 [   0/1251 (  0%)]  Loss: 4.491 (4.49)  Time: 1.742s,  587.96/s  (1.742s,  587.96/s)  LR: 6.989e-04  Data: 1.608 (1.608)
Train: 111 [  50/1251 (  4%)]  Loss: 4.649 (4.57)  Time: 0.158s, 6497.06/s  (0.226s, 4533.59/s)  LR: 6.989e-04  Data: 0.034 (0.064)
Train: 111 [ 100/1251 (  8%)]  Loss: 4.371 (4.50)  Time: 0.158s, 6498.37/s  (0.204s, 5010.26/s)  LR: 6.989e-04  Data: 0.030 (0.047)
Train: 111 [ 150/1251 ( 12%)]  Loss: 4.327 (4.46)  Time: 0.158s, 6493.62/s  (0.199s, 5141.06/s)  LR: 6.989e-04  Data: 0.029 (0.040)
Train: 111 [ 200/1251 ( 16%)]  Loss: 4.055 (4.38)  Time: 0.177s, 5791.58/s  (0.198s, 5177.58/s)  LR: 6.989e-04  Data: 0.037 (0.037)
Train: 111 [ 250/1251 ( 20%)]  Loss: 4.043 (4.32)  Time: 0.186s, 5491.10/s  (0.195s, 5252.77/s)  LR: 6.989e-04  Data: 0.030 (0.035)
Train: 111 [ 300/1251 ( 24%)]  Loss: 4.327 (4.32)  Time: 0.179s, 5708.69/s  (0.194s, 5280.45/s)  LR: 6.989e-04  Data: 0.029 (0.034)
Train: 111 [ 350/1251 ( 28%)]  Loss: 4.352 (4.33)  Time: 0.177s, 5775.15/s  (0.193s, 5296.57/s)  LR: 6.989e-04  Data: 0.027 (0.034)
Train: 111 [ 400/1251 ( 32%)]  Loss: 4.202 (4.31)  Time: 0.188s, 5439.85/s  (0.193s, 5301.14/s)  LR: 6.989e-04  Data: 0.029 (0.033)
Train: 111 [ 450/1251 ( 36%)]  Loss: 3.820 (4.26)  Time: 0.181s, 5664.31/s  (0.193s, 5312.12/s)  LR: 6.989e-04  Data: 0.028 (0.033)
Train: 111 [ 500/1251 ( 40%)]  Loss: 4.245 (4.26)  Time: 0.163s, 6269.06/s  (0.192s, 5319.48/s)  LR: 6.989e-04  Data: 0.029 (0.032)
Train: 111 [ 550/1251 ( 44%)]  Loss: 4.625 (4.29)  Time: 0.184s, 5552.52/s  (0.192s, 5325.53/s)  LR: 6.989e-04  Data: 0.018 (0.032)
Train: 111 [ 600/1251 ( 48%)]  Loss: 4.361 (4.30)  Time: 0.165s, 6209.86/s  (0.193s, 5318.41/s)  LR: 6.989e-04  Data: 0.034 (0.031)
Train: 111 [ 650/1251 ( 52%)]  Loss: 4.769 (4.33)  Time: 0.168s, 6080.49/s  (0.192s, 5327.62/s)  LR: 6.989e-04  Data: 0.029 (0.031)
Train: 111 [ 700/1251 ( 56%)]  Loss: 4.451 (4.34)  Time: 0.170s, 6012.84/s  (0.191s, 5352.65/s)  LR: 6.989e-04  Data: 0.022 (0.031)
Train: 111 [ 750/1251 ( 60%)]  Loss: 3.976 (4.32)  Time: 0.166s, 6183.91/s  (0.191s, 5353.35/s)  LR: 6.989e-04  Data: 0.030 (0.031)
Train: 111 [ 800/1251 ( 64%)]  Loss: 4.436 (4.32)  Time: 0.155s, 6593.02/s  (0.192s, 5336.60/s)  LR: 6.989e-04  Data: 0.029 (0.031)
Train: 111 [ 850/1251 ( 68%)]  Loss: 4.224 (4.32)  Time: 0.171s, 5975.85/s  (0.192s, 5338.22/s)  LR: 6.989e-04  Data: 0.038 (0.031)
Train: 111 [ 900/1251 ( 72%)]  Loss: 4.014 (4.30)  Time: 0.172s, 5966.30/s  (0.192s, 5335.07/s)  LR: 6.989e-04  Data: 0.026 (0.031)
Train: 111 [ 950/1251 ( 76%)]  Loss: 4.626 (4.32)  Time: 0.362s, 2828.09/s  (0.192s, 5332.19/s)  LR: 6.989e-04  Data: 0.033 (0.030)
Train: 111 [1000/1251 ( 80%)]  Loss: 4.272 (4.32)  Time: 0.168s, 6113.36/s  (0.192s, 5327.99/s)  LR: 6.989e-04  Data: 0.026 (0.030)
Train: 111 [1050/1251 ( 84%)]  Loss: 4.042 (4.30)  Time: 0.187s, 5480.83/s  (0.192s, 5321.04/s)  LR: 6.989e-04  Data: 0.055 (0.030)
Train: 111 [1100/1251 ( 88%)]  Loss: 4.172 (4.30)  Time: 0.161s, 6345.80/s  (0.193s, 5314.48/s)  LR: 6.989e-04  Data: 0.031 (0.030)
Train: 111 [1150/1251 ( 92%)]  Loss: 4.264 (4.30)  Time: 0.231s, 4442.36/s  (0.193s, 5315.55/s)  LR: 6.989e-04  Data: 0.029 (0.030)
Train: 111 [1200/1251 ( 96%)]  Loss: 4.253 (4.29)  Time: 0.174s, 5890.02/s  (0.193s, 5310.66/s)  LR: 6.989e-04  Data: 0.022 (0.030)
Train: 111 [1250/1251 (100%)]  Loss: 4.185 (4.29)  Time: 0.113s, 9071.72/s  (0.192s, 5324.51/s)  LR: 6.989e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.773 (1.773)  Loss:  1.1977 (1.1977)  Acc@1: 77.8320 (77.8320)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.2187 (1.8451)  Acc@1: 81.9576 (63.4460)  Acc@5: 93.2783 (85.2520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-103.pth.tar', 62.974000131835936)

Train: 112 [   0/1251 (  0%)]  Loss: 4.687 (4.69)  Time: 1.635s,  626.20/s  (1.635s,  626.20/s)  LR: 6.941e-04  Data: 1.500 (1.500)
Train: 112 [  50/1251 (  4%)]  Loss: 4.193 (4.44)  Time: 0.161s, 6348.60/s  (0.224s, 4574.73/s)  LR: 6.941e-04  Data: 0.032 (0.074)
Train: 112 [ 100/1251 (  8%)]  Loss: 4.218 (4.37)  Time: 0.157s, 6523.90/s  (0.205s, 4988.57/s)  LR: 6.941e-04  Data: 0.031 (0.058)
Train: 112 [ 150/1251 ( 12%)]  Loss: 4.635 (4.43)  Time: 0.179s, 5718.68/s  (0.200s, 5128.80/s)  LR: 6.941e-04  Data: 0.028 (0.050)
Train: 112 [ 200/1251 ( 16%)]  Loss: 4.583 (4.46)  Time: 0.178s, 5737.64/s  (0.199s, 5158.37/s)  LR: 6.941e-04  Data: 0.034 (0.044)
Train: 112 [ 250/1251 ( 20%)]  Loss: 4.371 (4.45)  Time: 0.163s, 6288.57/s  (0.197s, 5197.22/s)  LR: 6.941e-04  Data: 0.030 (0.041)
Train: 112 [ 300/1251 ( 24%)]  Loss: 4.478 (4.45)  Time: 0.187s, 5464.94/s  (0.196s, 5225.32/s)  LR: 6.941e-04  Data: 0.030 (0.039)
Train: 112 [ 350/1251 ( 28%)]  Loss: 4.061 (4.40)  Time: 0.342s, 2992.22/s  (0.195s, 5238.88/s)  LR: 6.941e-04  Data: 0.032 (0.037)
Train: 112 [ 400/1251 ( 32%)]  Loss: 4.427 (4.41)  Time: 0.186s, 5519.71/s  (0.194s, 5272.47/s)  LR: 6.941e-04  Data: 0.025 (0.036)
Train: 112 [ 450/1251 ( 36%)]  Loss: 4.348 (4.40)  Time: 0.181s, 5672.88/s  (0.194s, 5272.20/s)  LR: 6.941e-04  Data: 0.027 (0.035)
Train: 112 [ 500/1251 ( 40%)]  Loss: 4.648 (4.42)  Time: 0.171s, 6000.07/s  (0.193s, 5296.79/s)  LR: 6.941e-04  Data: 0.027 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 112 [ 550/1251 ( 44%)]  Loss: 4.065 (4.39)  Time: 0.283s, 3612.87/s  (0.193s, 5293.96/s)  LR: 6.941e-04  Data: 0.043 (0.034)
Train: 112 [ 600/1251 ( 48%)]  Loss: 4.007 (4.36)  Time: 0.158s, 6465.70/s  (0.193s, 5304.11/s)  LR: 6.941e-04  Data: 0.031 (0.034)
Train: 112 [ 650/1251 ( 52%)]  Loss: 4.516 (4.37)  Time: 0.165s, 6224.79/s  (0.193s, 5302.37/s)  LR: 6.941e-04  Data: 0.024 (0.033)
Train: 112 [ 700/1251 ( 56%)]  Loss: 3.954 (4.35)  Time: 0.174s, 5892.35/s  (0.193s, 5307.01/s)  LR: 6.941e-04  Data: 0.026 (0.033)
Train: 112 [ 750/1251 ( 60%)]  Loss: 4.195 (4.34)  Time: 0.246s, 4154.57/s  (0.193s, 5306.34/s)  LR: 6.941e-04  Data: 0.024 (0.033)
Train: 112 [ 800/1251 ( 64%)]  Loss: 4.058 (4.32)  Time: 0.176s, 5805.88/s  (0.193s, 5302.38/s)  LR: 6.941e-04  Data: 0.024 (0.032)
Train: 112 [ 850/1251 ( 68%)]  Loss: 4.327 (4.32)  Time: 0.178s, 5768.56/s  (0.193s, 5303.94/s)  LR: 6.941e-04  Data: 0.038 (0.032)
Train: 112 [ 900/1251 ( 72%)]  Loss: 4.537 (4.33)  Time: 0.171s, 5986.44/s  (0.193s, 5310.55/s)  LR: 6.941e-04  Data: 0.028 (0.032)
Train: 112 [ 950/1251 ( 76%)]  Loss: 4.224 (4.33)  Time: 0.280s, 3662.21/s  (0.193s, 5315.22/s)  LR: 6.941e-04  Data: 0.027 (0.032)
Train: 112 [1000/1251 ( 80%)]  Loss: 4.455 (4.33)  Time: 0.166s, 6169.49/s  (0.193s, 5319.32/s)  LR: 6.941e-04  Data: 0.027 (0.032)
Train: 112 [1050/1251 ( 84%)]  Loss: 4.324 (4.33)  Time: 0.167s, 6123.51/s  (0.193s, 5304.48/s)  LR: 6.941e-04  Data: 0.025 (0.032)
Train: 112 [1100/1251 ( 88%)]  Loss: 4.511 (4.34)  Time: 0.147s, 6960.63/s  (0.193s, 5302.52/s)  LR: 6.941e-04  Data: 0.022 (0.031)
Train: 112 [1150/1251 ( 92%)]  Loss: 4.207 (4.33)  Time: 0.169s, 6057.37/s  (0.193s, 5299.71/s)  LR: 6.941e-04  Data: 0.036 (0.031)
Train: 112 [1200/1251 ( 96%)]  Loss: 4.300 (4.33)  Time: 0.148s, 6918.63/s  (0.193s, 5295.16/s)  LR: 6.941e-04  Data: 0.023 (0.031)
Train: 112 [1250/1251 (100%)]  Loss: 3.902 (4.32)  Time: 0.114s, 9017.41/s  (0.193s, 5310.55/s)  LR: 6.941e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.834 (1.834)  Loss:  1.1519 (1.1519)  Acc@1: 78.6133 (78.6133)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2207 (1.8118)  Acc@1: 78.4198 (63.1220)  Acc@5: 93.2783 (84.9620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-112.pth.tar', 63.121999934082034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)

Train: 113 [   0/1251 (  0%)]  Loss: 4.594 (4.59)  Time: 1.965s,  521.20/s  (1.965s,  521.20/s)  LR: 6.892e-04  Data: 1.817 (1.817)
Train: 113 [  50/1251 (  4%)]  Loss: 4.183 (4.39)  Time: 0.162s, 6312.64/s  (0.221s, 4630.06/s)  LR: 6.892e-04  Data: 0.024 (0.078)
Train: 113 [ 100/1251 (  8%)]  Loss: 4.418 (4.40)  Time: 0.153s, 6676.39/s  (0.205s, 5004.07/s)  LR: 6.892e-04  Data: 0.027 (0.061)
Train: 113 [ 150/1251 ( 12%)]  Loss: 4.213 (4.35)  Time: 0.171s, 5974.83/s  (0.199s, 5138.13/s)  LR: 6.892e-04  Data: 0.040 (0.052)
Train: 113 [ 200/1251 ( 16%)]  Loss: 3.784 (4.24)  Time: 0.207s, 4951.62/s  (0.198s, 5180.27/s)  LR: 6.892e-04  Data: 0.022 (0.050)
Train: 113 [ 250/1251 ( 20%)]  Loss: 4.505 (4.28)  Time: 0.161s, 6379.50/s  (0.196s, 5221.76/s)  LR: 6.892e-04  Data: 0.020 (0.049)
Train: 113 [ 300/1251 ( 24%)]  Loss: 4.463 (4.31)  Time: 0.153s, 6676.16/s  (0.195s, 5253.57/s)  LR: 6.892e-04  Data: 0.025 (0.045)
Train: 113 [ 350/1251 ( 28%)]  Loss: 4.328 (4.31)  Time: 0.187s, 5471.15/s  (0.194s, 5271.55/s)  LR: 6.892e-04  Data: 0.031 (0.043)
Train: 113 [ 400/1251 ( 32%)]  Loss: 4.685 (4.35)  Time: 0.200s, 5116.30/s  (0.195s, 5264.72/s)  LR: 6.892e-04  Data: 0.030 (0.041)
Train: 113 [ 450/1251 ( 36%)]  Loss: 3.939 (4.31)  Time: 0.154s, 6633.67/s  (0.193s, 5294.82/s)  LR: 6.892e-04  Data: 0.029 (0.040)
Train: 113 [ 500/1251 ( 40%)]  Loss: 4.510 (4.33)  Time: 0.164s, 6246.15/s  (0.193s, 5316.95/s)  LR: 6.892e-04  Data: 0.032 (0.039)
Train: 113 [ 550/1251 ( 44%)]  Loss: 4.353 (4.33)  Time: 0.175s, 5851.67/s  (0.192s, 5326.10/s)  LR: 6.892e-04  Data: 0.024 (0.038)
Train: 113 [ 600/1251 ( 48%)]  Loss: 4.499 (4.34)  Time: 0.166s, 6153.07/s  (0.193s, 5315.18/s)  LR: 6.892e-04  Data: 0.032 (0.037)
Train: 113 [ 650/1251 ( 52%)]  Loss: 4.438 (4.35)  Time: 0.304s, 3362.94/s  (0.193s, 5319.34/s)  LR: 6.892e-04  Data: 0.025 (0.036)
Train: 113 [ 700/1251 ( 56%)]  Loss: 4.465 (4.36)  Time: 0.178s, 5749.31/s  (0.192s, 5320.64/s)  LR: 6.892e-04  Data: 0.028 (0.035)
Train: 113 [ 750/1251 ( 60%)]  Loss: 4.466 (4.37)  Time: 0.160s, 6403.59/s  (0.193s, 5312.59/s)  LR: 6.892e-04  Data: 0.029 (0.035)
Train: 113 [ 800/1251 ( 64%)]  Loss: 4.413 (4.37)  Time: 0.200s, 5113.77/s  (0.192s, 5327.42/s)  LR: 6.892e-04  Data: 0.026 (0.035)
Train: 113 [ 850/1251 ( 68%)]  Loss: 4.170 (4.36)  Time: 0.366s, 2797.35/s  (0.192s, 5322.64/s)  LR: 6.892e-04  Data: 0.026 (0.034)
Train: 113 [ 900/1251 ( 72%)]  Loss: 4.359 (4.36)  Time: 0.172s, 5963.63/s  (0.192s, 5328.84/s)  LR: 6.892e-04  Data: 0.030 (0.034)
Train: 113 [ 950/1251 ( 76%)]  Loss: 4.367 (4.36)  Time: 0.176s, 5806.06/s  (0.192s, 5331.05/s)  LR: 6.892e-04  Data: 0.028 (0.034)
Train: 113 [1000/1251 ( 80%)]  Loss: 4.121 (4.35)  Time: 0.146s, 7033.71/s  (0.192s, 5322.68/s)  LR: 6.892e-04  Data: 0.027 (0.033)
Train: 113 [1050/1251 ( 84%)]  Loss: 4.458 (4.35)  Time: 0.162s, 6317.27/s  (0.192s, 5319.67/s)  LR: 6.892e-04  Data: 0.028 (0.033)
Train: 113 [1100/1251 ( 88%)]  Loss: 4.061 (4.34)  Time: 0.157s, 6503.44/s  (0.193s, 5309.13/s)  LR: 6.892e-04  Data: 0.030 (0.033)
Train: 113 [1150/1251 ( 92%)]  Loss: 4.380 (4.34)  Time: 0.172s, 5952.59/s  (0.193s, 5310.79/s)  LR: 6.892e-04  Data: 0.032 (0.033)
Train: 113 [1200/1251 ( 96%)]  Loss: 4.178 (4.33)  Time: 0.191s, 5372.71/s  (0.193s, 5313.35/s)  LR: 6.892e-04  Data: 0.019 (0.033)
Train: 113 [1250/1251 (100%)]  Loss: 4.522 (4.34)  Time: 0.114s, 8958.21/s  (0.192s, 5328.97/s)  LR: 6.892e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.825 (1.825)  Loss:  1.2827 (1.2827)  Acc@1: 79.7852 (79.7852)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.3511 (1.8918)  Acc@1: 79.0094 (62.7760)  Acc@5: 93.1604 (85.0680)
Train: 114 [   0/1251 (  0%)]  Loss: 4.720 (4.72)  Time: 1.702s,  601.48/s  (1.702s,  601.48/s)  LR: 6.844e-04  Data: 1.583 (1.583)
Train: 114 [  50/1251 (  4%)]  Loss: 4.364 (4.54)  Time: 0.150s, 6840.38/s  (0.226s, 4535.06/s)  LR: 6.844e-04  Data: 0.033 (0.065)
Train: 114 [ 100/1251 (  8%)]  Loss: 4.189 (4.42)  Time: 0.164s, 6244.35/s  (0.207s, 4939.42/s)  LR: 6.844e-04  Data: 0.027 (0.047)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 114 [ 150/1251 ( 12%)]  Loss: 4.369 (4.41)  Time: 0.246s, 4169.43/s  (0.203s, 5048.58/s)  LR: 6.844e-04  Data: 0.031 (0.043)
Train: 114 [ 200/1251 ( 16%)]  Loss: 4.371 (4.40)  Time: 0.192s, 5342.47/s  (0.199s, 5146.66/s)  LR: 6.844e-04  Data: 0.036 (0.043)
Train: 114 [ 250/1251 ( 20%)]  Loss: 4.222 (4.37)  Time: 0.158s, 6462.21/s  (0.196s, 5232.89/s)  LR: 6.844e-04  Data: 0.031 (0.042)
Train: 114 [ 300/1251 ( 24%)]  Loss: 4.795 (4.43)  Time: 0.156s, 6568.11/s  (0.195s, 5264.47/s)  LR: 6.844e-04  Data: 0.030 (0.039)
Train: 114 [ 350/1251 ( 28%)]  Loss: 4.166 (4.40)  Time: 0.167s, 6138.76/s  (0.193s, 5294.59/s)  LR: 6.844e-04  Data: 0.023 (0.039)
Train: 114 [ 400/1251 ( 32%)]  Loss: 4.414 (4.40)  Time: 0.161s, 6360.42/s  (0.193s, 5295.83/s)  LR: 6.844e-04  Data: 0.029 (0.038)
Train: 114 [ 450/1251 ( 36%)]  Loss: 4.604 (4.42)  Time: 0.183s, 5583.27/s  (0.193s, 5295.72/s)  LR: 6.844e-04  Data: 0.021 (0.039)
Train: 114 [ 500/1251 ( 40%)]  Loss: 4.221 (4.40)  Time: 0.193s, 5314.70/s  (0.192s, 5320.54/s)  LR: 6.844e-04  Data: 0.031 (0.039)
Train: 114 [ 550/1251 ( 44%)]  Loss: 4.392 (4.40)  Time: 0.204s, 5008.62/s  (0.193s, 5316.40/s)  LR: 6.844e-04  Data: 0.026 (0.039)
Train: 114 [ 600/1251 ( 48%)]  Loss: 4.100 (4.38)  Time: 0.181s, 5648.16/s  (0.193s, 5318.06/s)  LR: 6.844e-04  Data: 0.030 (0.038)
Train: 114 [ 650/1251 ( 52%)]  Loss: 4.040 (4.35)  Time: 0.183s, 5597.43/s  (0.193s, 5317.21/s)  LR: 6.844e-04  Data: 0.020 (0.038)
Train: 114 [ 700/1251 ( 56%)]  Loss: 4.472 (4.36)  Time: 0.162s, 6332.30/s  (0.192s, 5332.08/s)  LR: 6.844e-04  Data: 0.030 (0.037)
Train: 114 [ 750/1251 ( 60%)]  Loss: 3.913 (4.33)  Time: 0.164s, 6244.06/s  (0.192s, 5335.20/s)  LR: 6.844e-04  Data: 0.022 (0.036)
Train: 114 [ 800/1251 ( 64%)]  Loss: 4.281 (4.33)  Time: 0.195s, 5244.10/s  (0.192s, 5323.23/s)  LR: 6.844e-04  Data: 0.032 (0.036)
Train: 114 [ 850/1251 ( 68%)]  Loss: 4.463 (4.34)  Time: 0.167s, 6124.88/s  (0.192s, 5324.67/s)  LR: 6.844e-04  Data: 0.034 (0.035)
Train: 114 [ 900/1251 ( 72%)]  Loss: 4.229 (4.33)  Time: 0.196s, 5236.76/s  (0.192s, 5324.81/s)  LR: 6.844e-04  Data: 0.029 (0.035)
Train: 114 [ 950/1251 ( 76%)]  Loss: 4.358 (4.33)  Time: 0.161s, 6366.62/s  (0.192s, 5327.05/s)  LR: 6.844e-04  Data: 0.030 (0.035)
Train: 114 [1000/1251 ( 80%)]  Loss: 4.136 (4.32)  Time: 0.161s, 6373.77/s  (0.193s, 5319.38/s)  LR: 6.844e-04  Data: 0.020 (0.034)
Train: 114 [1050/1251 ( 84%)]  Loss: 4.214 (4.32)  Time: 0.155s, 6589.26/s  (0.192s, 5319.97/s)  LR: 6.844e-04  Data: 0.031 (0.034)
Train: 114 [1100/1251 ( 88%)]  Loss: 4.430 (4.32)  Time: 0.154s, 6657.03/s  (0.193s, 5318.28/s)  LR: 6.844e-04  Data: 0.025 (0.034)
Train: 114 [1150/1251 ( 92%)]  Loss: 4.104 (4.32)  Time: 0.167s, 6132.83/s  (0.193s, 5315.91/s)  LR: 6.844e-04  Data: 0.029 (0.034)
Train: 114 [1200/1251 ( 96%)]  Loss: 4.800 (4.33)  Time: 0.287s, 3570.63/s  (0.193s, 5313.14/s)  LR: 6.844e-04  Data: 0.030 (0.034)
Train: 114 [1250/1251 (100%)]  Loss: 4.312 (4.33)  Time: 0.113s, 9034.31/s  (0.192s, 5326.22/s)  LR: 6.844e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.784 (1.784)  Loss:  1.2063 (1.2063)  Acc@1: 79.1016 (79.1016)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.2022 (1.7997)  Acc@1: 80.1887 (63.6340)  Acc@5: 92.3349 (85.3460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-112.pth.tar', 63.121999934082034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-97.pth.tar', 62.994000087890626)

Train: 115 [   0/1251 (  0%)]  Loss: 4.667 (4.67)  Time: 1.657s,  618.10/s  (1.657s,  618.10/s)  LR: 6.795e-04  Data: 1.524 (1.524)
Train: 115 [  50/1251 (  4%)]  Loss: 4.500 (4.58)  Time: 0.174s, 5889.52/s  (0.226s, 4529.39/s)  LR: 6.795e-04  Data: 0.031 (0.065)
Train: 115 [ 100/1251 (  8%)]  Loss: 4.209 (4.46)  Time: 0.165s, 6220.07/s  (0.206s, 4965.16/s)  LR: 6.795e-04  Data: 0.028 (0.048)
Train: 115 [ 150/1251 ( 12%)]  Loss: 4.279 (4.41)  Time: 0.153s, 6694.68/s  (0.202s, 5073.23/s)  LR: 6.795e-04  Data: 0.028 (0.047)
Train: 115 [ 200/1251 ( 16%)]  Loss: 4.056 (4.34)  Time: 0.176s, 5819.60/s  (0.198s, 5176.52/s)  LR: 6.795e-04  Data: 0.026 (0.045)
Train: 115 [ 250/1251 ( 20%)]  Loss: 4.692 (4.40)  Time: 0.170s, 6009.96/s  (0.196s, 5233.03/s)  LR: 6.795e-04  Data: 0.039 (0.043)
Train: 115 [ 300/1251 ( 24%)]  Loss: 4.658 (4.44)  Time: 0.163s, 6284.82/s  (0.195s, 5260.10/s)  LR: 6.795e-04  Data: 0.037 (0.041)
Train: 115 [ 350/1251 ( 28%)]  Loss: 4.388 (4.43)  Time: 0.178s, 5761.09/s  (0.193s, 5292.19/s)  LR: 6.795e-04  Data: 0.028 (0.039)
Train: 115 [ 400/1251 ( 32%)]  Loss: 4.340 (4.42)  Time: 0.165s, 6219.75/s  (0.194s, 5284.24/s)  LR: 6.795e-04  Data: 0.023 (0.038)
Train: 115 [ 450/1251 ( 36%)]  Loss: 4.233 (4.40)  Time: 0.268s, 3826.46/s  (0.193s, 5298.98/s)  LR: 6.795e-04  Data: 0.031 (0.037)
Train: 115 [ 500/1251 ( 40%)]  Loss: 4.071 (4.37)  Time: 0.187s, 5486.41/s  (0.194s, 5286.65/s)  LR: 6.795e-04  Data: 0.037 (0.036)
Train: 115 [ 550/1251 ( 44%)]  Loss: 4.057 (4.35)  Time: 0.173s, 5914.23/s  (0.193s, 5307.24/s)  LR: 6.795e-04  Data: 0.022 (0.035)
Train: 115 [ 600/1251 ( 48%)]  Loss: 4.490 (4.36)  Time: 0.242s, 4237.44/s  (0.192s, 5326.40/s)  LR: 6.795e-04  Data: 0.032 (0.035)
Train: 115 [ 650/1251 ( 52%)]  Loss: 4.148 (4.34)  Time: 0.156s, 6572.15/s  (0.193s, 5312.61/s)  LR: 6.795e-04  Data: 0.028 (0.034)
Train: 115 [ 700/1251 ( 56%)]  Loss: 4.421 (4.35)  Time: 0.168s, 6109.25/s  (0.192s, 5320.21/s)  LR: 6.795e-04  Data: 0.023 (0.034)
Train: 115 [ 750/1251 ( 60%)]  Loss: 4.166 (4.34)  Time: 0.170s, 6030.40/s  (0.192s, 5332.34/s)  LR: 6.795e-04  Data: 0.024 (0.033)
Train: 115 [ 800/1251 ( 64%)]  Loss: 4.426 (4.34)  Time: 0.178s, 5746.70/s  (0.192s, 5332.96/s)  LR: 6.795e-04  Data: 0.025 (0.033)
Train: 115 [ 850/1251 ( 68%)]  Loss: 4.555 (4.35)  Time: 0.173s, 5912.34/s  (0.192s, 5337.76/s)  LR: 6.795e-04  Data: 0.034 (0.033)
Train: 115 [ 900/1251 ( 72%)]  Loss: 4.109 (4.34)  Time: 0.192s, 5339.05/s  (0.192s, 5338.24/s)  LR: 6.795e-04  Data: 0.027 (0.033)
Train: 115 [ 950/1251 ( 76%)]  Loss: 4.209 (4.33)  Time: 0.182s, 5625.70/s  (0.192s, 5329.34/s)  LR: 6.795e-04  Data: 0.031 (0.033)
Train: 115 [1000/1251 ( 80%)]  Loss: 4.616 (4.35)  Time: 0.158s, 6478.45/s  (0.192s, 5326.85/s)  LR: 6.795e-04  Data: 0.028 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 115 [1050/1251 ( 84%)]  Loss: 4.643 (4.36)  Time: 0.175s, 5858.18/s  (0.192s, 5321.72/s)  LR: 6.795e-04  Data: 0.027 (0.034)
Train: 115 [1100/1251 ( 88%)]  Loss: 4.036 (4.35)  Time: 0.184s, 5559.38/s  (0.192s, 5322.75/s)  LR: 6.795e-04  Data: 0.023 (0.035)
Train: 115 [1150/1251 ( 92%)]  Loss: 4.184 (4.34)  Time: 0.160s, 6389.54/s  (0.193s, 5309.37/s)  LR: 6.795e-04  Data: 0.022 (0.036)
Train: 115 [1200/1251 ( 96%)]  Loss: 4.183 (4.33)  Time: 0.173s, 5925.24/s  (0.193s, 5309.71/s)  LR: 6.795e-04  Data: 0.027 (0.036)
Train: 115 [1250/1251 (100%)]  Loss: 4.391 (4.34)  Time: 0.114s, 9004.45/s  (0.192s, 5329.74/s)  LR: 6.795e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.911 (1.911)  Loss:  1.1262 (1.1262)  Acc@1: 79.8828 (79.8828)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1721 (1.8199)  Acc@1: 78.6557 (63.2620)  Acc@5: 92.5708 (85.2200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-115.pth.tar', 63.262000166015625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-112.pth.tar', 63.121999934082034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-95.pth.tar', 62.996000061035154)

Train: 116 [   0/1251 (  0%)]  Loss: 4.312 (4.31)  Time: 1.782s,  574.55/s  (1.782s,  574.55/s)  LR: 6.746e-04  Data: 1.644 (1.644)
Train: 116 [  50/1251 (  4%)]  Loss: 4.080 (4.20)  Time: 0.165s, 6197.57/s  (0.229s, 4471.80/s)  LR: 6.746e-04  Data: 0.033 (0.070)
Train: 116 [ 100/1251 (  8%)]  Loss: 4.280 (4.22)  Time: 0.189s, 5421.97/s  (0.209s, 4894.11/s)  LR: 6.746e-04  Data: 0.058 (0.050)
Train: 116 [ 150/1251 ( 12%)]  Loss: 4.687 (4.34)  Time: 0.187s, 5463.17/s  (0.202s, 5057.58/s)  LR: 6.746e-04  Data: 0.059 (0.043)
Train: 116 [ 200/1251 ( 16%)]  Loss: 4.482 (4.37)  Time: 0.276s, 3712.51/s  (0.198s, 5174.71/s)  LR: 6.746e-04  Data: 0.027 (0.040)
Train: 116 [ 250/1251 ( 20%)]  Loss: 4.200 (4.34)  Time: 0.173s, 5909.43/s  (0.196s, 5213.18/s)  LR: 6.746e-04  Data: 0.035 (0.038)
Train: 116 [ 300/1251 ( 24%)]  Loss: 4.321 (4.34)  Time: 0.196s, 5225.54/s  (0.196s, 5222.41/s)  LR: 6.746e-04  Data: 0.026 (0.036)
Train: 116 [ 350/1251 ( 28%)]  Loss: 4.370 (4.34)  Time: 0.170s, 6039.04/s  (0.195s, 5264.51/s)  LR: 6.746e-04  Data: 0.033 (0.035)
Train: 116 [ 400/1251 ( 32%)]  Loss: 3.864 (4.29)  Time: 0.171s, 5975.37/s  (0.193s, 5306.41/s)  LR: 6.746e-04  Data: 0.034 (0.034)
Train: 116 [ 450/1251 ( 36%)]  Loss: 3.903 (4.25)  Time: 0.161s, 6365.06/s  (0.193s, 5306.96/s)  LR: 6.746e-04  Data: 0.026 (0.034)
Train: 116 [ 500/1251 ( 40%)]  Loss: 4.130 (4.24)  Time: 0.165s, 6213.57/s  (0.193s, 5298.86/s)  LR: 6.746e-04  Data: 0.025 (0.033)
Train: 116 [ 550/1251 ( 44%)]  Loss: 4.589 (4.27)  Time: 0.193s, 5314.92/s  (0.192s, 5323.08/s)  LR: 6.746e-04  Data: 0.031 (0.033)
Train: 116 [ 600/1251 ( 48%)]  Loss: 4.147 (4.26)  Time: 0.171s, 5974.98/s  (0.192s, 5339.98/s)  LR: 6.746e-04  Data: 0.031 (0.032)
Train: 116 [ 650/1251 ( 52%)]  Loss: 4.196 (4.25)  Time: 0.175s, 5867.33/s  (0.192s, 5334.78/s)  LR: 6.746e-04  Data: 0.029 (0.032)
Train: 116 [ 700/1251 ( 56%)]  Loss: 4.360 (4.26)  Time: 0.249s, 4120.14/s  (0.192s, 5325.42/s)  LR: 6.746e-04  Data: 0.024 (0.032)
Train: 116 [ 750/1251 ( 60%)]  Loss: 4.373 (4.27)  Time: 0.169s, 6063.79/s  (0.192s, 5322.54/s)  LR: 6.746e-04  Data: 0.020 (0.032)
Train: 116 [ 800/1251 ( 64%)]  Loss: 4.582 (4.29)  Time: 0.166s, 6162.05/s  (0.192s, 5322.40/s)  LR: 6.746e-04  Data: 0.028 (0.031)
Train: 116 [ 850/1251 ( 68%)]  Loss: 3.995 (4.27)  Time: 0.170s, 6028.96/s  (0.193s, 5316.26/s)  LR: 6.746e-04  Data: 0.039 (0.031)
Train: 116 [ 900/1251 ( 72%)]  Loss: 4.010 (4.26)  Time: 0.180s, 5703.85/s  (0.193s, 5318.20/s)  LR: 6.746e-04  Data: 0.023 (0.031)
Train: 116 [ 950/1251 ( 76%)]  Loss: 4.303 (4.26)  Time: 0.194s, 5278.52/s  (0.193s, 5312.14/s)  LR: 6.746e-04  Data: 0.019 (0.031)
Train: 116 [1000/1251 ( 80%)]  Loss: 4.178 (4.26)  Time: 0.179s, 5715.98/s  (0.193s, 5304.89/s)  LR: 6.746e-04  Data: 0.026 (0.031)
Train: 116 [1050/1251 ( 84%)]  Loss: 4.426 (4.26)  Time: 0.173s, 5910.34/s  (0.193s, 5304.31/s)  LR: 6.746e-04  Data: 0.021 (0.031)
Train: 116 [1100/1251 ( 88%)]  Loss: 4.109 (4.26)  Time: 0.176s, 5816.78/s  (0.193s, 5307.01/s)  LR: 6.746e-04  Data: 0.032 (0.030)
Train: 116 [1150/1251 ( 92%)]  Loss: 4.246 (4.26)  Time: 0.185s, 5543.57/s  (0.193s, 5304.51/s)  LR: 6.746e-04  Data: 0.028 (0.030)
Train: 116 [1200/1251 ( 96%)]  Loss: 3.909 (4.24)  Time: 0.155s, 6600.53/s  (0.193s, 5303.89/s)  LR: 6.746e-04  Data: 0.029 (0.030)
Train: 116 [1250/1251 (100%)]  Loss: 4.253 (4.24)  Time: 0.113s, 9051.98/s  (0.193s, 5315.67/s)  LR: 6.746e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.803 (1.803)  Loss:  1.1899 (1.1899)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.1561 (1.7948)  Acc@1: 80.0708 (63.7840)  Acc@5: 93.1604 (85.3540)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-115.pth.tar', 63.262000166015625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-112.pth.tar', 63.121999934082034)

Train: 117 [   0/1251 (  0%)]  Loss: 4.526 (4.53)  Time: 1.782s,  574.56/s  (1.782s,  574.56/s)  LR: 6.697e-04  Data: 1.543 (1.543)
Train: 117 [  50/1251 (  4%)]  Loss: 4.228 (4.38)  Time: 0.166s, 6160.49/s  (0.220s, 4649.33/s)  LR: 6.697e-04  Data: 0.023 (0.060)
Train: 117 [ 100/1251 (  8%)]  Loss: 4.322 (4.36)  Time: 0.178s, 5739.36/s  (0.207s, 4936.13/s)  LR: 6.697e-04  Data: 0.023 (0.044)
Train: 117 [ 150/1251 ( 12%)]  Loss: 3.905 (4.25)  Time: 0.171s, 5982.11/s  (0.201s, 5102.65/s)  LR: 6.697e-04  Data: 0.029 (0.039)
Train: 117 [ 200/1251 ( 16%)]  Loss: 4.102 (4.22)  Time: 0.176s, 5824.73/s  (0.199s, 5145.35/s)  LR: 6.697e-04  Data: 0.026 (0.036)
Train: 117 [ 250/1251 ( 20%)]  Loss: 4.258 (4.22)  Time: 0.177s, 5780.10/s  (0.197s, 5195.70/s)  LR: 6.697e-04  Data: 0.025 (0.034)
Train: 117 [ 300/1251 ( 24%)]  Loss: 4.274 (4.23)  Time: 0.180s, 5673.72/s  (0.195s, 5240.85/s)  LR: 6.697e-04  Data: 0.026 (0.033)
Train: 117 [ 350/1251 ( 28%)]  Loss: 4.362 (4.25)  Time: 0.165s, 6193.53/s  (0.195s, 5248.62/s)  LR: 6.697e-04  Data: 0.026 (0.033)
Train: 117 [ 400/1251 ( 32%)]  Loss: 4.482 (4.27)  Time: 0.181s, 5663.96/s  (0.195s, 5255.26/s)  LR: 6.697e-04  Data: 0.032 (0.032)
Train: 117 [ 450/1251 ( 36%)]  Loss: 4.048 (4.25)  Time: 0.168s, 6081.69/s  (0.194s, 5285.88/s)  LR: 6.697e-04  Data: 0.028 (0.032)
Train: 117 [ 500/1251 ( 40%)]  Loss: 4.407 (4.26)  Time: 0.170s, 6035.62/s  (0.193s, 5305.41/s)  LR: 6.697e-04  Data: 0.023 (0.032)
Train: 117 [ 550/1251 ( 44%)]  Loss: 4.835 (4.31)  Time: 0.161s, 6357.40/s  (0.193s, 5303.63/s)  LR: 6.697e-04  Data: 0.030 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 117 [ 600/1251 ( 48%)]  Loss: 4.515 (4.33)  Time: 0.168s, 6100.10/s  (0.192s, 5320.36/s)  LR: 6.697e-04  Data: 0.025 (0.031)
Train: 117 [ 650/1251 ( 52%)]  Loss: 4.117 (4.31)  Time: 0.169s, 6066.89/s  (0.193s, 5311.55/s)  LR: 6.697e-04  Data: 0.024 (0.031)
Train: 117 [ 700/1251 ( 56%)]  Loss: 4.402 (4.32)  Time: 0.174s, 5890.93/s  (0.193s, 5307.65/s)  LR: 6.697e-04  Data: 0.030 (0.031)
Train: 117 [ 750/1251 ( 60%)]  Loss: 4.219 (4.31)  Time: 0.174s, 5872.69/s  (0.192s, 5323.45/s)  LR: 6.697e-04  Data: 0.032 (0.031)
Train: 117 [ 800/1251 ( 64%)]  Loss: 4.501 (4.32)  Time: 0.166s, 6181.39/s  (0.193s, 5318.20/s)  LR: 6.697e-04  Data: 0.029 (0.031)
Train: 117 [ 850/1251 ( 68%)]  Loss: 4.410 (4.33)  Time: 0.157s, 6501.91/s  (0.193s, 5318.97/s)  LR: 6.697e-04  Data: 0.031 (0.030)
Train: 117 [ 900/1251 ( 72%)]  Loss: 4.307 (4.33)  Time: 0.182s, 5611.59/s  (0.193s, 5318.70/s)  LR: 6.697e-04  Data: 0.023 (0.030)
Train: 117 [ 950/1251 ( 76%)]  Loss: 4.520 (4.34)  Time: 0.405s, 2525.69/s  (0.193s, 5314.93/s)  LR: 6.697e-04  Data: 0.260 (0.031)
Train: 117 [1000/1251 ( 80%)]  Loss: 4.448 (4.34)  Time: 0.154s, 6650.79/s  (0.193s, 5316.76/s)  LR: 6.697e-04  Data: 0.028 (0.031)
Train: 117 [1050/1251 ( 84%)]  Loss: 4.310 (4.34)  Time: 0.176s, 5816.18/s  (0.193s, 5305.68/s)  LR: 6.697e-04  Data: 0.024 (0.031)
Train: 117 [1100/1251 ( 88%)]  Loss: 4.193 (4.33)  Time: 0.181s, 5651.83/s  (0.193s, 5306.60/s)  LR: 6.697e-04  Data: 0.023 (0.030)
Train: 117 [1150/1251 ( 92%)]  Loss: 4.666 (4.35)  Time: 0.185s, 5520.50/s  (0.193s, 5307.17/s)  LR: 6.697e-04  Data: 0.036 (0.030)
Train: 117 [1200/1251 ( 96%)]  Loss: 4.417 (4.35)  Time: 0.161s, 6366.91/s  (0.193s, 5307.58/s)  LR: 6.697e-04  Data: 0.026 (0.030)
Train: 117 [1250/1251 (100%)]  Loss: 3.756 (4.33)  Time: 0.113s, 9037.70/s  (0.193s, 5313.75/s)  LR: 6.697e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.870 (1.870)  Loss:  1.1680 (1.1680)  Acc@1: 79.8828 (79.8828)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2935 (1.8402)  Acc@1: 76.8868 (63.3860)  Acc@5: 92.2170 (85.3200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-115.pth.tar', 63.262000166015625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-108.pth.tar', 63.128000085449216)

Train: 118 [   0/1251 (  0%)]  Loss: 4.239 (4.24)  Time: 1.951s,  524.77/s  (1.951s,  524.77/s)  LR: 6.648e-04  Data: 1.830 (1.830)
Train: 118 [  50/1251 (  4%)]  Loss: 4.372 (4.31)  Time: 0.160s, 6384.63/s  (0.221s, 4638.20/s)  LR: 6.648e-04  Data: 0.029 (0.069)
Train: 118 [ 100/1251 (  8%)]  Loss: 4.381 (4.33)  Time: 0.187s, 5463.33/s  (0.206s, 4968.10/s)  LR: 6.648e-04  Data: 0.036 (0.049)
Train: 118 [ 150/1251 ( 12%)]  Loss: 4.253 (4.31)  Time: 0.182s, 5611.82/s  (0.200s, 5125.93/s)  LR: 6.648e-04  Data: 0.032 (0.043)
Train: 118 [ 200/1251 ( 16%)]  Loss: 4.546 (4.36)  Time: 0.453s, 2258.86/s  (0.198s, 5169.13/s)  LR: 6.648e-04  Data: 0.034 (0.039)
Train: 118 [ 250/1251 ( 20%)]  Loss: 4.179 (4.33)  Time: 0.198s, 5181.03/s  (0.197s, 5199.43/s)  LR: 6.648e-04  Data: 0.023 (0.037)
Train: 118 [ 300/1251 ( 24%)]  Loss: 4.019 (4.28)  Time: 0.149s, 6858.42/s  (0.197s, 5192.50/s)  LR: 6.648e-04  Data: 0.021 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 118 [ 350/1251 ( 28%)]  Loss: 3.981 (4.25)  Time: 0.166s, 6181.69/s  (0.195s, 5258.94/s)  LR: 6.648e-04  Data: 0.031 (0.034)
Train: 118 [ 400/1251 ( 32%)]  Loss: 4.376 (4.26)  Time: 0.193s, 5295.32/s  (0.194s, 5272.36/s)  LR: 6.648e-04  Data: 0.023 (0.033)
Train: 118 [ 450/1251 ( 36%)]  Loss: 4.178 (4.25)  Time: 0.169s, 6042.37/s  (0.193s, 5317.64/s)  LR: 6.648e-04  Data: 0.025 (0.033)
Train: 118 [ 500/1251 ( 40%)]  Loss: 3.825 (4.21)  Time: 0.172s, 5951.88/s  (0.193s, 5308.46/s)  LR: 6.648e-04  Data: 0.028 (0.033)
Train: 118 [ 550/1251 ( 44%)]  Loss: 3.947 (4.19)  Time: 0.192s, 5322.73/s  (0.193s, 5292.96/s)  LR: 6.648e-04  Data: 0.038 (0.032)
Train: 118 [ 600/1251 ( 48%)]  Loss: 4.331 (4.20)  Time: 0.171s, 5972.04/s  (0.192s, 5330.72/s)  LR: 6.648e-04  Data: 0.029 (0.032)
Train: 118 [ 650/1251 ( 52%)]  Loss: 4.414 (4.22)  Time: 0.173s, 5930.11/s  (0.192s, 5326.77/s)  LR: 6.648e-04  Data: 0.027 (0.032)
Train: 118 [ 700/1251 ( 56%)]  Loss: 4.703 (4.25)  Time: 0.190s, 5386.99/s  (0.192s, 5331.20/s)  LR: 6.648e-04  Data: 0.020 (0.031)
Train: 118 [ 750/1251 ( 60%)]  Loss: 4.069 (4.24)  Time: 0.189s, 5421.41/s  (0.192s, 5325.30/s)  LR: 6.648e-04  Data: 0.031 (0.031)
Train: 118 [ 800/1251 ( 64%)]  Loss: 3.978 (4.22)  Time: 0.171s, 5977.83/s  (0.193s, 5302.58/s)  LR: 6.648e-04  Data: 0.025 (0.031)
Train: 118 [ 850/1251 ( 68%)]  Loss: 4.349 (4.23)  Time: 0.226s, 4540.55/s  (0.193s, 5310.44/s)  LR: 6.648e-04  Data: 0.025 (0.031)
Train: 118 [ 900/1251 ( 72%)]  Loss: 4.326 (4.24)  Time: 0.182s, 5626.97/s  (0.193s, 5304.45/s)  LR: 6.648e-04  Data: 0.028 (0.031)
Train: 118 [ 950/1251 ( 76%)]  Loss: 4.188 (4.23)  Time: 0.163s, 6278.96/s  (0.193s, 5308.62/s)  LR: 6.648e-04  Data: 0.027 (0.031)
Train: 118 [1000/1251 ( 80%)]  Loss: 4.124 (4.23)  Time: 0.156s, 6563.08/s  (0.193s, 5304.89/s)  LR: 6.648e-04  Data: 0.031 (0.030)
Train: 118 [1050/1251 ( 84%)]  Loss: 3.913 (4.21)  Time: 0.166s, 6172.43/s  (0.193s, 5302.38/s)  LR: 6.648e-04  Data: 0.027 (0.030)
Train: 118 [1100/1251 ( 88%)]  Loss: 4.295 (4.22)  Time: 0.187s, 5467.68/s  (0.193s, 5294.20/s)  LR: 6.648e-04  Data: 0.028 (0.030)
Train: 118 [1150/1251 ( 92%)]  Loss: 4.386 (4.22)  Time: 0.185s, 5546.09/s  (0.193s, 5294.43/s)  LR: 6.648e-04  Data: 0.024 (0.030)
Train: 118 [1200/1251 ( 96%)]  Loss: 4.275 (4.23)  Time: 0.174s, 5894.10/s  (0.193s, 5295.46/s)  LR: 6.648e-04  Data: 0.022 (0.030)
Train: 118 [1250/1251 (100%)]  Loss: 4.401 (4.23)  Time: 0.114s, 9017.58/s  (0.193s, 5307.91/s)  LR: 6.648e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1111 (1.1111)  Acc@1: 81.1523 (81.1523)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1740 (1.9063)  Acc@1: 80.1887 (63.1200)  Acc@5: 94.3396 (85.1860)
Train: 119 [   0/1251 (  0%)]  Loss: 4.673 (4.67)  Time: 2.019s,  507.18/s  (2.019s,  507.18/s)  LR: 6.598e-04  Data: 1.624 (1.624)
Train: 119 [  50/1251 (  4%)]  Loss: 4.150 (4.41)  Time: 0.161s, 6347.06/s  (0.220s, 4655.81/s)  LR: 6.598e-04  Data: 0.027 (0.060)
Train: 119 [ 100/1251 (  8%)]  Loss: 4.145 (4.32)  Time: 0.190s, 5390.24/s  (0.204s, 5015.49/s)  LR: 6.598e-04  Data: 0.023 (0.044)
Train: 119 [ 150/1251 ( 12%)]  Loss: 4.428 (4.35)  Time: 0.190s, 5379.06/s  (0.199s, 5133.03/s)  LR: 6.598e-04  Data: 0.028 (0.039)
Train: 119 [ 200/1251 ( 16%)]  Loss: 4.150 (4.31)  Time: 0.201s, 5088.32/s  (0.197s, 5198.38/s)  LR: 6.598e-04  Data: 0.033 (0.039)
Train: 119 [ 250/1251 ( 20%)]  Loss: 4.423 (4.33)  Time: 0.180s, 5685.04/s  (0.196s, 5223.31/s)  LR: 6.598e-04  Data: 0.027 (0.041)
Train: 119 [ 300/1251 ( 24%)]  Loss: 4.411 (4.34)  Time: 0.175s, 5841.10/s  (0.195s, 5259.74/s)  LR: 6.598e-04  Data: 0.021 (0.041)
Train: 119 [ 350/1251 ( 28%)]  Loss: 4.445 (4.35)  Time: 0.160s, 6401.30/s  (0.195s, 5259.18/s)  LR: 6.598e-04  Data: 0.022 (0.042)
Train: 119 [ 400/1251 ( 32%)]  Loss: 4.100 (4.32)  Time: 0.178s, 5761.53/s  (0.194s, 5277.56/s)  LR: 6.598e-04  Data: 0.021 (0.043)
Train: 119 [ 450/1251 ( 36%)]  Loss: 4.324 (4.32)  Time: 0.156s, 6575.39/s  (0.194s, 5286.88/s)  LR: 6.598e-04  Data: 0.027 (0.043)
Train: 119 [ 500/1251 ( 40%)]  Loss: 4.314 (4.32)  Time: 0.177s, 5775.27/s  (0.194s, 5287.61/s)  LR: 6.598e-04  Data: 0.021 (0.043)
Train: 119 [ 550/1251 ( 44%)]  Loss: 4.327 (4.32)  Time: 0.161s, 6365.22/s  (0.192s, 5320.64/s)  LR: 6.598e-04  Data: 0.027 (0.042)
Train: 119 [ 600/1251 ( 48%)]  Loss: 4.480 (4.34)  Time: 0.176s, 5815.37/s  (0.192s, 5328.05/s)  LR: 6.598e-04  Data: 0.027 (0.042)
Train: 119 [ 650/1251 ( 52%)]  Loss: 4.397 (4.34)  Time: 0.160s, 6381.67/s  (0.193s, 5318.22/s)  LR: 6.598e-04  Data: 0.020 (0.042)
Train: 119 [ 700/1251 ( 56%)]  Loss: 4.188 (4.33)  Time: 0.169s, 6059.01/s  (0.192s, 5330.84/s)  LR: 6.598e-04  Data: 0.025 (0.042)
Train: 119 [ 750/1251 ( 60%)]  Loss: 4.321 (4.33)  Time: 0.178s, 5752.04/s  (0.192s, 5334.44/s)  LR: 6.598e-04  Data: 0.031 (0.041)
Train: 119 [ 800/1251 ( 64%)]  Loss: 4.134 (4.32)  Time: 0.188s, 5456.08/s  (0.192s, 5334.57/s)  LR: 6.598e-04  Data: 0.019 (0.041)
Train: 119 [ 850/1251 ( 68%)]  Loss: 4.762 (4.34)  Time: 0.165s, 6222.25/s  (0.193s, 5315.60/s)  LR: 6.598e-04  Data: 0.031 (0.040)
Train: 119 [ 900/1251 ( 72%)]  Loss: 4.491 (4.35)  Time: 0.155s, 6591.84/s  (0.193s, 5316.44/s)  LR: 6.598e-04  Data: 0.033 (0.039)
Train: 119 [ 950/1251 ( 76%)]  Loss: 3.985 (4.33)  Time: 0.169s, 6050.76/s  (0.193s, 5319.44/s)  LR: 6.598e-04  Data: 0.032 (0.039)
Train: 119 [1000/1251 ( 80%)]  Loss: 4.122 (4.32)  Time: 0.176s, 5818.46/s  (0.192s, 5320.93/s)  LR: 6.598e-04  Data: 0.028 (0.038)
Train: 119 [1050/1251 ( 84%)]  Loss: 4.134 (4.31)  Time: 0.163s, 6297.95/s  (0.193s, 5316.38/s)  LR: 6.598e-04  Data: 0.029 (0.038)
Train: 119 [1100/1251 ( 88%)]  Loss: 4.696 (4.33)  Time: 0.169s, 6052.56/s  (0.193s, 5312.18/s)  LR: 6.598e-04  Data: 0.030 (0.037)
Train: 119 [1150/1251 ( 92%)]  Loss: 4.308 (4.33)  Time: 0.160s, 6390.19/s  (0.193s, 5313.28/s)  LR: 6.598e-04  Data: 0.021 (0.037)
Train: 119 [1200/1251 ( 96%)]  Loss: 4.111 (4.32)  Time: 0.567s, 1806.92/s  (0.193s, 5303.07/s)  LR: 6.598e-04  Data: 0.028 (0.037)
Train: 119 [1250/1251 (100%)]  Loss: 3.970 (4.31)  Time: 0.114s, 8997.26/s  (0.192s, 5321.02/s)  LR: 6.598e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.0979 (1.0979)  Acc@1: 79.1016 (79.1016)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1791 (1.8218)  Acc@1: 79.8349 (63.6680)  Acc@5: 93.7500 (85.6860)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-115.pth.tar', 63.262000166015625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-102.pth.tar', 63.14799995361328)

Train: 120 [   0/1251 (  0%)]  Loss: 4.184 (4.18)  Time: 1.807s,  566.80/s  (1.807s,  566.80/s)  LR: 6.549e-04  Data: 1.677 (1.677)
Train: 120 [  50/1251 (  4%)]  Loss: 4.331 (4.26)  Time: 0.180s, 5675.29/s  (0.222s, 4605.08/s)  LR: 6.549e-04  Data: 0.021 (0.064)
Train: 120 [ 100/1251 (  8%)]  Loss: 3.853 (4.12)  Time: 0.169s, 6056.93/s  (0.207s, 4949.44/s)  LR: 6.549e-04  Data: 0.031 (0.046)
Train: 120 [ 150/1251 ( 12%)]  Loss: 3.847 (4.05)  Time: 0.178s, 5761.30/s  (0.200s, 5114.25/s)  LR: 6.549e-04  Data: 0.029 (0.040)
Train: 120 [ 200/1251 ( 16%)]  Loss: 4.577 (4.16)  Time: 0.181s, 5664.94/s  (0.198s, 5169.27/s)  LR: 6.549e-04  Data: 0.025 (0.037)
Train: 120 [ 250/1251 ( 20%)]  Loss: 4.201 (4.17)  Time: 0.368s, 2785.85/s  (0.197s, 5199.61/s)  LR: 6.549e-04  Data: 0.020 (0.035)
Train: 120 [ 300/1251 ( 24%)]  Loss: 4.350 (4.19)  Time: 0.260s, 3943.16/s  (0.196s, 5222.35/s)  LR: 6.549e-04  Data: 0.029 (0.034)
Train: 120 [ 350/1251 ( 28%)]  Loss: 4.123 (4.18)  Time: 0.191s, 5354.79/s  (0.194s, 5271.94/s)  LR: 6.549e-04  Data: 0.030 (0.033)
Train: 120 [ 400/1251 ( 32%)]  Loss: 3.616 (4.12)  Time: 0.169s, 6071.05/s  (0.194s, 5266.07/s)  LR: 6.549e-04  Data: 0.026 (0.032)
Train: 120 [ 450/1251 ( 36%)]  Loss: 4.549 (4.16)  Time: 0.240s, 4269.46/s  (0.194s, 5289.15/s)  LR: 6.549e-04  Data: 0.022 (0.032)
Train: 120 [ 500/1251 ( 40%)]  Loss: 4.683 (4.21)  Time: 0.202s, 5065.49/s  (0.193s, 5300.34/s)  LR: 6.549e-04  Data: 0.029 (0.031)
Train: 120 [ 550/1251 ( 44%)]  Loss: 4.685 (4.25)  Time: 0.186s, 5497.61/s  (0.193s, 5312.23/s)  LR: 6.549e-04  Data: 0.026 (0.031)
Train: 120 [ 600/1251 ( 48%)]  Loss: 3.715 (4.21)  Time: 0.183s, 5606.18/s  (0.193s, 5307.82/s)  LR: 6.549e-04  Data: 0.027 (0.031)
Train: 120 [ 650/1251 ( 52%)]  Loss: 4.045 (4.20)  Time: 0.173s, 5933.45/s  (0.193s, 5312.02/s)  LR: 6.549e-04  Data: 0.029 (0.031)
Train: 120 [ 700/1251 ( 56%)]  Loss: 3.995 (4.18)  Time: 0.154s, 6629.05/s  (0.192s, 5321.88/s)  LR: 6.549e-04  Data: 0.028 (0.031)
Train: 120 [ 750/1251 ( 60%)]  Loss: 4.410 (4.20)  Time: 0.157s, 6521.97/s  (0.193s, 5319.14/s)  LR: 6.549e-04  Data: 0.028 (0.030)
Train: 120 [ 800/1251 ( 64%)]  Loss: 4.378 (4.21)  Time: 0.181s, 5670.40/s  (0.192s, 5328.66/s)  LR: 6.549e-04  Data: 0.034 (0.030)
Train: 120 [ 850/1251 ( 68%)]  Loss: 4.082 (4.20)  Time: 0.164s, 6225.84/s  (0.192s, 5325.82/s)  LR: 6.549e-04  Data: 0.025 (0.030)
Train: 120 [ 900/1251 ( 72%)]  Loss: 4.278 (4.21)  Time: 0.202s, 5068.94/s  (0.192s, 5323.91/s)  LR: 6.549e-04  Data: 0.033 (0.030)
Train: 120 [ 950/1251 ( 76%)]  Loss: 4.323 (4.21)  Time: 0.167s, 6129.32/s  (0.193s, 5313.03/s)  LR: 6.549e-04  Data: 0.025 (0.030)
Train: 120 [1000/1251 ( 80%)]  Loss: 4.462 (4.22)  Time: 0.171s, 5987.26/s  (0.192s, 5321.53/s)  LR: 6.549e-04  Data: 0.022 (0.030)
Train: 120 [1050/1251 ( 84%)]  Loss: 4.365 (4.23)  Time: 0.204s, 5022.04/s  (0.193s, 5309.04/s)  LR: 6.549e-04  Data: 0.027 (0.030)
Train: 120 [1100/1251 ( 88%)]  Loss: 4.610 (4.25)  Time: 0.171s, 5995.33/s  (0.193s, 5312.66/s)  LR: 6.549e-04  Data: 0.039 (0.030)
Train: 120 [1150/1251 ( 92%)]  Loss: 4.479 (4.26)  Time: 0.168s, 6086.15/s  (0.193s, 5306.74/s)  LR: 6.549e-04  Data: 0.021 (0.030)
Train: 120 [1200/1251 ( 96%)]  Loss: 4.336 (4.26)  Time: 0.185s, 5529.61/s  (0.193s, 5302.82/s)  LR: 6.549e-04  Data: 0.043 (0.030)
Train: 120 [1250/1251 (100%)]  Loss: 4.025 (4.25)  Time: 0.114s, 8991.42/s  (0.192s, 5322.36/s)  LR: 6.549e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.872 (1.872)  Loss:  1.1390 (1.1390)  Acc@1: 79.0039 (79.0039)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1720 (1.7934)  Acc@1: 78.6557 (63.8960)  Acc@5: 93.2783 (85.8700)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-115.pth.tar', 63.262000166015625)

Train: 121 [   0/1251 (  0%)]  Loss: 4.141 (4.14)  Time: 1.831s,  559.37/s  (1.831s,  559.37/s)  LR: 6.499e-04  Data: 1.578 (1.578)
Train: 121 [  50/1251 (  4%)]  Loss: 4.438 (4.29)  Time: 0.198s, 5182.93/s  (0.227s, 4517.08/s)  LR: 6.499e-04  Data: 0.027 (0.065)
Train: 121 [ 100/1251 (  8%)]  Loss: 4.053 (4.21)  Time: 0.163s, 6281.47/s  (0.207s, 4954.35/s)  LR: 6.499e-04  Data: 0.023 (0.049)
Train: 121 [ 150/1251 ( 12%)]  Loss: 4.288 (4.23)  Time: 0.161s, 6377.46/s  (0.200s, 5122.66/s)  LR: 6.499e-04  Data: 0.029 (0.043)
Train: 121 [ 200/1251 ( 16%)]  Loss: 4.265 (4.24)  Time: 0.165s, 6218.57/s  (0.198s, 5176.28/s)  LR: 6.499e-04  Data: 0.024 (0.039)
Train: 121 [ 250/1251 ( 20%)]  Loss: 3.825 (4.17)  Time: 0.167s, 6114.57/s  (0.196s, 5231.81/s)  LR: 6.499e-04  Data: 0.028 (0.038)
Train: 121 [ 300/1251 ( 24%)]  Loss: 4.325 (4.19)  Time: 0.168s, 6105.80/s  (0.195s, 5255.22/s)  LR: 6.499e-04  Data: 0.026 (0.037)
Train: 121 [ 350/1251 ( 28%)]  Loss: 4.459 (4.22)  Time: 0.194s, 5274.41/s  (0.194s, 5277.58/s)  LR: 6.499e-04  Data: 0.028 (0.036)
Train: 121 [ 400/1251 ( 32%)]  Loss: 4.666 (4.27)  Time: 0.164s, 6255.49/s  (0.194s, 5283.77/s)  LR: 6.499e-04  Data: 0.036 (0.035)
Train: 121 [ 450/1251 ( 36%)]  Loss: 3.868 (4.23)  Time: 0.166s, 6175.22/s  (0.193s, 5309.49/s)  LR: 6.499e-04  Data: 0.032 (0.034)
Train: 121 [ 500/1251 ( 40%)]  Loss: 4.335 (4.24)  Time: 0.163s, 6290.76/s  (0.192s, 5324.03/s)  LR: 6.499e-04  Data: 0.025 (0.035)
Train: 121 [ 550/1251 ( 44%)]  Loss: 4.200 (4.24)  Time: 0.172s, 5967.89/s  (0.192s, 5330.37/s)  LR: 6.499e-04  Data: 0.029 (0.034)
Train: 121 [ 600/1251 ( 48%)]  Loss: 4.200 (4.24)  Time: 0.159s, 6430.33/s  (0.192s, 5328.87/s)  LR: 6.499e-04  Data: 0.031 (0.034)
Train: 121 [ 650/1251 ( 52%)]  Loss: 3.776 (4.20)  Time: 0.170s, 6037.96/s  (0.192s, 5320.33/s)  LR: 6.499e-04  Data: 0.036 (0.033)
Train: 121 [ 700/1251 ( 56%)]  Loss: 4.368 (4.21)  Time: 0.161s, 6347.42/s  (0.192s, 5323.38/s)  LR: 6.499e-04  Data: 0.027 (0.033)
Train: 121 [ 750/1251 ( 60%)]  Loss: 4.156 (4.21)  Time: 0.174s, 5882.27/s  (0.192s, 5333.96/s)  LR: 6.499e-04  Data: 0.030 (0.033)
Train: 121 [ 800/1251 ( 64%)]  Loss: 4.216 (4.21)  Time: 0.186s, 5519.62/s  (0.192s, 5337.96/s)  LR: 6.499e-04  Data: 0.023 (0.032)
Train: 121 [ 850/1251 ( 68%)]  Loss: 4.236 (4.21)  Time: 0.182s, 5641.02/s  (0.192s, 5327.87/s)  LR: 6.499e-04  Data: 0.020 (0.032)
Train: 121 [ 900/1251 ( 72%)]  Loss: 4.055 (4.20)  Time: 0.205s, 4984.66/s  (0.192s, 5332.08/s)  LR: 6.499e-04  Data: 0.023 (0.032)
Train: 121 [ 950/1251 ( 76%)]  Loss: 4.064 (4.20)  Time: 0.177s, 5787.11/s  (0.192s, 5326.19/s)  LR: 6.499e-04  Data: 0.036 (0.032)
Train: 121 [1000/1251 ( 80%)]  Loss: 4.620 (4.22)  Time: 0.175s, 5835.04/s  (0.192s, 5329.21/s)  LR: 6.499e-04  Data: 0.026 (0.032)
Train: 121 [1050/1251 ( 84%)]  Loss: 4.373 (4.22)  Time: 0.171s, 6004.35/s  (0.192s, 5333.06/s)  LR: 6.499e-04  Data: 0.028 (0.031)
Train: 121 [1100/1251 ( 88%)]  Loss: 4.358 (4.23)  Time: 0.176s, 5810.68/s  (0.192s, 5325.72/s)  LR: 6.499e-04  Data: 0.039 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 121 [1150/1251 ( 92%)]  Loss: 4.145 (4.23)  Time: 0.382s, 2679.65/s  (0.193s, 5304.56/s)  LR: 6.499e-04  Data: 0.025 (0.031)
Train: 121 [1200/1251 ( 96%)]  Loss: 4.210 (4.23)  Time: 0.159s, 6439.20/s  (0.193s, 5308.56/s)  LR: 6.499e-04  Data: 0.028 (0.031)
Train: 121 [1250/1251 (100%)]  Loss: 4.432 (4.23)  Time: 0.113s, 9031.62/s  (0.192s, 5321.71/s)  LR: 6.499e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.863 (1.863)  Loss:  1.0499 (1.0499)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.2610 (1.8283)  Acc@1: 78.8915 (63.8080)  Acc@5: 92.9245 (85.5900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-104.pth.tar', 63.28400008300781)

Train: 122 [   0/1251 (  0%)]  Loss: 4.322 (4.32)  Time: 1.760s,  581.98/s  (1.760s,  581.98/s)  LR: 6.449e-04  Data: 1.630 (1.630)
Train: 122 [  50/1251 (  4%)]  Loss: 4.420 (4.37)  Time: 0.169s, 6073.24/s  (0.228s, 4486.25/s)  LR: 6.449e-04  Data: 0.025 (0.081)
Train: 122 [ 100/1251 (  8%)]  Loss: 4.450 (4.40)  Time: 0.166s, 6168.24/s  (0.207s, 4946.66/s)  LR: 6.449e-04  Data: 0.026 (0.061)
Train: 122 [ 150/1251 ( 12%)]  Loss: 4.246 (4.36)  Time: 0.192s, 5340.86/s  (0.202s, 5072.62/s)  LR: 6.449e-04  Data: 0.026 (0.050)
Train: 122 [ 200/1251 ( 16%)]  Loss: 4.345 (4.36)  Time: 0.185s, 5541.06/s  (0.198s, 5181.54/s)  LR: 6.449e-04  Data: 0.036 (0.045)
Train: 122 [ 250/1251 ( 20%)]  Loss: 4.029 (4.30)  Time: 0.175s, 5851.86/s  (0.196s, 5235.73/s)  LR: 6.449e-04  Data: 0.034 (0.042)
Train: 122 [ 300/1251 ( 24%)]  Loss: 4.143 (4.28)  Time: 0.200s, 5131.39/s  (0.194s, 5282.29/s)  LR: 6.449e-04  Data: 0.022 (0.039)
Train: 122 [ 350/1251 ( 28%)]  Loss: 4.366 (4.29)  Time: 0.164s, 6240.73/s  (0.193s, 5302.95/s)  LR: 6.449e-04  Data: 0.027 (0.039)
Train: 122 [ 400/1251 ( 32%)]  Loss: 4.499 (4.31)  Time: 0.269s, 3808.84/s  (0.194s, 5280.22/s)  LR: 6.449e-04  Data: 0.031 (0.038)
Train: 122 [ 450/1251 ( 36%)]  Loss: 4.355 (4.32)  Time: 0.187s, 5468.58/s  (0.193s, 5302.10/s)  LR: 6.449e-04  Data: 0.025 (0.037)
Train: 122 [ 500/1251 ( 40%)]  Loss: 4.053 (4.29)  Time: 0.175s, 5857.25/s  (0.193s, 5306.80/s)  LR: 6.449e-04  Data: 0.027 (0.036)
Train: 122 [ 550/1251 ( 44%)]  Loss: 4.313 (4.30)  Time: 0.175s, 5852.14/s  (0.192s, 5323.61/s)  LR: 6.449e-04  Data: 0.027 (0.035)
Train: 122 [ 600/1251 ( 48%)]  Loss: 3.689 (4.25)  Time: 0.468s, 2189.05/s  (0.193s, 5305.49/s)  LR: 6.449e-04  Data: 0.026 (0.035)
Train: 122 [ 650/1251 ( 52%)]  Loss: 4.307 (4.25)  Time: 0.181s, 5666.68/s  (0.193s, 5317.91/s)  LR: 6.449e-04  Data: 0.027 (0.034)
Train: 122 [ 700/1251 ( 56%)]  Loss: 4.261 (4.25)  Time: 0.187s, 5474.98/s  (0.192s, 5325.81/s)  LR: 6.449e-04  Data: 0.032 (0.034)
Train: 122 [ 750/1251 ( 60%)]  Loss: 4.329 (4.26)  Time: 0.308s, 3328.93/s  (0.193s, 5301.78/s)  LR: 6.449e-04  Data: 0.028 (0.033)
Train: 122 [ 800/1251 ( 64%)]  Loss: 4.415 (4.27)  Time: 0.161s, 6355.78/s  (0.193s, 5309.36/s)  LR: 6.449e-04  Data: 0.031 (0.033)
Train: 122 [ 850/1251 ( 68%)]  Loss: 3.899 (4.25)  Time: 0.173s, 5928.31/s  (0.192s, 5321.54/s)  LR: 6.449e-04  Data: 0.026 (0.033)
Train: 122 [ 900/1251 ( 72%)]  Loss: 4.015 (4.23)  Time: 0.189s, 5405.81/s  (0.192s, 5327.38/s)  LR: 6.449e-04  Data: 0.020 (0.033)
Train: 122 [ 950/1251 ( 76%)]  Loss: 4.442 (4.25)  Time: 0.175s, 5847.87/s  (0.193s, 5318.45/s)  LR: 6.449e-04  Data: 0.029 (0.033)
Train: 122 [1000/1251 ( 80%)]  Loss: 4.198 (4.24)  Time: 0.189s, 5429.58/s  (0.192s, 5321.33/s)  LR: 6.449e-04  Data: 0.033 (0.033)
Train: 122 [1050/1251 ( 84%)]  Loss: 4.469 (4.25)  Time: 0.170s, 6031.99/s  (0.193s, 5310.72/s)  LR: 6.449e-04  Data: 0.034 (0.035)
Train: 122 [1100/1251 ( 88%)]  Loss: 4.363 (4.26)  Time: 0.172s, 5950.51/s  (0.193s, 5308.40/s)  LR: 6.449e-04  Data: 0.029 (0.035)
Train: 122 [1150/1251 ( 92%)]  Loss: 4.128 (4.25)  Time: 0.368s, 2781.16/s  (0.193s, 5302.68/s)  LR: 6.449e-04  Data: 0.025 (0.035)
Train: 122 [1200/1251 ( 96%)]  Loss: 4.411 (4.26)  Time: 0.160s, 6410.43/s  (0.193s, 5300.84/s)  LR: 6.449e-04  Data: 0.037 (0.035)
Train: 122 [1250/1251 (100%)]  Loss: 4.260 (4.26)  Time: 0.113s, 9062.09/s  (0.193s, 5308.65/s)  LR: 6.449e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  1.1251 (1.1251)  Acc@1: 81.0547 (81.0547)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3064 (1.8682)  Acc@1: 78.3019 (63.6320)  Acc@5: 92.5708 (85.4060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-122.pth.tar', 63.632000141601566)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-106.pth.tar', 63.288000009765625)

Train: 123 [   0/1251 (  0%)]  Loss: 3.940 (3.94)  Time: 1.649s,  621.07/s  (1.649s,  621.07/s)  LR: 6.399e-04  Data: 1.474 (1.474)
Train: 123 [  50/1251 (  4%)]  Loss: 4.466 (4.20)  Time: 0.190s, 5394.71/s  (0.222s, 4617.81/s)  LR: 6.399e-04  Data: 0.022 (0.063)
Train: 123 [ 100/1251 (  8%)]  Loss: 4.286 (4.23)  Time: 0.198s, 5182.58/s  (0.209s, 4893.60/s)  LR: 6.399e-04  Data: 0.072 (0.047)
Train: 123 [ 150/1251 ( 12%)]  Loss: 4.288 (4.25)  Time: 0.173s, 5915.94/s  (0.201s, 5099.42/s)  LR: 6.399e-04  Data: 0.032 (0.041)
Train: 123 [ 200/1251 ( 16%)]  Loss: 4.283 (4.25)  Time: 0.170s, 6012.03/s  (0.198s, 5171.62/s)  LR: 6.399e-04  Data: 0.021 (0.038)
Train: 123 [ 250/1251 ( 20%)]  Loss: 4.297 (4.26)  Time: 0.172s, 5959.95/s  (0.197s, 5200.57/s)  LR: 6.399e-04  Data: 0.024 (0.036)
Train: 123 [ 300/1251 ( 24%)]  Loss: 3.739 (4.19)  Time: 0.177s, 5799.91/s  (0.196s, 5237.81/s)  LR: 6.399e-04  Data: 0.028 (0.034)
Train: 123 [ 350/1251 ( 28%)]  Loss: 4.203 (4.19)  Time: 0.172s, 5952.84/s  (0.194s, 5285.89/s)  LR: 6.399e-04  Data: 0.034 (0.034)
Train: 123 [ 400/1251 ( 32%)]  Loss: 4.328 (4.20)  Time: 0.179s, 5719.85/s  (0.193s, 5296.95/s)  LR: 6.399e-04  Data: 0.029 (0.033)
Train: 123 [ 450/1251 ( 36%)]  Loss: 4.328 (4.22)  Time: 0.219s, 4665.40/s  (0.193s, 5315.97/s)  LR: 6.399e-04  Data: 0.020 (0.033)
Train: 123 [ 500/1251 ( 40%)]  Loss: 4.317 (4.22)  Time: 0.515s, 1988.23/s  (0.193s, 5311.93/s)  LR: 6.399e-04  Data: 0.023 (0.032)
Train: 123 [ 550/1251 ( 44%)]  Loss: 4.213 (4.22)  Time: 0.173s, 5902.95/s  (0.193s, 5313.25/s)  LR: 6.399e-04  Data: 0.031 (0.032)
Train: 123 [ 600/1251 ( 48%)]  Loss: 4.472 (4.24)  Time: 0.172s, 5955.12/s  (0.192s, 5330.78/s)  LR: 6.399e-04  Data: 0.028 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 123 [ 650/1251 ( 52%)]  Loss: 4.280 (4.25)  Time: 0.402s, 2550.25/s  (0.192s, 5333.33/s)  LR: 6.399e-04  Data: 0.029 (0.032)
Train: 123 [ 700/1251 ( 56%)]  Loss: 4.222 (4.24)  Time: 0.172s, 5938.93/s  (0.192s, 5325.84/s)  LR: 6.399e-04  Data: 0.022 (0.031)
Train: 123 [ 750/1251 ( 60%)]  Loss: 4.187 (4.24)  Time: 0.168s, 6099.25/s  (0.193s, 5319.22/s)  LR: 6.399e-04  Data: 0.026 (0.032)
Train: 123 [ 800/1251 ( 64%)]  Loss: 4.556 (4.26)  Time: 0.175s, 5857.12/s  (0.192s, 5329.80/s)  LR: 6.399e-04  Data: 0.024 (0.032)
Train: 123 [ 850/1251 ( 68%)]  Loss: 4.269 (4.26)  Time: 0.166s, 6172.12/s  (0.192s, 5329.44/s)  LR: 6.399e-04  Data: 0.023 (0.032)
Train: 123 [ 900/1251 ( 72%)]  Loss: 4.423 (4.27)  Time: 0.148s, 6929.38/s  (0.192s, 5330.27/s)  LR: 6.399e-04  Data: 0.034 (0.032)
Train: 123 [ 950/1251 ( 76%)]  Loss: 4.030 (4.26)  Time: 0.168s, 6108.04/s  (0.192s, 5322.86/s)  LR: 6.399e-04  Data: 0.024 (0.032)
Train: 123 [1000/1251 ( 80%)]  Loss: 4.233 (4.26)  Time: 0.193s, 5294.32/s  (0.192s, 5321.11/s)  LR: 6.399e-04  Data: 0.024 (0.032)
Train: 123 [1050/1251 ( 84%)]  Loss: 4.060 (4.25)  Time: 0.180s, 5687.54/s  (0.192s, 5321.20/s)  LR: 6.399e-04  Data: 0.021 (0.032)
Train: 123 [1100/1251 ( 88%)]  Loss: 4.018 (4.24)  Time: 0.186s, 5514.35/s  (0.193s, 5319.14/s)  LR: 6.399e-04  Data: 0.026 (0.032)
Train: 123 [1150/1251 ( 92%)]  Loss: 4.091 (4.23)  Time: 0.156s, 6561.46/s  (0.193s, 5316.47/s)  LR: 6.399e-04  Data: 0.027 (0.032)
Train: 123 [1200/1251 ( 96%)]  Loss: 4.364 (4.24)  Time: 0.166s, 6163.89/s  (0.193s, 5307.52/s)  LR: 6.399e-04  Data: 0.022 (0.031)
Train: 123 [1250/1251 (100%)]  Loss: 3.939 (4.22)  Time: 0.136s, 7517.38/s  (0.192s, 5320.84/s)  LR: 6.399e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.869 (1.869)  Loss:  1.0675 (1.0675)  Acc@1: 82.1289 (82.1289)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2823 (1.8146)  Acc@1: 78.3019 (64.0880)  Acc@5: 93.0425 (85.9360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-122.pth.tar', 63.632000141601566)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-107.pth.tar', 63.31400004150391)

Train: 124 [   0/1251 (  0%)]  Loss: 4.077 (4.08)  Time: 1.845s,  554.90/s  (1.845s,  554.90/s)  LR: 6.348e-04  Data: 1.714 (1.714)
Train: 124 [  50/1251 (  4%)]  Loss: 4.430 (4.25)  Time: 0.170s, 6040.33/s  (0.223s, 4586.99/s)  LR: 6.348e-04  Data: 0.026 (0.065)
Train: 124 [ 100/1251 (  8%)]  Loss: 4.337 (4.28)  Time: 0.186s, 5505.74/s  (0.208s, 4932.43/s)  LR: 6.348e-04  Data: 0.033 (0.047)
Train: 124 [ 150/1251 ( 12%)]  Loss: 4.268 (4.28)  Time: 0.177s, 5794.76/s  (0.198s, 5176.19/s)  LR: 6.348e-04  Data: 0.022 (0.041)
Train: 124 [ 200/1251 ( 16%)]  Loss: 4.384 (4.30)  Time: 0.171s, 5976.75/s  (0.197s, 5208.62/s)  LR: 6.348e-04  Data: 0.031 (0.038)
Train: 124 [ 250/1251 ( 20%)]  Loss: 4.599 (4.35)  Time: 0.164s, 6251.78/s  (0.195s, 5255.07/s)  LR: 6.348e-04  Data: 0.030 (0.036)
Train: 124 [ 300/1251 ( 24%)]  Loss: 3.761 (4.27)  Time: 0.198s, 5178.30/s  (0.194s, 5271.88/s)  LR: 6.348e-04  Data: 0.035 (0.034)
Train: 124 [ 350/1251 ( 28%)]  Loss: 4.238 (4.26)  Time: 0.162s, 6324.31/s  (0.193s, 5295.48/s)  LR: 6.348e-04  Data: 0.030 (0.033)
Train: 124 [ 400/1251 ( 32%)]  Loss: 4.093 (4.24)  Time: 0.161s, 6368.63/s  (0.193s, 5302.15/s)  LR: 6.348e-04  Data: 0.031 (0.033)
Train: 124 [ 450/1251 ( 36%)]  Loss: 3.994 (4.22)  Time: 0.173s, 5919.07/s  (0.192s, 5322.07/s)  LR: 6.348e-04  Data: 0.031 (0.032)
Train: 124 [ 500/1251 ( 40%)]  Loss: 4.561 (4.25)  Time: 0.368s, 2783.15/s  (0.192s, 5331.98/s)  LR: 6.348e-04  Data: 0.025 (0.032)
Train: 124 [ 550/1251 ( 44%)]  Loss: 4.711 (4.29)  Time: 0.197s, 5209.55/s  (0.192s, 5323.34/s)  LR: 6.348e-04  Data: 0.026 (0.033)
Train: 124 [ 600/1251 ( 48%)]  Loss: 4.375 (4.29)  Time: 0.174s, 5881.91/s  (0.192s, 5324.92/s)  LR: 6.348e-04  Data: 0.026 (0.034)
Train: 124 [ 650/1251 ( 52%)]  Loss: 4.467 (4.31)  Time: 0.178s, 5746.81/s  (0.192s, 5328.08/s)  LR: 6.348e-04  Data: 0.033 (0.035)
Train: 124 [ 700/1251 ( 56%)]  Loss: 4.322 (4.31)  Time: 0.168s, 6089.38/s  (0.192s, 5333.63/s)  LR: 6.348e-04  Data: 0.025 (0.036)
Train: 124 [ 750/1251 ( 60%)]  Loss: 3.904 (4.28)  Time: 0.171s, 5985.30/s  (0.192s, 5334.72/s)  LR: 6.348e-04  Data: 0.020 (0.037)
Train: 124 [ 800/1251 ( 64%)]  Loss: 4.369 (4.29)  Time: 0.169s, 6048.88/s  (0.192s, 5330.52/s)  LR: 6.348e-04  Data: 0.025 (0.037)
Train: 124 [ 850/1251 ( 68%)]  Loss: 4.326 (4.29)  Time: 0.161s, 6343.04/s  (0.192s, 5336.98/s)  LR: 6.348e-04  Data: 0.028 (0.037)
Train: 124 [ 900/1251 ( 72%)]  Loss: 4.413 (4.30)  Time: 0.168s, 6079.61/s  (0.192s, 5320.37/s)  LR: 6.348e-04  Data: 0.024 (0.039)
Train: 124 [ 950/1251 ( 76%)]  Loss: 4.428 (4.30)  Time: 0.176s, 5808.58/s  (0.192s, 5322.42/s)  LR: 6.348e-04  Data: 0.024 (0.039)
Train: 124 [1000/1251 ( 80%)]  Loss: 4.610 (4.32)  Time: 0.169s, 6059.99/s  (0.192s, 5330.43/s)  LR: 6.348e-04  Data: 0.027 (0.039)
Train: 124 [1050/1251 ( 84%)]  Loss: 4.745 (4.34)  Time: 0.211s, 4851.85/s  (0.192s, 5322.64/s)  LR: 6.348e-04  Data: 0.027 (0.040)
Train: 124 [1100/1251 ( 88%)]  Loss: 4.337 (4.34)  Time: 0.184s, 5576.71/s  (0.192s, 5324.89/s)  LR: 6.348e-04  Data: 0.029 (0.040)
Train: 124 [1150/1251 ( 92%)]  Loss: 3.934 (4.32)  Time: 0.163s, 6275.40/s  (0.192s, 5324.13/s)  LR: 6.348e-04  Data: 0.019 (0.041)
Train: 124 [1200/1251 ( 96%)]  Loss: 4.316 (4.32)  Time: 0.163s, 6278.81/s  (0.192s, 5323.79/s)  LR: 6.348e-04  Data: 0.023 (0.040)
Train: 124 [1250/1251 (100%)]  Loss: 4.180 (4.31)  Time: 0.114s, 8995.37/s  (0.192s, 5333.96/s)  LR: 6.348e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.741 (1.741)  Loss:  1.2228 (1.2228)  Acc@1: 80.0781 (80.0781)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1774 (1.8500)  Acc@1: 79.2453 (64.2340)  Acc@5: 93.3962 (85.8660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-122.pth.tar', 63.632000141601566)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-117.pth.tar', 63.386000043945316)

Train: 125 [   0/1251 (  0%)]  Loss: 3.831 (3.83)  Time: 1.877s,  545.68/s  (1.877s,  545.68/s)  LR: 6.298e-04  Data: 1.750 (1.750)
Train: 125 [  50/1251 (  4%)]  Loss: 4.278 (4.05)  Time: 0.177s, 5769.49/s  (0.226s, 4538.66/s)  LR: 6.298e-04  Data: 0.034 (0.083)
Train: 125 [ 100/1251 (  8%)]  Loss: 4.589 (4.23)  Time: 0.158s, 6471.46/s  (0.207s, 4946.33/s)  LR: 6.298e-04  Data: 0.026 (0.060)
Train: 125 [ 150/1251 ( 12%)]  Loss: 4.142 (4.21)  Time: 0.163s, 6283.83/s  (0.199s, 5144.42/s)  LR: 6.298e-04  Data: 0.026 (0.053)
Train: 125 [ 200/1251 ( 16%)]  Loss: 3.979 (4.16)  Time: 0.182s, 5615.97/s  (0.199s, 5154.57/s)  LR: 6.298e-04  Data: 0.020 (0.053)
Train: 125 [ 250/1251 ( 20%)]  Loss: 4.276 (4.18)  Time: 0.170s, 6032.25/s  (0.196s, 5226.62/s)  LR: 6.298e-04  Data: 0.029 (0.050)
Train: 125 [ 300/1251 ( 24%)]  Loss: 4.597 (4.24)  Time: 0.187s, 5487.32/s  (0.195s, 5244.51/s)  LR: 6.298e-04  Data: 0.032 (0.049)
Train: 125 [ 350/1251 ( 28%)]  Loss: 4.575 (4.28)  Time: 0.163s, 6288.24/s  (0.195s, 5256.74/s)  LR: 6.298e-04  Data: 0.029 (0.046)
Train: 125 [ 400/1251 ( 32%)]  Loss: 4.454 (4.30)  Time: 0.195s, 5245.08/s  (0.194s, 5282.51/s)  LR: 6.298e-04  Data: 0.028 (0.044)
Train: 125 [ 450/1251 ( 36%)]  Loss: 4.311 (4.30)  Time: 0.177s, 5801.14/s  (0.193s, 5300.84/s)  LR: 6.298e-04  Data: 0.025 (0.043)
Train: 125 [ 500/1251 ( 40%)]  Loss: 4.052 (4.28)  Time: 0.163s, 6292.21/s  (0.193s, 5311.88/s)  LR: 6.298e-04  Data: 0.037 (0.041)
Train: 125 [ 550/1251 ( 44%)]  Loss: 4.456 (4.29)  Time: 0.158s, 6481.98/s  (0.192s, 5323.10/s)  LR: 6.298e-04  Data: 0.026 (0.040)
Train: 125 [ 600/1251 ( 48%)]  Loss: 4.461 (4.31)  Time: 0.164s, 6241.12/s  (0.193s, 5310.46/s)  LR: 6.298e-04  Data: 0.030 (0.039)
Train: 125 [ 650/1251 ( 52%)]  Loss: 4.237 (4.30)  Time: 0.153s, 6681.57/s  (0.193s, 5310.50/s)  LR: 6.298e-04  Data: 0.022 (0.038)
Train: 125 [ 700/1251 ( 56%)]  Loss: 4.297 (4.30)  Time: 0.161s, 6350.16/s  (0.193s, 5317.27/s)  LR: 6.298e-04  Data: 0.034 (0.037)
Train: 125 [ 750/1251 ( 60%)]  Loss: 4.099 (4.29)  Time: 0.182s, 5614.09/s  (0.192s, 5323.03/s)  LR: 6.298e-04  Data: 0.028 (0.037)
Train: 125 [ 800/1251 ( 64%)]  Loss: 4.432 (4.30)  Time: 0.166s, 6156.35/s  (0.193s, 5318.29/s)  LR: 6.298e-04  Data: 0.025 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 125 [ 850/1251 ( 68%)]  Loss: 4.164 (4.29)  Time: 0.186s, 5498.29/s  (0.193s, 5311.03/s)  LR: 6.298e-04  Data: 0.028 (0.036)
Train: 125 [ 900/1251 ( 72%)]  Loss: 4.496 (4.30)  Time: 0.175s, 5852.77/s  (0.193s, 5313.29/s)  LR: 6.298e-04  Data: 0.032 (0.035)
Train: 125 [ 950/1251 ( 76%)]  Loss: 4.451 (4.31)  Time: 0.149s, 6866.51/s  (0.193s, 5316.38/s)  LR: 6.298e-04  Data: 0.023 (0.035)
Train: 125 [1000/1251 ( 80%)]  Loss: 4.575 (4.32)  Time: 0.173s, 5934.82/s  (0.193s, 5309.42/s)  LR: 6.298e-04  Data: 0.023 (0.035)
Train: 125 [1050/1251 ( 84%)]  Loss: 3.948 (4.30)  Time: 0.185s, 5524.78/s  (0.193s, 5304.71/s)  LR: 6.298e-04  Data: 0.023 (0.034)
Train: 125 [1100/1251 ( 88%)]  Loss: 4.161 (4.30)  Time: 0.168s, 6103.16/s  (0.193s, 5306.79/s)  LR: 6.298e-04  Data: 0.022 (0.034)
Train: 125 [1150/1251 ( 92%)]  Loss: 3.992 (4.29)  Time: 0.155s, 6606.30/s  (0.193s, 5315.26/s)  LR: 6.298e-04  Data: 0.023 (0.034)
Train: 125 [1200/1251 ( 96%)]  Loss: 4.347 (4.29)  Time: 0.178s, 5754.06/s  (0.193s, 5306.73/s)  LR: 6.298e-04  Data: 0.027 (0.034)
Train: 125 [1250/1251 (100%)]  Loss: 4.278 (4.29)  Time: 0.113s, 9047.88/s  (0.193s, 5318.26/s)  LR: 6.298e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.807 (1.807)  Loss:  1.1856 (1.1856)  Acc@1: 80.7617 (80.7617)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.1793 (1.8069)  Acc@1: 79.4811 (64.1000)  Acc@5: 93.8679 (85.6620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-122.pth.tar', 63.632000141601566)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-111.pth.tar', 63.446000048828125)

Train: 126 [   0/1251 (  0%)]  Loss: 4.331 (4.33)  Time: 1.723s,  594.37/s  (1.723s,  594.37/s)  LR: 6.247e-04  Data: 1.567 (1.567)
Train: 126 [  50/1251 (  4%)]  Loss: 4.357 (4.34)  Time: 0.185s, 5526.06/s  (0.220s, 4662.25/s)  LR: 6.247e-04  Data: 0.030 (0.066)
Train: 126 [ 100/1251 (  8%)]  Loss: 4.277 (4.32)  Time: 0.450s, 2273.55/s  (0.209s, 4899.71/s)  LR: 6.247e-04  Data: 0.025 (0.048)
Train: 126 [ 150/1251 ( 12%)]  Loss: 4.364 (4.33)  Time: 0.172s, 5946.39/s  (0.202s, 5071.55/s)  LR: 6.247e-04  Data: 0.023 (0.041)
Train: 126 [ 200/1251 ( 16%)]  Loss: 4.306 (4.33)  Time: 0.169s, 6064.75/s  (0.199s, 5145.64/s)  LR: 6.247e-04  Data: 0.033 (0.038)
Train: 126 [ 250/1251 ( 20%)]  Loss: 4.311 (4.32)  Time: 0.179s, 5718.76/s  (0.197s, 5196.05/s)  LR: 6.247e-04  Data: 0.026 (0.036)
Train: 126 [ 300/1251 ( 24%)]  Loss: 3.920 (4.27)  Time: 0.324s, 3155.85/s  (0.195s, 5246.88/s)  LR: 6.247e-04  Data: 0.023 (0.035)
Train: 126 [ 350/1251 ( 28%)]  Loss: 4.315 (4.27)  Time: 0.198s, 5166.31/s  (0.194s, 5286.47/s)  LR: 6.247e-04  Data: 0.028 (0.034)
Train: 126 [ 400/1251 ( 32%)]  Loss: 3.912 (4.23)  Time: 0.191s, 5353.28/s  (0.194s, 5287.62/s)  LR: 6.247e-04  Data: 0.030 (0.033)
Train: 126 [ 450/1251 ( 36%)]  Loss: 4.303 (4.24)  Time: 0.187s, 5482.45/s  (0.194s, 5278.97/s)  LR: 6.247e-04  Data: 0.028 (0.033)
Train: 126 [ 500/1251 ( 40%)]  Loss: 4.204 (4.24)  Time: 0.183s, 5604.09/s  (0.193s, 5302.53/s)  LR: 6.247e-04  Data: 0.028 (0.033)
Train: 126 [ 550/1251 ( 44%)]  Loss: 4.224 (4.24)  Time: 0.191s, 5348.94/s  (0.193s, 5307.95/s)  LR: 6.247e-04  Data: 0.025 (0.032)
Train: 126 [ 600/1251 ( 48%)]  Loss: 3.896 (4.21)  Time: 0.357s, 2870.42/s  (0.193s, 5309.40/s)  LR: 6.247e-04  Data: 0.025 (0.032)
Train: 126 [ 650/1251 ( 52%)]  Loss: 4.151 (4.21)  Time: 0.240s, 4269.76/s  (0.193s, 5313.54/s)  LR: 6.247e-04  Data: 0.030 (0.031)
Train: 126 [ 700/1251 ( 56%)]  Loss: 4.217 (4.21)  Time: 0.156s, 6567.75/s  (0.193s, 5311.54/s)  LR: 6.247e-04  Data: 0.025 (0.031)
Train: 126 [ 750/1251 ( 60%)]  Loss: 4.516 (4.23)  Time: 0.161s, 6353.02/s  (0.193s, 5318.04/s)  LR: 6.247e-04  Data: 0.028 (0.031)
Train: 126 [ 800/1251 ( 64%)]  Loss: 4.357 (4.23)  Time: 0.337s, 3042.63/s  (0.193s, 5318.20/s)  LR: 6.247e-04  Data: 0.037 (0.031)
Train: 126 [ 850/1251 ( 68%)]  Loss: 4.341 (4.24)  Time: 0.251s, 4074.63/s  (0.192s, 5322.65/s)  LR: 6.247e-04  Data: 0.023 (0.031)
Train: 126 [ 900/1251 ( 72%)]  Loss: 4.250 (4.24)  Time: 0.182s, 5640.74/s  (0.193s, 5313.64/s)  LR: 6.247e-04  Data: 0.028 (0.030)
Train: 126 [ 950/1251 ( 76%)]  Loss: 3.914 (4.22)  Time: 0.170s, 6013.41/s  (0.192s, 5321.09/s)  LR: 6.247e-04  Data: 0.024 (0.030)
Train: 126 [1000/1251 ( 80%)]  Loss: 4.191 (4.22)  Time: 0.163s, 6286.14/s  (0.193s, 5315.05/s)  LR: 6.247e-04  Data: 0.041 (0.030)
Train: 126 [1050/1251 ( 84%)]  Loss: 4.326 (4.23)  Time: 0.447s, 2291.16/s  (0.193s, 5303.62/s)  LR: 6.247e-04  Data: 0.022 (0.030)
Train: 126 [1100/1251 ( 88%)]  Loss: 4.479 (4.24)  Time: 0.187s, 5482.79/s  (0.193s, 5306.66/s)  LR: 6.247e-04  Data: 0.027 (0.030)
Train: 126 [1150/1251 ( 92%)]  Loss: 4.367 (4.24)  Time: 0.181s, 5650.92/s  (0.193s, 5300.89/s)  LR: 6.247e-04  Data: 0.027 (0.030)
Train: 126 [1200/1251 ( 96%)]  Loss: 4.616 (4.26)  Time: 0.171s, 5993.53/s  (0.193s, 5300.33/s)  LR: 6.247e-04  Data: 0.027 (0.030)
Train: 126 [1250/1251 (100%)]  Loss: 4.139 (4.25)  Time: 0.113s, 9077.47/s  (0.193s, 5317.29/s)  LR: 6.247e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.682 (1.682)  Loss:  1.2026 (1.2026)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.1252 (1.8496)  Acc@1: 80.3066 (63.9940)  Acc@5: 94.5755 (85.8000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-122.pth.tar', 63.632000141601566)

Train: 127 [   0/1251 (  0%)]  Loss: 4.267 (4.27)  Time: 1.811s,  565.29/s  (1.811s,  565.29/s)  LR: 6.196e-04  Data: 1.687 (1.687)
Train: 127 [  50/1251 (  4%)]  Loss: 4.265 (4.27)  Time: 0.155s, 6606.72/s  (0.230s, 4458.57/s)  LR: 6.196e-04  Data: 0.025 (0.075)
Train: 127 [ 100/1251 (  8%)]  Loss: 4.182 (4.24)  Time: 0.179s, 5728.79/s  (0.209s, 4902.47/s)  LR: 6.196e-04  Data: 0.021 (0.052)
Train: 127 [ 150/1251 ( 12%)]  Loss: 4.154 (4.22)  Time: 0.179s, 5736.48/s  (0.202s, 5063.70/s)  LR: 6.196e-04  Data: 0.033 (0.044)
Train: 127 [ 200/1251 ( 16%)]  Loss: 4.247 (4.22)  Time: 0.173s, 5920.03/s  (0.200s, 5129.73/s)  LR: 6.196e-04  Data: 0.028 (0.041)
Train: 127 [ 250/1251 ( 20%)]  Loss: 4.444 (4.26)  Time: 0.180s, 5685.75/s  (0.197s, 5188.21/s)  LR: 6.196e-04  Data: 0.029 (0.038)
Train: 127 [ 300/1251 ( 24%)]  Loss: 4.022 (4.23)  Time: 0.165s, 6202.57/s  (0.196s, 5212.80/s)  LR: 6.196e-04  Data: 0.029 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 127 [ 350/1251 ( 28%)]  Loss: 4.461 (4.26)  Time: 0.194s, 5288.80/s  (0.195s, 5263.46/s)  LR: 6.196e-04  Data: 0.025 (0.035)
Train: 127 [ 400/1251 ( 32%)]  Loss: 3.909 (4.22)  Time: 0.156s, 6558.38/s  (0.194s, 5269.07/s)  LR: 6.196e-04  Data: 0.030 (0.034)
Train: 127 [ 450/1251 ( 36%)]  Loss: 4.171 (4.21)  Time: 0.169s, 6043.78/s  (0.194s, 5285.00/s)  LR: 6.196e-04  Data: 0.040 (0.033)
Train: 127 [ 500/1251 ( 40%)]  Loss: 4.440 (4.23)  Time: 0.175s, 5856.89/s  (0.194s, 5289.29/s)  LR: 6.196e-04  Data: 0.028 (0.033)
Train: 127 [ 550/1251 ( 44%)]  Loss: 4.333 (4.24)  Time: 0.168s, 6080.76/s  (0.193s, 5308.00/s)  LR: 6.196e-04  Data: 0.021 (0.033)
Train: 127 [ 600/1251 ( 48%)]  Loss: 4.341 (4.25)  Time: 0.168s, 6082.27/s  (0.193s, 5317.31/s)  LR: 6.196e-04  Data: 0.028 (0.032)
Train: 127 [ 650/1251 ( 52%)]  Loss: 4.097 (4.24)  Time: 0.238s, 4307.37/s  (0.193s, 5310.05/s)  LR: 6.196e-04  Data: 0.104 (0.032)
Train: 127 [ 700/1251 ( 56%)]  Loss: 3.972 (4.22)  Time: 0.175s, 5847.82/s  (0.193s, 5313.41/s)  LR: 6.196e-04  Data: 0.027 (0.032)
Train: 127 [ 750/1251 ( 60%)]  Loss: 4.080 (4.21)  Time: 0.186s, 5520.01/s  (0.193s, 5310.73/s)  LR: 6.196e-04  Data: 0.026 (0.033)
Train: 127 [ 800/1251 ( 64%)]  Loss: 4.279 (4.22)  Time: 0.172s, 5950.83/s  (0.192s, 5320.67/s)  LR: 6.196e-04  Data: 0.025 (0.034)
Train: 127 [ 850/1251 ( 68%)]  Loss: 4.488 (4.23)  Time: 0.191s, 5351.67/s  (0.193s, 5311.10/s)  LR: 6.196e-04  Data: 0.031 (0.035)
Train: 127 [ 900/1251 ( 72%)]  Loss: 3.776 (4.21)  Time: 0.172s, 5967.92/s  (0.193s, 5310.80/s)  LR: 6.196e-04  Data: 0.025 (0.036)
Train: 127 [ 950/1251 ( 76%)]  Loss: 4.205 (4.21)  Time: 0.150s, 6830.93/s  (0.193s, 5311.86/s)  LR: 6.196e-04  Data: 0.029 (0.036)
Train: 127 [1000/1251 ( 80%)]  Loss: 4.166 (4.20)  Time: 0.170s, 6021.85/s  (0.193s, 5311.18/s)  LR: 6.196e-04  Data: 0.028 (0.036)
Train: 127 [1050/1251 ( 84%)]  Loss: 4.174 (4.20)  Time: 0.154s, 6639.40/s  (0.193s, 5311.85/s)  LR: 6.196e-04  Data: 0.030 (0.037)
Train: 127 [1100/1251 ( 88%)]  Loss: 4.539 (4.22)  Time: 0.187s, 5477.69/s  (0.193s, 5309.31/s)  LR: 6.196e-04  Data: 0.020 (0.037)
Train: 127 [1150/1251 ( 92%)]  Loss: 4.335 (4.22)  Time: 0.164s, 6236.94/s  (0.193s, 5299.30/s)  LR: 6.196e-04  Data: 0.031 (0.037)
Train: 127 [1200/1251 ( 96%)]  Loss: 4.566 (4.24)  Time: 0.174s, 5882.51/s  (0.193s, 5300.79/s)  LR: 6.196e-04  Data: 0.030 (0.038)
Train: 127 [1250/1251 (100%)]  Loss: 4.470 (4.25)  Time: 0.113s, 9026.93/s  (0.193s, 5312.57/s)  LR: 6.196e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.837 (1.837)  Loss:  1.0893 (1.0893)  Acc@1: 81.0547 (81.0547)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3381 (1.8411)  Acc@1: 78.5377 (64.2780)  Acc@5: 93.0425 (85.7900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-114.pth.tar', 63.634000185546874)

Train: 128 [   0/1251 (  0%)]  Loss: 4.216 (4.22)  Time: 1.882s,  544.15/s  (1.882s,  544.15/s)  LR: 6.146e-04  Data: 1.761 (1.761)
Train: 128 [  50/1251 (  4%)]  Loss: 3.846 (4.03)  Time: 0.170s, 6010.98/s  (0.226s, 4525.56/s)  LR: 6.146e-04  Data: 0.025 (0.070)
Train: 128 [ 100/1251 (  8%)]  Loss: 4.260 (4.11)  Time: 0.174s, 5870.71/s  (0.206s, 4963.51/s)  LR: 6.146e-04  Data: 0.021 (0.051)
Train: 128 [ 150/1251 ( 12%)]  Loss: 3.975 (4.07)  Time: 0.163s, 6277.57/s  (0.201s, 5092.58/s)  LR: 6.146e-04  Data: 0.028 (0.045)
Train: 128 [ 200/1251 ( 16%)]  Loss: 4.615 (4.18)  Time: 0.275s, 3727.04/s  (0.200s, 5121.89/s)  LR: 6.146e-04  Data: 0.026 (0.041)
Train: 128 [ 250/1251 ( 20%)]  Loss: 4.325 (4.21)  Time: 0.180s, 5694.82/s  (0.197s, 5192.41/s)  LR: 6.146e-04  Data: 0.020 (0.040)
Train: 128 [ 300/1251 ( 24%)]  Loss: 4.389 (4.23)  Time: 0.189s, 5421.00/s  (0.195s, 5252.75/s)  LR: 6.146e-04  Data: 0.025 (0.040)
Train: 128 [ 350/1251 ( 28%)]  Loss: 3.877 (4.19)  Time: 0.162s, 6301.61/s  (0.194s, 5278.37/s)  LR: 6.146e-04  Data: 0.031 (0.039)
Train: 128 [ 400/1251 ( 32%)]  Loss: 4.158 (4.18)  Time: 0.155s, 6603.86/s  (0.194s, 5279.25/s)  LR: 6.146e-04  Data: 0.026 (0.038)
Train: 128 [ 450/1251 ( 36%)]  Loss: 4.505 (4.22)  Time: 0.160s, 6388.95/s  (0.193s, 5297.06/s)  LR: 6.146e-04  Data: 0.031 (0.037)
Train: 128 [ 500/1251 ( 40%)]  Loss: 4.298 (4.22)  Time: 0.166s, 6159.64/s  (0.193s, 5296.55/s)  LR: 6.146e-04  Data: 0.030 (0.036)
Train: 128 [ 550/1251 ( 44%)]  Loss: 4.371 (4.24)  Time: 0.168s, 6087.36/s  (0.193s, 5307.51/s)  LR: 6.146e-04  Data: 0.030 (0.035)
Train: 128 [ 600/1251 ( 48%)]  Loss: 4.506 (4.26)  Time: 0.157s, 6518.99/s  (0.193s, 5308.93/s)  LR: 6.146e-04  Data: 0.025 (0.035)
Train: 128 [ 650/1251 ( 52%)]  Loss: 4.133 (4.25)  Time: 0.178s, 5750.14/s  (0.193s, 5318.35/s)  LR: 6.146e-04  Data: 0.025 (0.034)
Train: 128 [ 700/1251 ( 56%)]  Loss: 4.492 (4.26)  Time: 0.190s, 5403.40/s  (0.193s, 5310.18/s)  LR: 6.146e-04  Data: 0.024 (0.034)
Train: 128 [ 750/1251 ( 60%)]  Loss: 4.086 (4.25)  Time: 0.509s, 2013.75/s  (0.193s, 5314.50/s)  LR: 6.146e-04  Data: 0.031 (0.033)
Train: 128 [ 800/1251 ( 64%)]  Loss: 4.221 (4.25)  Time: 0.170s, 6037.17/s  (0.193s, 5312.94/s)  LR: 6.146e-04  Data: 0.025 (0.033)
Train: 128 [ 850/1251 ( 68%)]  Loss: 4.234 (4.25)  Time: 0.164s, 6260.28/s  (0.193s, 5306.45/s)  LR: 6.146e-04  Data: 0.025 (0.033)
Train: 128 [ 900/1251 ( 72%)]  Loss: 4.003 (4.24)  Time: 0.150s, 6824.94/s  (0.193s, 5311.67/s)  LR: 6.146e-04  Data: 0.026 (0.032)
Train: 128 [ 950/1251 ( 76%)]  Loss: 3.933 (4.22)  Time: 0.255s, 4011.59/s  (0.193s, 5308.21/s)  LR: 6.146e-04  Data: 0.025 (0.032)
Train: 128 [1000/1251 ( 80%)]  Loss: 4.173 (4.22)  Time: 0.166s, 6170.04/s  (0.193s, 5313.12/s)  LR: 6.146e-04  Data: 0.029 (0.032)
Train: 128 [1050/1251 ( 84%)]  Loss: 4.105 (4.21)  Time: 0.164s, 6241.61/s  (0.193s, 5311.92/s)  LR: 6.146e-04  Data: 0.027 (0.032)
Train: 128 [1100/1251 ( 88%)]  Loss: 3.944 (4.20)  Time: 0.185s, 5530.55/s  (0.193s, 5306.38/s)  LR: 6.146e-04  Data: 0.032 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 128 [1150/1251 ( 92%)]  Loss: 4.121 (4.20)  Time: 0.173s, 5915.79/s  (0.193s, 5302.79/s)  LR: 6.146e-04  Data: 0.026 (0.031)
Train: 128 [1200/1251 ( 96%)]  Loss: 3.982 (4.19)  Time: 0.177s, 5792.88/s  (0.193s, 5299.13/s)  LR: 6.146e-04  Data: 0.032 (0.031)
Train: 128 [1250/1251 (100%)]  Loss: 3.947 (4.18)  Time: 0.114s, 8999.16/s  (0.193s, 5309.54/s)  LR: 6.146e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.835 (1.835)  Loss:  1.2162 (1.2162)  Acc@1: 78.6133 (78.6133)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1119 (1.8478)  Acc@1: 80.5425 (63.4080)  Acc@5: 94.4576 (85.2500)
Train: 129 [   0/1251 (  0%)]  Loss: 4.153 (4.15)  Time: 1.793s,  571.02/s  (1.793s,  571.02/s)  LR: 6.095e-04  Data: 1.665 (1.665)
Train: 129 [  50/1251 (  4%)]  Loss: 4.307 (4.23)  Time: 0.182s, 5638.41/s  (0.231s, 4439.33/s)  LR: 6.095e-04  Data: 0.026 (0.072)
Train: 129 [ 100/1251 (  8%)]  Loss: 4.329 (4.26)  Time: 0.165s, 6198.64/s  (0.209s, 4906.86/s)  LR: 6.095e-04  Data: 0.032 (0.050)
Train: 129 [ 150/1251 ( 12%)]  Loss: 4.128 (4.23)  Time: 0.164s, 6231.95/s  (0.202s, 5081.71/s)  LR: 6.095e-04  Data: 0.026 (0.043)
Train: 129 [ 200/1251 ( 16%)]  Loss: 3.849 (4.15)  Time: 0.169s, 6067.35/s  (0.199s, 5143.60/s)  LR: 6.095e-04  Data: 0.030 (0.039)
Train: 129 [ 250/1251 ( 20%)]  Loss: 4.218 (4.16)  Time: 0.221s, 4636.54/s  (0.197s, 5204.46/s)  LR: 6.095e-04  Data: 0.025 (0.037)
Train: 129 [ 300/1251 ( 24%)]  Loss: 4.194 (4.17)  Time: 0.172s, 5963.18/s  (0.195s, 5256.41/s)  LR: 6.095e-04  Data: 0.026 (0.035)
Train: 129 [ 350/1251 ( 28%)]  Loss: 4.284 (4.18)  Time: 0.168s, 6090.40/s  (0.194s, 5273.20/s)  LR: 6.095e-04  Data: 0.034 (0.034)
Train: 129 [ 400/1251 ( 32%)]  Loss: 4.354 (4.20)  Time: 0.166s, 6168.64/s  (0.193s, 5310.83/s)  LR: 6.095e-04  Data: 0.024 (0.034)
Train: 129 [ 450/1251 ( 36%)]  Loss: 3.896 (4.17)  Time: 0.246s, 4154.95/s  (0.194s, 5286.29/s)  LR: 6.095e-04  Data: 0.031 (0.033)
Train: 129 [ 500/1251 ( 40%)]  Loss: 4.257 (4.18)  Time: 0.157s, 6511.31/s  (0.194s, 5288.22/s)  LR: 6.095e-04  Data: 0.024 (0.033)
Train: 129 [ 550/1251 ( 44%)]  Loss: 4.006 (4.16)  Time: 0.176s, 5805.58/s  (0.193s, 5293.71/s)  LR: 6.095e-04  Data: 0.036 (0.033)
Train: 129 [ 600/1251 ( 48%)]  Loss: 4.736 (4.21)  Time: 0.173s, 5919.10/s  (0.193s, 5319.24/s)  LR: 6.095e-04  Data: 0.025 (0.033)
Train: 129 [ 650/1251 ( 52%)]  Loss: 4.238 (4.21)  Time: 0.333s, 3071.92/s  (0.192s, 5327.29/s)  LR: 6.095e-04  Data: 0.028 (0.032)
Train: 129 [ 700/1251 ( 56%)]  Loss: 4.208 (4.21)  Time: 0.171s, 5996.40/s  (0.193s, 5317.39/s)  LR: 6.095e-04  Data: 0.026 (0.032)
Train: 129 [ 750/1251 ( 60%)]  Loss: 4.450 (4.23)  Time: 0.180s, 5679.46/s  (0.192s, 5333.17/s)  LR: 6.095e-04  Data: 0.035 (0.032)
Train: 129 [ 800/1251 ( 64%)]  Loss: 4.211 (4.22)  Time: 0.179s, 5711.20/s  (0.192s, 5322.69/s)  LR: 6.095e-04  Data: 0.033 (0.032)
Train: 129 [ 850/1251 ( 68%)]  Loss: 4.545 (4.24)  Time: 0.164s, 6261.23/s  (0.193s, 5316.69/s)  LR: 6.095e-04  Data: 0.029 (0.031)
Train: 129 [ 900/1251 ( 72%)]  Loss: 4.105 (4.24)  Time: 0.165s, 6207.64/s  (0.192s, 5329.61/s)  LR: 6.095e-04  Data: 0.028 (0.031)
Train: 129 [ 950/1251 ( 76%)]  Loss: 4.416 (4.24)  Time: 0.184s, 5557.03/s  (0.192s, 5322.05/s)  LR: 6.095e-04  Data: 0.037 (0.031)
Train: 129 [1000/1251 ( 80%)]  Loss: 4.174 (4.24)  Time: 0.169s, 6061.48/s  (0.193s, 5310.73/s)  LR: 6.095e-04  Data: 0.024 (0.031)
Train: 129 [1050/1251 ( 84%)]  Loss: 4.235 (4.24)  Time: 0.163s, 6272.22/s  (0.193s, 5297.92/s)  LR: 6.095e-04  Data: 0.031 (0.031)
Train: 129 [1100/1251 ( 88%)]  Loss: 4.629 (4.26)  Time: 0.161s, 6372.44/s  (0.193s, 5302.81/s)  LR: 6.095e-04  Data: 0.032 (0.031)
Train: 129 [1150/1251 ( 92%)]  Loss: 4.272 (4.26)  Time: 0.167s, 6141.96/s  (0.193s, 5295.40/s)  LR: 6.095e-04  Data: 0.026 (0.031)
Train: 129 [1200/1251 ( 96%)]  Loss: 4.343 (4.26)  Time: 0.200s, 5113.31/s  (0.193s, 5293.09/s)  LR: 6.095e-04  Data: 0.023 (0.031)
Train: 129 [1250/1251 (100%)]  Loss: 4.799 (4.28)  Time: 0.114s, 8994.02/s  (0.193s, 5311.77/s)  LR: 6.095e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.810 (1.810)  Loss:  1.1736 (1.1736)  Acc@1: 79.1992 (79.1992)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2552 (1.8517)  Acc@1: 79.5991 (63.9960)  Acc@5: 93.3962 (85.7220)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-119.pth.tar', 63.66799990234375)

Train: 130 [   0/1251 (  0%)]  Loss: 3.984 (3.98)  Time: 1.812s,  565.02/s  (1.812s,  565.02/s)  LR: 6.044e-04  Data: 1.675 (1.675)
Train: 130 [  50/1251 (  4%)]  Loss: 4.342 (4.16)  Time: 0.169s, 6064.32/s  (0.227s, 4512.29/s)  LR: 6.044e-04  Data: 0.024 (0.075)
Train: 130 [ 100/1251 (  8%)]  Loss: 4.255 (4.19)  Time: 0.194s, 5265.20/s  (0.207s, 4939.68/s)  LR: 6.044e-04  Data: 0.025 (0.057)
Train: 130 [ 150/1251 ( 12%)]  Loss: 4.394 (4.24)  Time: 0.174s, 5899.29/s  (0.200s, 5126.66/s)  LR: 6.044e-04  Data: 0.026 (0.050)
Train: 130 [ 200/1251 ( 16%)]  Loss: 4.310 (4.26)  Time: 0.178s, 5761.71/s  (0.198s, 5177.30/s)  LR: 6.044e-04  Data: 0.042 (0.049)
Train: 130 [ 250/1251 ( 20%)]  Loss: 4.610 (4.32)  Time: 0.164s, 6226.01/s  (0.196s, 5227.33/s)  LR: 6.044e-04  Data: 0.027 (0.048)
Train: 130 [ 300/1251 ( 24%)]  Loss: 3.940 (4.26)  Time: 0.175s, 5864.30/s  (0.195s, 5249.32/s)  LR: 6.044e-04  Data: 0.020 (0.046)
Train: 130 [ 350/1251 ( 28%)]  Loss: 4.150 (4.25)  Time: 0.175s, 5846.82/s  (0.194s, 5272.23/s)  LR: 6.044e-04  Data: 0.021 (0.043)
Train: 130 [ 400/1251 ( 32%)]  Loss: 4.462 (4.27)  Time: 0.173s, 5904.44/s  (0.194s, 5284.91/s)  LR: 6.044e-04  Data: 0.030 (0.043)
Train: 130 [ 450/1251 ( 36%)]  Loss: 4.488 (4.29)  Time: 0.168s, 6109.52/s  (0.193s, 5297.26/s)  LR: 6.044e-04  Data: 0.030 (0.042)
Train: 130 [ 500/1251 ( 40%)]  Loss: 4.495 (4.31)  Time: 0.158s, 6501.24/s  (0.194s, 5281.74/s)  LR: 6.044e-04  Data: 0.027 (0.041)
Train: 130 [ 550/1251 ( 44%)]  Loss: 3.951 (4.28)  Time: 0.164s, 6240.57/s  (0.194s, 5284.52/s)  LR: 6.044e-04  Data: 0.026 (0.040)
Train: 130 [ 600/1251 ( 48%)]  Loss: 3.903 (4.25)  Time: 0.170s, 6028.31/s  (0.193s, 5309.48/s)  LR: 6.044e-04  Data: 0.035 (0.039)
Train: 130 [ 650/1251 ( 52%)]  Loss: 4.064 (4.24)  Time: 0.170s, 6027.20/s  (0.193s, 5312.24/s)  LR: 6.044e-04  Data: 0.023 (0.038)
Train: 130 [ 700/1251 ( 56%)]  Loss: 4.294 (4.24)  Time: 0.176s, 5834.38/s  (0.193s, 5316.49/s)  LR: 6.044e-04  Data: 0.024 (0.038)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 130 [ 750/1251 ( 60%)]  Loss: 4.508 (4.26)  Time: 0.179s, 5719.30/s  (0.192s, 5321.91/s)  LR: 6.044e-04  Data: 0.033 (0.037)
Train: 130 [ 800/1251 ( 64%)]  Loss: 4.108 (4.25)  Time: 0.163s, 6269.10/s  (0.192s, 5320.19/s)  LR: 6.044e-04  Data: 0.029 (0.037)
Train: 130 [ 850/1251 ( 68%)]  Loss: 4.084 (4.24)  Time: 0.173s, 5916.27/s  (0.193s, 5314.54/s)  LR: 6.044e-04  Data: 0.026 (0.036)
Train: 130 [ 900/1251 ( 72%)]  Loss: 4.352 (4.25)  Time: 0.178s, 5744.84/s  (0.193s, 5316.84/s)  LR: 6.044e-04  Data: 0.020 (0.036)
Train: 130 [ 950/1251 ( 76%)]  Loss: 4.364 (4.25)  Time: 0.195s, 5258.46/s  (0.193s, 5315.89/s)  LR: 6.044e-04  Data: 0.021 (0.035)
Train: 130 [1000/1251 ( 80%)]  Loss: 4.273 (4.25)  Time: 0.224s, 4575.62/s  (0.193s, 5314.53/s)  LR: 6.044e-04  Data: 0.029 (0.035)
Train: 130 [1050/1251 ( 84%)]  Loss: 4.177 (4.25)  Time: 0.289s, 3537.56/s  (0.193s, 5311.82/s)  LR: 6.044e-04  Data: 0.038 (0.034)
Train: 130 [1100/1251 ( 88%)]  Loss: 4.176 (4.25)  Time: 0.166s, 6176.91/s  (0.193s, 5304.74/s)  LR: 6.044e-04  Data: 0.024 (0.034)
Train: 130 [1150/1251 ( 92%)]  Loss: 4.216 (4.25)  Time: 0.174s, 5900.49/s  (0.193s, 5302.61/s)  LR: 6.044e-04  Data: 0.026 (0.034)
Train: 130 [1200/1251 ( 96%)]  Loss: 4.254 (4.25)  Time: 0.167s, 6137.19/s  (0.193s, 5299.39/s)  LR: 6.044e-04  Data: 0.028 (0.034)
Train: 130 [1250/1251 (100%)]  Loss: 4.175 (4.24)  Time: 0.113s, 9092.11/s  (0.193s, 5306.26/s)  LR: 6.044e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.760 (1.760)  Loss:  1.2068 (1.2068)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2569 (1.8702)  Acc@1: 79.0094 (63.7280)  Acc@5: 93.1604 (85.4460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-130.pth.tar', 63.728000061035154)

Train: 131 [   0/1251 (  0%)]  Loss: 4.153 (4.15)  Time: 1.682s,  608.76/s  (1.682s,  608.76/s)  LR: 5.992e-04  Data: 1.500 (1.500)
Train: 131 [  50/1251 (  4%)]  Loss: 4.621 (4.39)  Time: 0.164s, 6255.60/s  (0.223s, 4587.25/s)  LR: 5.992e-04  Data: 0.030 (0.065)
Train: 131 [ 100/1251 (  8%)]  Loss: 4.592 (4.45)  Time: 0.181s, 5656.62/s  (0.207s, 4941.51/s)  LR: 5.992e-04  Data: 0.034 (0.047)
Train: 131 [ 150/1251 ( 12%)]  Loss: 4.163 (4.38)  Time: 0.166s, 6179.61/s  (0.199s, 5151.37/s)  LR: 5.992e-04  Data: 0.029 (0.041)
Train: 131 [ 200/1251 ( 16%)]  Loss: 4.139 (4.33)  Time: 0.176s, 5813.21/s  (0.197s, 5208.79/s)  LR: 5.992e-04  Data: 0.030 (0.038)
Train: 131 [ 250/1251 ( 20%)]  Loss: 4.322 (4.33)  Time: 0.173s, 5911.09/s  (0.197s, 5205.78/s)  LR: 5.992e-04  Data: 0.023 (0.036)
Train: 131 [ 300/1251 ( 24%)]  Loss: 4.325 (4.33)  Time: 0.184s, 5571.82/s  (0.195s, 5258.57/s)  LR: 5.992e-04  Data: 0.033 (0.034)
Train: 131 [ 350/1251 ( 28%)]  Loss: 4.115 (4.30)  Time: 0.189s, 5426.98/s  (0.194s, 5273.68/s)  LR: 5.992e-04  Data: 0.027 (0.033)
Train: 131 [ 400/1251 ( 32%)]  Loss: 4.074 (4.28)  Time: 0.313s, 3269.29/s  (0.194s, 5280.92/s)  LR: 5.992e-04  Data: 0.043 (0.033)
Train: 131 [ 450/1251 ( 36%)]  Loss: 3.912 (4.24)  Time: 0.173s, 5934.05/s  (0.193s, 5297.41/s)  LR: 5.992e-04  Data: 0.018 (0.032)
Train: 131 [ 500/1251 ( 40%)]  Loss: 4.273 (4.24)  Time: 0.154s, 6663.30/s  (0.193s, 5314.78/s)  LR: 5.992e-04  Data: 0.030 (0.032)
Train: 131 [ 550/1251 ( 44%)]  Loss: 3.925 (4.22)  Time: 0.171s, 5986.76/s  (0.193s, 5314.60/s)  LR: 5.992e-04  Data: 0.032 (0.031)
Train: 131 [ 600/1251 ( 48%)]  Loss: 3.941 (4.20)  Time: 0.162s, 6339.85/s  (0.193s, 5312.21/s)  LR: 5.992e-04  Data: 0.026 (0.031)
Train: 131 [ 650/1251 ( 52%)]  Loss: 4.283 (4.20)  Time: 0.181s, 5650.41/s  (0.193s, 5315.77/s)  LR: 5.992e-04  Data: 0.024 (0.031)
Train: 131 [ 700/1251 ( 56%)]  Loss: 4.333 (4.21)  Time: 0.176s, 5827.01/s  (0.193s, 5303.18/s)  LR: 5.992e-04  Data: 0.025 (0.031)
Train: 131 [ 750/1251 ( 60%)]  Loss: 4.622 (4.24)  Time: 0.182s, 5611.89/s  (0.193s, 5318.61/s)  LR: 5.992e-04  Data: 0.025 (0.032)
Train: 131 [ 800/1251 ( 64%)]  Loss: 4.502 (4.25)  Time: 0.186s, 5500.25/s  (0.193s, 5318.98/s)  LR: 5.992e-04  Data: 0.021 (0.032)
Train: 131 [ 850/1251 ( 68%)]  Loss: 4.202 (4.25)  Time: 0.157s, 6522.85/s  (0.193s, 5318.28/s)  LR: 5.992e-04  Data: 0.027 (0.032)
Train: 131 [ 900/1251 ( 72%)]  Loss: 4.300 (4.25)  Time: 0.196s, 5230.00/s  (0.192s, 5328.81/s)  LR: 5.992e-04  Data: 0.019 (0.032)
Train: 131 [ 950/1251 ( 76%)]  Loss: 4.582 (4.27)  Time: 0.175s, 5867.34/s  (0.192s, 5319.79/s)  LR: 5.992e-04  Data: 0.028 (0.032)
Train: 131 [1000/1251 ( 80%)]  Loss: 4.172 (4.26)  Time: 0.245s, 4182.43/s  (0.193s, 5312.74/s)  LR: 5.992e-04  Data: 0.023 (0.032)
Train: 131 [1050/1251 ( 84%)]  Loss: 3.963 (4.25)  Time: 0.179s, 5707.30/s  (0.193s, 5314.80/s)  LR: 5.992e-04  Data: 0.023 (0.032)
Train: 131 [1100/1251 ( 88%)]  Loss: 4.304 (4.25)  Time: 0.184s, 5570.66/s  (0.193s, 5310.65/s)  LR: 5.992e-04  Data: 0.028 (0.032)
Train: 131 [1150/1251 ( 92%)]  Loss: 4.432 (4.26)  Time: 0.170s, 6021.91/s  (0.193s, 5313.18/s)  LR: 5.992e-04  Data: 0.030 (0.032)
Train: 131 [1200/1251 ( 96%)]  Loss: 4.561 (4.27)  Time: 0.446s, 2298.22/s  (0.193s, 5312.45/s)  LR: 5.992e-04  Data: 0.029 (0.032)
Train: 131 [1250/1251 (100%)]  Loss: 4.349 (4.28)  Time: 0.114s, 9018.53/s  (0.192s, 5323.41/s)  LR: 5.992e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.1848 (1.1848)  Acc@1: 80.3711 (80.3711)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1820 (1.7916)  Acc@1: 80.3066 (64.4800)  Acc@5: 93.8679 (86.1380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-116.pth.tar', 63.78400013427734)

Train: 132 [   0/1251 (  0%)]  Loss: 4.341 (4.34)  Time: 1.802s,  568.14/s  (1.802s,  568.14/s)  LR: 5.941e-04  Data: 1.673 (1.673)
Train: 132 [  50/1251 (  4%)]  Loss: 4.507 (4.42)  Time: 0.162s, 6317.18/s  (0.222s, 4621.70/s)  LR: 5.941e-04  Data: 0.020 (0.077)
Train: 132 [ 100/1251 (  8%)]  Loss: 4.174 (4.34)  Time: 0.188s, 5446.40/s  (0.207s, 4953.65/s)  LR: 5.941e-04  Data: 0.027 (0.052)
Train: 132 [ 150/1251 ( 12%)]  Loss: 4.827 (4.46)  Time: 0.162s, 6305.14/s  (0.199s, 5147.27/s)  LR: 5.941e-04  Data: 0.029 (0.045)
Train: 132 [ 200/1251 ( 16%)]  Loss: 4.726 (4.51)  Time: 0.196s, 5234.16/s  (0.196s, 5213.01/s)  LR: 5.941e-04  Data: 0.022 (0.040)
Train: 132 [ 250/1251 ( 20%)]  Loss: 4.499 (4.51)  Time: 0.170s, 6023.75/s  (0.197s, 5194.34/s)  LR: 5.941e-04  Data: 0.026 (0.038)
Train: 132 [ 300/1251 ( 24%)]  Loss: 4.443 (4.50)  Time: 0.299s, 3424.45/s  (0.196s, 5223.14/s)  LR: 5.941e-04  Data: 0.032 (0.036)
Train: 132 [ 350/1251 ( 28%)]  Loss: 4.672 (4.52)  Time: 0.182s, 5613.68/s  (0.195s, 5261.04/s)  LR: 5.941e-04  Data: 0.025 (0.035)
Train: 132 [ 400/1251 ( 32%)]  Loss: 4.037 (4.47)  Time: 0.160s, 6404.68/s  (0.195s, 5262.72/s)  LR: 5.941e-04  Data: 0.026 (0.034)
Train: 132 [ 450/1251 ( 36%)]  Loss: 4.069 (4.43)  Time: 0.157s, 6506.78/s  (0.193s, 5293.39/s)  LR: 5.941e-04  Data: 0.031 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 132 [ 500/1251 ( 40%)]  Loss: 4.647 (4.45)  Time: 0.407s, 2516.66/s  (0.193s, 5292.68/s)  LR: 5.941e-04  Data: 0.025 (0.033)
Train: 132 [ 550/1251 ( 44%)]  Loss: 4.298 (4.44)  Time: 0.185s, 5543.25/s  (0.193s, 5310.78/s)  LR: 5.941e-04  Data: 0.026 (0.033)
Train: 132 [ 600/1251 ( 48%)]  Loss: 4.563 (4.45)  Time: 0.197s, 5205.02/s  (0.192s, 5320.72/s)  LR: 5.941e-04  Data: 0.024 (0.032)
Train: 132 [ 650/1251 ( 52%)]  Loss: 4.382 (4.44)  Time: 0.162s, 6324.96/s  (0.193s, 5313.76/s)  LR: 5.941e-04  Data: 0.029 (0.032)
Train: 132 [ 700/1251 ( 56%)]  Loss: 4.145 (4.42)  Time: 0.182s, 5629.81/s  (0.193s, 5305.84/s)  LR: 5.941e-04  Data: 0.030 (0.031)
Train: 132 [ 750/1251 ( 60%)]  Loss: 4.541 (4.43)  Time: 0.189s, 5423.99/s  (0.193s, 5319.07/s)  LR: 5.941e-04  Data: 0.026 (0.031)
Train: 132 [ 800/1251 ( 64%)]  Loss: 4.238 (4.42)  Time: 0.173s, 5926.18/s  (0.192s, 5322.21/s)  LR: 5.941e-04  Data: 0.035 (0.031)
Train: 132 [ 850/1251 ( 68%)]  Loss: 4.246 (4.41)  Time: 0.173s, 5931.20/s  (0.192s, 5324.89/s)  LR: 5.941e-04  Data: 0.025 (0.031)
Train: 132 [ 900/1251 ( 72%)]  Loss: 4.573 (4.42)  Time: 0.163s, 6263.92/s  (0.193s, 5313.18/s)  LR: 5.941e-04  Data: 0.023 (0.031)
Train: 132 [ 950/1251 ( 76%)]  Loss: 4.217 (4.41)  Time: 0.216s, 4743.88/s  (0.193s, 5312.53/s)  LR: 5.941e-04  Data: 0.028 (0.031)
Train: 132 [1000/1251 ( 80%)]  Loss: 4.216 (4.40)  Time: 0.207s, 4952.57/s  (0.193s, 5310.64/s)  LR: 5.941e-04  Data: 0.033 (0.031)
Train: 132 [1050/1251 ( 84%)]  Loss: 4.119 (4.39)  Time: 0.157s, 6521.47/s  (0.193s, 5311.64/s)  LR: 5.941e-04  Data: 0.025 (0.031)
Train: 132 [1100/1251 ( 88%)]  Loss: 4.268 (4.38)  Time: 0.369s, 2778.00/s  (0.193s, 5312.11/s)  LR: 5.941e-04  Data: 0.021 (0.030)
Train: 132 [1150/1251 ( 92%)]  Loss: 4.275 (4.38)  Time: 0.179s, 5729.77/s  (0.193s, 5317.10/s)  LR: 5.941e-04  Data: 0.019 (0.030)
Train: 132 [1200/1251 ( 96%)]  Loss: 4.123 (4.37)  Time: 0.179s, 5705.51/s  (0.193s, 5307.73/s)  LR: 5.941e-04  Data: 0.021 (0.030)
Train: 132 [1250/1251 (100%)]  Loss: 4.366 (4.37)  Time: 0.114s, 9007.28/s  (0.193s, 5319.31/s)  LR: 5.941e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.855 (1.855)  Loss:  1.1446 (1.1446)  Acc@1: 80.9570 (80.9570)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.2282 (1.7898)  Acc@1: 80.3066 (64.8760)  Acc@5: 92.2170 (86.1460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-121.pth.tar', 63.80800000976563)

Train: 133 [   0/1251 (  0%)]  Loss: 4.459 (4.46)  Time: 1.770s,  578.47/s  (1.770s,  578.47/s)  LR: 5.890e-04  Data: 1.622 (1.622)
Train: 133 [  50/1251 (  4%)]  Loss: 3.943 (4.20)  Time: 0.166s, 6181.94/s  (0.225s, 4554.53/s)  LR: 5.890e-04  Data: 0.030 (0.079)
Train: 133 [ 100/1251 (  8%)]  Loss: 4.308 (4.24)  Time: 0.190s, 5382.51/s  (0.209s, 4897.60/s)  LR: 5.890e-04  Data: 0.030 (0.064)
Train: 133 [ 150/1251 ( 12%)]  Loss: 3.989 (4.17)  Time: 0.159s, 6451.31/s  (0.202s, 5078.51/s)  LR: 5.890e-04  Data: 0.025 (0.057)
Train: 133 [ 200/1251 ( 16%)]  Loss: 3.768 (4.09)  Time: 0.163s, 6301.21/s  (0.200s, 5114.72/s)  LR: 5.890e-04  Data: 0.041 (0.052)
Train: 133 [ 250/1251 ( 20%)]  Loss: 4.441 (4.15)  Time: 0.163s, 6279.37/s  (0.197s, 5196.04/s)  LR: 5.890e-04  Data: 0.025 (0.047)
Train: 133 [ 300/1251 ( 24%)]  Loss: 4.452 (4.19)  Time: 0.162s, 6335.66/s  (0.195s, 5263.30/s)  LR: 5.890e-04  Data: 0.025 (0.045)
Train: 133 [ 350/1251 ( 28%)]  Loss: 4.191 (4.19)  Time: 0.167s, 6144.07/s  (0.194s, 5277.87/s)  LR: 5.890e-04  Data: 0.028 (0.042)
Train: 133 [ 400/1251 ( 32%)]  Loss: 4.194 (4.19)  Time: 0.183s, 5594.50/s  (0.193s, 5295.54/s)  LR: 5.890e-04  Data: 0.036 (0.042)
Train: 133 [ 450/1251 ( 36%)]  Loss: 4.456 (4.22)  Time: 0.164s, 6256.66/s  (0.193s, 5297.17/s)  LR: 5.890e-04  Data: 0.024 (0.042)
Train: 133 [ 500/1251 ( 40%)]  Loss: 4.446 (4.24)  Time: 0.190s, 5398.87/s  (0.193s, 5295.57/s)  LR: 5.890e-04  Data: 0.023 (0.041)
Train: 133 [ 550/1251 ( 44%)]  Loss: 4.126 (4.23)  Time: 0.166s, 6180.84/s  (0.193s, 5294.04/s)  LR: 5.890e-04  Data: 0.025 (0.042)
Train: 133 [ 600/1251 ( 48%)]  Loss: 4.311 (4.24)  Time: 0.278s, 3683.09/s  (0.194s, 5290.92/s)  LR: 5.890e-04  Data: 0.150 (0.042)
Train: 133 [ 650/1251 ( 52%)]  Loss: 4.352 (4.25)  Time: 0.187s, 5490.19/s  (0.193s, 5298.32/s)  LR: 5.890e-04  Data: 0.024 (0.042)
Train: 133 [ 700/1251 ( 56%)]  Loss: 4.149 (4.24)  Time: 0.171s, 5986.38/s  (0.193s, 5311.58/s)  LR: 5.890e-04  Data: 0.026 (0.042)
Train: 133 [ 750/1251 ( 60%)]  Loss: 3.997 (4.22)  Time: 0.169s, 6074.84/s  (0.193s, 5312.40/s)  LR: 5.890e-04  Data: 0.027 (0.043)
Train: 133 [ 800/1251 ( 64%)]  Loss: 4.107 (4.22)  Time: 0.261s, 3919.82/s  (0.193s, 5307.82/s)  LR: 5.890e-04  Data: 0.127 (0.043)
Train: 133 [ 850/1251 ( 68%)]  Loss: 4.273 (4.22)  Time: 0.195s, 5255.84/s  (0.193s, 5310.49/s)  LR: 5.890e-04  Data: 0.030 (0.043)
Train: 133 [ 900/1251 ( 72%)]  Loss: 4.026 (4.21)  Time: 0.165s, 6211.00/s  (0.193s, 5306.35/s)  LR: 5.890e-04  Data: 0.026 (0.044)
Train: 133 [ 950/1251 ( 76%)]  Loss: 4.538 (4.23)  Time: 0.185s, 5533.42/s  (0.193s, 5318.11/s)  LR: 5.890e-04  Data: 0.025 (0.043)
Train: 133 [1000/1251 ( 80%)]  Loss: 4.446 (4.24)  Time: 0.356s, 2880.06/s  (0.193s, 5307.37/s)  LR: 5.890e-04  Data: 0.225 (0.044)
Train: 133 [1050/1251 ( 84%)]  Loss: 4.185 (4.23)  Time: 0.160s, 6417.49/s  (0.193s, 5308.92/s)  LR: 5.890e-04  Data: 0.026 (0.044)
Train: 133 [1100/1251 ( 88%)]  Loss: 4.349 (4.24)  Time: 0.161s, 6363.66/s  (0.193s, 5315.13/s)  LR: 5.890e-04  Data: 0.018 (0.044)
Train: 133 [1150/1251 ( 92%)]  Loss: 4.442 (4.25)  Time: 0.165s, 6195.80/s  (0.193s, 5318.19/s)  LR: 5.890e-04  Data: 0.018 (0.044)
Train: 133 [1200/1251 ( 96%)]  Loss: 4.134 (4.24)  Time: 0.317s, 3235.19/s  (0.193s, 5315.48/s)  LR: 5.890e-04  Data: 0.188 (0.044)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 133 [1250/1251 (100%)]  Loss: 4.519 (4.25)  Time: 0.114s, 9007.18/s  (0.192s, 5328.38/s)  LR: 5.890e-04  Data: 0.000 (0.044)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.751 (1.751)  Loss:  1.1406 (1.1406)  Acc@1: 81.4453 (81.4453)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.2081 (1.8094)  Acc@1: 78.3019 (64.2620)  Acc@5: 93.6321 (85.8680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-120.pth.tar', 63.896000036621096)

Train: 134 [   0/1251 (  0%)]  Loss: 4.168 (4.17)  Time: 1.717s,  596.50/s  (1.717s,  596.50/s)  LR: 5.838e-04  Data: 1.600 (1.600)
Train: 134 [  50/1251 (  4%)]  Loss: 3.847 (4.01)  Time: 0.174s, 5888.21/s  (0.225s, 4544.84/s)  LR: 5.838e-04  Data: 0.021 (0.075)
Train: 134 [ 100/1251 (  8%)]  Loss: 4.359 (4.12)  Time: 0.180s, 5693.71/s  (0.206s, 4960.11/s)  LR: 5.838e-04  Data: 0.031 (0.051)
Train: 134 [ 150/1251 ( 12%)]  Loss: 4.179 (4.14)  Time: 0.159s, 6431.04/s  (0.200s, 5128.07/s)  LR: 5.838e-04  Data: 0.028 (0.044)
Train: 134 [ 200/1251 ( 16%)]  Loss: 4.492 (4.21)  Time: 0.336s, 3044.71/s  (0.197s, 5190.36/s)  LR: 5.838e-04  Data: 0.024 (0.040)
Train: 134 [ 250/1251 ( 20%)]  Loss: 4.070 (4.19)  Time: 0.159s, 6421.55/s  (0.198s, 5180.00/s)  LR: 5.838e-04  Data: 0.024 (0.037)
Train: 134 [ 300/1251 ( 24%)]  Loss: 4.429 (4.22)  Time: 0.186s, 5517.01/s  (0.195s, 5246.78/s)  LR: 5.838e-04  Data: 0.028 (0.036)
Train: 134 [ 350/1251 ( 28%)]  Loss: 4.138 (4.21)  Time: 0.178s, 5740.57/s  (0.195s, 5262.49/s)  LR: 5.838e-04  Data: 0.020 (0.035)
Train: 134 [ 400/1251 ( 32%)]  Loss: 4.540 (4.25)  Time: 0.204s, 5018.34/s  (0.195s, 5260.87/s)  LR: 5.838e-04  Data: 0.030 (0.034)
Train: 134 [ 450/1251 ( 36%)]  Loss: 4.243 (4.25)  Time: 0.164s, 6230.77/s  (0.195s, 5262.86/s)  LR: 5.838e-04  Data: 0.033 (0.034)
Train: 134 [ 500/1251 ( 40%)]  Loss: 4.321 (4.25)  Time: 0.168s, 6097.85/s  (0.194s, 5276.47/s)  LR: 5.838e-04  Data: 0.032 (0.033)
Train: 134 [ 550/1251 ( 44%)]  Loss: 4.484 (4.27)  Time: 0.216s, 4743.09/s  (0.193s, 5299.35/s)  LR: 5.838e-04  Data: 0.033 (0.033)
Train: 134 [ 600/1251 ( 48%)]  Loss: 4.195 (4.27)  Time: 0.495s, 2067.73/s  (0.194s, 5281.47/s)  LR: 5.838e-04  Data: 0.028 (0.033)
Train: 134 [ 650/1251 ( 52%)]  Loss: 4.231 (4.26)  Time: 0.167s, 6120.37/s  (0.194s, 5288.15/s)  LR: 5.838e-04  Data: 0.029 (0.032)
Train: 134 [ 700/1251 ( 56%)]  Loss: 4.146 (4.26)  Time: 0.178s, 5753.20/s  (0.193s, 5306.25/s)  LR: 5.838e-04  Data: 0.026 (0.032)
Train: 134 [ 750/1251 ( 60%)]  Loss: 4.457 (4.27)  Time: 0.181s, 5665.68/s  (0.193s, 5307.58/s)  LR: 5.838e-04  Data: 0.034 (0.032)
Train: 134 [ 800/1251 ( 64%)]  Loss: 4.027 (4.25)  Time: 0.253s, 4050.43/s  (0.193s, 5310.33/s)  LR: 5.838e-04  Data: 0.031 (0.032)
Train: 134 [ 850/1251 ( 68%)]  Loss: 4.239 (4.25)  Time: 0.172s, 5953.76/s  (0.193s, 5306.32/s)  LR: 5.838e-04  Data: 0.025 (0.031)
Train: 134 [ 900/1251 ( 72%)]  Loss: 4.098 (4.25)  Time: 0.196s, 5219.74/s  (0.193s, 5302.20/s)  LR: 5.838e-04  Data: 0.031 (0.031)
Train: 134 [ 950/1251 ( 76%)]  Loss: 4.642 (4.27)  Time: 0.161s, 6357.42/s  (0.193s, 5301.04/s)  LR: 5.838e-04  Data: 0.027 (0.031)
Train: 134 [1000/1251 ( 80%)]  Loss: 4.054 (4.26)  Time: 0.323s, 3169.97/s  (0.193s, 5297.29/s)  LR: 5.838e-04  Data: 0.024 (0.031)
Train: 134 [1050/1251 ( 84%)]  Loss: 4.463 (4.26)  Time: 0.157s, 6522.47/s  (0.193s, 5303.86/s)  LR: 5.838e-04  Data: 0.030 (0.031)
Train: 134 [1100/1251 ( 88%)]  Loss: 4.472 (4.27)  Time: 0.175s, 5860.64/s  (0.193s, 5297.40/s)  LR: 5.838e-04  Data: 0.019 (0.031)
Train: 134 [1150/1251 ( 92%)]  Loss: 4.575 (4.29)  Time: 0.193s, 5304.80/s  (0.193s, 5300.69/s)  LR: 5.838e-04  Data: 0.028 (0.031)
Train: 134 [1200/1251 ( 96%)]  Loss: 4.016 (4.28)  Time: 0.325s, 3146.08/s  (0.193s, 5295.35/s)  LR: 5.838e-04  Data: 0.024 (0.031)
Train: 134 [1250/1251 (100%)]  Loss: 4.412 (4.28)  Time: 0.114s, 9006.92/s  (0.193s, 5311.21/s)  LR: 5.838e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.836 (1.836)  Loss:  1.2072 (1.2072)  Acc@1: 79.3945 (79.3945)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.2580 (1.8298)  Acc@1: 79.0094 (64.6860)  Acc@5: 92.9245 (86.2260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-126.pth.tar', 63.99399997802735)

Train: 135 [   0/1251 (  0%)]  Loss: 4.229 (4.23)  Time: 2.004s,  510.96/s  (2.004s,  510.96/s)  LR: 5.786e-04  Data: 1.864 (1.864)
Train: 135 [  50/1251 (  4%)]  Loss: 4.246 (4.24)  Time: 0.207s, 4958.17/s  (0.222s, 4614.21/s)  LR: 5.786e-04  Data: 0.029 (0.074)
Train: 135 [ 100/1251 (  8%)]  Loss: 4.360 (4.28)  Time: 0.163s, 6270.37/s  (0.207s, 4954.12/s)  LR: 5.786e-04  Data: 0.030 (0.060)
Train: 135 [ 150/1251 ( 12%)]  Loss: 4.179 (4.25)  Time: 0.159s, 6428.41/s  (0.203s, 5040.99/s)  LR: 5.786e-04  Data: 0.030 (0.057)
Train: 135 [ 200/1251 ( 16%)]  Loss: 4.425 (4.29)  Time: 0.178s, 5754.78/s  (0.199s, 5148.92/s)  LR: 5.786e-04  Data: 0.030 (0.052)
Train: 135 [ 250/1251 ( 20%)]  Loss: 4.336 (4.30)  Time: 0.312s, 3277.12/s  (0.197s, 5193.54/s)  LR: 5.786e-04  Data: 0.185 (0.050)
Train: 135 [ 300/1251 ( 24%)]  Loss: 4.239 (4.29)  Time: 0.176s, 5823.49/s  (0.196s, 5213.05/s)  LR: 5.786e-04  Data: 0.019 (0.048)
Train: 135 [ 350/1251 ( 28%)]  Loss: 4.133 (4.27)  Time: 0.165s, 6218.71/s  (0.196s, 5232.30/s)  LR: 5.786e-04  Data: 0.025 (0.047)
Train: 135 [ 400/1251 ( 32%)]  Loss: 4.452 (4.29)  Time: 0.183s, 5601.95/s  (0.195s, 5263.08/s)  LR: 5.786e-04  Data: 0.021 (0.046)
Train: 135 [ 450/1251 ( 36%)]  Loss: 4.517 (4.31)  Time: 0.270s, 3793.56/s  (0.194s, 5280.99/s)  LR: 5.786e-04  Data: 0.148 (0.046)
Train: 135 [ 500/1251 ( 40%)]  Loss: 4.269 (4.31)  Time: 0.162s, 6317.48/s  (0.194s, 5274.92/s)  LR: 5.786e-04  Data: 0.035 (0.046)
Train: 135 [ 550/1251 ( 44%)]  Loss: 4.475 (4.32)  Time: 0.179s, 5719.44/s  (0.194s, 5291.97/s)  LR: 5.786e-04  Data: 0.023 (0.046)
Train: 135 [ 600/1251 ( 48%)]  Loss: 4.155 (4.31)  Time: 0.166s, 6176.81/s  (0.193s, 5303.24/s)  LR: 5.786e-04  Data: 0.035 (0.045)
Train: 135 [ 650/1251 ( 52%)]  Loss: 4.123 (4.30)  Time: 0.275s, 3728.92/s  (0.193s, 5304.70/s)  LR: 5.786e-04  Data: 0.149 (0.045)
Train: 135 [ 700/1251 ( 56%)]  Loss: 4.383 (4.30)  Time: 0.198s, 5170.64/s  (0.193s, 5309.36/s)  LR: 5.786e-04  Data: 0.020 (0.045)
Train: 135 [ 750/1251 ( 60%)]  Loss: 4.502 (4.31)  Time: 0.165s, 6218.87/s  (0.193s, 5310.20/s)  LR: 5.786e-04  Data: 0.023 (0.045)
Train: 135 [ 800/1251 ( 64%)]  Loss: 4.334 (4.32)  Time: 0.146s, 7018.24/s  (0.193s, 5311.98/s)  LR: 5.786e-04  Data: 0.024 (0.045)
Train: 135 [ 850/1251 ( 68%)]  Loss: 4.509 (4.33)  Time: 0.163s, 6273.66/s  (0.193s, 5311.30/s)  LR: 5.786e-04  Data: 0.026 (0.045)
Train: 135 [ 900/1251 ( 72%)]  Loss: 4.107 (4.31)  Time: 0.177s, 5786.29/s  (0.193s, 5310.04/s)  LR: 5.786e-04  Data: 0.026 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 135 [ 950/1251 ( 76%)]  Loss: 4.002 (4.30)  Time: 0.170s, 6009.45/s  (0.193s, 5312.59/s)  LR: 5.786e-04  Data: 0.022 (0.045)
Train: 135 [1000/1251 ( 80%)]  Loss: 4.233 (4.30)  Time: 0.159s, 6446.44/s  (0.193s, 5314.40/s)  LR: 5.786e-04  Data: 0.023 (0.045)
Train: 135 [1050/1251 ( 84%)]  Loss: 4.518 (4.31)  Time: 0.205s, 4991.04/s  (0.193s, 5302.72/s)  LR: 5.786e-04  Data: 0.025 (0.046)
Train: 135 [1100/1251 ( 88%)]  Loss: 4.000 (4.29)  Time: 0.165s, 6209.45/s  (0.193s, 5312.85/s)  LR: 5.786e-04  Data: 0.023 (0.045)
Train: 135 [1150/1251 ( 92%)]  Loss: 4.288 (4.29)  Time: 0.166s, 6171.58/s  (0.193s, 5313.69/s)  LR: 5.786e-04  Data: 0.022 (0.045)
Train: 135 [1200/1251 ( 96%)]  Loss: 4.442 (4.30)  Time: 0.744s, 1376.25/s  (0.193s, 5303.80/s)  LR: 5.786e-04  Data: 0.034 (0.045)
Train: 135 [1250/1251 (100%)]  Loss: 4.256 (4.30)  Time: 0.114s, 9009.98/s  (0.193s, 5318.50/s)  LR: 5.786e-04  Data: 0.000 (0.045)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.870 (1.870)  Loss:  1.1029 (1.1029)  Acc@1: 81.7383 (81.7383)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.1767 (1.8130)  Acc@1: 79.3632 (64.6000)  Acc@5: 93.7500 (86.0840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-129.pth.tar', 63.99600005859375)

Train: 136 [   0/1251 (  0%)]  Loss: 3.913 (3.91)  Time: 1.809s,  565.91/s  (1.809s,  565.91/s)  LR: 5.735e-04  Data: 1.644 (1.644)
Train: 136 [  50/1251 (  4%)]  Loss: 4.294 (4.10)  Time: 0.168s, 6095.76/s  (0.223s, 4589.26/s)  LR: 5.735e-04  Data: 0.030 (0.079)
Train: 136 [ 100/1251 (  8%)]  Loss: 4.464 (4.22)  Time: 0.162s, 6331.69/s  (0.209s, 4907.90/s)  LR: 5.735e-04  Data: 0.029 (0.060)
Train: 136 [ 150/1251 ( 12%)]  Loss: 4.162 (4.21)  Time: 0.178s, 5759.79/s  (0.202s, 5074.18/s)  LR: 5.735e-04  Data: 0.033 (0.051)
Train: 136 [ 200/1251 ( 16%)]  Loss: 3.970 (4.16)  Time: 0.214s, 4781.28/s  (0.199s, 5153.81/s)  LR: 5.735e-04  Data: 0.025 (0.046)
Train: 136 [ 250/1251 ( 20%)]  Loss: 4.530 (4.22)  Time: 0.173s, 5921.28/s  (0.198s, 5181.10/s)  LR: 5.735e-04  Data: 0.026 (0.046)
Train: 136 [ 300/1251 ( 24%)]  Loss: 4.327 (4.24)  Time: 0.169s, 6072.93/s  (0.196s, 5236.92/s)  LR: 5.735e-04  Data: 0.028 (0.045)
Train: 136 [ 350/1251 ( 28%)]  Loss: 4.307 (4.25)  Time: 0.180s, 5684.47/s  (0.195s, 5253.70/s)  LR: 5.735e-04  Data: 0.018 (0.045)
Train: 136 [ 400/1251 ( 32%)]  Loss: 4.025 (4.22)  Time: 0.179s, 5715.14/s  (0.193s, 5317.20/s)  LR: 5.735e-04  Data: 0.049 (0.043)
Train: 136 [ 450/1251 ( 36%)]  Loss: 4.206 (4.22)  Time: 0.365s, 2807.32/s  (0.193s, 5304.94/s)  LR: 5.735e-04  Data: 0.028 (0.043)
Train: 136 [ 500/1251 ( 40%)]  Loss: 4.073 (4.21)  Time: 0.156s, 6546.84/s  (0.193s, 5303.37/s)  LR: 5.735e-04  Data: 0.028 (0.041)
Train: 136 [ 550/1251 ( 44%)]  Loss: 3.808 (4.17)  Time: 0.174s, 5897.06/s  (0.193s, 5301.51/s)  LR: 5.735e-04  Data: 0.023 (0.040)
Train: 136 [ 600/1251 ( 48%)]  Loss: 4.211 (4.18)  Time: 0.323s, 3171.12/s  (0.193s, 5314.43/s)  LR: 5.735e-04  Data: 0.036 (0.039)
Train: 136 [ 650/1251 ( 52%)]  Loss: 4.120 (4.17)  Time: 0.198s, 5177.68/s  (0.193s, 5314.82/s)  LR: 5.735e-04  Data: 0.022 (0.038)
Train: 136 [ 700/1251 ( 56%)]  Loss: 4.535 (4.20)  Time: 0.173s, 5929.60/s  (0.193s, 5309.46/s)  LR: 5.735e-04  Data: 0.032 (0.038)
Train: 136 [ 750/1251 ( 60%)]  Loss: 4.265 (4.20)  Time: 0.178s, 5750.41/s  (0.193s, 5313.26/s)  LR: 5.735e-04  Data: 0.029 (0.037)
Train: 136 [ 800/1251 ( 64%)]  Loss: 4.715 (4.23)  Time: 0.179s, 5720.25/s  (0.193s, 5312.97/s)  LR: 5.735e-04  Data: 0.026 (0.036)
Train: 136 [ 850/1251 ( 68%)]  Loss: 4.362 (4.24)  Time: 0.174s, 5888.73/s  (0.192s, 5325.20/s)  LR: 5.735e-04  Data: 0.023 (0.036)
Train: 136 [ 900/1251 ( 72%)]  Loss: 3.984 (4.22)  Time: 0.171s, 5992.62/s  (0.192s, 5323.22/s)  LR: 5.735e-04  Data: 0.040 (0.036)
Train: 136 [ 950/1251 ( 76%)]  Loss: 4.144 (4.22)  Time: 0.168s, 6085.82/s  (0.193s, 5319.07/s)  LR: 5.735e-04  Data: 0.023 (0.035)
Train: 136 [1000/1251 ( 80%)]  Loss: 4.435 (4.23)  Time: 0.189s, 5426.14/s  (0.193s, 5312.34/s)  LR: 5.735e-04  Data: 0.025 (0.035)
Train: 136 [1050/1251 ( 84%)]  Loss: 4.392 (4.24)  Time: 0.169s, 6070.66/s  (0.193s, 5315.96/s)  LR: 5.735e-04  Data: 0.031 (0.034)
Train: 136 [1100/1251 ( 88%)]  Loss: 4.146 (4.23)  Time: 0.175s, 5836.16/s  (0.193s, 5311.19/s)  LR: 5.735e-04  Data: 0.043 (0.034)
Train: 136 [1150/1251 ( 92%)]  Loss: 4.098 (4.23)  Time: 0.166s, 6162.33/s  (0.193s, 5314.23/s)  LR: 5.735e-04  Data: 0.026 (0.034)
Train: 136 [1200/1251 ( 96%)]  Loss: 3.738 (4.21)  Time: 0.428s, 2393.58/s  (0.193s, 5308.44/s)  LR: 5.735e-04  Data: 0.028 (0.034)
Train: 136 [1250/1251 (100%)]  Loss: 4.180 (4.21)  Time: 0.114s, 9010.28/s  (0.192s, 5320.03/s)  LR: 5.735e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.790 (1.790)  Loss:  1.0798 (1.0798)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1403 (1.7679)  Acc@1: 78.0660 (64.0440)  Acc@5: 92.6887 (85.7380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-136.pth.tar', 64.04400016845703)

Train: 137 [   0/1251 (  0%)]  Loss: 4.395 (4.39)  Time: 1.630s,  628.36/s  (1.630s,  628.36/s)  LR: 5.683e-04  Data: 1.508 (1.508)
Train: 137 [  50/1251 (  4%)]  Loss: 4.279 (4.34)  Time: 0.185s, 5530.78/s  (0.223s, 4590.71/s)  LR: 5.683e-04  Data: 0.022 (0.057)
Train: 137 [ 100/1251 (  8%)]  Loss: 4.668 (4.45)  Time: 0.178s, 5756.73/s  (0.206s, 4973.23/s)  LR: 5.683e-04  Data: 0.028 (0.042)
Train: 137 [ 150/1251 ( 12%)]  Loss: 4.286 (4.41)  Time: 0.157s, 6508.13/s  (0.201s, 5107.10/s)  LR: 5.683e-04  Data: 0.025 (0.038)
Train: 137 [ 200/1251 ( 16%)]  Loss: 3.891 (4.30)  Time: 0.163s, 6300.62/s  (0.198s, 5180.89/s)  LR: 5.683e-04  Data: 0.026 (0.036)
Train: 137 [ 250/1251 ( 20%)]  Loss: 4.472 (4.33)  Time: 0.168s, 6087.66/s  (0.195s, 5246.14/s)  LR: 5.683e-04  Data: 0.028 (0.035)
Train: 137 [ 300/1251 ( 24%)]  Loss: 4.362 (4.34)  Time: 0.185s, 5527.98/s  (0.196s, 5233.45/s)  LR: 5.683e-04  Data: 0.023 (0.033)
Train: 137 [ 350/1251 ( 28%)]  Loss: 4.426 (4.35)  Time: 0.223s, 4599.36/s  (0.194s, 5282.56/s)  LR: 5.683e-04  Data: 0.027 (0.033)
Train: 137 [ 400/1251 ( 32%)]  Loss: 4.324 (4.34)  Time: 0.179s, 5705.39/s  (0.193s, 5298.45/s)  LR: 5.683e-04  Data: 0.032 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 137 [ 450/1251 ( 36%)]  Loss: 4.204 (4.33)  Time: 0.166s, 6186.81/s  (0.193s, 5306.61/s)  LR: 5.683e-04  Data: 0.027 (0.032)
Train: 137 [ 500/1251 ( 40%)]  Loss: 4.397 (4.34)  Time: 0.153s, 6675.85/s  (0.192s, 5324.16/s)  LR: 5.683e-04  Data: 0.028 (0.032)
Train: 137 [ 550/1251 ( 44%)]  Loss: 4.329 (4.34)  Time: 0.181s, 5655.96/s  (0.193s, 5314.23/s)  LR: 5.683e-04  Data: 0.026 (0.031)
Train: 137 [ 600/1251 ( 48%)]  Loss: 4.022 (4.31)  Time: 0.156s, 6564.55/s  (0.192s, 5320.93/s)  LR: 5.683e-04  Data: 0.030 (0.031)
Train: 137 [ 650/1251 ( 52%)]  Loss: 4.306 (4.31)  Time: 0.199s, 5142.63/s  (0.192s, 5327.01/s)  LR: 5.683e-04  Data: 0.023 (0.031)
Train: 137 [ 700/1251 ( 56%)]  Loss: 4.501 (4.32)  Time: 0.159s, 6457.65/s  (0.192s, 5332.57/s)  LR: 5.683e-04  Data: 0.029 (0.031)
Train: 137 [ 750/1251 ( 60%)]  Loss: 4.110 (4.31)  Time: 0.303s, 3381.99/s  (0.192s, 5338.06/s)  LR: 5.683e-04  Data: 0.037 (0.031)
Train: 137 [ 800/1251 ( 64%)]  Loss: 4.316 (4.31)  Time: 0.191s, 5372.94/s  (0.192s, 5336.53/s)  LR: 5.683e-04  Data: 0.030 (0.030)
Train: 137 [ 850/1251 ( 68%)]  Loss: 4.396 (4.32)  Time: 0.433s, 2362.63/s  (0.193s, 5318.85/s)  LR: 5.683e-04  Data: 0.031 (0.030)
Train: 137 [ 900/1251 ( 72%)]  Loss: 4.299 (4.31)  Time: 0.165s, 6217.89/s  (0.192s, 5320.39/s)  LR: 5.683e-04  Data: 0.032 (0.030)
Train: 137 [ 950/1251 ( 76%)]  Loss: 4.200 (4.31)  Time: 0.161s, 6371.28/s  (0.193s, 5317.51/s)  LR: 5.683e-04  Data: 0.025 (0.030)
Train: 137 [1000/1251 ( 80%)]  Loss: 4.368 (4.31)  Time: 0.177s, 5774.99/s  (0.193s, 5318.11/s)  LR: 5.683e-04  Data: 0.022 (0.030)
Train: 137 [1050/1251 ( 84%)]  Loss: 4.339 (4.31)  Time: 0.189s, 5420.95/s  (0.193s, 5319.12/s)  LR: 5.683e-04  Data: 0.025 (0.030)
Train: 137 [1100/1251 ( 88%)]  Loss: 4.004 (4.30)  Time: 0.174s, 5869.94/s  (0.192s, 5323.50/s)  LR: 5.683e-04  Data: 0.025 (0.030)
Train: 137 [1150/1251 ( 92%)]  Loss: 4.236 (4.30)  Time: 0.170s, 6016.62/s  (0.193s, 5317.38/s)  LR: 5.683e-04  Data: 0.029 (0.030)
Train: 137 [1200/1251 ( 96%)]  Loss: 4.247 (4.30)  Time: 0.183s, 5588.69/s  (0.193s, 5304.16/s)  LR: 5.683e-04  Data: 0.060 (0.030)
Train: 137 [1250/1251 (100%)]  Loss: 4.149 (4.29)  Time: 0.114s, 8978.49/s  (0.193s, 5318.36/s)  LR: 5.683e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.843 (1.843)  Loss:  1.2457 (1.2457)  Acc@1: 80.0781 (80.0781)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.2940 (1.9001)  Acc@1: 78.0660 (63.7560)  Acc@5: 93.3962 (85.7060)
Train: 138 [   0/1251 (  0%)]  Loss: 4.124 (4.12)  Time: 2.286s,  447.95/s  (2.286s,  447.95/s)  LR: 5.631e-04  Data: 2.161 (2.161)
Train: 138 [  50/1251 (  4%)]  Loss: 4.191 (4.16)  Time: 0.183s, 5587.44/s  (0.231s, 4438.10/s)  LR: 5.631e-04  Data: 0.026 (0.085)
Train: 138 [ 100/1251 (  8%)]  Loss: 4.061 (4.13)  Time: 0.174s, 5880.29/s  (0.207s, 4942.73/s)  LR: 5.631e-04  Data: 0.028 (0.060)
Train: 138 [ 150/1251 ( 12%)]  Loss: 4.366 (4.19)  Time: 0.155s, 6590.74/s  (0.201s, 5104.74/s)  LR: 5.631e-04  Data: 0.030 (0.050)
Train: 138 [ 200/1251 ( 16%)]  Loss: 4.359 (4.22)  Time: 0.167s, 6132.37/s  (0.199s, 5155.85/s)  LR: 5.631e-04  Data: 0.030 (0.045)
Train: 138 [ 250/1251 ( 20%)]  Loss: 4.149 (4.21)  Time: 0.172s, 5947.34/s  (0.196s, 5215.86/s)  LR: 5.631e-04  Data: 0.023 (0.042)
Train: 138 [ 300/1251 ( 24%)]  Loss: 4.172 (4.20)  Time: 0.173s, 5922.22/s  (0.194s, 5278.57/s)  LR: 5.631e-04  Data: 0.022 (0.039)
Train: 138 [ 350/1251 ( 28%)]  Loss: 4.402 (4.23)  Time: 0.166s, 6168.54/s  (0.193s, 5292.12/s)  LR: 5.631e-04  Data: 0.033 (0.039)
Train: 138 [ 400/1251 ( 32%)]  Loss: 4.448 (4.25)  Time: 0.160s, 6411.29/s  (0.193s, 5297.73/s)  LR: 5.631e-04  Data: 0.026 (0.038)
Train: 138 [ 450/1251 ( 36%)]  Loss: 4.235 (4.25)  Time: 0.163s, 6280.80/s  (0.193s, 5297.24/s)  LR: 5.631e-04  Data: 0.025 (0.037)
Train: 138 [ 500/1251 ( 40%)]  Loss: 4.398 (4.26)  Time: 0.211s, 4843.00/s  (0.194s, 5281.84/s)  LR: 5.631e-04  Data: 0.020 (0.036)
Train: 138 [ 550/1251 ( 44%)]  Loss: 4.431 (4.28)  Time: 0.188s, 5436.50/s  (0.193s, 5304.58/s)  LR: 5.631e-04  Data: 0.024 (0.035)
Train: 138 [ 600/1251 ( 48%)]  Loss: 3.930 (4.25)  Time: 0.181s, 5662.55/s  (0.192s, 5328.90/s)  LR: 5.631e-04  Data: 0.022 (0.034)
Train: 138 [ 650/1251 ( 52%)]  Loss: 4.347 (4.26)  Time: 0.179s, 5725.46/s  (0.192s, 5333.75/s)  LR: 5.631e-04  Data: 0.026 (0.034)
Train: 138 [ 700/1251 ( 56%)]  Loss: 4.317 (4.26)  Time: 0.163s, 6289.63/s  (0.192s, 5326.30/s)  LR: 5.631e-04  Data: 0.027 (0.033)
Train: 138 [ 750/1251 ( 60%)]  Loss: 3.934 (4.24)  Time: 0.183s, 5587.31/s  (0.192s, 5327.02/s)  LR: 5.631e-04  Data: 0.022 (0.033)
Train: 138 [ 800/1251 ( 64%)]  Loss: 3.854 (4.22)  Time: 0.172s, 5953.52/s  (0.192s, 5334.15/s)  LR: 5.631e-04  Data: 0.027 (0.033)
Train: 138 [ 850/1251 ( 68%)]  Loss: 4.440 (4.23)  Time: 0.368s, 2780.21/s  (0.192s, 5329.82/s)  LR: 5.631e-04  Data: 0.027 (0.032)
Train: 138 [ 900/1251 ( 72%)]  Loss: 4.142 (4.23)  Time: 0.193s, 5312.40/s  (0.193s, 5319.24/s)  LR: 5.631e-04  Data: 0.030 (0.032)
Train: 138 [ 950/1251 ( 76%)]  Loss: 4.392 (4.23)  Time: 0.155s, 6625.47/s  (0.192s, 5324.99/s)  LR: 5.631e-04  Data: 0.029 (0.032)
Train: 138 [1000/1251 ( 80%)]  Loss: 3.861 (4.22)  Time: 0.230s, 4461.20/s  (0.193s, 5315.73/s)  LR: 5.631e-04  Data: 0.025 (0.032)
Train: 138 [1050/1251 ( 84%)]  Loss: 4.183 (4.22)  Time: 0.163s, 6288.49/s  (0.193s, 5314.96/s)  LR: 5.631e-04  Data: 0.030 (0.032)
Train: 138 [1100/1251 ( 88%)]  Loss: 4.285 (4.22)  Time: 0.180s, 5689.29/s  (0.193s, 5309.23/s)  LR: 5.631e-04  Data: 0.024 (0.032)
Train: 138 [1150/1251 ( 92%)]  Loss: 4.098 (4.21)  Time: 0.173s, 5932.17/s  (0.193s, 5315.02/s)  LR: 5.631e-04  Data: 0.025 (0.032)
Train: 138 [1200/1251 ( 96%)]  Loss: 3.884 (4.20)  Time: 0.210s, 4880.25/s  (0.193s, 5311.63/s)  LR: 5.631e-04  Data: 0.028 (0.031)
Train: 138 [1250/1251 (100%)]  Loss: 4.107 (4.20)  Time: 0.113s, 9022.59/s  (0.192s, 5321.52/s)  LR: 5.631e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.871 (1.871)  Loss:  1.1738 (1.1738)  Acc@1: 80.1758 (80.1758)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1153 (1.8021)  Acc@1: 79.8349 (64.6520)  Acc@5: 93.3962 (86.1760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-123.pth.tar', 64.0879998828125)

Train: 139 [   0/1251 (  0%)]  Loss: 4.406 (4.41)  Time: 1.709s,  599.29/s  (1.709s,  599.29/s)  LR: 5.579e-04  Data: 1.561 (1.561)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Train: 139 [  50/1251 (  4%)]  Loss: 4.542 (4.47)  Time: 0.156s, 6548.87/s  (0.226s, 4535.98/s)  LR: 5.579e-04  Data: 0.028 (0.063)
Train: 139 [ 100/1251 (  8%)]  Loss: 4.157 (4.37)  Time: 0.159s, 6437.44/s  (0.208s, 4919.86/s)  LR: 5.579e-04  Data: 0.026 (0.045)
Train: 139 [ 150/1251 ( 12%)]  Loss: 4.118 (4.31)  Time: 0.182s, 5612.92/s  (0.203s, 5055.84/s)  LR: 5.579e-04  Data: 0.031 (0.039)
Train: 139 [ 200/1251 ( 16%)]  Loss: 4.111 (4.27)  Time: 0.257s, 3980.15/s  (0.200s, 5115.89/s)  LR: 5.579e-04  Data: 0.037 (0.037)
Train: 139 [ 250/1251 ( 20%)]  Loss: 4.155 (4.25)  Time: 0.166s, 6165.53/s  (0.197s, 5205.05/s)  LR: 5.579e-04  Data: 0.027 (0.035)
Train: 139 [ 300/1251 ( 24%)]  Loss: 3.823 (4.19)  Time: 0.166s, 6175.79/s  (0.196s, 5227.03/s)  LR: 5.579e-04  Data: 0.037 (0.034)
Train: 139 [ 350/1251 ( 28%)]  Loss: 4.363 (4.21)  Time: 0.168s, 6104.72/s  (0.193s, 5292.10/s)  LR: 5.579e-04  Data: 0.025 (0.033)
Train: 139 [ 400/1251 ( 32%)]  Loss: 4.390 (4.23)  Time: 0.189s, 5420.40/s  (0.194s, 5273.42/s)  LR: 5.579e-04  Data: 0.028 (0.033)
Train: 139 [ 450/1251 ( 36%)]  Loss: 3.883 (4.19)  Time: 0.168s, 6109.28/s  (0.194s, 5277.16/s)  LR: 5.579e-04  Data: 0.027 (0.035)
Train: 139 [ 500/1251 ( 40%)]  Loss: 4.203 (4.20)  Time: 0.165s, 6220.56/s  (0.193s, 5294.91/s)  LR: 5.579e-04  Data: 0.025 (0.036)
Train: 139 [ 550/1251 ( 44%)]  Loss: 4.312 (4.21)  Time: 0.185s, 5541.26/s  (0.193s, 5292.41/s)  LR: 5.579e-04  Data: 0.024 (0.037)
Train: 139 [ 600/1251 ( 48%)]  Loss: 4.235 (4.21)  Time: 0.164s, 6229.20/s  (0.193s, 5309.09/s)  LR: 5.579e-04  Data: 0.032 (0.037)
Train: 139 [ 650/1251 ( 52%)]  Loss: 4.376 (4.22)  Time: 0.165s, 6217.13/s  (0.193s, 5318.49/s)  LR: 5.579e-04  Data: 0.027 (0.037)
Train: 139 [ 700/1251 ( 56%)]  Loss: 4.435 (4.23)  Time: 0.181s, 5649.85/s  (0.193s, 5316.24/s)  LR: 5.579e-04  Data: 0.026 (0.038)
Train: 139 [ 750/1251 ( 60%)]  Loss: 4.013 (4.22)  Time: 0.159s, 6448.16/s  (0.193s, 5317.00/s)  LR: 5.579e-04  Data: 0.026 (0.039)
Train: 139 [ 800/1251 ( 64%)]  Loss: 4.295 (4.22)  Time: 0.167s, 6126.83/s  (0.192s, 5330.84/s)  LR: 5.579e-04  Data: 0.022 (0.039)
Train: 139 [ 850/1251 ( 68%)]  Loss: 4.230 (4.22)  Time: 0.190s, 5389.19/s  (0.192s, 5329.18/s)  LR: 5.579e-04  Data: 0.023 (0.039)
Train: 139 [ 900/1251 ( 72%)]  Loss: 4.625 (4.25)  Time: 0.168s, 6080.35/s  (0.192s, 5332.96/s)  LR: 5.579e-04  Data: 0.024 (0.040)
Train: 139 [ 950/1251 ( 76%)]  Loss: 4.369 (4.25)  Time: 0.155s, 6622.77/s  (0.192s, 5323.05/s)  LR: 5.579e-04  Data: 0.027 (0.039)
Train: 139 [1000/1251 ( 80%)]  Loss: 3.958 (4.24)  Time: 0.175s, 5836.81/s  (0.192s, 5321.24/s)  LR: 5.579e-04  Data: 0.020 (0.038)
Train: 139 [1050/1251 ( 84%)]  Loss: 3.905 (4.22)  Time: 0.164s, 6232.61/s  (0.193s, 5317.70/s)  LR: 5.579e-04  Data: 0.051 (0.038)
Train: 139 [1100/1251 ( 88%)]  Loss: 4.124 (4.22)  Time: 0.176s, 5815.49/s  (0.193s, 5313.57/s)  LR: 5.579e-04  Data: 0.035 (0.038)
Train: 139 [1150/1251 ( 92%)]  Loss: 4.116 (4.21)  Time: 0.437s, 2345.48/s  (0.193s, 5314.78/s)  LR: 5.579e-04  Data: 0.026 (0.037)
Train: 139 [1200/1251 ( 96%)]  Loss: 4.421 (4.22)  Time: 0.167s, 6132.48/s  (0.192s, 5319.68/s)  LR: 5.579e-04  Data: 0.026 (0.037)
Train: 139 [1250/1251 (100%)]  Loss: 4.377 (4.23)  Time: 0.114s, 9020.25/s  (0.192s, 5330.87/s)  LR: 5.579e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.872 (1.872)  Loss:  1.1064 (1.1064)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1588 (1.7763)  Acc@1: 79.2453 (64.6760)  Acc@5: 92.6887 (86.2420)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-125.pth.tar', 64.10000000732421)

Train: 140 [   0/1251 (  0%)]  Loss: 4.197 (4.20)  Time: 2.132s,  480.27/s  (2.132s,  480.27/s)  LR: 5.527e-04  Data: 2.006 (2.006)
Train: 140 [  50/1251 (  4%)]  Loss: 4.320 (4.26)  Time: 0.168s, 6082.86/s  (0.221s, 4635.99/s)  LR: 5.527e-04  Data: 0.029 (0.075)
Train: 140 [ 100/1251 (  8%)]  Loss: 4.425 (4.31)  Time: 0.184s, 5571.62/s  (0.206s, 4980.25/s)  LR: 5.527e-04  Data: 0.034 (0.054)
Train: 140 [ 150/1251 ( 12%)]  Loss: 3.993 (4.23)  Time: 0.166s, 6154.45/s  (0.202s, 5077.52/s)  LR: 5.527e-04  Data: 0.029 (0.049)
Train: 140 [ 200/1251 ( 16%)]  Loss: 4.403 (4.27)  Time: 0.360s, 2844.07/s  (0.197s, 5189.66/s)  LR: 5.527e-04  Data: 0.025 (0.044)
Train: 140 [ 250/1251 ( 20%)]  Loss: 4.268 (4.27)  Time: 0.175s, 5838.81/s  (0.195s, 5250.54/s)  LR: 5.527e-04  Data: 0.020 (0.040)
Train: 140 [ 300/1251 ( 24%)]  Loss: 4.157 (4.25)  Time: 0.168s, 6108.97/s  (0.194s, 5282.33/s)  LR: 5.527e-04  Data: 0.029 (0.038)
Train: 140 [ 350/1251 ( 28%)]  Loss: 4.406 (4.27)  Time: 0.165s, 6197.75/s  (0.193s, 5308.24/s)  LR: 5.527e-04  Data: 0.024 (0.037)
Train: 140 [ 400/1251 ( 32%)]  Loss: 4.154 (4.26)  Time: 0.279s, 3675.07/s  (0.194s, 5291.17/s)  LR: 5.527e-04  Data: 0.029 (0.036)
Train: 140 [ 450/1251 ( 36%)]  Loss: 4.388 (4.27)  Time: 0.169s, 6057.12/s  (0.193s, 5312.12/s)  LR: 5.527e-04  Data: 0.039 (0.035)
Train: 140 [ 500/1251 ( 40%)]  Loss: 3.994 (4.25)  Time: 0.174s, 5884.77/s  (0.193s, 5309.93/s)  LR: 5.527e-04  Data: 0.029 (0.034)
Train: 140 [ 550/1251 ( 44%)]  Loss: 4.504 (4.27)  Time: 0.198s, 5166.50/s  (0.193s, 5306.30/s)  LR: 5.527e-04  Data: 0.021 (0.034)
Train: 140 [ 600/1251 ( 48%)]  Loss: 4.306 (4.27)  Time: 0.356s, 2873.25/s  (0.193s, 5305.47/s)  LR: 5.527e-04  Data: 0.022 (0.033)
Train: 140 [ 650/1251 ( 52%)]  Loss: 4.486 (4.29)  Time: 0.175s, 5866.90/s  (0.193s, 5314.79/s)  LR: 5.527e-04  Data: 0.020 (0.033)
Train: 140 [ 700/1251 ( 56%)]  Loss: 4.207 (4.28)  Time: 0.182s, 5631.13/s  (0.192s, 5323.32/s)  LR: 5.527e-04  Data: 0.026 (0.033)
Train: 140 [ 750/1251 ( 60%)]  Loss: 4.293 (4.28)  Time: 0.178s, 5748.78/s  (0.192s, 5328.31/s)  LR: 5.527e-04  Data: 0.024 (0.033)
Train: 140 [ 800/1251 ( 64%)]  Loss: 4.176 (4.28)  Time: 0.195s, 5251.46/s  (0.192s, 5322.22/s)  LR: 5.527e-04  Data: 0.028 (0.032)
Train: 140 [ 850/1251 ( 68%)]  Loss: 4.576 (4.29)  Time: 0.183s, 5586.97/s  (0.192s, 5334.53/s)  LR: 5.527e-04  Data: 0.025 (0.032)
Train: 140 [ 900/1251 ( 72%)]  Loss: 4.213 (4.29)  Time: 0.198s, 5176.04/s  (0.192s, 5326.47/s)  LR: 5.527e-04  Data: 0.027 (0.032)
Train: 140 [ 950/1251 ( 76%)]  Loss: 4.236 (4.29)  Time: 0.155s, 6601.62/s  (0.192s, 5323.22/s)  LR: 5.527e-04  Data: 0.029 (0.032)
Train: 140 [1000/1251 ( 80%)]  Loss: 4.340 (4.29)  Time: 0.568s, 1802.62/s  (0.193s, 5315.15/s)  LR: 5.527e-04  Data: 0.030 (0.032)
Train: 140 [1050/1251 ( 84%)]  Loss: 4.104 (4.28)  Time: 0.189s, 5420.10/s  (0.193s, 5314.79/s)  LR: 5.527e-04  Data: 0.033 (0.031)
Train: 140 [1100/1251 ( 88%)]  Loss: 4.292 (4.28)  Time: 0.171s, 5971.62/s  (0.193s, 5309.70/s)  LR: 5.527e-04  Data: 0.043 (0.031)
Train: 140 [1150/1251 ( 92%)]  Loss: 4.287 (4.28)  Time: 0.158s, 6467.31/s  (0.193s, 5311.74/s)  LR: 5.527e-04  Data: 0.030 (0.031)
Train: 140 [1200/1251 ( 96%)]  Loss: 4.019 (4.27)  Time: 0.161s, 6362.29/s  (0.193s, 5313.97/s)  LR: 5.527e-04  Data: 0.028 (0.031)
Train: 140 [1250/1251 (100%)]  Loss: 4.136 (4.26)  Time: 0.114s, 8978.83/s  (0.192s, 5326.50/s)  LR: 5.527e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.829 (1.829)  Loss:  1.1573 (1.1573)  Acc@1: 79.7852 (79.7852)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.2846 (1.8112)  Acc@1: 77.9481 (64.9060)  Acc@5: 93.3962 (86.2340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-124.pth.tar', 64.23400003417969)

Train: 141 [   0/1251 (  0%)]  Loss: 4.040 (4.04)  Time: 1.868s,  548.27/s  (1.868s,  548.27/s)  LR: 5.475e-04  Data: 1.582 (1.582)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 141 [  50/1251 (  4%)]  Loss: 4.201 (4.12)  Time: 0.178s, 5762.86/s  (0.228s, 4484.55/s)  LR: 5.475e-04  Data: 0.031 (0.079)
Train: 141 [ 100/1251 (  8%)]  Loss: 4.178 (4.14)  Time: 0.169s, 6073.41/s  (0.210s, 4884.36/s)  LR: 5.475e-04  Data: 0.031 (0.057)
Train: 141 [ 150/1251 ( 12%)]  Loss: 4.093 (4.13)  Time: 0.175s, 5859.20/s  (0.203s, 5052.21/s)  LR: 5.475e-04  Data: 0.024 (0.047)
Train: 141 [ 200/1251 ( 16%)]  Loss: 4.093 (4.12)  Time: 0.158s, 6479.54/s  (0.199s, 5147.11/s)  LR: 5.475e-04  Data: 0.029 (0.042)
Train: 141 [ 250/1251 ( 20%)]  Loss: 4.157 (4.13)  Time: 0.167s, 6115.13/s  (0.198s, 5168.83/s)  LR: 5.475e-04  Data: 0.023 (0.039)
Train: 141 [ 300/1251 ( 24%)]  Loss: 3.938 (4.10)  Time: 0.203s, 5038.94/s  (0.196s, 5211.86/s)  LR: 5.475e-04  Data: 0.022 (0.037)
Train: 141 [ 350/1251 ( 28%)]  Loss: 4.726 (4.18)  Time: 0.171s, 5989.72/s  (0.196s, 5236.23/s)  LR: 5.475e-04  Data: 0.030 (0.036)
Train: 141 [ 400/1251 ( 32%)]  Loss: 4.490 (4.21)  Time: 0.154s, 6665.80/s  (0.194s, 5271.81/s)  LR: 5.475e-04  Data: 0.026 (0.035)
Train: 141 [ 450/1251 ( 36%)]  Loss: 4.437 (4.24)  Time: 0.197s, 5196.72/s  (0.194s, 5264.95/s)  LR: 5.475e-04  Data: 0.024 (0.034)
Train: 141 [ 500/1251 ( 40%)]  Loss: 4.566 (4.27)  Time: 0.176s, 5808.50/s  (0.193s, 5293.32/s)  LR: 5.475e-04  Data: 0.020 (0.034)
Train: 141 [ 550/1251 ( 44%)]  Loss: 4.474 (4.28)  Time: 0.172s, 5967.30/s  (0.193s, 5300.14/s)  LR: 5.475e-04  Data: 0.032 (0.033)
Train: 141 [ 600/1251 ( 48%)]  Loss: 3.888 (4.25)  Time: 0.173s, 5921.56/s  (0.193s, 5314.04/s)  LR: 5.475e-04  Data: 0.028 (0.033)
Train: 141 [ 650/1251 ( 52%)]  Loss: 4.119 (4.24)  Time: 0.179s, 5729.23/s  (0.193s, 5308.18/s)  LR: 5.475e-04  Data: 0.022 (0.032)
Train: 141 [ 700/1251 ( 56%)]  Loss: 4.206 (4.24)  Time: 0.154s, 6630.27/s  (0.193s, 5310.78/s)  LR: 5.475e-04  Data: 0.028 (0.032)
Train: 141 [ 750/1251 ( 60%)]  Loss: 4.019 (4.23)  Time: 0.197s, 5190.25/s  (0.193s, 5316.07/s)  LR: 5.475e-04  Data: 0.022 (0.032)
Train: 141 [ 800/1251 ( 64%)]  Loss: 4.448 (4.24)  Time: 0.185s, 5540.75/s  (0.193s, 5309.08/s)  LR: 5.475e-04  Data: 0.030 (0.032)
Train: 141 [ 850/1251 ( 68%)]  Loss: 4.431 (4.25)  Time: 0.189s, 5425.54/s  (0.193s, 5310.46/s)  LR: 5.475e-04  Data: 0.027 (0.031)
Train: 141 [ 900/1251 ( 72%)]  Loss: 4.313 (4.25)  Time: 0.173s, 5914.09/s  (0.192s, 5320.90/s)  LR: 5.475e-04  Data: 0.027 (0.031)
Train: 141 [ 950/1251 ( 76%)]  Loss: 3.919 (4.24)  Time: 0.246s, 4169.35/s  (0.193s, 5316.41/s)  LR: 5.475e-04  Data: 0.028 (0.031)
Train: 141 [1000/1251 ( 80%)]  Loss: 3.835 (4.22)  Time: 0.172s, 5940.02/s  (0.193s, 5311.47/s)  LR: 5.475e-04  Data: 0.034 (0.031)
Train: 141 [1050/1251 ( 84%)]  Loss: 3.951 (4.21)  Time: 0.189s, 5408.97/s  (0.193s, 5316.35/s)  LR: 5.475e-04  Data: 0.026 (0.031)
Train: 141 [1100/1251 ( 88%)]  Loss: 4.351 (4.21)  Time: 0.172s, 5951.40/s  (0.193s, 5307.29/s)  LR: 5.475e-04  Data: 0.029 (0.031)
Train: 141 [1150/1251 ( 92%)]  Loss: 4.091 (4.21)  Time: 0.167s, 6122.62/s  (0.193s, 5314.23/s)  LR: 5.475e-04  Data: 0.035 (0.031)
Train: 141 [1200/1251 ( 96%)]  Loss: 4.225 (4.21)  Time: 0.160s, 6396.15/s  (0.193s, 5314.55/s)  LR: 5.475e-04  Data: 0.035 (0.031)
Train: 141 [1250/1251 (100%)]  Loss: 4.223 (4.21)  Time: 0.114s, 9020.60/s  (0.192s, 5321.79/s)  LR: 5.475e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.744 (1.744)  Loss:  1.1521 (1.1521)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1783 (1.7721)  Acc@1: 80.3066 (64.6200)  Acc@5: 92.9245 (86.1140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-141.pth.tar', 64.61999997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-133.pth.tar', 64.2619998828125)

Train: 142 [   0/1251 (  0%)]  Loss: 3.970 (3.97)  Time: 1.698s,  603.15/s  (1.698s,  603.15/s)  LR: 5.423e-04  Data: 1.562 (1.562)
Train: 142 [  50/1251 (  4%)]  Loss: 3.971 (3.97)  Time: 0.159s, 6450.43/s  (0.220s, 4665.11/s)  LR: 5.423e-04  Data: 0.030 (0.063)
Train: 142 [ 100/1251 (  8%)]  Loss: 4.169 (4.04)  Time: 0.176s, 5831.36/s  (0.206s, 4960.22/s)  LR: 5.423e-04  Data: 0.039 (0.046)
Train: 142 [ 150/1251 ( 12%)]  Loss: 4.472 (4.15)  Time: 0.175s, 5841.10/s  (0.199s, 5147.97/s)  LR: 5.423e-04  Data: 0.029 (0.041)
Train: 142 [ 200/1251 ( 16%)]  Loss: 4.387 (4.19)  Time: 0.178s, 5750.54/s  (0.198s, 5165.91/s)  LR: 5.423e-04  Data: 0.034 (0.039)
Train: 142 [ 250/1251 ( 20%)]  Loss: 4.105 (4.18)  Time: 0.213s, 4814.44/s  (0.195s, 5240.27/s)  LR: 5.423e-04  Data: 0.024 (0.036)
Train: 142 [ 300/1251 ( 24%)]  Loss: 4.437 (4.22)  Time: 0.174s, 5891.67/s  (0.194s, 5265.42/s)  LR: 5.423e-04  Data: 0.035 (0.035)
Train: 142 [ 350/1251 ( 28%)]  Loss: 4.451 (4.25)  Time: 0.165s, 6205.50/s  (0.194s, 5284.98/s)  LR: 5.423e-04  Data: 0.033 (0.035)
Train: 142 [ 400/1251 ( 32%)]  Loss: 4.369 (4.26)  Time: 0.209s, 4906.33/s  (0.193s, 5308.18/s)  LR: 5.423e-04  Data: 0.022 (0.034)
Train: 142 [ 450/1251 ( 36%)]  Loss: 4.312 (4.26)  Time: 0.184s, 5554.60/s  (0.193s, 5311.61/s)  LR: 5.423e-04  Data: 0.049 (0.033)
Train: 142 [ 500/1251 ( 40%)]  Loss: 3.959 (4.24)  Time: 0.158s, 6474.09/s  (0.193s, 5306.23/s)  LR: 5.423e-04  Data: 0.031 (0.033)
Train: 142 [ 550/1251 ( 44%)]  Loss: 4.249 (4.24)  Time: 0.169s, 6063.43/s  (0.192s, 5336.19/s)  LR: 5.423e-04  Data: 0.037 (0.033)
Train: 142 [ 600/1251 ( 48%)]  Loss: 4.412 (4.25)  Time: 0.209s, 4893.67/s  (0.192s, 5342.23/s)  LR: 5.423e-04  Data: 0.033 (0.032)
Train: 142 [ 650/1251 ( 52%)]  Loss: 4.305 (4.25)  Time: 0.158s, 6463.83/s  (0.192s, 5342.35/s)  LR: 5.423e-04  Data: 0.030 (0.032)
Train: 142 [ 700/1251 ( 56%)]  Loss: 3.869 (4.23)  Time: 0.178s, 5745.25/s  (0.192s, 5338.39/s)  LR: 5.423e-04  Data: 0.036 (0.032)
Train: 142 [ 750/1251 ( 60%)]  Loss: 4.005 (4.22)  Time: 0.294s, 3482.95/s  (0.192s, 5327.26/s)  LR: 5.423e-04  Data: 0.032 (0.031)
Train: 142 [ 800/1251 ( 64%)]  Loss: 3.870 (4.19)  Time: 0.177s, 5770.05/s  (0.192s, 5326.30/s)  LR: 5.423e-04  Data: 0.037 (0.031)
Train: 142 [ 850/1251 ( 68%)]  Loss: 4.251 (4.20)  Time: 0.187s, 5467.55/s  (0.192s, 5328.72/s)  LR: 5.423e-04  Data: 0.042 (0.031)
Train: 142 [ 900/1251 ( 72%)]  Loss: 4.269 (4.20)  Time: 0.253s, 4039.74/s  (0.192s, 5324.12/s)  LR: 5.423e-04  Data: 0.028 (0.031)
Train: 142 [ 950/1251 ( 76%)]  Loss: 4.614 (4.22)  Time: 0.168s, 6093.68/s  (0.192s, 5321.19/s)  LR: 5.423e-04  Data: 0.026 (0.031)
Train: 142 [1000/1251 ( 80%)]  Loss: 4.017 (4.21)  Time: 0.173s, 5918.36/s  (0.192s, 5325.19/s)  LR: 5.423e-04  Data: 0.031 (0.031)
Train: 142 [1050/1251 ( 84%)]  Loss: 3.956 (4.20)  Time: 0.173s, 5924.82/s  (0.192s, 5321.01/s)  LR: 5.423e-04  Data: 0.025 (0.030)
Train: 142 [1100/1251 ( 88%)]  Loss: 4.024 (4.19)  Time: 0.188s, 5449.00/s  (0.193s, 5311.10/s)  LR: 5.423e-04  Data: 0.027 (0.030)
Train: 142 [1150/1251 ( 92%)]  Loss: 4.213 (4.19)  Time: 0.172s, 5965.72/s  (0.193s, 5308.47/s)  LR: 5.423e-04  Data: 0.032 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 142 [1200/1251 ( 96%)]  Loss: 4.295 (4.20)  Time: 0.171s, 6004.58/s  (0.193s, 5317.84/s)  LR: 5.423e-04  Data: 0.029 (0.030)
Train: 142 [1250/1251 (100%)]  Loss: 4.438 (4.21)  Time: 0.114s, 9005.28/s  (0.192s, 5328.53/s)  LR: 5.423e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.826 (1.826)  Loss:  1.0702 (1.0702)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2660 (1.8096)  Acc@1: 79.0094 (64.9240)  Acc@5: 93.3962 (86.2460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-141.pth.tar', 64.61999997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-127.pth.tar', 64.27799998535156)

Train: 143 [   0/1251 (  0%)]  Loss: 4.266 (4.27)  Time: 1.815s,  564.10/s  (1.815s,  564.10/s)  LR: 5.371e-04  Data: 1.691 (1.691)
Train: 143 [  50/1251 (  4%)]  Loss: 4.374 (4.32)  Time: 0.179s, 5729.28/s  (0.220s, 4646.88/s)  LR: 5.371e-04  Data: 0.031 (0.072)
Train: 143 [ 100/1251 (  8%)]  Loss: 4.283 (4.31)  Time: 0.170s, 6020.37/s  (0.204s, 5022.02/s)  LR: 5.371e-04  Data: 0.028 (0.051)
Train: 143 [ 150/1251 ( 12%)]  Loss: 4.331 (4.31)  Time: 0.165s, 6224.09/s  (0.199s, 5145.40/s)  LR: 5.371e-04  Data: 0.026 (0.048)
Train: 143 [ 200/1251 ( 16%)]  Loss: 3.865 (4.22)  Time: 0.183s, 5593.37/s  (0.198s, 5174.36/s)  LR: 5.371e-04  Data: 0.026 (0.048)
Train: 143 [ 250/1251 ( 20%)]  Loss: 4.465 (4.26)  Time: 0.164s, 6254.86/s  (0.196s, 5237.73/s)  LR: 5.371e-04  Data: 0.027 (0.047)
Train: 143 [ 300/1251 ( 24%)]  Loss: 4.404 (4.28)  Time: 0.194s, 5269.82/s  (0.194s, 5275.70/s)  LR: 5.371e-04  Data: 0.041 (0.044)
Train: 143 [ 350/1251 ( 28%)]  Loss: 4.137 (4.27)  Time: 0.193s, 5300.24/s  (0.193s, 5303.34/s)  LR: 5.371e-04  Data: 0.027 (0.042)
Train: 143 [ 400/1251 ( 32%)]  Loss: 4.112 (4.25)  Time: 0.178s, 5765.99/s  (0.193s, 5316.44/s)  LR: 5.371e-04  Data: 0.028 (0.040)
Train: 143 [ 450/1251 ( 36%)]  Loss: 4.041 (4.23)  Time: 0.183s, 5582.69/s  (0.192s, 5319.57/s)  LR: 5.371e-04  Data: 0.030 (0.039)
Train: 143 [ 500/1251 ( 40%)]  Loss: 4.210 (4.23)  Time: 0.195s, 5263.41/s  (0.193s, 5311.86/s)  LR: 5.371e-04  Data: 0.034 (0.038)
Train: 143 [ 550/1251 ( 44%)]  Loss: 4.274 (4.23)  Time: 0.157s, 6541.47/s  (0.192s, 5319.69/s)  LR: 5.371e-04  Data: 0.030 (0.037)
Train: 143 [ 600/1251 ( 48%)]  Loss: 4.431 (4.25)  Time: 0.197s, 5204.16/s  (0.192s, 5332.66/s)  LR: 5.371e-04  Data: 0.028 (0.036)
Train: 143 [ 650/1251 ( 52%)]  Loss: 4.163 (4.24)  Time: 0.189s, 5429.90/s  (0.192s, 5343.00/s)  LR: 5.371e-04  Data: 0.023 (0.036)
Train: 143 [ 700/1251 ( 56%)]  Loss: 4.356 (4.25)  Time: 0.182s, 5628.60/s  (0.192s, 5331.18/s)  LR: 5.371e-04  Data: 0.025 (0.035)
Train: 143 [ 750/1251 ( 60%)]  Loss: 4.460 (4.26)  Time: 0.190s, 5382.58/s  (0.192s, 5330.55/s)  LR: 5.371e-04  Data: 0.027 (0.035)
Train: 143 [ 800/1251 ( 64%)]  Loss: 4.130 (4.25)  Time: 0.191s, 5352.37/s  (0.192s, 5334.56/s)  LR: 5.371e-04  Data: 0.032 (0.034)
Train: 143 [ 850/1251 ( 68%)]  Loss: 4.181 (4.25)  Time: 0.170s, 6030.83/s  (0.192s, 5326.42/s)  LR: 5.371e-04  Data: 0.026 (0.034)
Train: 143 [ 900/1251 ( 72%)]  Loss: 4.151 (4.24)  Time: 0.168s, 6097.37/s  (0.192s, 5342.11/s)  LR: 5.371e-04  Data: 0.035 (0.034)
Train: 143 [ 950/1251 ( 76%)]  Loss: 4.227 (4.24)  Time: 0.185s, 5544.18/s  (0.192s, 5340.73/s)  LR: 5.371e-04  Data: 0.022 (0.034)
Train: 143 [1000/1251 ( 80%)]  Loss: 4.472 (4.25)  Time: 0.188s, 5433.86/s  (0.192s, 5328.98/s)  LR: 5.371e-04  Data: 0.026 (0.034)
Train: 143 [1050/1251 ( 84%)]  Loss: 4.677 (4.27)  Time: 0.174s, 5871.56/s  (0.192s, 5320.52/s)  LR: 5.371e-04  Data: 0.032 (0.034)
Train: 143 [1100/1251 ( 88%)]  Loss: 4.282 (4.27)  Time: 0.183s, 5602.92/s  (0.193s, 5315.02/s)  LR: 5.371e-04  Data: 0.033 (0.034)
Train: 143 [1150/1251 ( 92%)]  Loss: 4.326 (4.28)  Time: 0.165s, 6204.93/s  (0.193s, 5308.95/s)  LR: 5.371e-04  Data: 0.031 (0.033)
Train: 143 [1200/1251 ( 96%)]  Loss: 4.111 (4.27)  Time: 0.169s, 6068.53/s  (0.193s, 5314.37/s)  LR: 5.371e-04  Data: 0.028 (0.033)
Train: 143 [1250/1251 (100%)]  Loss: 4.095 (4.26)  Time: 0.113s, 9040.27/s  (0.192s, 5321.70/s)  LR: 5.371e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  1.0526 (1.0526)  Acc@1: 81.5430 (81.5430)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1804 (1.7851)  Acc@1: 79.9528 (65.0120)  Acc@5: 93.5142 (86.4400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-141.pth.tar', 64.61999997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-131.pth.tar', 64.48000010742187)

Train: 144 [   0/1251 (  0%)]  Loss: 4.197 (4.20)  Time: 2.083s,  491.53/s  (2.083s,  491.53/s)  LR: 5.319e-04  Data: 1.959 (1.959)
Train: 144 [  50/1251 (  4%)]  Loss: 3.962 (4.08)  Time: 0.160s, 6419.48/s  (0.228s, 4483.02/s)  LR: 5.319e-04  Data: 0.027 (0.083)
Train: 144 [ 100/1251 (  8%)]  Loss: 4.310 (4.16)  Time: 0.167s, 6139.72/s  (0.207s, 4937.26/s)  LR: 5.319e-04  Data: 0.033 (0.062)
Train: 144 [ 150/1251 ( 12%)]  Loss: 3.877 (4.09)  Time: 0.185s, 5544.57/s  (0.202s, 5058.87/s)  LR: 5.319e-04  Data: 0.029 (0.054)
Train: 144 [ 200/1251 ( 16%)]  Loss: 4.184 (4.11)  Time: 0.173s, 5914.35/s  (0.201s, 5083.07/s)  LR: 5.319e-04  Data: 0.024 (0.047)
Train: 144 [ 250/1251 ( 20%)]  Loss: 4.059 (4.10)  Time: 0.177s, 5782.54/s  (0.198s, 5176.27/s)  LR: 5.319e-04  Data: 0.027 (0.044)
Train: 144 [ 300/1251 ( 24%)]  Loss: 3.749 (4.05)  Time: 0.173s, 5936.23/s  (0.194s, 5265.14/s)  LR: 5.319e-04  Data: 0.026 (0.042)
Train: 144 [ 350/1251 ( 28%)]  Loss: 4.395 (4.09)  Time: 0.177s, 5798.78/s  (0.195s, 5249.27/s)  LR: 5.319e-04  Data: 0.030 (0.040)
Train: 144 [ 400/1251 ( 32%)]  Loss: 4.071 (4.09)  Time: 0.176s, 5810.80/s  (0.195s, 5254.66/s)  LR: 5.319e-04  Data: 0.042 (0.039)
Train: 144 [ 450/1251 ( 36%)]  Loss: 4.065 (4.09)  Time: 0.159s, 6437.74/s  (0.194s, 5271.40/s)  LR: 5.319e-04  Data: 0.025 (0.038)
Train: 144 [ 500/1251 ( 40%)]  Loss: 4.038 (4.08)  Time: 0.176s, 5803.19/s  (0.194s, 5270.82/s)  LR: 5.319e-04  Data: 0.025 (0.038)
Train: 144 [ 550/1251 ( 44%)]  Loss: 4.369 (4.11)  Time: 0.179s, 5734.31/s  (0.194s, 5291.01/s)  LR: 5.319e-04  Data: 0.025 (0.038)
Train: 144 [ 600/1251 ( 48%)]  Loss: 4.253 (4.12)  Time: 0.267s, 3829.94/s  (0.193s, 5295.77/s)  LR: 5.319e-04  Data: 0.038 (0.038)
Train: 144 [ 650/1251 ( 52%)]  Loss: 4.302 (4.13)  Time: 0.167s, 6123.74/s  (0.193s, 5300.73/s)  LR: 5.319e-04  Data: 0.026 (0.038)
Train: 144 [ 700/1251 ( 56%)]  Loss: 4.473 (4.15)  Time: 0.164s, 6240.36/s  (0.193s, 5307.18/s)  LR: 5.319e-04  Data: 0.024 (0.037)
Train: 144 [ 750/1251 ( 60%)]  Loss: 4.263 (4.16)  Time: 0.179s, 5717.16/s  (0.193s, 5301.22/s)  LR: 5.319e-04  Data: 0.024 (0.036)
Train: 144 [ 800/1251 ( 64%)]  Loss: 4.364 (4.17)  Time: 0.168s, 6099.37/s  (0.193s, 5299.86/s)  LR: 5.319e-04  Data: 0.022 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 144 [ 850/1251 ( 68%)]  Loss: 4.286 (4.18)  Time: 0.180s, 5699.36/s  (0.193s, 5309.82/s)  LR: 5.319e-04  Data: 0.030 (0.035)
Train: 144 [ 900/1251 ( 72%)]  Loss: 4.417 (4.19)  Time: 0.196s, 5222.75/s  (0.193s, 5306.22/s)  LR: 5.319e-04  Data: 0.027 (0.035)
Train: 144 [ 950/1251 ( 76%)]  Loss: 4.265 (4.19)  Time: 0.182s, 5638.90/s  (0.193s, 5300.60/s)  LR: 5.319e-04  Data: 0.023 (0.036)
Train: 144 [1000/1251 ( 80%)]  Loss: 4.146 (4.19)  Time: 0.164s, 6257.76/s  (0.193s, 5303.02/s)  LR: 5.319e-04  Data: 0.026 (0.036)
Train: 144 [1050/1251 ( 84%)]  Loss: 4.095 (4.19)  Time: 0.182s, 5632.94/s  (0.193s, 5303.88/s)  LR: 5.319e-04  Data: 0.022 (0.036)
Train: 144 [1100/1251 ( 88%)]  Loss: 4.141 (4.19)  Time: 0.175s, 5853.20/s  (0.193s, 5309.27/s)  LR: 5.319e-04  Data: 0.030 (0.037)
Train: 144 [1150/1251 ( 92%)]  Loss: 4.630 (4.20)  Time: 0.164s, 6238.99/s  (0.193s, 5297.65/s)  LR: 5.319e-04  Data: 0.026 (0.037)
Train: 144 [1200/1251 ( 96%)]  Loss: 4.023 (4.20)  Time: 0.177s, 5786.01/s  (0.193s, 5294.21/s)  LR: 5.319e-04  Data: 0.029 (0.038)
Train: 144 [1250/1251 (100%)]  Loss: 4.165 (4.20)  Time: 0.113s, 9066.97/s  (0.193s, 5309.66/s)  LR: 5.319e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.790 (1.790)  Loss:  1.1515 (1.1515)  Acc@1: 81.1523 (81.1523)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.2000 (1.8008)  Acc@1: 79.2453 (64.6220)  Acc@5: 93.5142 (86.2040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-144.pth.tar', 64.62200003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-141.pth.tar', 64.61999997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-135.pth.tar', 64.60000008544922)

Train: 145 [   0/1251 (  0%)]  Loss: 4.226 (4.23)  Time: 1.864s,  549.46/s  (1.864s,  549.46/s)  LR: 5.266e-04  Data: 1.740 (1.740)
Train: 145 [  50/1251 (  4%)]  Loss: 4.395 (4.31)  Time: 0.167s, 6121.54/s  (0.222s, 4605.78/s)  LR: 5.266e-04  Data: 0.030 (0.073)
Train: 145 [ 100/1251 (  8%)]  Loss: 4.480 (4.37)  Time: 0.184s, 5566.83/s  (0.205s, 4996.51/s)  LR: 5.266e-04  Data: 0.023 (0.051)
Train: 145 [ 150/1251 ( 12%)]  Loss: 4.017 (4.28)  Time: 0.195s, 5238.61/s  (0.200s, 5123.98/s)  LR: 5.266e-04  Data: 0.027 (0.044)
Train: 145 [ 200/1251 ( 16%)]  Loss: 4.619 (4.35)  Time: 0.212s, 4833.72/s  (0.198s, 5181.94/s)  LR: 5.266e-04  Data: 0.022 (0.040)
Train: 145 [ 250/1251 ( 20%)]  Loss: 3.894 (4.27)  Time: 0.163s, 6300.94/s  (0.196s, 5213.68/s)  LR: 5.266e-04  Data: 0.021 (0.037)
Train: 145 [ 300/1251 ( 24%)]  Loss: 4.533 (4.31)  Time: 0.335s, 3060.82/s  (0.195s, 5244.70/s)  LR: 5.266e-04  Data: 0.024 (0.036)
Train: 145 [ 350/1251 ( 28%)]  Loss: 4.019 (4.27)  Time: 0.182s, 5635.49/s  (0.194s, 5277.96/s)  LR: 5.266e-04  Data: 0.031 (0.035)
Train: 145 [ 400/1251 ( 32%)]  Loss: 4.525 (4.30)  Time: 0.164s, 6257.73/s  (0.193s, 5297.29/s)  LR: 5.266e-04  Data: 0.021 (0.034)
Train: 145 [ 450/1251 ( 36%)]  Loss: 4.250 (4.30)  Time: 0.188s, 5452.90/s  (0.193s, 5318.80/s)  LR: 5.266e-04  Data: 0.024 (0.033)
Train: 145 [ 500/1251 ( 40%)]  Loss: 4.169 (4.28)  Time: 0.162s, 6308.76/s  (0.192s, 5322.10/s)  LR: 5.266e-04  Data: 0.033 (0.033)
Train: 145 [ 550/1251 ( 44%)]  Loss: 4.226 (4.28)  Time: 0.475s, 2154.29/s  (0.192s, 5340.90/s)  LR: 5.266e-04  Data: 0.029 (0.033)
Train: 145 [ 600/1251 ( 48%)]  Loss: 3.870 (4.25)  Time: 0.166s, 6173.53/s  (0.191s, 5356.36/s)  LR: 5.266e-04  Data: 0.033 (0.032)
Train: 145 [ 650/1251 ( 52%)]  Loss: 4.532 (4.27)  Time: 0.159s, 6450.03/s  (0.192s, 5344.89/s)  LR: 5.266e-04  Data: 0.028 (0.032)
Train: 145 [ 700/1251 ( 56%)]  Loss: 4.342 (4.27)  Time: 0.170s, 6030.03/s  (0.192s, 5343.75/s)  LR: 5.266e-04  Data: 0.031 (0.032)
Train: 145 [ 750/1251 ( 60%)]  Loss: 4.227 (4.27)  Time: 0.359s, 2851.22/s  (0.191s, 5352.27/s)  LR: 5.266e-04  Data: 0.024 (0.031)
Train: 145 [ 800/1251 ( 64%)]  Loss: 4.479 (4.28)  Time: 0.179s, 5706.97/s  (0.192s, 5343.15/s)  LR: 5.266e-04  Data: 0.023 (0.031)
Train: 145 [ 850/1251 ( 68%)]  Loss: 4.051 (4.27)  Time: 0.177s, 5780.18/s  (0.192s, 5346.24/s)  LR: 5.266e-04  Data: 0.028 (0.031)
Train: 145 [ 900/1251 ( 72%)]  Loss: 4.103 (4.26)  Time: 0.181s, 5666.16/s  (0.192s, 5340.37/s)  LR: 5.266e-04  Data: 0.051 (0.031)
Train: 145 [ 950/1251 ( 76%)]  Loss: 4.245 (4.26)  Time: 0.315s, 3252.83/s  (0.192s, 5342.24/s)  LR: 5.266e-04  Data: 0.023 (0.031)
Train: 145 [1000/1251 ( 80%)]  Loss: 4.099 (4.25)  Time: 0.178s, 5760.24/s  (0.192s, 5341.04/s)  LR: 5.266e-04  Data: 0.023 (0.031)
Train: 145 [1050/1251 ( 84%)]  Loss: 4.343 (4.26)  Time: 0.168s, 6104.47/s  (0.192s, 5337.55/s)  LR: 5.266e-04  Data: 0.039 (0.031)
Train: 145 [1100/1251 ( 88%)]  Loss: 4.337 (4.26)  Time: 0.170s, 6032.49/s  (0.192s, 5333.81/s)  LR: 5.266e-04  Data: 0.026 (0.031)
Train: 145 [1150/1251 ( 92%)]  Loss: 4.139 (4.25)  Time: 0.174s, 5872.58/s  (0.192s, 5325.15/s)  LR: 5.266e-04  Data: 0.019 (0.030)
Train: 145 [1200/1251 ( 96%)]  Loss: 4.316 (4.26)  Time: 0.211s, 4860.40/s  (0.192s, 5323.92/s)  LR: 5.266e-04  Data: 0.027 (0.030)
Train: 145 [1250/1251 (100%)]  Loss: 4.339 (4.26)  Time: 0.114s, 9020.44/s  (0.192s, 5337.91/s)  LR: 5.266e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.786 (1.786)  Loss:  1.0409 (1.0409)  Acc@1: 81.6406 (81.6406)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1439 (1.7522)  Acc@1: 80.1887 (65.1520)  Acc@5: 94.3396 (86.4880)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-144.pth.tar', 64.62200003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-141.pth.tar', 64.61999997802734)

Train: 146 [   0/1251 (  0%)]  Loss: 4.125 (4.13)  Time: 1.805s,  567.30/s  (1.805s,  567.30/s)  LR: 5.214e-04  Data: 1.679 (1.679)
Train: 146 [  50/1251 (  4%)]  Loss: 4.158 (4.14)  Time: 0.172s, 5948.54/s  (0.223s, 4594.91/s)  LR: 5.214e-04  Data: 0.031 (0.073)
Train: 146 [ 100/1251 (  8%)]  Loss: 4.430 (4.24)  Time: 0.174s, 5882.53/s  (0.206s, 4973.65/s)  LR: 5.214e-04  Data: 0.025 (0.052)
Train: 146 [ 150/1251 ( 12%)]  Loss: 4.301 (4.25)  Time: 0.199s, 5147.54/s  (0.199s, 5136.75/s)  LR: 5.214e-04  Data: 0.026 (0.046)
Train: 146 [ 200/1251 ( 16%)]  Loss: 4.195 (4.24)  Time: 0.192s, 5338.78/s  (0.196s, 5226.93/s)  LR: 5.214e-04  Data: 0.033 (0.044)
Train: 146 [ 250/1251 ( 20%)]  Loss: 4.269 (4.25)  Time: 0.179s, 5712.30/s  (0.195s, 5253.27/s)  LR: 5.214e-04  Data: 0.026 (0.044)
Train: 146 [ 300/1251 ( 24%)]  Loss: 4.469 (4.28)  Time: 0.167s, 6142.43/s  (0.193s, 5293.44/s)  LR: 5.214e-04  Data: 0.027 (0.042)
Train: 146 [ 350/1251 ( 28%)]  Loss: 4.000 (4.24)  Time: 0.165s, 6222.43/s  (0.192s, 5322.76/s)  LR: 5.214e-04  Data: 0.031 (0.041)
Train: 146 [ 400/1251 ( 32%)]  Loss: 4.007 (4.22)  Time: 0.237s, 4322.64/s  (0.192s, 5319.83/s)  LR: 5.214e-04  Data: 0.027 (0.040)
Train: 146 [ 450/1251 ( 36%)]  Loss: 4.287 (4.22)  Time: 0.181s, 5658.21/s  (0.192s, 5321.34/s)  LR: 5.214e-04  Data: 0.025 (0.038)
Train: 146 [ 500/1251 ( 40%)]  Loss: 4.281 (4.23)  Time: 0.178s, 5748.93/s  (0.192s, 5324.03/s)  LR: 5.214e-04  Data: 0.024 (0.038)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 146 [ 550/1251 ( 44%)]  Loss: 4.184 (4.23)  Time: 0.173s, 5927.07/s  (0.193s, 5318.11/s)  LR: 5.214e-04  Data: 0.029 (0.037)
Train: 146 [ 600/1251 ( 48%)]  Loss: 4.371 (4.24)  Time: 0.166s, 6185.92/s  (0.192s, 5331.15/s)  LR: 5.214e-04  Data: 0.026 (0.036)
Train: 146 [ 650/1251 ( 52%)]  Loss: 4.594 (4.26)  Time: 0.163s, 6289.29/s  (0.192s, 5339.04/s)  LR: 5.214e-04  Data: 0.026 (0.035)
Train: 146 [ 700/1251 ( 56%)]  Loss: 4.092 (4.25)  Time: 0.176s, 5826.43/s  (0.192s, 5328.67/s)  LR: 5.214e-04  Data: 0.027 (0.035)
Train: 146 [ 750/1251 ( 60%)]  Loss: 4.270 (4.25)  Time: 0.162s, 6327.46/s  (0.192s, 5329.81/s)  LR: 5.214e-04  Data: 0.026 (0.034)
Train: 146 [ 800/1251 ( 64%)]  Loss: 3.932 (4.23)  Time: 0.166s, 6174.90/s  (0.192s, 5333.25/s)  LR: 5.214e-04  Data: 0.023 (0.034)
Train: 146 [ 850/1251 ( 68%)]  Loss: 4.201 (4.23)  Time: 0.181s, 5662.18/s  (0.192s, 5328.90/s)  LR: 5.214e-04  Data: 0.025 (0.034)
Train: 146 [ 900/1251 ( 72%)]  Loss: 4.044 (4.22)  Time: 0.165s, 6197.73/s  (0.192s, 5329.07/s)  LR: 5.214e-04  Data: 0.023 (0.033)
Train: 146 [ 950/1251 ( 76%)]  Loss: 4.227 (4.22)  Time: 0.194s, 5284.07/s  (0.192s, 5323.37/s)  LR: 5.214e-04  Data: 0.029 (0.033)
Train: 146 [1000/1251 ( 80%)]  Loss: 4.360 (4.23)  Time: 0.164s, 6252.59/s  (0.192s, 5324.19/s)  LR: 5.214e-04  Data: 0.028 (0.033)
Train: 146 [1050/1251 ( 84%)]  Loss: 3.898 (4.21)  Time: 0.166s, 6163.33/s  (0.193s, 5315.32/s)  LR: 5.214e-04  Data: 0.032 (0.032)
Train: 146 [1100/1251 ( 88%)]  Loss: 4.047 (4.21)  Time: 0.165s, 6188.36/s  (0.193s, 5311.10/s)  LR: 5.214e-04  Data: 0.035 (0.032)
Train: 146 [1150/1251 ( 92%)]  Loss: 3.976 (4.20)  Time: 0.409s, 2505.47/s  (0.193s, 5299.18/s)  LR: 5.214e-04  Data: 0.025 (0.032)
Train: 146 [1200/1251 ( 96%)]  Loss: 4.330 (4.20)  Time: 0.183s, 5585.15/s  (0.193s, 5302.75/s)  LR: 5.214e-04  Data: 0.053 (0.032)
Train: 146 [1250/1251 (100%)]  Loss: 4.169 (4.20)  Time: 0.113s, 9025.13/s  (0.193s, 5316.10/s)  LR: 5.214e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.875 (1.875)  Loss:  1.1296 (1.1296)  Acc@1: 82.1289 (82.1289)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2428 (1.8009)  Acc@1: 79.4811 (64.6180)  Acc@5: 93.9859 (86.2940)
Train: 147 [   0/1251 (  0%)]  Loss: 4.239 (4.24)  Time: 1.880s,  544.70/s  (1.880s,  544.70/s)  LR: 5.162e-04  Data: 1.760 (1.760)
Train: 147 [  50/1251 (  4%)]  Loss: 4.012 (4.13)  Time: 0.178s, 5753.34/s  (0.223s, 4593.56/s)  LR: 5.162e-04  Data: 0.033 (0.069)
Train: 147 [ 100/1251 (  8%)]  Loss: 3.964 (4.07)  Time: 0.157s, 6521.01/s  (0.205s, 4990.86/s)  LR: 5.162e-04  Data: 0.029 (0.048)
Train: 147 [ 150/1251 ( 12%)]  Loss: 4.464 (4.17)  Time: 0.163s, 6272.53/s  (0.202s, 5064.40/s)  LR: 5.162e-04  Data: 0.033 (0.042)
Train: 147 [ 200/1251 ( 16%)]  Loss: 4.359 (4.21)  Time: 0.184s, 5577.82/s  (0.199s, 5155.02/s)  LR: 5.162e-04  Data: 0.034 (0.039)
Train: 147 [ 250/1251 ( 20%)]  Loss: 4.010 (4.17)  Time: 0.183s, 5589.42/s  (0.197s, 5209.11/s)  LR: 5.162e-04  Data: 0.026 (0.037)
Train: 147 [ 300/1251 ( 24%)]  Loss: 3.830 (4.13)  Time: 0.175s, 5849.54/s  (0.194s, 5273.16/s)  LR: 5.162e-04  Data: 0.025 (0.036)
Train: 147 [ 350/1251 ( 28%)]  Loss: 4.223 (4.14)  Time: 0.159s, 6458.00/s  (0.194s, 5290.97/s)  LR: 5.162e-04  Data: 0.030 (0.035)
Train: 147 [ 400/1251 ( 32%)]  Loss: 4.108 (4.13)  Time: 0.193s, 5301.41/s  (0.193s, 5305.38/s)  LR: 5.162e-04  Data: 0.031 (0.034)
Train: 147 [ 450/1251 ( 36%)]  Loss: 4.252 (4.15)  Time: 0.181s, 5648.71/s  (0.193s, 5301.27/s)  LR: 5.162e-04  Data: 0.025 (0.033)
Train: 147 [ 500/1251 ( 40%)]  Loss: 4.360 (4.17)  Time: 0.184s, 5563.10/s  (0.193s, 5306.83/s)  LR: 5.162e-04  Data: 0.028 (0.033)
Train: 147 [ 550/1251 ( 44%)]  Loss: 4.534 (4.20)  Time: 0.179s, 5717.13/s  (0.193s, 5313.40/s)  LR: 5.162e-04  Data: 0.027 (0.032)
Train: 147 [ 600/1251 ( 48%)]  Loss: 4.150 (4.19)  Time: 0.166s, 6151.67/s  (0.193s, 5310.77/s)  LR: 5.162e-04  Data: 0.032 (0.032)
Train: 147 [ 650/1251 ( 52%)]  Loss: 4.211 (4.19)  Time: 0.162s, 6311.90/s  (0.192s, 5333.78/s)  LR: 5.162e-04  Data: 0.024 (0.032)
Train: 147 [ 700/1251 ( 56%)]  Loss: 4.631 (4.22)  Time: 0.159s, 6436.38/s  (0.192s, 5328.79/s)  LR: 5.162e-04  Data: 0.026 (0.031)
Train: 147 [ 750/1251 ( 60%)]  Loss: 4.419 (4.24)  Time: 0.165s, 6203.00/s  (0.193s, 5307.54/s)  LR: 5.162e-04  Data: 0.032 (0.031)
Train: 147 [ 800/1251 ( 64%)]  Loss: 4.330 (4.24)  Time: 0.182s, 5623.42/s  (0.193s, 5318.28/s)  LR: 5.162e-04  Data: 0.024 (0.031)
Train: 147 [ 850/1251 ( 68%)]  Loss: 4.253 (4.24)  Time: 0.177s, 5783.83/s  (0.193s, 5316.18/s)  LR: 5.162e-04  Data: 0.023 (0.031)
Train: 147 [ 900/1251 ( 72%)]  Loss: 4.227 (4.24)  Time: 0.171s, 5990.29/s  (0.193s, 5308.90/s)  LR: 5.162e-04  Data: 0.031 (0.031)
Train: 147 [ 950/1251 ( 76%)]  Loss: 3.765 (4.22)  Time: 0.204s, 5029.83/s  (0.193s, 5298.23/s)  LR: 5.162e-04  Data: 0.026 (0.030)
Train: 147 [1000/1251 ( 80%)]  Loss: 3.346 (4.18)  Time: 0.165s, 6202.41/s  (0.193s, 5311.21/s)  LR: 5.162e-04  Data: 0.029 (0.030)
Train: 147 [1050/1251 ( 84%)]  Loss: 4.365 (4.18)  Time: 0.162s, 6334.50/s  (0.193s, 5301.16/s)  LR: 5.162e-04  Data: 0.026 (0.030)
Train: 147 [1100/1251 ( 88%)]  Loss: 4.369 (4.19)  Time: 0.181s, 5644.45/s  (0.193s, 5299.56/s)  LR: 5.162e-04  Data: 0.038 (0.030)
Train: 147 [1150/1251 ( 92%)]  Loss: 4.239 (4.19)  Time: 0.172s, 5961.26/s  (0.193s, 5306.00/s)  LR: 5.162e-04  Data: 0.027 (0.030)
Train: 147 [1200/1251 ( 96%)]  Loss: 4.078 (4.19)  Time: 0.187s, 5471.98/s  (0.193s, 5306.34/s)  LR: 5.162e-04  Data: 0.040 (0.030)
Train: 147 [1250/1251 (100%)]  Loss: 4.222 (4.19)  Time: 0.113s, 9050.86/s  (0.193s, 5315.42/s)  LR: 5.162e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.796 (1.796)  Loss:  1.2372 (1.2372)  Acc@1: 80.8594 (80.8594)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.2466 (1.8565)  Acc@1: 79.5991 (65.0960)  Acc@5: 93.3962 (86.4280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-144.pth.tar', 64.62200003417969)

Train: 148 [   0/1251 (  0%)]  Loss: 4.227 (4.23)  Time: 1.704s,  601.07/s  (1.704s,  601.07/s)  LR: 5.110e-04  Data: 1.566 (1.566)
Train: 148 [  50/1251 (  4%)]  Loss: 4.322 (4.27)  Time: 0.162s, 6302.88/s  (0.223s, 4589.14/s)  LR: 5.110e-04  Data: 0.029 (0.074)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 148 [ 100/1251 (  8%)]  Loss: 4.181 (4.24)  Time: 0.161s, 6355.37/s  (0.206s, 4974.81/s)  LR: 5.110e-04  Data: 0.024 (0.060)
Train: 148 [ 150/1251 ( 12%)]  Loss: 4.514 (4.31)  Time: 0.181s, 5649.33/s  (0.199s, 5156.75/s)  LR: 5.110e-04  Data: 0.021 (0.053)
Train: 148 [ 200/1251 ( 16%)]  Loss: 3.898 (4.23)  Time: 0.173s, 5919.19/s  (0.196s, 5218.27/s)  LR: 5.110e-04  Data: 0.028 (0.048)
Train: 148 [ 250/1251 ( 20%)]  Loss: 4.050 (4.20)  Time: 0.179s, 5729.88/s  (0.196s, 5212.53/s)  LR: 5.110e-04  Data: 0.025 (0.047)
Train: 148 [ 300/1251 ( 24%)]  Loss: 3.661 (4.12)  Time: 0.175s, 5843.00/s  (0.195s, 5241.98/s)  LR: 5.110e-04  Data: 0.031 (0.044)
Train: 148 [ 350/1251 ( 28%)]  Loss: 4.173 (4.13)  Time: 0.150s, 6812.08/s  (0.194s, 5277.85/s)  LR: 5.110e-04  Data: 0.022 (0.044)
Train: 148 [ 400/1251 ( 32%)]  Loss: 4.058 (4.12)  Time: 0.390s, 2624.59/s  (0.194s, 5291.73/s)  LR: 5.110e-04  Data: 0.030 (0.043)
Train: 148 [ 450/1251 ( 36%)]  Loss: 4.447 (4.15)  Time: 0.179s, 5728.17/s  (0.193s, 5313.71/s)  LR: 5.110e-04  Data: 0.025 (0.041)
Train: 148 [ 500/1251 ( 40%)]  Loss: 4.233 (4.16)  Time: 0.172s, 5958.16/s  (0.192s, 5319.87/s)  LR: 5.110e-04  Data: 0.032 (0.040)
Train: 148 [ 550/1251 ( 44%)]  Loss: 4.765 (4.21)  Time: 0.180s, 5687.53/s  (0.193s, 5310.35/s)  LR: 5.110e-04  Data: 0.021 (0.039)
Train: 148 [ 600/1251 ( 48%)]  Loss: 4.339 (4.22)  Time: 0.188s, 5457.95/s  (0.192s, 5323.96/s)  LR: 5.110e-04  Data: 0.023 (0.038)
Train: 148 [ 650/1251 ( 52%)]  Loss: 4.191 (4.22)  Time: 0.196s, 5225.24/s  (0.192s, 5327.78/s)  LR: 5.110e-04  Data: 0.025 (0.037)
Train: 148 [ 700/1251 ( 56%)]  Loss: 4.490 (4.24)  Time: 0.157s, 6533.31/s  (0.192s, 5321.55/s)  LR: 5.110e-04  Data: 0.021 (0.036)
Train: 148 [ 750/1251 ( 60%)]  Loss: 3.942 (4.22)  Time: 0.184s, 5552.79/s  (0.192s, 5321.35/s)  LR: 5.110e-04  Data: 0.031 (0.036)
Train: 148 [ 800/1251 ( 64%)]  Loss: 4.162 (4.21)  Time: 0.231s, 4435.53/s  (0.192s, 5320.43/s)  LR: 5.110e-04  Data: 0.028 (0.035)
Train: 148 [ 850/1251 ( 68%)]  Loss: 3.822 (4.19)  Time: 0.167s, 6124.59/s  (0.192s, 5322.59/s)  LR: 5.110e-04  Data: 0.028 (0.035)
Train: 148 [ 900/1251 ( 72%)]  Loss: 4.414 (4.20)  Time: 0.185s, 5530.31/s  (0.192s, 5320.92/s)  LR: 5.110e-04  Data: 0.024 (0.034)
Train: 148 [ 950/1251 ( 76%)]  Loss: 4.071 (4.20)  Time: 0.179s, 5709.08/s  (0.192s, 5327.65/s)  LR: 5.110e-04  Data: 0.032 (0.034)
Train: 148 [1000/1251 ( 80%)]  Loss: 4.564 (4.22)  Time: 0.202s, 5078.42/s  (0.192s, 5325.99/s)  LR: 5.110e-04  Data: 0.020 (0.034)
Train: 148 [1050/1251 ( 84%)]  Loss: 4.113 (4.21)  Time: 0.159s, 6444.30/s  (0.192s, 5328.48/s)  LR: 5.110e-04  Data: 0.027 (0.033)
Train: 148 [1100/1251 ( 88%)]  Loss: 4.547 (4.23)  Time: 0.163s, 6281.60/s  (0.192s, 5330.27/s)  LR: 5.110e-04  Data: 0.032 (0.033)
Train: 148 [1150/1251 ( 92%)]  Loss: 4.687 (4.24)  Time: 0.179s, 5716.01/s  (0.192s, 5327.91/s)  LR: 5.110e-04  Data: 0.024 (0.034)
Train: 148 [1200/1251 ( 96%)]  Loss: 4.078 (4.24)  Time: 0.167s, 6138.97/s  (0.192s, 5321.99/s)  LR: 5.110e-04  Data: 0.031 (0.034)
Train: 148 [1250/1251 (100%)]  Loss: 3.987 (4.23)  Time: 0.114s, 9010.53/s  (0.192s, 5340.21/s)  LR: 5.110e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.857 (1.857)  Loss:  1.1743 (1.1743)  Acc@1: 82.2266 (82.2266)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.3563 (1.8114)  Acc@1: 78.6557 (64.9920)  Acc@5: 92.6887 (86.3260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-138.pth.tar', 64.65200003173828)

Train: 149 [   0/1251 (  0%)]  Loss: 4.327 (4.33)  Time: 1.903s,  538.00/s  (1.903s,  538.00/s)  LR: 5.057e-04  Data: 1.774 (1.774)
Train: 149 [  50/1251 (  4%)]  Loss: 4.183 (4.25)  Time: 0.164s, 6255.01/s  (0.224s, 4567.95/s)  LR: 5.057e-04  Data: 0.029 (0.082)
Train: 149 [ 100/1251 (  8%)]  Loss: 3.997 (4.17)  Time: 0.158s, 6470.65/s  (0.206s, 4962.55/s)  LR: 5.057e-04  Data: 0.026 (0.062)
Train: 149 [ 150/1251 ( 12%)]  Loss: 4.401 (4.23)  Time: 0.180s, 5689.44/s  (0.200s, 5122.39/s)  LR: 5.057e-04  Data: 0.024 (0.055)
Train: 149 [ 200/1251 ( 16%)]  Loss: 4.078 (4.20)  Time: 0.161s, 6341.50/s  (0.198s, 5178.61/s)  LR: 5.057e-04  Data: 0.029 (0.048)
Train: 149 [ 250/1251 ( 20%)]  Loss: 4.509 (4.25)  Time: 0.168s, 6101.23/s  (0.195s, 5245.37/s)  LR: 5.057e-04  Data: 0.025 (0.044)
Train: 149 [ 300/1251 ( 24%)]  Loss: 4.099 (4.23)  Time: 0.163s, 6293.74/s  (0.194s, 5276.15/s)  LR: 5.057e-04  Data: 0.027 (0.042)
Train: 149 [ 350/1251 ( 28%)]  Loss: 4.424 (4.25)  Time: 0.172s, 5950.09/s  (0.194s, 5267.99/s)  LR: 5.057e-04  Data: 0.025 (0.043)
Train: 149 [ 400/1251 ( 32%)]  Loss: 4.353 (4.26)  Time: 0.185s, 5528.70/s  (0.193s, 5300.28/s)  LR: 5.057e-04  Data: 0.025 (0.041)
Train: 149 [ 450/1251 ( 36%)]  Loss: 4.313 (4.27)  Time: 0.180s, 5696.15/s  (0.193s, 5293.55/s)  LR: 5.057e-04  Data: 0.031 (0.039)
Train: 149 [ 500/1251 ( 40%)]  Loss: 4.410 (4.28)  Time: 0.169s, 6070.87/s  (0.193s, 5308.66/s)  LR: 5.057e-04  Data: 0.024 (0.038)
Train: 149 [ 550/1251 ( 44%)]  Loss: 4.259 (4.28)  Time: 0.175s, 5843.76/s  (0.192s, 5321.19/s)  LR: 5.057e-04  Data: 0.045 (0.037)
Train: 149 [ 600/1251 ( 48%)]  Loss: 4.254 (4.28)  Time: 0.374s, 2737.25/s  (0.193s, 5306.75/s)  LR: 5.057e-04  Data: 0.033 (0.036)
Train: 149 [ 650/1251 ( 52%)]  Loss: 4.054 (4.26)  Time: 0.178s, 5755.35/s  (0.192s, 5327.74/s)  LR: 5.057e-04  Data: 0.029 (0.036)
Train: 149 [ 700/1251 ( 56%)]  Loss: 3.912 (4.24)  Time: 0.170s, 6014.48/s  (0.192s, 5324.92/s)  LR: 5.057e-04  Data: 0.024 (0.035)
Train: 149 [ 750/1251 ( 60%)]  Loss: 4.164 (4.23)  Time: 0.195s, 5246.38/s  (0.192s, 5324.24/s)  LR: 5.057e-04  Data: 0.058 (0.035)
Train: 149 [ 800/1251 ( 64%)]  Loss: 4.152 (4.23)  Time: 0.191s, 5355.81/s  (0.192s, 5322.22/s)  LR: 5.057e-04  Data: 0.035 (0.034)
Train: 149 [ 850/1251 ( 68%)]  Loss: 4.051 (4.22)  Time: 0.178s, 5737.20/s  (0.192s, 5322.15/s)  LR: 5.057e-04  Data: 0.029 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 149 [ 900/1251 ( 72%)]  Loss: 4.290 (4.22)  Time: 0.187s, 5477.81/s  (0.192s, 5321.48/s)  LR: 5.057e-04  Data: 0.025 (0.034)
Train: 149 [ 950/1251 ( 76%)]  Loss: 4.046 (4.21)  Time: 0.156s, 6550.38/s  (0.193s, 5317.05/s)  LR: 5.057e-04  Data: 0.028 (0.034)
Train: 149 [1000/1251 ( 80%)]  Loss: 3.765 (4.19)  Time: 0.209s, 4892.37/s  (0.192s, 5320.81/s)  LR: 5.057e-04  Data: 0.030 (0.033)
Train: 149 [1050/1251 ( 84%)]  Loss: 4.350 (4.20)  Time: 0.180s, 5679.27/s  (0.193s, 5309.81/s)  LR: 5.057e-04  Data: 0.024 (0.033)
Train: 149 [1100/1251 ( 88%)]  Loss: 4.576 (4.22)  Time: 0.184s, 5550.72/s  (0.193s, 5309.17/s)  LR: 5.057e-04  Data: 0.026 (0.033)
Train: 149 [1150/1251 ( 92%)]  Loss: 3.955 (4.21)  Time: 0.163s, 6283.60/s  (0.193s, 5309.47/s)  LR: 5.057e-04  Data: 0.032 (0.033)
Train: 149 [1200/1251 ( 96%)]  Loss: 4.001 (4.20)  Time: 0.408s, 2507.22/s  (0.193s, 5306.17/s)  LR: 5.057e-04  Data: 0.035 (0.033)
Train: 149 [1250/1251 (100%)]  Loss: 4.066 (4.19)  Time: 0.113s, 9028.48/s  (0.192s, 5325.48/s)  LR: 5.057e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.795 (1.795)  Loss:  0.9392 (0.9392)  Acc@1: 81.1523 (81.1523)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1247 (1.6991)  Acc@1: 79.0094 (65.6620)  Acc@5: 93.0425 (86.8980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-139.pth.tar', 64.6760000341797)

Train: 150 [   0/1251 (  0%)]  Loss: 4.134 (4.13)  Time: 1.849s,  553.89/s  (1.849s,  553.89/s)  LR: 5.005e-04  Data: 1.716 (1.716)
Train: 150 [  50/1251 (  4%)]  Loss: 4.195 (4.16)  Time: 0.158s, 6477.65/s  (0.220s, 4656.83/s)  LR: 5.005e-04  Data: 0.027 (0.067)
Train: 150 [ 100/1251 (  8%)]  Loss: 3.938 (4.09)  Time: 0.163s, 6293.96/s  (0.205s, 4999.85/s)  LR: 5.005e-04  Data: 0.020 (0.049)
Train: 150 [ 150/1251 ( 12%)]  Loss: 4.382 (4.16)  Time: 0.158s, 6491.94/s  (0.202s, 5069.25/s)  LR: 5.005e-04  Data: 0.030 (0.042)
Train: 150 [ 200/1251 ( 16%)]  Loss: 4.129 (4.16)  Time: 0.164s, 6227.16/s  (0.197s, 5190.96/s)  LR: 5.005e-04  Data: 0.028 (0.038)
Train: 150 [ 250/1251 ( 20%)]  Loss: 4.227 (4.17)  Time: 0.155s, 6616.77/s  (0.194s, 5264.97/s)  LR: 5.005e-04  Data: 0.023 (0.036)
Train: 150 [ 300/1251 ( 24%)]  Loss: 4.063 (4.15)  Time: 0.167s, 6117.74/s  (0.194s, 5284.57/s)  LR: 5.005e-04  Data: 0.035 (0.036)
Train: 150 [ 350/1251 ( 28%)]  Loss: 4.016 (4.14)  Time: 0.328s, 3118.84/s  (0.195s, 5264.58/s)  LR: 5.005e-04  Data: 0.034 (0.036)
Train: 150 [ 400/1251 ( 32%)]  Loss: 4.391 (4.16)  Time: 0.192s, 5323.04/s  (0.193s, 5299.14/s)  LR: 5.005e-04  Data: 0.026 (0.035)
Train: 150 [ 450/1251 ( 36%)]  Loss: 4.381 (4.19)  Time: 0.165s, 6200.58/s  (0.193s, 5304.45/s)  LR: 5.005e-04  Data: 0.027 (0.034)
Train: 150 [ 500/1251 ( 40%)]  Loss: 3.984 (4.17)  Time: 0.179s, 5724.39/s  (0.193s, 5317.77/s)  LR: 5.005e-04  Data: 0.042 (0.033)
Train: 150 [ 550/1251 ( 44%)]  Loss: 4.472 (4.19)  Time: 0.270s, 3786.50/s  (0.193s, 5314.41/s)  LR: 5.005e-04  Data: 0.025 (0.033)
Train: 150 [ 600/1251 ( 48%)]  Loss: 4.141 (4.19)  Time: 0.201s, 5103.58/s  (0.192s, 5323.61/s)  LR: 5.005e-04  Data: 0.023 (0.034)
Train: 150 [ 650/1251 ( 52%)]  Loss: 4.298 (4.20)  Time: 0.166s, 6161.17/s  (0.192s, 5325.63/s)  LR: 5.005e-04  Data: 0.028 (0.035)
Train: 150 [ 700/1251 ( 56%)]  Loss: 4.499 (4.22)  Time: 0.276s, 3704.17/s  (0.192s, 5325.38/s)  LR: 5.005e-04  Data: 0.149 (0.036)
Train: 150 [ 750/1251 ( 60%)]  Loss: 4.071 (4.21)  Time: 0.166s, 6152.07/s  (0.192s, 5323.16/s)  LR: 5.005e-04  Data: 0.030 (0.036)
Train: 150 [ 800/1251 ( 64%)]  Loss: 4.222 (4.21)  Time: 0.154s, 6661.49/s  (0.192s, 5334.61/s)  LR: 5.005e-04  Data: 0.023 (0.036)
Train: 150 [ 850/1251 ( 68%)]  Loss: 4.399 (4.22)  Time: 0.167s, 6143.17/s  (0.192s, 5334.39/s)  LR: 5.005e-04  Data: 0.030 (0.037)
Train: 150 [ 900/1251 ( 72%)]  Loss: 4.300 (4.22)  Time: 0.204s, 5007.58/s  (0.192s, 5330.47/s)  LR: 5.005e-04  Data: 0.087 (0.038)
Train: 150 [ 950/1251 ( 76%)]  Loss: 4.628 (4.24)  Time: 0.175s, 5835.58/s  (0.192s, 5339.75/s)  LR: 5.005e-04  Data: 0.032 (0.038)
Train: 150 [1000/1251 ( 80%)]  Loss: 4.114 (4.24)  Time: 0.158s, 6465.08/s  (0.192s, 5330.36/s)  LR: 5.005e-04  Data: 0.032 (0.037)
Train: 150 [1050/1251 ( 84%)]  Loss: 4.201 (4.24)  Time: 0.181s, 5657.75/s  (0.192s, 5321.39/s)  LR: 5.005e-04  Data: 0.020 (0.036)
Train: 150 [1100/1251 ( 88%)]  Loss: 4.373 (4.24)  Time: 0.421s, 2431.50/s  (0.193s, 5302.45/s)  LR: 5.005e-04  Data: 0.027 (0.036)
Train: 150 [1150/1251 ( 92%)]  Loss: 4.145 (4.24)  Time: 0.198s, 5177.07/s  (0.193s, 5310.86/s)  LR: 5.005e-04  Data: 0.028 (0.036)
Train: 150 [1200/1251 ( 96%)]  Loss: 4.018 (4.23)  Time: 0.167s, 6123.67/s  (0.193s, 5308.81/s)  LR: 5.005e-04  Data: 0.036 (0.036)
Train: 150 [1250/1251 (100%)]  Loss: 4.214 (4.23)  Time: 0.114s, 9019.55/s  (0.192s, 5322.96/s)  LR: 5.005e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.838 (1.838)  Loss:  1.2138 (1.2138)  Acc@1: 79.7852 (79.7852)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.1734 (1.7836)  Acc@1: 79.8349 (64.8540)  Acc@5: 92.9245 (86.1280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-150.pth.tar', 64.85400016113282)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-134.pth.tar', 64.68600006103516)

Train: 151 [   0/1251 (  0%)]  Loss: 3.863 (3.86)  Time: 1.714s,  597.37/s  (1.714s,  597.37/s)  LR: 4.953e-04  Data: 1.585 (1.585)
Train: 151 [  50/1251 (  4%)]  Loss: 3.981 (3.92)  Time: 0.178s, 5768.78/s  (0.225s, 4549.72/s)  LR: 4.953e-04  Data: 0.028 (0.080)
Train: 151 [ 100/1251 (  8%)]  Loss: 4.302 (4.05)  Time: 0.161s, 6352.28/s  (0.206s, 4966.97/s)  LR: 4.953e-04  Data: 0.025 (0.061)
Train: 151 [ 150/1251 ( 12%)]  Loss: 4.022 (4.04)  Time: 0.175s, 5863.05/s  (0.200s, 5117.11/s)  LR: 4.953e-04  Data: 0.022 (0.056)
Train: 151 [ 200/1251 ( 16%)]  Loss: 4.152 (4.06)  Time: 0.170s, 6024.90/s  (0.198s, 5176.24/s)  LR: 4.953e-04  Data: 0.030 (0.053)
Train: 151 [ 250/1251 ( 20%)]  Loss: 4.247 (4.09)  Time: 0.166s, 6180.84/s  (0.196s, 5233.33/s)  LR: 4.953e-04  Data: 0.025 (0.048)
Train: 151 [ 300/1251 ( 24%)]  Loss: 3.732 (4.04)  Time: 0.160s, 6384.14/s  (0.195s, 5259.71/s)  LR: 4.953e-04  Data: 0.030 (0.045)
Train: 151 [ 350/1251 ( 28%)]  Loss: 4.190 (4.06)  Time: 0.180s, 5695.98/s  (0.194s, 5278.00/s)  LR: 4.953e-04  Data: 0.026 (0.042)
Train: 151 [ 400/1251 ( 32%)]  Loss: 4.373 (4.10)  Time: 0.177s, 5774.07/s  (0.193s, 5301.78/s)  LR: 4.953e-04  Data: 0.025 (0.041)
Train: 151 [ 450/1251 ( 36%)]  Loss: 4.352 (4.12)  Time: 0.252s, 4066.55/s  (0.193s, 5296.66/s)  LR: 4.953e-04  Data: 0.036 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 151 [ 500/1251 ( 40%)]  Loss: 4.586 (4.16)  Time: 0.157s, 6517.53/s  (0.193s, 5314.95/s)  LR: 4.953e-04  Data: 0.028 (0.038)
Train: 151 [ 550/1251 ( 44%)]  Loss: 4.054 (4.15)  Time: 0.174s, 5879.95/s  (0.193s, 5318.11/s)  LR: 4.953e-04  Data: 0.032 (0.037)
Train: 151 [ 600/1251 ( 48%)]  Loss: 4.024 (4.14)  Time: 0.186s, 5518.22/s  (0.192s, 5327.23/s)  LR: 4.953e-04  Data: 0.023 (0.036)
Train: 151 [ 650/1251 ( 52%)]  Loss: 4.153 (4.15)  Time: 0.192s, 5342.93/s  (0.192s, 5325.73/s)  LR: 4.953e-04  Data: 0.024 (0.036)
Train: 151 [ 700/1251 ( 56%)]  Loss: 4.229 (4.15)  Time: 0.151s, 6793.57/s  (0.192s, 5327.29/s)  LR: 4.953e-04  Data: 0.026 (0.035)
Train: 151 [ 750/1251 ( 60%)]  Loss: 4.233 (4.16)  Time: 0.190s, 5391.45/s  (0.193s, 5316.19/s)  LR: 4.953e-04  Data: 0.030 (0.035)
Train: 151 [ 800/1251 ( 64%)]  Loss: 4.145 (4.16)  Time: 0.196s, 5224.28/s  (0.193s, 5316.92/s)  LR: 4.953e-04  Data: 0.028 (0.034)
Train: 151 [ 850/1251 ( 68%)]  Loss: 4.069 (4.15)  Time: 0.245s, 4174.01/s  (0.193s, 5318.47/s)  LR: 4.953e-04  Data: 0.028 (0.034)
Train: 151 [ 900/1251 ( 72%)]  Loss: 3.712 (4.13)  Time: 0.171s, 6002.27/s  (0.192s, 5334.21/s)  LR: 4.953e-04  Data: 0.020 (0.034)
Train: 151 [ 950/1251 ( 76%)]  Loss: 4.038 (4.12)  Time: 0.184s, 5557.05/s  (0.193s, 5319.29/s)  LR: 4.953e-04  Data: 0.037 (0.033)
Train: 151 [1000/1251 ( 80%)]  Loss: 3.961 (4.12)  Time: 0.170s, 6028.29/s  (0.192s, 5327.59/s)  LR: 4.953e-04  Data: 0.026 (0.033)
Train: 151 [1050/1251 ( 84%)]  Loss: 3.912 (4.11)  Time: 0.424s, 2417.51/s  (0.192s, 5319.57/s)  LR: 4.953e-04  Data: 0.023 (0.033)
Train: 151 [1100/1251 ( 88%)]  Loss: 4.004 (4.10)  Time: 0.171s, 5973.84/s  (0.192s, 5320.58/s)  LR: 4.953e-04  Data: 0.030 (0.033)
Train: 151 [1150/1251 ( 92%)]  Loss: 4.431 (4.12)  Time: 0.201s, 5091.88/s  (0.192s, 5323.94/s)  LR: 4.953e-04  Data: 0.025 (0.032)
Train: 151 [1200/1251 ( 96%)]  Loss: 3.716 (4.10)  Time: 0.164s, 6260.37/s  (0.192s, 5319.72/s)  LR: 4.953e-04  Data: 0.025 (0.032)
Train: 151 [1250/1251 (100%)]  Loss: 4.161 (4.10)  Time: 0.113s, 9048.78/s  (0.192s, 5330.87/s)  LR: 4.953e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.812 (1.812)  Loss:  1.1104 (1.1104)  Acc@1: 81.0547 (81.0547)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1855 (1.8430)  Acc@1: 80.6604 (64.5080)  Acc@5: 93.6321 (86.1220)
Train: 152 [   0/1251 (  0%)]  Loss: 4.264 (4.26)  Time: 1.640s,  624.38/s  (1.640s,  624.38/s)  LR: 4.900e-04  Data: 1.519 (1.519)
Train: 152 [  50/1251 (  4%)]  Loss: 4.532 (4.40)  Time: 0.173s, 5906.26/s  (0.220s, 4650.85/s)  LR: 4.900e-04  Data: 0.027 (0.071)
Train: 152 [ 100/1251 (  8%)]  Loss: 4.162 (4.32)  Time: 0.162s, 6334.33/s  (0.204s, 5012.10/s)  LR: 4.900e-04  Data: 0.028 (0.050)
Train: 152 [ 150/1251 ( 12%)]  Loss: 4.218 (4.29)  Time: 0.174s, 5895.18/s  (0.200s, 5131.90/s)  LR: 4.900e-04  Data: 0.025 (0.047)
Train: 152 [ 200/1251 ( 16%)]  Loss: 3.972 (4.23)  Time: 0.166s, 6179.28/s  (0.196s, 5222.14/s)  LR: 4.900e-04  Data: 0.029 (0.044)
Train: 152 [ 250/1251 ( 20%)]  Loss: 4.228 (4.23)  Time: 0.192s, 5333.47/s  (0.196s, 5232.80/s)  LR: 4.900e-04  Data: 0.021 (0.042)
Train: 152 [ 300/1251 ( 24%)]  Loss: 4.394 (4.25)  Time: 0.174s, 5881.52/s  (0.194s, 5278.98/s)  LR: 4.900e-04  Data: 0.028 (0.041)
Train: 152 [ 350/1251 ( 28%)]  Loss: 3.913 (4.21)  Time: 0.167s, 6137.04/s  (0.193s, 5297.63/s)  LR: 4.900e-04  Data: 0.019 (0.039)
Train: 152 [ 400/1251 ( 32%)]  Loss: 3.730 (4.16)  Time: 0.161s, 6349.90/s  (0.194s, 5291.53/s)  LR: 4.900e-04  Data: 0.028 (0.038)
Train: 152 [ 450/1251 ( 36%)]  Loss: 4.048 (4.15)  Time: 0.168s, 6091.90/s  (0.192s, 5330.72/s)  LR: 4.900e-04  Data: 0.026 (0.036)
Train: 152 [ 500/1251 ( 40%)]  Loss: 3.962 (4.13)  Time: 0.167s, 6141.44/s  (0.192s, 5326.08/s)  LR: 4.900e-04  Data: 0.026 (0.036)
Train: 152 [ 550/1251 ( 44%)]  Loss: 4.168 (4.13)  Time: 0.203s, 5038.94/s  (0.192s, 5319.68/s)  LR: 4.900e-04  Data: 0.029 (0.035)
Train: 152 [ 600/1251 ( 48%)]  Loss: 4.160 (4.13)  Time: 0.180s, 5674.60/s  (0.193s, 5318.35/s)  LR: 4.900e-04  Data: 0.029 (0.034)
Train: 152 [ 650/1251 ( 52%)]  Loss: 4.164 (4.14)  Time: 0.177s, 5793.85/s  (0.192s, 5323.31/s)  LR: 4.900e-04  Data: 0.020 (0.034)
Train: 152 [ 700/1251 ( 56%)]  Loss: 4.147 (4.14)  Time: 0.185s, 5538.19/s  (0.193s, 5311.14/s)  LR: 4.900e-04  Data: 0.027 (0.034)
Train: 152 [ 750/1251 ( 60%)]  Loss: 4.384 (4.15)  Time: 0.164s, 6225.08/s  (0.192s, 5322.03/s)  LR: 4.900e-04  Data: 0.028 (0.033)
Train: 152 [ 800/1251 ( 64%)]  Loss: 4.199 (4.16)  Time: 0.326s, 3137.21/s  (0.193s, 5298.89/s)  LR: 4.900e-04  Data: 0.025 (0.033)
Train: 152 [ 850/1251 ( 68%)]  Loss: 4.226 (4.16)  Time: 0.189s, 5425.61/s  (0.192s, 5322.44/s)  LR: 4.900e-04  Data: 0.026 (0.033)
Train: 152 [ 900/1251 ( 72%)]  Loss: 4.406 (4.17)  Time: 0.171s, 5988.06/s  (0.193s, 5315.11/s)  LR: 4.900e-04  Data: 0.032 (0.032)
Train: 152 [ 950/1251 ( 76%)]  Loss: 3.819 (4.15)  Time: 0.160s, 6387.82/s  (0.193s, 5316.18/s)  LR: 4.900e-04  Data: 0.029 (0.032)
Train: 152 [1000/1251 ( 80%)]  Loss: 4.196 (4.16)  Time: 0.176s, 5814.76/s  (0.192s, 5323.27/s)  LR: 4.900e-04  Data: 0.030 (0.032)
Train: 152 [1050/1251 ( 84%)]  Loss: 4.180 (4.16)  Time: 0.222s, 4604.88/s  (0.192s, 5325.52/s)  LR: 4.900e-04  Data: 0.035 (0.032)
Train: 152 [1100/1251 ( 88%)]  Loss: 4.260 (4.16)  Time: 0.447s, 2290.38/s  (0.193s, 5312.49/s)  LR: 4.900e-04  Data: 0.026 (0.032)
Train: 152 [1150/1251 ( 92%)]  Loss: 4.057 (4.16)  Time: 0.155s, 6609.19/s  (0.193s, 5313.92/s)  LR: 4.900e-04  Data: 0.027 (0.032)
Train: 152 [1200/1251 ( 96%)]  Loss: 4.311 (4.16)  Time: 0.170s, 6016.84/s  (0.193s, 5312.76/s)  LR: 4.900e-04  Data: 0.021 (0.031)
Train: 152 [1250/1251 (100%)]  Loss: 4.429 (4.17)  Time: 0.113s, 9022.91/s  (0.192s, 5327.38/s)  LR: 4.900e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.834 (1.834)  Loss:  1.1522 (1.1522)  Acc@1: 80.6641 (80.6641)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2206 (1.8075)  Acc@1: 80.5425 (65.5780)  Acc@5: 94.2217 (86.6740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-150.pth.tar', 64.85400016113282)

Train: 153 [   0/1251 (  0%)]  Loss: 3.840 (3.84)  Time: 1.826s,  560.69/s  (1.826s,  560.69/s)  LR: 4.848e-04  Data: 1.696 (1.696)
Train: 153 [  50/1251 (  4%)]  Loss: 4.421 (4.13)  Time: 0.170s, 6030.48/s  (0.224s, 4576.45/s)  LR: 4.848e-04  Data: 0.026 (0.076)
Train: 153 [ 100/1251 (  8%)]  Loss: 4.366 (4.21)  Time: 0.171s, 6004.17/s  (0.209s, 4900.84/s)  LR: 4.848e-04  Data: 0.028 (0.063)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 153 [ 150/1251 ( 12%)]  Loss: 4.325 (4.24)  Time: 0.187s, 5479.90/s  (0.201s, 5095.39/s)  LR: 4.848e-04  Data: 0.027 (0.054)
Train: 153 [ 200/1251 ( 16%)]  Loss: 4.575 (4.31)  Time: 0.186s, 5492.24/s  (0.198s, 5177.09/s)  LR: 4.848e-04  Data: 0.031 (0.048)
Train: 153 [ 250/1251 ( 20%)]  Loss: 4.265 (4.30)  Time: 0.180s, 5674.32/s  (0.196s, 5222.54/s)  LR: 4.848e-04  Data: 0.025 (0.044)
Train: 153 [ 300/1251 ( 24%)]  Loss: 4.535 (4.33)  Time: 0.194s, 5273.77/s  (0.196s, 5237.17/s)  LR: 4.848e-04  Data: 0.021 (0.042)
Train: 153 [ 350/1251 ( 28%)]  Loss: 4.339 (4.33)  Time: 0.175s, 5843.49/s  (0.194s, 5282.80/s)  LR: 4.848e-04  Data: 0.029 (0.040)
Train: 153 [ 400/1251 ( 32%)]  Loss: 3.894 (4.28)  Time: 0.174s, 5879.63/s  (0.194s, 5287.27/s)  LR: 4.848e-04  Data: 0.034 (0.038)
Train: 153 [ 450/1251 ( 36%)]  Loss: 4.088 (4.26)  Time: 0.161s, 6371.23/s  (0.193s, 5298.59/s)  LR: 4.848e-04  Data: 0.025 (0.037)
Train: 153 [ 500/1251 ( 40%)]  Loss: 3.985 (4.24)  Time: 0.181s, 5661.39/s  (0.193s, 5311.79/s)  LR: 4.848e-04  Data: 0.021 (0.036)
Train: 153 [ 550/1251 ( 44%)]  Loss: 3.995 (4.22)  Time: 0.185s, 5526.95/s  (0.193s, 5317.37/s)  LR: 4.848e-04  Data: 0.025 (0.036)
Train: 153 [ 600/1251 ( 48%)]  Loss: 4.016 (4.20)  Time: 0.164s, 6229.20/s  (0.192s, 5324.83/s)  LR: 4.848e-04  Data: 0.027 (0.035)
Train: 153 [ 650/1251 ( 52%)]  Loss: 4.216 (4.20)  Time: 0.181s, 5671.38/s  (0.192s, 5333.41/s)  LR: 4.848e-04  Data: 0.025 (0.035)
Train: 153 [ 700/1251 ( 56%)]  Loss: 3.907 (4.18)  Time: 0.163s, 6286.23/s  (0.192s, 5330.08/s)  LR: 4.848e-04  Data: 0.026 (0.034)
Train: 153 [ 750/1251 ( 60%)]  Loss: 4.466 (4.20)  Time: 0.335s, 3061.17/s  (0.192s, 5330.51/s)  LR: 4.848e-04  Data: 0.025 (0.034)
Train: 153 [ 800/1251 ( 64%)]  Loss: 4.041 (4.19)  Time: 0.160s, 6402.08/s  (0.192s, 5321.16/s)  LR: 4.848e-04  Data: 0.026 (0.033)
Train: 153 [ 850/1251 ( 68%)]  Loss: 4.064 (4.19)  Time: 0.164s, 6245.85/s  (0.193s, 5319.23/s)  LR: 4.848e-04  Data: 0.030 (0.033)
Train: 153 [ 900/1251 ( 72%)]  Loss: 4.188 (4.19)  Time: 0.201s, 5092.19/s  (0.192s, 5330.77/s)  LR: 4.848e-04  Data: 0.027 (0.033)
Train: 153 [ 950/1251 ( 76%)]  Loss: 4.279 (4.19)  Time: 0.174s, 5883.26/s  (0.193s, 5314.32/s)  LR: 4.848e-04  Data: 0.026 (0.033)
Train: 153 [1000/1251 ( 80%)]  Loss: 4.253 (4.19)  Time: 0.210s, 4881.64/s  (0.193s, 5312.57/s)  LR: 4.848e-04  Data: 0.020 (0.032)
Train: 153 [1050/1251 ( 84%)]  Loss: 4.275 (4.20)  Time: 0.177s, 5799.36/s  (0.193s, 5315.97/s)  LR: 4.848e-04  Data: 0.024 (0.032)
Train: 153 [1100/1251 ( 88%)]  Loss: 4.334 (4.20)  Time: 0.173s, 5911.00/s  (0.193s, 5310.31/s)  LR: 4.848e-04  Data: 0.036 (0.032)
Train: 153 [1150/1251 ( 92%)]  Loss: 4.155 (4.20)  Time: 0.163s, 6263.04/s  (0.193s, 5307.03/s)  LR: 4.848e-04  Data: 0.029 (0.032)
Train: 153 [1200/1251 ( 96%)]  Loss: 4.260 (4.20)  Time: 0.163s, 6276.39/s  (0.193s, 5302.66/s)  LR: 4.848e-04  Data: 0.038 (0.032)
Train: 153 [1250/1251 (100%)]  Loss: 4.105 (4.20)  Time: 0.112s, 9106.28/s  (0.193s, 5318.56/s)  LR: 4.848e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.675 (1.675)  Loss:  1.1215 (1.1215)  Acc@1: 80.2734 (80.2734)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1261 (1.7339)  Acc@1: 80.0708 (65.4800)  Acc@5: 93.3962 (86.6980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-132.pth.tar', 64.87599997802734)

Train: 154 [   0/1251 (  0%)]  Loss: 4.367 (4.37)  Time: 1.852s,  552.96/s  (1.852s,  552.96/s)  LR: 4.796e-04  Data: 1.725 (1.725)
Train: 154 [  50/1251 (  4%)]  Loss: 3.853 (4.11)  Time: 0.193s, 5295.78/s  (0.221s, 4633.31/s)  LR: 4.796e-04  Data: 0.026 (0.067)
Train: 154 [ 100/1251 (  8%)]  Loss: 4.460 (4.23)  Time: 0.164s, 6242.98/s  (0.206s, 4968.12/s)  LR: 4.796e-04  Data: 0.027 (0.051)
Train: 154 [ 150/1251 ( 12%)]  Loss: 3.902 (4.15)  Time: 0.179s, 5710.66/s  (0.199s, 5137.01/s)  LR: 4.796e-04  Data: 0.028 (0.046)
Train: 154 [ 200/1251 ( 16%)]  Loss: 4.380 (4.19)  Time: 0.162s, 6303.92/s  (0.197s, 5195.63/s)  LR: 4.796e-04  Data: 0.027 (0.042)
Train: 154 [ 250/1251 ( 20%)]  Loss: 4.286 (4.21)  Time: 0.175s, 5839.69/s  (0.196s, 5213.22/s)  LR: 4.796e-04  Data: 0.031 (0.039)
Train: 154 [ 300/1251 ( 24%)]  Loss: 4.354 (4.23)  Time: 0.165s, 6189.33/s  (0.195s, 5255.24/s)  LR: 4.796e-04  Data: 0.026 (0.039)
Train: 154 [ 350/1251 ( 28%)]  Loss: 4.169 (4.22)  Time: 0.156s, 6562.38/s  (0.194s, 5267.05/s)  LR: 4.796e-04  Data: 0.032 (0.038)
Train: 154 [ 400/1251 ( 32%)]  Loss: 4.223 (4.22)  Time: 0.167s, 6133.29/s  (0.194s, 5284.28/s)  LR: 4.796e-04  Data: 0.033 (0.037)
Train: 154 [ 450/1251 ( 36%)]  Loss: 4.084 (4.21)  Time: 0.187s, 5488.05/s  (0.193s, 5292.34/s)  LR: 4.796e-04  Data: 0.025 (0.036)
Train: 154 [ 500/1251 ( 40%)]  Loss: 4.307 (4.22)  Time: 0.195s, 5239.49/s  (0.193s, 5294.95/s)  LR: 4.796e-04  Data: 0.031 (0.035)
Train: 154 [ 550/1251 ( 44%)]  Loss: 4.217 (4.22)  Time: 0.187s, 5472.96/s  (0.192s, 5320.41/s)  LR: 4.796e-04  Data: 0.023 (0.034)
Train: 154 [ 600/1251 ( 48%)]  Loss: 4.278 (4.22)  Time: 0.180s, 5693.36/s  (0.193s, 5312.00/s)  LR: 4.796e-04  Data: 0.029 (0.034)
Train: 154 [ 650/1251 ( 52%)]  Loss: 4.244 (4.22)  Time: 0.416s, 2459.24/s  (0.193s, 5306.31/s)  LR: 4.796e-04  Data: 0.026 (0.033)
Train: 154 [ 700/1251 ( 56%)]  Loss: 4.392 (4.23)  Time: 0.162s, 6313.10/s  (0.193s, 5312.79/s)  LR: 4.796e-04  Data: 0.028 (0.033)
Train: 154 [ 750/1251 ( 60%)]  Loss: 4.381 (4.24)  Time: 0.162s, 6328.30/s  (0.192s, 5323.15/s)  LR: 4.796e-04  Data: 0.032 (0.033)
Train: 154 [ 800/1251 ( 64%)]  Loss: 4.555 (4.26)  Time: 0.159s, 6452.04/s  (0.192s, 5335.45/s)  LR: 4.796e-04  Data: 0.026 (0.033)
Train: 154 [ 850/1251 ( 68%)]  Loss: 4.428 (4.27)  Time: 0.172s, 5957.85/s  (0.192s, 5328.33/s)  LR: 4.796e-04  Data: 0.032 (0.033)
Train: 154 [ 900/1251 ( 72%)]  Loss: 4.228 (4.27)  Time: 0.160s, 6412.70/s  (0.192s, 5322.94/s)  LR: 4.796e-04  Data: 0.020 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 154 [ 950/1251 ( 76%)]  Loss: 4.114 (4.26)  Time: 0.173s, 5907.32/s  (0.193s, 5318.45/s)  LR: 4.796e-04  Data: 0.030 (0.033)
Train: 154 [1000/1251 ( 80%)]  Loss: 3.947 (4.25)  Time: 0.378s, 2709.85/s  (0.192s, 5320.55/s)  LR: 4.796e-04  Data: 0.032 (0.032)
Train: 154 [1050/1251 ( 84%)]  Loss: 4.575 (4.26)  Time: 0.179s, 5710.66/s  (0.192s, 5325.54/s)  LR: 4.796e-04  Data: 0.018 (0.032)
Train: 154 [1100/1251 ( 88%)]  Loss: 4.602 (4.28)  Time: 0.186s, 5492.12/s  (0.192s, 5325.21/s)  LR: 4.796e-04  Data: 0.023 (0.032)
Train: 154 [1150/1251 ( 92%)]  Loss: 4.487 (4.28)  Time: 0.155s, 6592.56/s  (0.192s, 5322.94/s)  LR: 4.796e-04  Data: 0.026 (0.031)
Train: 154 [1200/1251 ( 96%)]  Loss: 4.289 (4.28)  Time: 0.163s, 6300.28/s  (0.193s, 5316.60/s)  LR: 4.796e-04  Data: 0.029 (0.031)
Train: 154 [1250/1251 (100%)]  Loss: 4.378 (4.29)  Time: 0.113s, 9047.87/s  (0.192s, 5332.89/s)  LR: 4.796e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.827 (1.827)  Loss:  1.0625 (1.0625)  Acc@1: 80.8594 (80.8594)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1000 (1.7312)  Acc@1: 79.8349 (65.5200)  Acc@5: 93.2783 (86.8100)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-140.pth.tar', 64.90599998779297)

Train: 155 [   0/1251 (  0%)]  Loss: 4.160 (4.16)  Time: 1.952s,  524.64/s  (1.952s,  524.64/s)  LR: 4.744e-04  Data: 1.824 (1.824)
Train: 155 [  50/1251 (  4%)]  Loss: 3.986 (4.07)  Time: 0.165s, 6205.57/s  (0.234s, 4379.65/s)  LR: 4.744e-04  Data: 0.026 (0.066)
Train: 155 [ 100/1251 (  8%)]  Loss: 4.126 (4.09)  Time: 0.205s, 5005.60/s  (0.209s, 4910.78/s)  LR: 4.744e-04  Data: 0.030 (0.048)
Train: 155 [ 150/1251 ( 12%)]  Loss: 4.388 (4.17)  Time: 0.184s, 5554.27/s  (0.200s, 5130.03/s)  LR: 4.744e-04  Data: 0.023 (0.041)
Train: 155 [ 200/1251 ( 16%)]  Loss: 4.294 (4.19)  Time: 0.163s, 6269.03/s  (0.199s, 5138.73/s)  LR: 4.744e-04  Data: 0.026 (0.038)
Train: 155 [ 250/1251 ( 20%)]  Loss: 4.301 (4.21)  Time: 0.178s, 5756.75/s  (0.197s, 5197.59/s)  LR: 4.744e-04  Data: 0.026 (0.036)
Train: 155 [ 300/1251 ( 24%)]  Loss: 4.383 (4.23)  Time: 0.173s, 5905.59/s  (0.195s, 5243.26/s)  LR: 4.744e-04  Data: 0.025 (0.035)
Train: 155 [ 350/1251 ( 28%)]  Loss: 4.289 (4.24)  Time: 0.160s, 6406.71/s  (0.195s, 5248.45/s)  LR: 4.744e-04  Data: 0.027 (0.034)
Train: 155 [ 400/1251 ( 32%)]  Loss: 3.852 (4.20)  Time: 0.186s, 5508.80/s  (0.194s, 5269.98/s)  LR: 4.744e-04  Data: 0.023 (0.034)
Train: 155 [ 450/1251 ( 36%)]  Loss: 4.261 (4.20)  Time: 0.163s, 6277.12/s  (0.194s, 5274.52/s)  LR: 4.744e-04  Data: 0.031 (0.033)
Train: 155 [ 500/1251 ( 40%)]  Loss: 3.983 (4.18)  Time: 0.170s, 6008.42/s  (0.193s, 5292.30/s)  LR: 4.744e-04  Data: 0.020 (0.033)
Train: 155 [ 550/1251 ( 44%)]  Loss: 4.198 (4.19)  Time: 0.170s, 6024.99/s  (0.193s, 5309.45/s)  LR: 4.744e-04  Data: 0.023 (0.032)
Train: 155 [ 600/1251 ( 48%)]  Loss: 3.896 (4.16)  Time: 0.173s, 5935.33/s  (0.193s, 5318.83/s)  LR: 4.744e-04  Data: 0.038 (0.032)
Train: 155 [ 650/1251 ( 52%)]  Loss: 4.268 (4.17)  Time: 0.160s, 6412.93/s  (0.192s, 5328.84/s)  LR: 4.744e-04  Data: 0.029 (0.032)
Train: 155 [ 700/1251 ( 56%)]  Loss: 4.201 (4.17)  Time: 0.166s, 6169.58/s  (0.192s, 5336.18/s)  LR: 4.744e-04  Data: 0.026 (0.032)
Train: 155 [ 750/1251 ( 60%)]  Loss: 3.841 (4.15)  Time: 0.164s, 6229.97/s  (0.192s, 5322.86/s)  LR: 4.744e-04  Data: 0.025 (0.033)
Train: 155 [ 800/1251 ( 64%)]  Loss: 4.294 (4.16)  Time: 0.200s, 5120.68/s  (0.192s, 5323.74/s)  LR: 4.744e-04  Data: 0.075 (0.034)
Train: 155 [ 850/1251 ( 68%)]  Loss: 3.846 (4.14)  Time: 0.160s, 6406.00/s  (0.192s, 5333.13/s)  LR: 4.744e-04  Data: 0.027 (0.033)
Train: 155 [ 900/1251 ( 72%)]  Loss: 4.238 (4.15)  Time: 0.180s, 5704.38/s  (0.192s, 5330.98/s)  LR: 4.744e-04  Data: 0.035 (0.033)
Train: 155 [ 950/1251 ( 76%)]  Loss: 4.264 (4.15)  Time: 0.193s, 5306.23/s  (0.192s, 5326.64/s)  LR: 4.744e-04  Data: 0.025 (0.033)
Train: 155 [1000/1251 ( 80%)]  Loss: 4.454 (4.17)  Time: 0.226s, 4531.95/s  (0.192s, 5328.81/s)  LR: 4.744e-04  Data: 0.078 (0.033)
Train: 155 [1050/1251 ( 84%)]  Loss: 3.908 (4.16)  Time: 0.162s, 6317.35/s  (0.192s, 5323.32/s)  LR: 4.744e-04  Data: 0.026 (0.033)
Train: 155 [1100/1251 ( 88%)]  Loss: 4.082 (4.15)  Time: 0.187s, 5471.33/s  (0.192s, 5324.71/s)  LR: 4.744e-04  Data: 0.030 (0.033)
Train: 155 [1150/1251 ( 92%)]  Loss: 4.007 (4.15)  Time: 0.169s, 6058.52/s  (0.193s, 5318.07/s)  LR: 4.744e-04  Data: 0.024 (0.034)
Train: 155 [1200/1251 ( 96%)]  Loss: 4.312 (4.15)  Time: 0.171s, 5997.92/s  (0.193s, 5309.56/s)  LR: 4.744e-04  Data: 0.043 (0.034)
Train: 155 [1250/1251 (100%)]  Loss: 4.259 (4.16)  Time: 0.114s, 9008.18/s  (0.192s, 5330.33/s)  LR: 4.744e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.922 (1.922)  Loss:  1.0624 (1.0624)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1107 (1.7744)  Acc@1: 80.0708 (65.4500)  Acc@5: 92.9245 (86.5900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-142.pth.tar', 64.92399993164062)

Train: 156 [   0/1251 (  0%)]  Loss: 4.067 (4.07)  Time: 1.743s,  587.66/s  (1.743s,  587.66/s)  LR: 4.691e-04  Data: 1.614 (1.614)
Train: 156 [  50/1251 (  4%)]  Loss: 4.210 (4.14)  Time: 0.186s, 5500.99/s  (0.218s, 4697.31/s)  LR: 4.691e-04  Data: 0.030 (0.065)
Train: 156 [ 100/1251 (  8%)]  Loss: 4.033 (4.10)  Time: 0.173s, 5928.74/s  (0.209s, 4893.11/s)  LR: 4.691e-04  Data: 0.025 (0.057)
Train: 156 [ 150/1251 ( 12%)]  Loss: 4.071 (4.10)  Time: 0.161s, 6348.28/s  (0.201s, 5090.70/s)  LR: 4.691e-04  Data: 0.025 (0.052)
Train: 156 [ 200/1251 ( 16%)]  Loss: 4.314 (4.14)  Time: 0.172s, 5958.62/s  (0.200s, 5108.98/s)  LR: 4.691e-04  Data: 0.039 (0.047)
Train: 156 [ 250/1251 ( 20%)]  Loss: 4.469 (4.19)  Time: 0.184s, 5576.46/s  (0.198s, 5177.43/s)  LR: 4.691e-04  Data: 0.030 (0.043)
Train: 156 [ 300/1251 ( 24%)]  Loss: 4.308 (4.21)  Time: 0.177s, 5796.16/s  (0.197s, 5209.58/s)  LR: 4.691e-04  Data: 0.031 (0.041)
Train: 156 [ 350/1251 ( 28%)]  Loss: 4.281 (4.22)  Time: 0.158s, 6476.72/s  (0.195s, 5255.43/s)  LR: 4.691e-04  Data: 0.026 (0.039)
Train: 156 [ 400/1251 ( 32%)]  Loss: 4.433 (4.24)  Time: 0.234s, 4383.35/s  (0.194s, 5273.20/s)  LR: 4.691e-04  Data: 0.024 (0.038)
Train: 156 [ 450/1251 ( 36%)]  Loss: 4.161 (4.23)  Time: 0.191s, 5375.25/s  (0.194s, 5288.13/s)  LR: 4.691e-04  Data: 0.033 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 156 [ 500/1251 ( 40%)]  Loss: 3.909 (4.21)  Time: 0.159s, 6426.63/s  (0.193s, 5293.99/s)  LR: 4.691e-04  Data: 0.026 (0.036)
Train: 156 [ 550/1251 ( 44%)]  Loss: 4.225 (4.21)  Time: 0.160s, 6415.99/s  (0.193s, 5295.84/s)  LR: 4.691e-04  Data: 0.029 (0.035)
Train: 156 [ 600/1251 ( 48%)]  Loss: 4.299 (4.21)  Time: 0.289s, 3547.31/s  (0.194s, 5286.86/s)  LR: 4.691e-04  Data: 0.026 (0.034)
Train: 156 [ 650/1251 ( 52%)]  Loss: 4.286 (4.22)  Time: 0.173s, 5902.88/s  (0.193s, 5309.81/s)  LR: 4.691e-04  Data: 0.025 (0.034)
Train: 156 [ 700/1251 ( 56%)]  Loss: 4.266 (4.22)  Time: 0.175s, 5835.18/s  (0.193s, 5309.08/s)  LR: 4.691e-04  Data: 0.027 (0.033)
Train: 156 [ 750/1251 ( 60%)]  Loss: 4.275 (4.23)  Time: 0.163s, 6275.27/s  (0.194s, 5291.30/s)  LR: 4.691e-04  Data: 0.028 (0.033)
Train: 156 [ 800/1251 ( 64%)]  Loss: 3.796 (4.20)  Time: 0.168s, 6110.86/s  (0.193s, 5297.99/s)  LR: 4.691e-04  Data: 0.024 (0.033)
Train: 156 [ 850/1251 ( 68%)]  Loss: 4.167 (4.20)  Time: 0.177s, 5776.02/s  (0.193s, 5303.49/s)  LR: 4.691e-04  Data: 0.029 (0.033)
Train: 156 [ 900/1251 ( 72%)]  Loss: 4.587 (4.22)  Time: 0.181s, 5659.25/s  (0.193s, 5305.38/s)  LR: 4.691e-04  Data: 0.032 (0.032)
Train: 156 [ 950/1251 ( 76%)]  Loss: 3.949 (4.21)  Time: 0.169s, 6057.57/s  (0.193s, 5317.62/s)  LR: 4.691e-04  Data: 0.022 (0.032)
Train: 156 [1000/1251 ( 80%)]  Loss: 3.961 (4.19)  Time: 0.398s, 2575.36/s  (0.193s, 5303.66/s)  LR: 4.691e-04  Data: 0.024 (0.032)
Train: 156 [1050/1251 ( 84%)]  Loss: 4.408 (4.20)  Time: 0.176s, 5810.31/s  (0.193s, 5304.00/s)  LR: 4.691e-04  Data: 0.029 (0.032)
Train: 156 [1100/1251 ( 88%)]  Loss: 4.241 (4.21)  Time: 0.164s, 6225.06/s  (0.193s, 5307.89/s)  LR: 4.691e-04  Data: 0.041 (0.032)
Train: 156 [1150/1251 ( 92%)]  Loss: 4.345 (4.21)  Time: 0.188s, 5439.56/s  (0.193s, 5302.63/s)  LR: 4.691e-04  Data: 0.028 (0.032)
Train: 156 [1200/1251 ( 96%)]  Loss: 3.959 (4.20)  Time: 0.478s, 2144.20/s  (0.193s, 5294.07/s)  LR: 4.691e-04  Data: 0.027 (0.031)
Train: 156 [1250/1251 (100%)]  Loss: 4.182 (4.20)  Time: 0.114s, 9021.58/s  (0.193s, 5312.45/s)  LR: 4.691e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.738 (1.738)  Loss:  1.1846 (1.1846)  Acc@1: 80.2734 (80.2734)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2537 (1.8078)  Acc@1: 79.1274 (64.9940)  Acc@5: 92.5708 (86.5520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-156.pth.tar', 64.99400011230469)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-148.pth.tar', 64.99200003662109)

Train: 157 [   0/1251 (  0%)]  Loss: 3.768 (3.77)  Time: 1.942s,  527.37/s  (1.942s,  527.37/s)  LR: 4.639e-04  Data: 1.815 (1.815)
Train: 157 [  50/1251 (  4%)]  Loss: 3.868 (3.82)  Time: 0.167s, 6145.70/s  (0.223s, 4595.06/s)  LR: 4.639e-04  Data: 0.025 (0.063)
Train: 157 [ 100/1251 (  8%)]  Loss: 4.334 (3.99)  Time: 0.176s, 5815.70/s  (0.206s, 4965.46/s)  LR: 4.639e-04  Data: 0.027 (0.046)
Train: 157 [ 150/1251 ( 12%)]  Loss: 4.325 (4.07)  Time: 0.185s, 5527.82/s  (0.201s, 5088.67/s)  LR: 4.639e-04  Data: 0.028 (0.040)
Train: 157 [ 200/1251 ( 16%)]  Loss: 4.183 (4.10)  Time: 0.174s, 5877.93/s  (0.198s, 5176.90/s)  LR: 4.639e-04  Data: 0.019 (0.037)
Train: 157 [ 250/1251 ( 20%)]  Loss: 4.166 (4.11)  Time: 0.175s, 5839.63/s  (0.196s, 5233.70/s)  LR: 4.639e-04  Data: 0.024 (0.035)
Train: 157 [ 300/1251 ( 24%)]  Loss: 4.642 (4.18)  Time: 0.158s, 6491.70/s  (0.195s, 5245.76/s)  LR: 4.639e-04  Data: 0.025 (0.034)
Train: 157 [ 350/1251 ( 28%)]  Loss: 4.452 (4.22)  Time: 0.161s, 6354.54/s  (0.193s, 5293.36/s)  LR: 4.639e-04  Data: 0.020 (0.033)
Train: 157 [ 400/1251 ( 32%)]  Loss: 4.086 (4.20)  Time: 0.172s, 5954.98/s  (0.194s, 5285.40/s)  LR: 4.639e-04  Data: 0.028 (0.033)
Train: 157 [ 450/1251 ( 36%)]  Loss: 4.465 (4.23)  Time: 0.168s, 6084.86/s  (0.193s, 5306.80/s)  LR: 4.639e-04  Data: 0.030 (0.032)
Train: 157 [ 500/1251 ( 40%)]  Loss: 4.559 (4.26)  Time: 0.169s, 6065.80/s  (0.193s, 5306.31/s)  LR: 4.639e-04  Data: 0.029 (0.032)
Train: 157 [ 550/1251 ( 44%)]  Loss: 3.953 (4.23)  Time: 0.169s, 6061.30/s  (0.193s, 5300.60/s)  LR: 4.639e-04  Data: 0.021 (0.032)
Train: 157 [ 600/1251 ( 48%)]  Loss: 4.451 (4.25)  Time: 0.193s, 5306.58/s  (0.193s, 5315.66/s)  LR: 4.639e-04  Data: 0.028 (0.031)
Train: 157 [ 650/1251 ( 52%)]  Loss: 4.293 (4.25)  Time: 0.175s, 5850.43/s  (0.192s, 5328.02/s)  LR: 4.639e-04  Data: 0.024 (0.031)
Train: 157 [ 700/1251 ( 56%)]  Loss: 4.324 (4.26)  Time: 0.177s, 5794.96/s  (0.192s, 5338.97/s)  LR: 4.639e-04  Data: 0.031 (0.031)
Train: 157 [ 750/1251 ( 60%)]  Loss: 3.964 (4.24)  Time: 0.353s, 2899.43/s  (0.192s, 5329.76/s)  LR: 4.639e-04  Data: 0.031 (0.031)
Train: 157 [ 800/1251 ( 64%)]  Loss: 4.257 (4.24)  Time: 0.187s, 5479.79/s  (0.192s, 5340.81/s)  LR: 4.639e-04  Data: 0.030 (0.030)
Train: 157 [ 850/1251 ( 68%)]  Loss: 4.165 (4.24)  Time: 0.441s, 2323.84/s  (0.192s, 5331.99/s)  LR: 4.639e-04  Data: 0.022 (0.030)
Train: 157 [ 900/1251 ( 72%)]  Loss: 3.994 (4.22)  Time: 0.167s, 6124.57/s  (0.192s, 5331.62/s)  LR: 4.639e-04  Data: 0.033 (0.030)
Train: 157 [ 950/1251 ( 76%)]  Loss: 4.391 (4.23)  Time: 0.174s, 5883.81/s  (0.192s, 5324.75/s)  LR: 4.639e-04  Data: 0.028 (0.030)
Train: 157 [1000/1251 ( 80%)]  Loss: 4.391 (4.24)  Time: 0.169s, 6045.72/s  (0.192s, 5325.52/s)  LR: 4.639e-04  Data: 0.030 (0.030)
Train: 157 [1050/1251 ( 84%)]  Loss: 4.339 (4.24)  Time: 0.334s, 3068.94/s  (0.192s, 5326.27/s)  LR: 4.639e-04  Data: 0.028 (0.030)
Train: 157 [1100/1251 ( 88%)]  Loss: 3.817 (4.23)  Time: 0.172s, 5957.88/s  (0.192s, 5328.10/s)  LR: 4.639e-04  Data: 0.025 (0.030)
Train: 157 [1150/1251 ( 92%)]  Loss: 4.648 (4.24)  Time: 0.166s, 6183.85/s  (0.192s, 5319.57/s)  LR: 4.639e-04  Data: 0.036 (0.030)
Train: 157 [1200/1251 ( 96%)]  Loss: 4.102 (4.24)  Time: 0.161s, 6350.83/s  (0.192s, 5320.54/s)  LR: 4.639e-04  Data: 0.027 (0.030)
Train: 157 [1250/1251 (100%)]  Loss: 3.920 (4.23)  Time: 0.114s, 8979.24/s  (0.192s, 5329.09/s)  LR: 4.639e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.776 (1.776)  Loss:  1.0859 (1.0859)  Acc@1: 79.7852 (79.7852)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.228)  Loss:  1.2869 (1.7908)  Acc@1: 79.0094 (65.4480)  Acc@5: 93.5142 (86.5700)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-157.pth.tar', 65.44800006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-156.pth.tar', 64.99400011230469)

Train: 158 [   0/1251 (  0%)]  Loss: 4.286 (4.29)  Time: 1.651s,  620.05/s  (1.651s,  620.05/s)  LR: 4.587e-04  Data: 1.524 (1.524)
Train: 158 [  50/1251 (  4%)]  Loss: 3.930 (4.11)  Time: 0.173s, 5933.79/s  (0.234s, 4381.61/s)  LR: 4.587e-04  Data: 0.025 (0.062)
Train: 158 [ 100/1251 (  8%)]  Loss: 4.349 (4.19)  Time: 0.183s, 5585.76/s  (0.209s, 4896.22/s)  LR: 4.587e-04  Data: 0.031 (0.046)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 158 [ 150/1251 ( 12%)]  Loss: 4.418 (4.25)  Time: 0.167s, 6131.77/s  (0.200s, 5115.70/s)  LR: 4.587e-04  Data: 0.024 (0.040)
Train: 158 [ 200/1251 ( 16%)]  Loss: 4.399 (4.28)  Time: 0.168s, 6110.95/s  (0.197s, 5189.02/s)  LR: 4.587e-04  Data: 0.029 (0.036)
Train: 158 [ 250/1251 ( 20%)]  Loss: 4.337 (4.29)  Time: 0.171s, 5984.80/s  (0.196s, 5217.51/s)  LR: 4.587e-04  Data: 0.026 (0.035)
Train: 158 [ 300/1251 ( 24%)]  Loss: 4.141 (4.27)  Time: 0.175s, 5846.40/s  (0.195s, 5244.43/s)  LR: 4.587e-04  Data: 0.028 (0.034)
Train: 158 [ 350/1251 ( 28%)]  Loss: 4.333 (4.27)  Time: 0.167s, 6144.67/s  (0.194s, 5291.81/s)  LR: 4.587e-04  Data: 0.025 (0.034)
Train: 158 [ 400/1251 ( 32%)]  Loss: 4.143 (4.26)  Time: 0.161s, 6370.53/s  (0.193s, 5317.23/s)  LR: 4.587e-04  Data: 0.033 (0.033)
Train: 158 [ 450/1251 ( 36%)]  Loss: 4.183 (4.25)  Time: 0.186s, 5497.52/s  (0.193s, 5319.40/s)  LR: 4.587e-04  Data: 0.029 (0.032)
Train: 158 [ 500/1251 ( 40%)]  Loss: 4.221 (4.25)  Time: 0.182s, 5639.58/s  (0.193s, 5314.44/s)  LR: 4.587e-04  Data: 0.031 (0.032)
Train: 158 [ 550/1251 ( 44%)]  Loss: 4.335 (4.26)  Time: 0.343s, 2981.64/s  (0.193s, 5319.06/s)  LR: 4.587e-04  Data: 0.026 (0.032)
Train: 158 [ 600/1251 ( 48%)]  Loss: 4.008 (4.24)  Time: 0.170s, 6010.67/s  (0.193s, 5313.83/s)  LR: 4.587e-04  Data: 0.025 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 158 [ 650/1251 ( 52%)]  Loss: 4.105 (4.23)  Time: 0.165s, 6215.72/s  (0.192s, 5329.18/s)  LR: 4.587e-04  Data: 0.021 (0.032)
Train: 158 [ 700/1251 ( 56%)]  Loss: 4.258 (4.23)  Time: 0.204s, 5017.23/s  (0.192s, 5323.64/s)  LR: 4.587e-04  Data: 0.058 (0.031)
Train: 158 [ 750/1251 ( 60%)]  Loss: 4.054 (4.22)  Time: 0.278s, 3682.10/s  (0.193s, 5313.91/s)  LR: 4.587e-04  Data: 0.022 (0.031)
Train: 158 [ 800/1251 ( 64%)]  Loss: 4.356 (4.23)  Time: 0.161s, 6350.47/s  (0.192s, 5327.10/s)  LR: 4.587e-04  Data: 0.025 (0.031)
Train: 158 [ 850/1251 ( 68%)]  Loss: 4.203 (4.23)  Time: 0.202s, 5064.33/s  (0.192s, 5331.14/s)  LR: 4.587e-04  Data: 0.025 (0.031)
Train: 158 [ 900/1251 ( 72%)]  Loss: 4.411 (4.24)  Time: 0.167s, 6147.59/s  (0.192s, 5335.57/s)  LR: 4.587e-04  Data: 0.025 (0.031)
Train: 158 [ 950/1251 ( 76%)]  Loss: 4.497 (4.25)  Time: 0.363s, 2823.43/s  (0.192s, 5322.84/s)  LR: 4.587e-04  Data: 0.023 (0.031)
Train: 158 [1000/1251 ( 80%)]  Loss: 4.123 (4.24)  Time: 0.179s, 5716.01/s  (0.192s, 5327.03/s)  LR: 4.587e-04  Data: 0.024 (0.031)
Train: 158 [1050/1251 ( 84%)]  Loss: 4.016 (4.23)  Time: 0.160s, 6384.79/s  (0.192s, 5330.08/s)  LR: 4.587e-04  Data: 0.028 (0.031)
Train: 158 [1100/1251 ( 88%)]  Loss: 4.565 (4.25)  Time: 0.171s, 5978.26/s  (0.192s, 5327.70/s)  LR: 4.587e-04  Data: 0.025 (0.031)
Train: 158 [1150/1251 ( 92%)]  Loss: 4.026 (4.24)  Time: 0.324s, 3160.57/s  (0.192s, 5322.28/s)  LR: 4.587e-04  Data: 0.020 (0.031)
Train: 158 [1200/1251 ( 96%)]  Loss: 4.485 (4.25)  Time: 0.152s, 6750.45/s  (0.193s, 5317.43/s)  LR: 4.587e-04  Data: 0.026 (0.030)
Train: 158 [1250/1251 (100%)]  Loss: 4.003 (4.24)  Time: 0.113s, 9027.93/s  (0.192s, 5333.16/s)  LR: 4.587e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1208 (1.1208)  Acc@1: 79.9805 (79.9805)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2780 (1.7500)  Acc@1: 78.8915 (65.8780)  Acc@5: 92.9245 (87.2360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-157.pth.tar', 65.44800006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-143.pth.tar', 65.01199995361328)

Train: 159 [   0/1251 (  0%)]  Loss: 4.238 (4.24)  Time: 1.855s,  552.05/s  (1.855s,  552.05/s)  LR: 4.535e-04  Data: 1.719 (1.719)
Train: 159 [  50/1251 (  4%)]  Loss: 3.767 (4.00)  Time: 0.167s, 6146.22/s  (0.232s, 4408.61/s)  LR: 4.535e-04  Data: 0.031 (0.089)
Train: 159 [ 100/1251 (  8%)]  Loss: 4.253 (4.09)  Time: 0.167s, 6133.60/s  (0.208s, 4917.72/s)  LR: 4.535e-04  Data: 0.027 (0.062)
Train: 159 [ 150/1251 ( 12%)]  Loss: 4.272 (4.13)  Time: 0.163s, 6284.32/s  (0.199s, 5136.10/s)  LR: 4.535e-04  Data: 0.030 (0.054)
Train: 159 [ 200/1251 ( 16%)]  Loss: 4.175 (4.14)  Time: 0.174s, 5872.53/s  (0.199s, 5153.43/s)  LR: 4.535e-04  Data: 0.025 (0.054)
Train: 159 [ 250/1251 ( 20%)]  Loss: 3.836 (4.09)  Time: 0.167s, 6145.36/s  (0.195s, 5241.31/s)  LR: 4.535e-04  Data: 0.021 (0.051)
Train: 159 [ 300/1251 ( 24%)]  Loss: 4.031 (4.08)  Time: 0.182s, 5612.11/s  (0.195s, 5254.50/s)  LR: 4.535e-04  Data: 0.028 (0.051)
Train: 159 [ 350/1251 ( 28%)]  Loss: 4.513 (4.14)  Time: 0.172s, 5944.18/s  (0.194s, 5278.56/s)  LR: 4.535e-04  Data: 0.024 (0.050)
Train: 159 [ 400/1251 ( 32%)]  Loss: 4.250 (4.15)  Time: 0.168s, 6080.90/s  (0.194s, 5277.99/s)  LR: 4.535e-04  Data: 0.028 (0.049)
Train: 159 [ 450/1251 ( 36%)]  Loss: 4.067 (4.14)  Time: 0.170s, 6037.04/s  (0.194s, 5277.20/s)  LR: 4.535e-04  Data: 0.035 (0.049)
Train: 159 [ 500/1251 ( 40%)]  Loss: 3.716 (4.10)  Time: 0.165s, 6195.36/s  (0.193s, 5306.82/s)  LR: 4.535e-04  Data: 0.022 (0.048)
Train: 159 [ 550/1251 ( 44%)]  Loss: 4.157 (4.11)  Time: 0.190s, 5402.97/s  (0.192s, 5324.54/s)  LR: 4.535e-04  Data: 0.039 (0.047)
Train: 159 [ 600/1251 ( 48%)]  Loss: 4.429 (4.13)  Time: 0.165s, 6198.65/s  (0.192s, 5325.26/s)  LR: 4.535e-04  Data: 0.031 (0.046)
Train: 159 [ 650/1251 ( 52%)]  Loss: 4.136 (4.13)  Time: 0.161s, 6365.47/s  (0.193s, 5311.18/s)  LR: 4.535e-04  Data: 0.033 (0.045)
Train: 159 [ 700/1251 ( 56%)]  Loss: 4.302 (4.14)  Time: 0.167s, 6124.08/s  (0.193s, 5316.64/s)  LR: 4.535e-04  Data: 0.031 (0.044)
Train: 159 [ 750/1251 ( 60%)]  Loss: 4.148 (4.14)  Time: 0.189s, 5406.87/s  (0.192s, 5319.51/s)  LR: 4.535e-04  Data: 0.024 (0.043)
Train: 159 [ 800/1251 ( 64%)]  Loss: 4.365 (4.16)  Time: 0.292s, 3504.63/s  (0.192s, 5326.72/s)  LR: 4.535e-04  Data: 0.024 (0.042)
Train: 159 [ 850/1251 ( 68%)]  Loss: 4.435 (4.17)  Time: 0.282s, 3626.33/s  (0.192s, 5323.15/s)  LR: 4.535e-04  Data: 0.031 (0.041)
Train: 159 [ 900/1251 ( 72%)]  Loss: 4.541 (4.19)  Time: 0.175s, 5856.92/s  (0.192s, 5323.13/s)  LR: 4.535e-04  Data: 0.025 (0.040)
Train: 159 [ 950/1251 ( 76%)]  Loss: 4.083 (4.19)  Time: 0.187s, 5461.77/s  (0.192s, 5324.05/s)  LR: 4.535e-04  Data: 0.023 (0.039)
Train: 159 [1000/1251 ( 80%)]  Loss: 4.313 (4.19)  Time: 0.182s, 5631.34/s  (0.192s, 5320.71/s)  LR: 4.535e-04  Data: 0.027 (0.039)
Train: 159 [1050/1251 ( 84%)]  Loss: 3.954 (4.18)  Time: 0.159s, 6440.62/s  (0.193s, 5318.33/s)  LR: 4.535e-04  Data: 0.027 (0.038)
Train: 159 [1100/1251 ( 88%)]  Loss: 3.855 (4.17)  Time: 0.179s, 5722.62/s  (0.193s, 5311.97/s)  LR: 4.535e-04  Data: 0.019 (0.038)
Train: 159 [1150/1251 ( 92%)]  Loss: 4.403 (4.18)  Time: 0.172s, 5947.68/s  (0.193s, 5318.32/s)  LR: 4.535e-04  Data: 0.024 (0.038)
Train: 159 [1200/1251 ( 96%)]  Loss: 4.669 (4.20)  Time: 0.159s, 6426.37/s  (0.193s, 5305.81/s)  LR: 4.535e-04  Data: 0.025 (0.037)
Train: 159 [1250/1251 (100%)]  Loss: 3.908 (4.19)  Time: 0.114s, 9001.20/s  (0.192s, 5326.50/s)  LR: 4.535e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.857 (1.857)  Loss:  0.9880 (0.9880)  Acc@1: 80.6641 (80.6641)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.1073 (1.6782)  Acc@1: 80.3066 (65.7820)  Acc@5: 93.2783 (86.8740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-157.pth.tar', 65.44800006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-147.pth.tar', 65.09600005859375)

Train: 160 [   0/1251 (  0%)]  Loss: 4.297 (4.30)  Time: 1.924s,  532.21/s  (1.924s,  532.21/s)  LR: 4.483e-04  Data: 1.798 (1.798)
Train: 160 [  50/1251 (  4%)]  Loss: 3.827 (4.06)  Time: 0.175s, 5864.21/s  (0.227s, 4510.95/s)  LR: 4.483e-04  Data: 0.030 (0.083)
Train: 160 [ 100/1251 (  8%)]  Loss: 4.307 (4.14)  Time: 0.177s, 5799.58/s  (0.207s, 4941.96/s)  LR: 4.483e-04  Data: 0.033 (0.062)
Train: 160 [ 150/1251 ( 12%)]  Loss: 4.526 (4.24)  Time: 0.167s, 6146.94/s  (0.201s, 5096.58/s)  LR: 4.483e-04  Data: 0.020 (0.057)
Train: 160 [ 200/1251 ( 16%)]  Loss: 4.057 (4.20)  Time: 0.166s, 6184.55/s  (0.200s, 5129.57/s)  LR: 4.483e-04  Data: 0.028 (0.050)
Train: 160 [ 250/1251 ( 20%)]  Loss: 4.397 (4.23)  Time: 0.165s, 6192.48/s  (0.198s, 5178.02/s)  LR: 4.483e-04  Data: 0.029 (0.046)
Train: 160 [ 300/1251 ( 24%)]  Loss: 4.325 (4.25)  Time: 0.186s, 5506.02/s  (0.194s, 5268.36/s)  LR: 4.483e-04  Data: 0.024 (0.043)
Train: 160 [ 350/1251 ( 28%)]  Loss: 4.369 (4.26)  Time: 0.171s, 5990.04/s  (0.196s, 5222.36/s)  LR: 4.483e-04  Data: 0.027 (0.041)
Train: 160 [ 400/1251 ( 32%)]  Loss: 4.007 (4.23)  Time: 0.169s, 6057.02/s  (0.195s, 5244.09/s)  LR: 4.483e-04  Data: 0.029 (0.039)
Train: 160 [ 450/1251 ( 36%)]  Loss: 4.557 (4.27)  Time: 0.174s, 5884.36/s  (0.194s, 5276.99/s)  LR: 4.483e-04  Data: 0.021 (0.038)
Train: 160 [ 500/1251 ( 40%)]  Loss: 3.989 (4.24)  Time: 0.180s, 5697.97/s  (0.193s, 5292.30/s)  LR: 4.483e-04  Data: 0.026 (0.037)
Train: 160 [ 550/1251 ( 44%)]  Loss: 4.084 (4.23)  Time: 0.156s, 6548.15/s  (0.194s, 5278.42/s)  LR: 4.483e-04  Data: 0.024 (0.036)
Train: 160 [ 600/1251 ( 48%)]  Loss: 4.747 (4.27)  Time: 0.167s, 6139.11/s  (0.194s, 5291.24/s)  LR: 4.483e-04  Data: 0.026 (0.036)
Train: 160 [ 650/1251 ( 52%)]  Loss: 4.442 (4.28)  Time: 0.181s, 5662.81/s  (0.193s, 5294.03/s)  LR: 4.483e-04  Data: 0.036 (0.035)
Train: 160 [ 700/1251 ( 56%)]  Loss: 4.249 (4.28)  Time: 0.163s, 6275.54/s  (0.193s, 5314.25/s)  LR: 4.483e-04  Data: 0.027 (0.035)
Train: 160 [ 750/1251 ( 60%)]  Loss: 4.173 (4.27)  Time: 0.166s, 6153.65/s  (0.192s, 5322.65/s)  LR: 4.483e-04  Data: 0.024 (0.034)
Train: 160 [ 800/1251 ( 64%)]  Loss: 4.406 (4.28)  Time: 0.176s, 5820.70/s  (0.193s, 5315.26/s)  LR: 4.483e-04  Data: 0.027 (0.034)
Train: 160 [ 850/1251 ( 68%)]  Loss: 4.144 (4.27)  Time: 0.180s, 5684.47/s  (0.192s, 5320.34/s)  LR: 4.483e-04  Data: 0.037 (0.034)
Train: 160 [ 900/1251 ( 72%)]  Loss: 4.318 (4.27)  Time: 0.204s, 5030.81/s  (0.193s, 5310.65/s)  LR: 4.483e-04  Data: 0.023 (0.033)
Train: 160 [ 950/1251 ( 76%)]  Loss: 4.467 (4.28)  Time: 0.186s, 5492.33/s  (0.193s, 5308.94/s)  LR: 4.483e-04  Data: 0.031 (0.033)
Train: 160 [1000/1251 ( 80%)]  Loss: 4.616 (4.30)  Time: 0.179s, 5715.97/s  (0.193s, 5309.41/s)  LR: 4.483e-04  Data: 0.022 (0.033)
Train: 160 [1050/1251 ( 84%)]  Loss: 4.240 (4.30)  Time: 0.285s, 3592.98/s  (0.193s, 5307.76/s)  LR: 4.483e-04  Data: 0.025 (0.033)
Train: 160 [1100/1251 ( 88%)]  Loss: 3.973 (4.28)  Time: 0.181s, 5644.37/s  (0.193s, 5307.78/s)  LR: 4.483e-04  Data: 0.032 (0.032)
Train: 160 [1150/1251 ( 92%)]  Loss: 3.736 (4.26)  Time: 0.168s, 6090.97/s  (0.193s, 5300.11/s)  LR: 4.483e-04  Data: 0.034 (0.032)
Train: 160 [1200/1251 ( 96%)]  Loss: 3.985 (4.25)  Time: 0.173s, 5932.48/s  (0.193s, 5309.10/s)  LR: 4.483e-04  Data: 0.028 (0.032)
Train: 160 [1250/1251 (100%)]  Loss: 4.058 (4.24)  Time: 0.114s, 9007.88/s  (0.193s, 5315.40/s)  LR: 4.483e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.813 (1.813)  Loss:  1.0571 (1.0571)  Acc@1: 79.5898 (79.5898)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.0869 (1.7317)  Acc@1: 81.6038 (65.8080)  Acc@5: 93.6321 (86.7480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-157.pth.tar', 65.44800006103516)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-145.pth.tar', 65.15200018554688)

Train: 161 [   0/1251 (  0%)]  Loss: 4.030 (4.03)  Time: 1.659s,  617.25/s  (1.659s,  617.25/s)  LR: 4.431e-04  Data: 1.534 (1.534)
Train: 161 [  50/1251 (  4%)]  Loss: 3.756 (3.89)  Time: 0.163s, 6267.36/s  (0.219s, 4665.54/s)  LR: 4.431e-04  Data: 0.025 (0.069)
Train: 161 [ 100/1251 (  8%)]  Loss: 4.286 (4.02)  Time: 0.194s, 5267.26/s  (0.207s, 4941.38/s)  LR: 4.431e-04  Data: 0.023 (0.057)
Train: 161 [ 150/1251 ( 12%)]  Loss: 4.125 (4.05)  Time: 0.173s, 5924.79/s  (0.203s, 5047.86/s)  LR: 4.431e-04  Data: 0.026 (0.054)
Train: 161 [ 200/1251 ( 16%)]  Loss: 3.983 (4.04)  Time: 0.155s, 6618.38/s  (0.199s, 5140.32/s)  LR: 4.431e-04  Data: 0.028 (0.050)
Train: 161 [ 250/1251 ( 20%)]  Loss: 3.944 (4.02)  Time: 0.166s, 6157.14/s  (0.197s, 5198.04/s)  LR: 4.431e-04  Data: 0.022 (0.048)
Train: 161 [ 300/1251 ( 24%)]  Loss: 3.810 (3.99)  Time: 0.177s, 5794.19/s  (0.195s, 5240.68/s)  LR: 4.431e-04  Data: 0.027 (0.046)
Train: 161 [ 350/1251 ( 28%)]  Loss: 4.359 (4.04)  Time: 0.159s, 6424.73/s  (0.194s, 5284.03/s)  LR: 4.431e-04  Data: 0.032 (0.044)
Train: 161 [ 400/1251 ( 32%)]  Loss: 4.168 (4.05)  Time: 0.167s, 6121.51/s  (0.193s, 5300.53/s)  LR: 4.431e-04  Data: 0.035 (0.042)
Train: 161 [ 450/1251 ( 36%)]  Loss: 4.224 (4.07)  Time: 0.159s, 6443.64/s  (0.192s, 5322.72/s)  LR: 4.431e-04  Data: 0.032 (0.040)
Train: 161 [ 500/1251 ( 40%)]  Loss: 4.444 (4.10)  Time: 0.175s, 5850.47/s  (0.193s, 5316.50/s)  LR: 4.431e-04  Data: 0.034 (0.041)
Train: 161 [ 550/1251 ( 44%)]  Loss: 4.205 (4.11)  Time: 0.179s, 5707.71/s  (0.193s, 5319.28/s)  LR: 4.431e-04  Data: 0.028 (0.041)
Train: 161 [ 600/1251 ( 48%)]  Loss: 4.093 (4.11)  Time: 0.169s, 6076.10/s  (0.192s, 5321.15/s)  LR: 4.431e-04  Data: 0.027 (0.040)
Train: 161 [ 650/1251 ( 52%)]  Loss: 4.498 (4.14)  Time: 0.164s, 6241.61/s  (0.192s, 5330.82/s)  LR: 4.431e-04  Data: 0.026 (0.039)
Train: 161 [ 700/1251 ( 56%)]  Loss: 4.208 (4.14)  Time: 0.180s, 5699.24/s  (0.193s, 5311.99/s)  LR: 4.431e-04  Data: 0.021 (0.039)
Train: 161 [ 750/1251 ( 60%)]  Loss: 3.961 (4.13)  Time: 0.179s, 5721.37/s  (0.193s, 5317.35/s)  LR: 4.431e-04  Data: 0.029 (0.038)
Train: 161 [ 800/1251 ( 64%)]  Loss: 4.259 (4.14)  Time: 0.186s, 5501.18/s  (0.192s, 5320.03/s)  LR: 4.431e-04  Data: 0.019 (0.037)
Train: 161 [ 850/1251 ( 68%)]  Loss: 4.022 (4.13)  Time: 0.282s, 3626.57/s  (0.193s, 5318.36/s)  LR: 4.431e-04  Data: 0.040 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 161 [ 900/1251 ( 72%)]  Loss: 3.986 (4.12)  Time: 0.164s, 6237.71/s  (0.193s, 5317.34/s)  LR: 4.431e-04  Data: 0.028 (0.036)
Train: 161 [ 950/1251 ( 76%)]  Loss: 4.291 (4.13)  Time: 0.168s, 6110.62/s  (0.193s, 5317.88/s)  LR: 4.431e-04  Data: 0.024 (0.036)
Train: 161 [1000/1251 ( 80%)]  Loss: 4.217 (4.14)  Time: 0.151s, 6779.54/s  (0.193s, 5319.07/s)  LR: 4.431e-04  Data: 0.027 (0.035)
Train: 161 [1050/1251 ( 84%)]  Loss: 4.413 (4.15)  Time: 0.179s, 5721.99/s  (0.193s, 5314.52/s)  LR: 4.431e-04  Data: 0.031 (0.035)
Train: 161 [1100/1251 ( 88%)]  Loss: 3.977 (4.14)  Time: 0.161s, 6341.68/s  (0.193s, 5318.82/s)  LR: 4.431e-04  Data: 0.030 (0.035)
Train: 161 [1150/1251 ( 92%)]  Loss: 4.284 (4.15)  Time: 0.184s, 5575.32/s  (0.193s, 5311.88/s)  LR: 4.431e-04  Data: 0.033 (0.035)
Train: 161 [1200/1251 ( 96%)]  Loss: 4.173 (4.15)  Time: 0.158s, 6465.84/s  (0.193s, 5308.58/s)  LR: 4.431e-04  Data: 0.024 (0.034)
Train: 161 [1250/1251 (100%)]  Loss: 4.107 (4.15)  Time: 0.113s, 9095.13/s  (0.193s, 5319.17/s)  LR: 4.431e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.791 (1.791)  Loss:  1.0855 (1.0855)  Acc@1: 81.8359 (81.8359)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.1822 (1.7200)  Acc@1: 79.4811 (65.8580)  Acc@5: 94.2217 (86.9480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-157.pth.tar', 65.44800006103516)

Train: 162 [   0/1251 (  0%)]  Loss: 4.395 (4.40)  Time: 1.827s,  560.50/s  (1.827s,  560.50/s)  LR: 4.379e-04  Data: 1.704 (1.704)
Train: 162 [  50/1251 (  4%)]  Loss: 3.985 (4.19)  Time: 0.174s, 5877.62/s  (0.226s, 4524.53/s)  LR: 4.379e-04  Data: 0.024 (0.079)
Train: 162 [ 100/1251 (  8%)]  Loss: 4.084 (4.15)  Time: 0.203s, 5049.11/s  (0.208s, 4916.94/s)  LR: 4.379e-04  Data: 0.022 (0.059)
Train: 162 [ 150/1251 ( 12%)]  Loss: 4.241 (4.18)  Time: 0.181s, 5665.96/s  (0.203s, 5050.18/s)  LR: 4.379e-04  Data: 0.024 (0.052)
Train: 162 [ 200/1251 ( 16%)]  Loss: 4.044 (4.15)  Time: 0.309s, 3309.01/s  (0.199s, 5145.51/s)  LR: 4.379e-04  Data: 0.023 (0.047)
Train: 162 [ 250/1251 ( 20%)]  Loss: 4.263 (4.17)  Time: 0.168s, 6107.44/s  (0.197s, 5210.77/s)  LR: 4.379e-04  Data: 0.024 (0.043)
Train: 162 [ 300/1251 ( 24%)]  Loss: 4.372 (4.20)  Time: 0.177s, 5801.24/s  (0.196s, 5224.45/s)  LR: 4.379e-04  Data: 0.023 (0.041)
Train: 162 [ 350/1251 ( 28%)]  Loss: 3.997 (4.17)  Time: 0.217s, 4724.75/s  (0.195s, 5246.13/s)  LR: 4.379e-04  Data: 0.021 (0.039)
Train: 162 [ 400/1251 ( 32%)]  Loss: 4.310 (4.19)  Time: 0.195s, 5240.98/s  (0.194s, 5275.60/s)  LR: 4.379e-04  Data: 0.033 (0.037)
Train: 162 [ 450/1251 ( 36%)]  Loss: 4.183 (4.19)  Time: 0.176s, 5801.78/s  (0.194s, 5280.76/s)  LR: 4.379e-04  Data: 0.025 (0.036)
Train: 162 [ 500/1251 ( 40%)]  Loss: 3.839 (4.16)  Time: 0.172s, 5952.50/s  (0.193s, 5312.68/s)  LR: 4.379e-04  Data: 0.032 (0.036)
Train: 162 [ 550/1251 ( 44%)]  Loss: 4.631 (4.20)  Time: 0.192s, 5340.15/s  (0.194s, 5291.88/s)  LR: 4.379e-04  Data: 0.026 (0.035)
Train: 162 [ 600/1251 ( 48%)]  Loss: 3.744 (4.16)  Time: 0.252s, 4063.09/s  (0.193s, 5296.38/s)  LR: 4.379e-04  Data: 0.054 (0.034)
Train: 162 [ 650/1251 ( 52%)]  Loss: 4.141 (4.16)  Time: 0.162s, 6340.46/s  (0.193s, 5293.79/s)  LR: 4.379e-04  Data: 0.029 (0.034)
Train: 162 [ 700/1251 ( 56%)]  Loss: 4.363 (4.17)  Time: 0.171s, 5993.93/s  (0.194s, 5286.87/s)  LR: 4.379e-04  Data: 0.031 (0.034)
Train: 162 [ 750/1251 ( 60%)]  Loss: 4.349 (4.18)  Time: 0.187s, 5475.49/s  (0.193s, 5301.52/s)  LR: 4.379e-04  Data: 0.032 (0.033)
Train: 162 [ 800/1251 ( 64%)]  Loss: 4.341 (4.19)  Time: 0.307s, 3334.42/s  (0.193s, 5307.93/s)  LR: 4.379e-04  Data: 0.039 (0.033)
Train: 162 [ 850/1251 ( 68%)]  Loss: 3.977 (4.18)  Time: 0.174s, 5879.94/s  (0.193s, 5298.48/s)  LR: 4.379e-04  Data: 0.030 (0.033)
Train: 162 [ 900/1251 ( 72%)]  Loss: 3.973 (4.17)  Time: 0.301s, 3405.49/s  (0.194s, 5290.69/s)  LR: 4.379e-04  Data: 0.023 (0.032)
Train: 162 [ 950/1251 ( 76%)]  Loss: 4.443 (4.18)  Time: 0.162s, 6317.96/s  (0.193s, 5297.58/s)  LR: 4.379e-04  Data: 0.026 (0.032)
Train: 162 [1000/1251 ( 80%)]  Loss: 4.316 (4.19)  Time: 0.181s, 5644.83/s  (0.193s, 5300.93/s)  LR: 4.379e-04  Data: 0.025 (0.032)
Train: 162 [1050/1251 ( 84%)]  Loss: 4.415 (4.20)  Time: 0.318s, 3223.50/s  (0.194s, 5290.23/s)  LR: 4.379e-04  Data: 0.022 (0.032)
Train: 162 [1100/1251 ( 88%)]  Loss: 4.252 (4.20)  Time: 0.169s, 6073.18/s  (0.193s, 5297.55/s)  LR: 4.379e-04  Data: 0.028 (0.032)
Train: 162 [1150/1251 ( 92%)]  Loss: 4.253 (4.20)  Time: 0.164s, 6228.16/s  (0.193s, 5299.64/s)  LR: 4.379e-04  Data: 0.026 (0.032)
Train: 162 [1200/1251 ( 96%)]  Loss: 4.053 (4.20)  Time: 0.167s, 6115.71/s  (0.193s, 5298.70/s)  LR: 4.379e-04  Data: 0.028 (0.031)
Train: 162 [1250/1251 (100%)]  Loss: 4.598 (4.21)  Time: 0.113s, 9056.76/s  (0.193s, 5305.86/s)  LR: 4.379e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.1142 (1.1142)  Acc@1: 81.6406 (81.6406)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.1652 (1.7090)  Acc@1: 79.9528 (65.7340)  Acc@5: 92.8066 (86.7940)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-155.pth.tar', 65.45000013427735)

Train: 163 [   0/1251 (  0%)]  Loss: 4.099 (4.10)  Time: 1.670s,  613.21/s  (1.670s,  613.21/s)  LR: 4.327e-04  Data: 1.508 (1.508)
Train: 163 [  50/1251 (  4%)]  Loss: 4.278 (4.19)  Time: 0.169s, 6073.23/s  (0.228s, 4489.58/s)  LR: 4.327e-04  Data: 0.029 (0.062)
Train: 163 [ 100/1251 (  8%)]  Loss: 4.167 (4.18)  Time: 0.172s, 5963.00/s  (0.208s, 4924.53/s)  LR: 4.327e-04  Data: 0.026 (0.044)
Train: 163 [ 150/1251 ( 12%)]  Loss: 4.337 (4.22)  Time: 0.164s, 6235.31/s  (0.199s, 5141.10/s)  LR: 4.327e-04  Data: 0.026 (0.039)
Train: 163 [ 200/1251 ( 16%)]  Loss: 4.260 (4.23)  Time: 0.287s, 3573.18/s  (0.196s, 5231.29/s)  LR: 4.327e-04  Data: 0.027 (0.036)
Train: 163 [ 250/1251 ( 20%)]  Loss: 4.194 (4.22)  Time: 0.189s, 5409.69/s  (0.196s, 5230.10/s)  LR: 4.327e-04  Data: 0.033 (0.035)
Train: 163 [ 300/1251 ( 24%)]  Loss: 4.116 (4.21)  Time: 0.191s, 5350.12/s  (0.195s, 5259.82/s)  LR: 4.327e-04  Data: 0.034 (0.034)
Train: 163 [ 350/1251 ( 28%)]  Loss: 4.356 (4.23)  Time: 0.175s, 5856.01/s  (0.194s, 5287.92/s)  LR: 4.327e-04  Data: 0.032 (0.033)
Train: 163 [ 400/1251 ( 32%)]  Loss: 4.280 (4.23)  Time: 0.333s, 3079.16/s  (0.193s, 5297.24/s)  LR: 4.327e-04  Data: 0.022 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 163 [ 450/1251 ( 36%)]  Loss: 4.214 (4.23)  Time: 0.178s, 5753.40/s  (0.193s, 5309.85/s)  LR: 4.327e-04  Data: 0.021 (0.032)
Train: 163 [ 500/1251 ( 40%)]  Loss: 4.528 (4.26)  Time: 0.167s, 6125.53/s  (0.193s, 5313.29/s)  LR: 4.327e-04  Data: 0.028 (0.032)
Train: 163 [ 550/1251 ( 44%)]  Loss: 4.214 (4.25)  Time: 0.165s, 6193.31/s  (0.193s, 5315.74/s)  LR: 4.327e-04  Data: 0.025 (0.031)
Train: 163 [ 600/1251 ( 48%)]  Loss: 3.870 (4.22)  Time: 0.163s, 6281.37/s  (0.193s, 5317.35/s)  LR: 4.327e-04  Data: 0.026 (0.031)
Train: 163 [ 650/1251 ( 52%)]  Loss: 3.902 (4.20)  Time: 0.166s, 6169.60/s  (0.192s, 5319.98/s)  LR: 4.327e-04  Data: 0.025 (0.031)
Train: 163 [ 700/1251 ( 56%)]  Loss: 4.138 (4.20)  Time: 0.176s, 5813.20/s  (0.192s, 5327.49/s)  LR: 4.327e-04  Data: 0.021 (0.031)
Train: 163 [ 750/1251 ( 60%)]  Loss: 3.976 (4.18)  Time: 0.161s, 6361.14/s  (0.192s, 5333.56/s)  LR: 4.327e-04  Data: 0.022 (0.030)
Train: 163 [ 800/1251 ( 64%)]  Loss: 3.898 (4.17)  Time: 0.379s, 2700.89/s  (0.192s, 5323.31/s)  LR: 4.327e-04  Data: 0.030 (0.030)
Train: 163 [ 850/1251 ( 68%)]  Loss: 3.755 (4.14)  Time: 0.178s, 5765.43/s  (0.192s, 5323.44/s)  LR: 4.327e-04  Data: 0.031 (0.030)
Train: 163 [ 900/1251 ( 72%)]  Loss: 3.998 (4.14)  Time: 0.183s, 5605.68/s  (0.193s, 5315.16/s)  LR: 4.327e-04  Data: 0.030 (0.030)
Train: 163 [ 950/1251 ( 76%)]  Loss: 4.324 (4.15)  Time: 0.155s, 6602.96/s  (0.193s, 5311.95/s)  LR: 4.327e-04  Data: 0.028 (0.030)
Train: 163 [1000/1251 ( 80%)]  Loss: 4.311 (4.15)  Time: 0.182s, 5616.01/s  (0.193s, 5308.14/s)  LR: 4.327e-04  Data: 0.025 (0.030)
Train: 163 [1050/1251 ( 84%)]  Loss: 4.068 (4.15)  Time: 0.161s, 6354.22/s  (0.193s, 5299.45/s)  LR: 4.327e-04  Data: 0.035 (0.030)
Train: 163 [1100/1251 ( 88%)]  Loss: 4.396 (4.16)  Time: 0.178s, 5749.28/s  (0.193s, 5299.93/s)  LR: 4.327e-04  Data: 0.029 (0.030)
Train: 163 [1150/1251 ( 92%)]  Loss: 3.995 (4.15)  Time: 0.169s, 6043.05/s  (0.193s, 5299.30/s)  LR: 4.327e-04  Data: 0.035 (0.030)
Train: 163 [1200/1251 ( 96%)]  Loss: 4.332 (4.16)  Time: 0.172s, 5952.00/s  (0.193s, 5297.50/s)  LR: 4.327e-04  Data: 0.028 (0.030)
Train: 163 [1250/1251 (100%)]  Loss: 4.402 (4.17)  Time: 0.114s, 8999.82/s  (0.193s, 5313.20/s)  LR: 4.327e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.813 (1.813)  Loss:  1.2372 (1.2372)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2560 (1.8392)  Acc@1: 81.3679 (65.6320)  Acc@5: 93.2783 (86.7300)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-163.pth.tar', 65.63200005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-153.pth.tar', 65.48000000488281)

Train: 164 [   0/1251 (  0%)]  Loss: 4.068 (4.07)  Time: 2.015s,  508.26/s  (2.015s,  508.26/s)  LR: 4.275e-04  Data: 1.875 (1.875)
Train: 164 [  50/1251 (  4%)]  Loss: 4.174 (4.12)  Time: 0.152s, 6752.23/s  (0.221s, 4632.42/s)  LR: 4.275e-04  Data: 0.030 (0.076)
Train: 164 [ 100/1251 (  8%)]  Loss: 4.461 (4.23)  Time: 0.178s, 5756.73/s  (0.205s, 4987.85/s)  LR: 4.275e-04  Data: 0.022 (0.054)
Train: 164 [ 150/1251 ( 12%)]  Loss: 3.843 (4.14)  Time: 0.169s, 6042.18/s  (0.201s, 5085.33/s)  LR: 4.275e-04  Data: 0.028 (0.047)
Train: 164 [ 200/1251 ( 16%)]  Loss: 4.168 (4.14)  Time: 0.172s, 5951.49/s  (0.200s, 5132.82/s)  LR: 4.275e-04  Data: 0.028 (0.042)
Train: 164 [ 250/1251 ( 20%)]  Loss: 4.109 (4.14)  Time: 0.172s, 5957.31/s  (0.198s, 5181.06/s)  LR: 4.275e-04  Data: 0.026 (0.039)
Train: 164 [ 300/1251 ( 24%)]  Loss: 4.223 (4.15)  Time: 0.174s, 5868.60/s  (0.197s, 5204.14/s)  LR: 4.275e-04  Data: 0.030 (0.037)
Train: 164 [ 350/1251 ( 28%)]  Loss: 4.255 (4.16)  Time: 0.177s, 5784.02/s  (0.195s, 5238.25/s)  LR: 4.275e-04  Data: 0.028 (0.036)
Train: 164 [ 400/1251 ( 32%)]  Loss: 4.435 (4.19)  Time: 0.173s, 5932.62/s  (0.194s, 5271.37/s)  LR: 4.275e-04  Data: 0.038 (0.035)
Train: 164 [ 450/1251 ( 36%)]  Loss: 4.373 (4.21)  Time: 0.352s, 2910.89/s  (0.195s, 5262.73/s)  LR: 4.275e-04  Data: 0.024 (0.034)
Train: 164 [ 500/1251 ( 40%)]  Loss: 4.003 (4.19)  Time: 0.175s, 5866.64/s  (0.194s, 5279.18/s)  LR: 4.275e-04  Data: 0.042 (0.034)
Train: 164 [ 550/1251 ( 44%)]  Loss: 4.268 (4.20)  Time: 0.166s, 6151.92/s  (0.194s, 5275.61/s)  LR: 4.275e-04  Data: 0.030 (0.033)
Train: 164 [ 600/1251 ( 48%)]  Loss: 4.062 (4.19)  Time: 0.172s, 5940.50/s  (0.194s, 5273.82/s)  LR: 4.275e-04  Data: 0.029 (0.033)
Train: 164 [ 650/1251 ( 52%)]  Loss: 3.983 (4.17)  Time: 0.168s, 6104.53/s  (0.194s, 5291.58/s)  LR: 4.275e-04  Data: 0.035 (0.033)
Train: 164 [ 700/1251 ( 56%)]  Loss: 4.323 (4.18)  Time: 0.177s, 5769.94/s  (0.194s, 5283.59/s)  LR: 4.275e-04  Data: 0.024 (0.032)
Train: 164 [ 750/1251 ( 60%)]  Loss: 4.317 (4.19)  Time: 0.171s, 5978.54/s  (0.194s, 5291.82/s)  LR: 4.275e-04  Data: 0.027 (0.032)
Train: 164 [ 800/1251 ( 64%)]  Loss: 4.220 (4.19)  Time: 0.336s, 3043.95/s  (0.193s, 5295.17/s)  LR: 4.275e-04  Data: 0.028 (0.032)
Train: 164 [ 850/1251 ( 68%)]  Loss: 4.220 (4.19)  Time: 0.189s, 5413.62/s  (0.193s, 5302.51/s)  LR: 4.275e-04  Data: 0.021 (0.031)
Train: 164 [ 900/1251 ( 72%)]  Loss: 3.901 (4.18)  Time: 0.301s, 3400.67/s  (0.193s, 5297.42/s)  LR: 4.275e-04  Data: 0.032 (0.031)
Train: 164 [ 950/1251 ( 76%)]  Loss: 4.015 (4.17)  Time: 0.173s, 5920.37/s  (0.193s, 5294.51/s)  LR: 4.275e-04  Data: 0.024 (0.031)
Train: 164 [1000/1251 ( 80%)]  Loss: 4.495 (4.19)  Time: 0.160s, 6404.63/s  (0.194s, 5291.56/s)  LR: 4.275e-04  Data: 0.028 (0.032)
Train: 164 [1050/1251 ( 84%)]  Loss: 4.221 (4.19)  Time: 0.174s, 5878.74/s  (0.193s, 5293.54/s)  LR: 4.275e-04  Data: 0.030 (0.032)
Train: 164 [1100/1251 ( 88%)]  Loss: 4.575 (4.20)  Time: 0.283s, 3615.30/s  (0.194s, 5287.11/s)  LR: 4.275e-04  Data: 0.028 (0.032)
Train: 164 [1150/1251 ( 92%)]  Loss: 3.948 (4.19)  Time: 0.165s, 6208.70/s  (0.194s, 5287.79/s)  LR: 4.275e-04  Data: 0.025 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 164 [1200/1251 ( 96%)]  Loss: 4.332 (4.20)  Time: 0.165s, 6199.88/s  (0.194s, 5289.15/s)  LR: 4.275e-04  Data: 0.024 (0.031)
Train: 164 [1250/1251 (100%)]  Loss: 3.798 (4.18)  Time: 0.113s, 9024.48/s  (0.193s, 5299.15/s)  LR: 4.275e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.871 (1.871)  Loss:  1.1518 (1.1518)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.7266 (94.7266)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1965 (1.7583)  Acc@1: 79.7170 (65.9860)  Acc@5: 94.4576 (86.8900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-163.pth.tar', 65.63200005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-154.pth.tar', 65.51999990234376)

Train: 165 [   0/1251 (  0%)]  Loss: 3.976 (3.98)  Time: 1.750s,  585.24/s  (1.750s,  585.24/s)  LR: 4.224e-04  Data: 1.620 (1.620)
Train: 165 [  50/1251 (  4%)]  Loss: 4.565 (4.27)  Time: 0.164s, 6246.73/s  (0.224s, 4575.25/s)  LR: 4.224e-04  Data: 0.021 (0.078)
Train: 165 [ 100/1251 (  8%)]  Loss: 3.891 (4.14)  Time: 0.167s, 6145.83/s  (0.210s, 4877.77/s)  LR: 4.224e-04  Data: 0.033 (0.065)
Train: 165 [ 150/1251 ( 12%)]  Loss: 4.095 (4.13)  Time: 0.178s, 5740.15/s  (0.200s, 5114.66/s)  LR: 4.224e-04  Data: 0.033 (0.053)
Train: 165 [ 200/1251 ( 16%)]  Loss: 4.266 (4.16)  Time: 0.175s, 5858.06/s  (0.200s, 5121.32/s)  LR: 4.224e-04  Data: 0.029 (0.054)
Train: 165 [ 250/1251 ( 20%)]  Loss: 4.557 (4.23)  Time: 0.188s, 5447.98/s  (0.197s, 5200.79/s)  LR: 4.224e-04  Data: 0.022 (0.051)
Train: 165 [ 300/1251 ( 24%)]  Loss: 4.368 (4.25)  Time: 0.177s, 5790.16/s  (0.195s, 5242.52/s)  LR: 4.224e-04  Data: 0.030 (0.049)
Train: 165 [ 350/1251 ( 28%)]  Loss: 4.251 (4.25)  Time: 0.162s, 6324.69/s  (0.195s, 5258.51/s)  LR: 4.224e-04  Data: 0.030 (0.046)
Train: 165 [ 400/1251 ( 32%)]  Loss: 4.297 (4.25)  Time: 0.169s, 6046.32/s  (0.194s, 5291.14/s)  LR: 4.224e-04  Data: 0.028 (0.044)
Train: 165 [ 450/1251 ( 36%)]  Loss: 4.596 (4.29)  Time: 0.174s, 5868.94/s  (0.194s, 5285.45/s)  LR: 4.224e-04  Data: 0.029 (0.043)
Train: 165 [ 500/1251 ( 40%)]  Loss: 4.099 (4.27)  Time: 0.166s, 6151.12/s  (0.193s, 5292.90/s)  LR: 4.224e-04  Data: 0.024 (0.043)
Train: 165 [ 550/1251 ( 44%)]  Loss: 4.209 (4.26)  Time: 0.189s, 5427.00/s  (0.193s, 5304.75/s)  LR: 4.224e-04  Data: 0.027 (0.043)
Train: 165 [ 600/1251 ( 48%)]  Loss: 4.571 (4.29)  Time: 0.165s, 6195.91/s  (0.193s, 5309.75/s)  LR: 4.224e-04  Data: 0.027 (0.043)
Train: 165 [ 650/1251 ( 52%)]  Loss: 4.180 (4.28)  Time: 0.170s, 6039.77/s  (0.192s, 5320.16/s)  LR: 4.224e-04  Data: 0.032 (0.042)
Train: 165 [ 700/1251 ( 56%)]  Loss: 4.099 (4.27)  Time: 0.157s, 6516.73/s  (0.193s, 5309.00/s)  LR: 4.224e-04  Data: 0.024 (0.041)
Train: 165 [ 750/1251 ( 60%)]  Loss: 4.285 (4.27)  Time: 0.189s, 5406.37/s  (0.193s, 5318.50/s)  LR: 4.224e-04  Data: 0.020 (0.041)
Train: 165 [ 800/1251 ( 64%)]  Loss: 4.294 (4.27)  Time: 0.153s, 6713.67/s  (0.193s, 5316.58/s)  LR: 4.224e-04  Data: 0.026 (0.040)
Train: 165 [ 850/1251 ( 68%)]  Loss: 4.252 (4.27)  Time: 0.182s, 5611.88/s  (0.193s, 5316.73/s)  LR: 4.224e-04  Data: 0.024 (0.039)
Train: 165 [ 900/1251 ( 72%)]  Loss: 4.360 (4.27)  Time: 0.171s, 5985.88/s  (0.193s, 5309.49/s)  LR: 4.224e-04  Data: 0.028 (0.039)
Train: 165 [ 950/1251 ( 76%)]  Loss: 4.241 (4.27)  Time: 0.175s, 5863.24/s  (0.193s, 5307.84/s)  LR: 4.224e-04  Data: 0.028 (0.038)
Train: 165 [1000/1251 ( 80%)]  Loss: 4.241 (4.27)  Time: 0.168s, 6088.70/s  (0.193s, 5306.92/s)  LR: 4.224e-04  Data: 0.019 (0.038)
Train: 165 [1050/1251 ( 84%)]  Loss: 3.973 (4.26)  Time: 0.169s, 6046.69/s  (0.193s, 5305.25/s)  LR: 4.224e-04  Data: 0.028 (0.037)
Train: 165 [1100/1251 ( 88%)]  Loss: 4.547 (4.27)  Time: 0.167s, 6142.78/s  (0.193s, 5299.05/s)  LR: 4.224e-04  Data: 0.026 (0.037)
Train: 165 [1150/1251 ( 92%)]  Loss: 4.486 (4.28)  Time: 0.164s, 6225.54/s  (0.193s, 5292.27/s)  LR: 4.224e-04  Data: 0.026 (0.037)
Train: 165 [1200/1251 ( 96%)]  Loss: 4.474 (4.29)  Time: 0.165s, 6198.34/s  (0.193s, 5295.44/s)  LR: 4.224e-04  Data: 0.026 (0.036)
Train: 165 [1250/1251 (100%)]  Loss: 4.498 (4.30)  Time: 0.114s, 8977.25/s  (0.193s, 5308.10/s)  LR: 4.224e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.736 (1.736)  Loss:  1.0725 (1.0725)  Acc@1: 82.2266 (82.2266)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1629 (1.7704)  Acc@1: 80.6604 (66.1860)  Acc@5: 94.5755 (87.2280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-163.pth.tar', 65.63200005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-152.pth.tar', 65.57800008056641)

Train: 166 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 1.697s,  603.54/s  (1.697s,  603.54/s)  LR: 4.172e-04  Data: 1.573 (1.573)
Train: 166 [  50/1251 (  4%)]  Loss: 3.923 (3.96)  Time: 0.169s, 6072.17/s  (0.222s, 4607.01/s)  LR: 4.172e-04  Data: 0.028 (0.080)
Train: 166 [ 100/1251 (  8%)]  Loss: 3.969 (3.96)  Time: 0.168s, 6096.03/s  (0.207s, 4949.74/s)  LR: 4.172e-04  Data: 0.034 (0.064)
Train: 166 [ 150/1251 ( 12%)]  Loss: 4.046 (3.98)  Time: 0.166s, 6165.54/s  (0.201s, 5096.89/s)  LR: 4.172e-04  Data: 0.022 (0.056)
Train: 166 [ 200/1251 ( 16%)]  Loss: 4.076 (4.00)  Time: 0.182s, 5641.50/s  (0.199s, 5138.33/s)  LR: 4.172e-04  Data: 0.020 (0.055)
Train: 166 [ 250/1251 ( 20%)]  Loss: 4.436 (4.07)  Time: 0.154s, 6629.03/s  (0.197s, 5199.57/s)  LR: 4.172e-04  Data: 0.025 (0.052)
Train: 166 [ 300/1251 ( 24%)]  Loss: 4.197 (4.09)  Time: 0.196s, 5221.74/s  (0.196s, 5226.55/s)  LR: 4.172e-04  Data: 0.025 (0.051)
Train: 166 [ 350/1251 ( 28%)]  Loss: 4.169 (4.10)  Time: 0.166s, 6156.14/s  (0.195s, 5254.55/s)  LR: 4.172e-04  Data: 0.028 (0.049)
Train: 166 [ 400/1251 ( 32%)]  Loss: 4.482 (4.14)  Time: 0.165s, 6207.19/s  (0.195s, 5254.06/s)  LR: 4.172e-04  Data: 0.023 (0.049)
Train: 166 [ 450/1251 ( 36%)]  Loss: 4.048 (4.13)  Time: 0.187s, 5470.11/s  (0.194s, 5284.96/s)  LR: 4.172e-04  Data: 0.028 (0.047)
Train: 166 [ 500/1251 ( 40%)]  Loss: 4.439 (4.16)  Time: 0.170s, 6030.48/s  (0.193s, 5319.31/s)  LR: 4.172e-04  Data: 0.034 (0.045)
Train: 166 [ 550/1251 ( 44%)]  Loss: 3.905 (4.14)  Time: 0.338s, 3032.76/s  (0.193s, 5292.46/s)  LR: 4.172e-04  Data: 0.029 (0.044)
Train: 166 [ 600/1251 ( 48%)]  Loss: 4.187 (4.14)  Time: 0.167s, 6115.29/s  (0.193s, 5299.79/s)  LR: 4.172e-04  Data: 0.024 (0.043)
Train: 166 [ 650/1251 ( 52%)]  Loss: 4.462 (4.17)  Time: 0.167s, 6115.49/s  (0.193s, 5311.62/s)  LR: 4.172e-04  Data: 0.027 (0.042)
Train: 166 [ 700/1251 ( 56%)]  Loss: 4.161 (4.17)  Time: 0.190s, 5389.38/s  (0.193s, 5314.21/s)  LR: 4.172e-04  Data: 0.029 (0.041)
Train: 166 [ 750/1251 ( 60%)]  Loss: 3.822 (4.14)  Time: 0.174s, 5878.12/s  (0.193s, 5313.39/s)  LR: 4.172e-04  Data: 0.032 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 166 [ 800/1251 ( 64%)]  Loss: 4.340 (4.16)  Time: 0.178s, 5736.74/s  (0.193s, 5316.18/s)  LR: 4.172e-04  Data: 0.024 (0.040)
Train: 166 [ 850/1251 ( 68%)]  Loss: 3.903 (4.14)  Time: 0.165s, 6189.78/s  (0.193s, 5316.44/s)  LR: 4.172e-04  Data: 0.034 (0.040)
Train: 166 [ 900/1251 ( 72%)]  Loss: 4.202 (4.15)  Time: 0.287s, 3570.25/s  (0.193s, 5312.58/s)  LR: 4.172e-04  Data: 0.163 (0.040)
Train: 166 [ 950/1251 ( 76%)]  Loss: 4.254 (4.15)  Time: 0.406s, 2524.99/s  (0.193s, 5304.61/s)  LR: 4.172e-04  Data: 0.031 (0.041)
Train: 166 [1000/1251 ( 80%)]  Loss: 4.362 (4.16)  Time: 0.199s, 5150.24/s  (0.193s, 5304.79/s)  LR: 4.172e-04  Data: 0.026 (0.040)
Train: 166 [1050/1251 ( 84%)]  Loss: 4.274 (4.17)  Time: 0.165s, 6212.67/s  (0.193s, 5303.16/s)  LR: 4.172e-04  Data: 0.030 (0.040)
Train: 166 [1100/1251 ( 88%)]  Loss: 4.261 (4.17)  Time: 0.253s, 4050.68/s  (0.193s, 5297.08/s)  LR: 4.172e-04  Data: 0.124 (0.040)
Train: 166 [1150/1251 ( 92%)]  Loss: 4.322 (4.18)  Time: 0.173s, 5905.72/s  (0.193s, 5292.94/s)  LR: 4.172e-04  Data: 0.032 (0.040)
Train: 166 [1200/1251 ( 96%)]  Loss: 4.103 (4.17)  Time: 0.178s, 5756.30/s  (0.194s, 5286.76/s)  LR: 4.172e-04  Data: 0.023 (0.039)
Train: 166 [1250/1251 (100%)]  Loss: 4.491 (4.19)  Time: 0.114s, 9009.28/s  (0.193s, 5305.29/s)  LR: 4.172e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.964 (1.964)  Loss:  1.1285 (1.1285)  Acc@1: 81.4453 (81.4453)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.2288 (1.7740)  Acc@1: 80.6604 (65.7800)  Acc@5: 93.2783 (86.9640)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-166.pth.tar', 65.78000000244141)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-163.pth.tar', 65.63200005126953)

Train: 167 [   0/1251 (  0%)]  Loss: 4.606 (4.61)  Time: 2.009s,  509.71/s  (2.009s,  509.71/s)  LR: 4.120e-04  Data: 1.878 (1.878)
Train: 167 [  50/1251 (  4%)]  Loss: 4.280 (4.44)  Time: 0.182s, 5628.43/s  (0.230s, 4444.74/s)  LR: 4.120e-04  Data: 0.026 (0.068)
Train: 167 [ 100/1251 (  8%)]  Loss: 4.294 (4.39)  Time: 0.172s, 5967.40/s  (0.211s, 4846.98/s)  LR: 4.120e-04  Data: 0.026 (0.049)
Train: 167 [ 150/1251 ( 12%)]  Loss: 4.498 (4.42)  Time: 0.160s, 6400.28/s  (0.203s, 5038.14/s)  LR: 4.120e-04  Data: 0.031 (0.042)
Train: 167 [ 200/1251 ( 16%)]  Loss: 3.967 (4.33)  Time: 0.167s, 6119.83/s  (0.199s, 5158.49/s)  LR: 4.120e-04  Data: 0.022 (0.039)
Train: 167 [ 250/1251 ( 20%)]  Loss: 4.003 (4.27)  Time: 0.172s, 5958.74/s  (0.196s, 5228.24/s)  LR: 4.120e-04  Data: 0.025 (0.037)
Train: 167 [ 300/1251 ( 24%)]  Loss: 4.052 (4.24)  Time: 0.183s, 5600.55/s  (0.196s, 5227.81/s)  LR: 4.120e-04  Data: 0.027 (0.040)
Train: 167 [ 350/1251 ( 28%)]  Loss: 4.098 (4.22)  Time: 0.278s, 3689.52/s  (0.195s, 5250.34/s)  LR: 4.120e-04  Data: 0.131 (0.041)
Train: 167 [ 400/1251 ( 32%)]  Loss: 4.395 (4.24)  Time: 0.177s, 5794.03/s  (0.194s, 5271.97/s)  LR: 4.120e-04  Data: 0.025 (0.041)
Train: 167 [ 450/1251 ( 36%)]  Loss: 4.299 (4.25)  Time: 0.162s, 6320.95/s  (0.194s, 5272.91/s)  LR: 4.120e-04  Data: 0.024 (0.042)
Train: 167 [ 500/1251 ( 40%)]  Loss: 4.416 (4.26)  Time: 0.184s, 5578.45/s  (0.193s, 5308.07/s)  LR: 4.120e-04  Data: 0.029 (0.041)
Train: 167 [ 550/1251 ( 44%)]  Loss: 3.836 (4.23)  Time: 0.173s, 5902.27/s  (0.194s, 5291.93/s)  LR: 4.120e-04  Data: 0.030 (0.040)
Train: 167 [ 600/1251 ( 48%)]  Loss: 4.576 (4.26)  Time: 0.258s, 3965.26/s  (0.193s, 5298.74/s)  LR: 4.120e-04  Data: 0.028 (0.039)
Train: 167 [ 650/1251 ( 52%)]  Loss: 4.195 (4.25)  Time: 0.163s, 6292.58/s  (0.193s, 5312.63/s)  LR: 4.120e-04  Data: 0.030 (0.038)
Train: 167 [ 700/1251 ( 56%)]  Loss: 3.928 (4.23)  Time: 0.173s, 5911.64/s  (0.193s, 5311.61/s)  LR: 4.120e-04  Data: 0.024 (0.037)
Train: 167 [ 750/1251 ( 60%)]  Loss: 4.148 (4.22)  Time: 0.256s, 3994.88/s  (0.193s, 5309.85/s)  LR: 4.120e-04  Data: 0.140 (0.038)
Train: 167 [ 800/1251 ( 64%)]  Loss: 4.412 (4.24)  Time: 0.271s, 3775.56/s  (0.193s, 5308.17/s)  LR: 4.120e-04  Data: 0.034 (0.038)
Train: 167 [ 850/1251 ( 68%)]  Loss: 4.103 (4.23)  Time: 0.167s, 6117.54/s  (0.193s, 5308.29/s)  LR: 4.120e-04  Data: 0.026 (0.038)
Train: 167 [ 900/1251 ( 72%)]  Loss: 4.377 (4.24)  Time: 0.158s, 6463.64/s  (0.193s, 5314.33/s)  LR: 4.120e-04  Data: 0.029 (0.038)
Train: 167 [ 950/1251 ( 76%)]  Loss: 4.447 (4.25)  Time: 0.185s, 5532.27/s  (0.193s, 5316.25/s)  LR: 4.120e-04  Data: 0.028 (0.038)
Train: 167 [1000/1251 ( 80%)]  Loss: 4.095 (4.24)  Time: 0.157s, 6517.25/s  (0.193s, 5314.89/s)  LR: 4.120e-04  Data: 0.032 (0.039)
Train: 167 [1050/1251 ( 84%)]  Loss: 4.470 (4.25)  Time: 0.163s, 6268.76/s  (0.193s, 5310.84/s)  LR: 4.120e-04  Data: 0.026 (0.038)
Train: 167 [1100/1251 ( 88%)]  Loss: 4.371 (4.26)  Time: 0.195s, 5246.21/s  (0.193s, 5299.88/s)  LR: 4.120e-04  Data: 0.026 (0.039)
Train: 167 [1150/1251 ( 92%)]  Loss: 4.490 (4.26)  Time: 0.165s, 6202.96/s  (0.193s, 5293.25/s)  LR: 4.120e-04  Data: 0.021 (0.040)
Train: 167 [1200/1251 ( 96%)]  Loss: 4.225 (4.26)  Time: 0.326s, 3143.01/s  (0.194s, 5287.69/s)  LR: 4.120e-04  Data: 0.199 (0.040)
Train: 167 [1250/1251 (100%)]  Loss: 4.134 (4.26)  Time: 0.117s, 8753.88/s  (0.193s, 5302.56/s)  LR: 4.120e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.749 (1.749)  Loss:  1.0865 (1.0865)  Acc@1: 80.7617 (80.7617)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2143 (1.7972)  Acc@1: 79.4811 (65.3600)  Acc@5: 92.8066 (86.3740)
Train: 168 [   0/1251 (  0%)]  Loss: 4.346 (4.35)  Time: 1.803s,  568.07/s  (1.803s,  568.07/s)  LR: 4.069e-04  Data: 1.592 (1.592)
Train: 168 [  50/1251 (  4%)]  Loss: 4.018 (4.18)  Time: 0.172s, 5948.52/s  (0.225s, 4543.88/s)  LR: 4.069e-04  Data: 0.023 (0.071)
Train: 168 [ 100/1251 (  8%)]  Loss: 4.337 (4.23)  Time: 0.170s, 6036.45/s  (0.207s, 4958.75/s)  LR: 4.069e-04  Data: 0.027 (0.050)
Train: 168 [ 150/1251 ( 12%)]  Loss: 3.994 (4.17)  Time: 0.175s, 5849.11/s  (0.202s, 5067.29/s)  LR: 4.069e-04  Data: 0.025 (0.043)
Train: 168 [ 200/1251 ( 16%)]  Loss: 4.138 (4.17)  Time: 0.363s, 2818.80/s  (0.199s, 5149.12/s)  LR: 4.069e-04  Data: 0.026 (0.040)
Train: 168 [ 250/1251 ( 20%)]  Loss: 4.264 (4.18)  Time: 0.164s, 6244.36/s  (0.197s, 5201.75/s)  LR: 4.069e-04  Data: 0.025 (0.037)
Train: 168 [ 300/1251 ( 24%)]  Loss: 4.201 (4.19)  Time: 0.367s, 2788.15/s  (0.196s, 5236.86/s)  LR: 4.069e-04  Data: 0.022 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 168 [ 350/1251 ( 28%)]  Loss: 4.050 (4.17)  Time: 0.178s, 5742.27/s  (0.195s, 5239.96/s)  LR: 4.069e-04  Data: 0.023 (0.035)
Train: 168 [ 400/1251 ( 32%)]  Loss: 4.144 (4.17)  Time: 0.184s, 5580.37/s  (0.196s, 5234.40/s)  LR: 4.069e-04  Data: 0.025 (0.034)
Train: 168 [ 450/1251 ( 36%)]  Loss: 4.469 (4.20)  Time: 0.172s, 5968.69/s  (0.194s, 5277.36/s)  LR: 4.069e-04  Data: 0.024 (0.034)
Train: 168 [ 500/1251 ( 40%)]  Loss: 4.428 (4.22)  Time: 0.163s, 6289.79/s  (0.194s, 5280.89/s)  LR: 4.069e-04  Data: 0.022 (0.033)
Train: 168 [ 550/1251 ( 44%)]  Loss: 3.819 (4.18)  Time: 0.190s, 5389.64/s  (0.194s, 5276.97/s)  LR: 4.069e-04  Data: 0.030 (0.033)
Train: 168 [ 600/1251 ( 48%)]  Loss: 4.411 (4.20)  Time: 0.173s, 5910.09/s  (0.194s, 5286.32/s)  LR: 4.069e-04  Data: 0.030 (0.032)
Train: 168 [ 650/1251 ( 52%)]  Loss: 4.426 (4.22)  Time: 0.181s, 5643.00/s  (0.193s, 5311.99/s)  LR: 4.069e-04  Data: 0.020 (0.032)
Train: 168 [ 700/1251 ( 56%)]  Loss: 4.049 (4.21)  Time: 0.173s, 5920.29/s  (0.193s, 5306.22/s)  LR: 4.069e-04  Data: 0.027 (0.032)
Train: 168 [ 750/1251 ( 60%)]  Loss: 4.503 (4.22)  Time: 0.154s, 6645.51/s  (0.193s, 5297.43/s)  LR: 4.069e-04  Data: 0.027 (0.032)
Train: 168 [ 800/1251 ( 64%)]  Loss: 4.645 (4.25)  Time: 0.409s, 2502.10/s  (0.194s, 5289.46/s)  LR: 4.069e-04  Data: 0.033 (0.031)
Train: 168 [ 850/1251 ( 68%)]  Loss: 4.305 (4.25)  Time: 0.156s, 6555.73/s  (0.193s, 5302.60/s)  LR: 4.069e-04  Data: 0.030 (0.031)
Train: 168 [ 900/1251 ( 72%)]  Loss: 4.173 (4.25)  Time: 0.161s, 6363.60/s  (0.193s, 5298.02/s)  LR: 4.069e-04  Data: 0.028 (0.031)
Train: 168 [ 950/1251 ( 76%)]  Loss: 4.302 (4.25)  Time: 0.165s, 6213.64/s  (0.193s, 5293.15/s)  LR: 4.069e-04  Data: 0.024 (0.031)
Train: 168 [1000/1251 ( 80%)]  Loss: 4.165 (4.25)  Time: 0.177s, 5773.42/s  (0.194s, 5284.19/s)  LR: 4.069e-04  Data: 0.027 (0.031)
Train: 168 [1050/1251 ( 84%)]  Loss: 3.922 (4.23)  Time: 0.174s, 5883.20/s  (0.193s, 5297.63/s)  LR: 4.069e-04  Data: 0.032 (0.030)
Train: 168 [1100/1251 ( 88%)]  Loss: 3.988 (4.22)  Time: 0.179s, 5727.68/s  (0.193s, 5292.81/s)  LR: 4.069e-04  Data: 0.022 (0.030)
Train: 168 [1150/1251 ( 92%)]  Loss: 3.907 (4.21)  Time: 0.186s, 5513.68/s  (0.194s, 5281.61/s)  LR: 4.069e-04  Data: 0.020 (0.030)
Train: 168 [1200/1251 ( 96%)]  Loss: 4.192 (4.21)  Time: 0.169s, 6051.86/s  (0.194s, 5281.71/s)  LR: 4.069e-04  Data: 0.029 (0.030)
Train: 168 [1250/1251 (100%)]  Loss: 4.196 (4.21)  Time: 0.125s, 8218.32/s  (0.193s, 5301.11/s)  LR: 4.069e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.751 (1.751)  Loss:  1.1596 (1.1596)  Acc@1: 80.5664 (80.5664)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0953 (1.7160)  Acc@1: 81.3679 (66.3220)  Acc@5: 94.3396 (87.4240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-166.pth.tar', 65.78000000244141)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-149.pth.tar', 65.66199993164062)

Train: 169 [   0/1251 (  0%)]  Loss: 4.024 (4.02)  Time: 1.651s,  620.39/s  (1.651s,  620.39/s)  LR: 4.018e-04  Data: 1.464 (1.464)
Train: 169 [  50/1251 (  4%)]  Loss: 4.024 (4.02)  Time: 0.167s, 6121.88/s  (0.226s, 4533.89/s)  LR: 4.018e-04  Data: 0.032 (0.059)
Train: 169 [ 100/1251 (  8%)]  Loss: 3.991 (4.01)  Time: 0.169s, 6050.19/s  (0.206s, 4964.15/s)  LR: 4.018e-04  Data: 0.026 (0.044)
Train: 169 [ 150/1251 ( 12%)]  Loss: 3.947 (4.00)  Time: 0.169s, 6048.74/s  (0.200s, 5112.98/s)  LR: 4.018e-04  Data: 0.021 (0.039)
Train: 169 [ 200/1251 ( 16%)]  Loss: 3.680 (3.93)  Time: 0.163s, 6273.42/s  (0.197s, 5196.20/s)  LR: 4.018e-04  Data: 0.033 (0.036)
Train: 169 [ 250/1251 ( 20%)]  Loss: 3.884 (3.93)  Time: 0.175s, 5848.08/s  (0.197s, 5189.71/s)  LR: 4.018e-04  Data: 0.021 (0.035)
Train: 169 [ 300/1251 ( 24%)]  Loss: 4.274 (3.98)  Time: 0.158s, 6488.41/s  (0.196s, 5230.13/s)  LR: 4.018e-04  Data: 0.034 (0.034)
Train: 169 [ 350/1251 ( 28%)]  Loss: 4.076 (3.99)  Time: 0.186s, 5518.77/s  (0.196s, 5234.09/s)  LR: 4.018e-04  Data: 0.025 (0.034)
Train: 169 [ 400/1251 ( 32%)]  Loss: 4.145 (4.01)  Time: 0.195s, 5246.45/s  (0.195s, 5247.70/s)  LR: 4.018e-04  Data: 0.056 (0.033)
Train: 169 [ 450/1251 ( 36%)]  Loss: 4.528 (4.06)  Time: 0.186s, 5506.85/s  (0.195s, 5247.50/s)  LR: 4.018e-04  Data: 0.036 (0.033)
Train: 169 [ 500/1251 ( 40%)]  Loss: 4.140 (4.07)  Time: 0.182s, 5627.81/s  (0.194s, 5276.85/s)  LR: 4.018e-04  Data: 0.028 (0.032)
Train: 169 [ 550/1251 ( 44%)]  Loss: 4.329 (4.09)  Time: 0.173s, 5909.88/s  (0.193s, 5298.08/s)  LR: 4.018e-04  Data: 0.031 (0.032)
Train: 169 [ 600/1251 ( 48%)]  Loss: 3.942 (4.08)  Time: 0.212s, 4840.56/s  (0.194s, 5284.57/s)  LR: 4.018e-04  Data: 0.028 (0.031)
Train: 169 [ 650/1251 ( 52%)]  Loss: 4.505 (4.11)  Time: 0.165s, 6224.64/s  (0.194s, 5275.69/s)  LR: 4.018e-04  Data: 0.030 (0.031)
Train: 169 [ 700/1251 ( 56%)]  Loss: 3.862 (4.09)  Time: 0.164s, 6237.58/s  (0.194s, 5281.87/s)  LR: 4.018e-04  Data: 0.032 (0.031)
Train: 169 [ 750/1251 ( 60%)]  Loss: 3.943 (4.08)  Time: 0.181s, 5650.53/s  (0.194s, 5279.11/s)  LR: 4.018e-04  Data: 0.026 (0.031)
Train: 169 [ 800/1251 ( 64%)]  Loss: 4.308 (4.09)  Time: 0.177s, 5791.49/s  (0.193s, 5293.55/s)  LR: 4.018e-04  Data: 0.030 (0.031)
Train: 169 [ 850/1251 ( 68%)]  Loss: 4.295 (4.11)  Time: 0.170s, 6022.72/s  (0.193s, 5293.82/s)  LR: 4.018e-04  Data: 0.020 (0.031)
Train: 169 [ 900/1251 ( 72%)]  Loss: 3.796 (4.09)  Time: 0.167s, 6142.47/s  (0.194s, 5284.46/s)  LR: 4.018e-04  Data: 0.019 (0.031)
Train: 169 [ 950/1251 ( 76%)]  Loss: 3.910 (4.08)  Time: 0.166s, 6182.44/s  (0.194s, 5287.59/s)  LR: 4.018e-04  Data: 0.041 (0.031)
Train: 169 [1000/1251 ( 80%)]  Loss: 4.164 (4.08)  Time: 0.172s, 5945.06/s  (0.194s, 5288.89/s)  LR: 4.018e-04  Data: 0.027 (0.031)
Train: 169 [1050/1251 ( 84%)]  Loss: 3.949 (4.08)  Time: 0.163s, 6276.48/s  (0.194s, 5290.83/s)  LR: 4.018e-04  Data: 0.028 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 169 [1100/1251 ( 88%)]  Loss: 4.612 (4.10)  Time: 0.221s, 4631.22/s  (0.194s, 5288.81/s)  LR: 4.018e-04  Data: 0.098 (0.031)
Train: 169 [1150/1251 ( 92%)]  Loss: 4.081 (4.10)  Time: 0.174s, 5883.87/s  (0.194s, 5286.96/s)  LR: 4.018e-04  Data: 0.023 (0.031)
Train: 169 [1200/1251 ( 96%)]  Loss: 4.356 (4.11)  Time: 0.187s, 5474.55/s  (0.194s, 5281.10/s)  LR: 4.018e-04  Data: 0.025 (0.031)
Train: 169 [1250/1251 (100%)]  Loss: 3.993 (4.11)  Time: 0.113s, 9033.95/s  (0.194s, 5291.31/s)  LR: 4.018e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.745 (1.745)  Loss:  1.0817 (1.0817)  Acc@1: 81.3477 (81.3477)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0875 (1.6951)  Acc@1: 78.6557 (66.1120)  Acc@5: 93.7500 (87.2020)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-166.pth.tar', 65.78000000244141)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-162.pth.tar', 65.73399995361328)

Train: 170 [   0/1251 (  0%)]  Loss: 3.649 (3.65)  Time: 1.651s,  620.19/s  (1.651s,  620.19/s)  LR: 3.966e-04  Data: 1.499 (1.499)
Train: 170 [  50/1251 (  4%)]  Loss: 4.307 (3.98)  Time: 0.173s, 5912.06/s  (0.220s, 4662.80/s)  LR: 3.966e-04  Data: 0.031 (0.068)
Train: 170 [ 100/1251 (  8%)]  Loss: 4.047 (4.00)  Time: 0.199s, 5143.66/s  (0.206s, 4978.42/s)  LR: 3.966e-04  Data: 0.020 (0.054)
Train: 170 [ 150/1251 ( 12%)]  Loss: 4.142 (4.04)  Time: 0.168s, 6083.19/s  (0.199s, 5143.73/s)  LR: 3.966e-04  Data: 0.025 (0.050)
Train: 170 [ 200/1251 ( 16%)]  Loss: 4.306 (4.09)  Time: 0.166s, 6164.95/s  (0.197s, 5190.67/s)  LR: 3.966e-04  Data: 0.032 (0.045)
Train: 170 [ 250/1251 ( 20%)]  Loss: 4.043 (4.08)  Time: 0.153s, 6696.34/s  (0.196s, 5222.87/s)  LR: 3.966e-04  Data: 0.021 (0.041)
Train: 170 [ 300/1251 ( 24%)]  Loss: 4.315 (4.12)  Time: 0.183s, 5608.62/s  (0.195s, 5260.26/s)  LR: 3.966e-04  Data: 0.026 (0.039)
Train: 170 [ 350/1251 ( 28%)]  Loss: 3.890 (4.09)  Time: 0.181s, 5658.45/s  (0.194s, 5274.75/s)  LR: 3.966e-04  Data: 0.026 (0.038)
Train: 170 [ 400/1251 ( 32%)]  Loss: 3.977 (4.07)  Time: 0.168s, 6109.72/s  (0.194s, 5279.93/s)  LR: 3.966e-04  Data: 0.034 (0.037)
Train: 170 [ 450/1251 ( 36%)]  Loss: 4.085 (4.08)  Time: 0.173s, 5902.62/s  (0.195s, 5249.41/s)  LR: 3.966e-04  Data: 0.027 (0.036)
Train: 170 [ 500/1251 ( 40%)]  Loss: 3.838 (4.05)  Time: 0.178s, 5766.81/s  (0.194s, 5273.32/s)  LR: 3.966e-04  Data: 0.026 (0.035)
Train: 170 [ 550/1251 ( 44%)]  Loss: 3.898 (4.04)  Time: 0.177s, 5790.78/s  (0.194s, 5276.86/s)  LR: 3.966e-04  Data: 0.025 (0.035)
Train: 170 [ 600/1251 ( 48%)]  Loss: 3.797 (4.02)  Time: 0.171s, 5984.00/s  (0.194s, 5287.65/s)  LR: 3.966e-04  Data: 0.031 (0.035)
Train: 170 [ 650/1251 ( 52%)]  Loss: 4.036 (4.02)  Time: 0.183s, 5581.04/s  (0.194s, 5290.75/s)  LR: 3.966e-04  Data: 0.033 (0.034)
Train: 170 [ 700/1251 ( 56%)]  Loss: 3.907 (4.02)  Time: 0.166s, 6153.87/s  (0.194s, 5289.99/s)  LR: 3.966e-04  Data: 0.027 (0.035)
Train: 170 [ 750/1251 ( 60%)]  Loss: 4.413 (4.04)  Time: 0.179s, 5706.83/s  (0.193s, 5298.16/s)  LR: 3.966e-04  Data: 0.033 (0.035)
Train: 170 [ 800/1251 ( 64%)]  Loss: 4.148 (4.05)  Time: 0.172s, 5966.90/s  (0.193s, 5305.93/s)  LR: 3.966e-04  Data: 0.037 (0.036)
Train: 170 [ 850/1251 ( 68%)]  Loss: 3.807 (4.03)  Time: 0.251s, 4083.08/s  (0.193s, 5292.90/s)  LR: 3.966e-04  Data: 0.021 (0.037)
Train: 170 [ 900/1251 ( 72%)]  Loss: 3.937 (4.03)  Time: 0.207s, 4942.69/s  (0.194s, 5280.13/s)  LR: 3.966e-04  Data: 0.029 (0.037)
Train: 170 [ 950/1251 ( 76%)]  Loss: 4.049 (4.03)  Time: 0.171s, 5975.53/s  (0.194s, 5280.32/s)  LR: 3.966e-04  Data: 0.033 (0.036)
Train: 170 [1000/1251 ( 80%)]  Loss: 4.622 (4.06)  Time: 0.189s, 5413.55/s  (0.194s, 5278.64/s)  LR: 3.966e-04  Data: 0.029 (0.036)
Train: 170 [1050/1251 ( 84%)]  Loss: 4.139 (4.06)  Time: 0.442s, 2319.09/s  (0.194s, 5271.21/s)  LR: 3.966e-04  Data: 0.026 (0.036)
Train: 170 [1100/1251 ( 88%)]  Loss: 4.194 (4.07)  Time: 0.163s, 6291.08/s  (0.194s, 5281.19/s)  LR: 3.966e-04  Data: 0.029 (0.035)
Train: 170 [1150/1251 ( 92%)]  Loss: 4.143 (4.07)  Time: 0.169s, 6046.37/s  (0.194s, 5280.77/s)  LR: 3.966e-04  Data: 0.022 (0.035)
Train: 170 [1200/1251 ( 96%)]  Loss: 4.008 (4.07)  Time: 0.161s, 6349.58/s  (0.194s, 5279.48/s)  LR: 3.966e-04  Data: 0.029 (0.035)
Train: 170 [1250/1251 (100%)]  Loss: 4.565 (4.09)  Time: 0.115s, 8937.31/s  (0.194s, 5279.53/s)  LR: 3.966e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.803 (1.803)  Loss:  1.1303 (1.1303)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.1849 (1.7887)  Acc@1: 81.3679 (65.6480)  Acc@5: 95.1651 (86.9480)
Train: 171 [   0/1251 (  0%)]  Loss: 3.894 (3.89)  Time: 1.761s,  581.55/s  (1.761s,  581.55/s)  LR: 3.915e-04  Data: 1.604 (1.604)
Train: 171 [  50/1251 (  4%)]  Loss: 4.420 (4.16)  Time: 0.178s, 5764.79/s  (0.226s, 4532.60/s)  LR: 3.915e-04  Data: 0.030 (0.081)
Train: 171 [ 100/1251 (  8%)]  Loss: 4.396 (4.24)  Time: 0.153s, 6685.51/s  (0.210s, 4875.44/s)  LR: 3.915e-04  Data: 0.023 (0.066)
Train: 171 [ 150/1251 ( 12%)]  Loss: 4.517 (4.31)  Time: 0.168s, 6094.24/s  (0.204s, 5029.26/s)  LR: 3.915e-04  Data: 0.028 (0.059)
Train: 171 [ 200/1251 ( 16%)]  Loss: 4.314 (4.31)  Time: 0.160s, 6403.57/s  (0.201s, 5084.17/s)  LR: 3.915e-04  Data: 0.029 (0.057)
Train: 171 [ 250/1251 ( 20%)]  Loss: 4.319 (4.31)  Time: 0.182s, 5618.69/s  (0.199s, 5154.90/s)  LR: 3.915e-04  Data: 0.026 (0.053)
Train: 171 [ 300/1251 ( 24%)]  Loss: 4.458 (4.33)  Time: 0.203s, 5040.70/s  (0.197s, 5209.06/s)  LR: 3.915e-04  Data: 0.019 (0.050)
Train: 171 [ 350/1251 ( 28%)]  Loss: 3.920 (4.28)  Time: 0.184s, 5576.08/s  (0.196s, 5230.42/s)  LR: 3.915e-04  Data: 0.019 (0.049)
Train: 171 [ 400/1251 ( 32%)]  Loss: 4.092 (4.26)  Time: 0.182s, 5626.84/s  (0.195s, 5237.89/s)  LR: 3.915e-04  Data: 0.024 (0.047)
Train: 171 [ 450/1251 ( 36%)]  Loss: 4.077 (4.24)  Time: 0.157s, 6505.53/s  (0.194s, 5272.59/s)  LR: 3.915e-04  Data: 0.033 (0.045)
Train: 171 [ 500/1251 ( 40%)]  Loss: 4.212 (4.24)  Time: 0.158s, 6460.67/s  (0.194s, 5269.07/s)  LR: 3.915e-04  Data: 0.026 (0.043)
Train: 171 [ 550/1251 ( 44%)]  Loss: 4.511 (4.26)  Time: 0.181s, 5666.75/s  (0.195s, 5262.34/s)  LR: 3.915e-04  Data: 0.027 (0.042)
Train: 171 [ 600/1251 ( 48%)]  Loss: 4.224 (4.26)  Time: 0.175s, 5843.38/s  (0.194s, 5275.05/s)  LR: 3.915e-04  Data: 0.019 (0.041)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 171 [ 650/1251 ( 52%)]  Loss: 4.373 (4.27)  Time: 0.183s, 5585.59/s  (0.194s, 5277.08/s)  LR: 3.915e-04  Data: 0.029 (0.040)
Train: 171 [ 700/1251 ( 56%)]  Loss: 4.379 (4.27)  Time: 0.184s, 5568.93/s  (0.194s, 5278.60/s)  LR: 3.915e-04  Data: 0.032 (0.039)
Train: 171 [ 750/1251 ( 60%)]  Loss: 4.453 (4.28)  Time: 0.239s, 4275.86/s  (0.194s, 5276.15/s)  LR: 3.915e-04  Data: 0.030 (0.038)
Train: 171 [ 800/1251 ( 64%)]  Loss: 4.138 (4.28)  Time: 0.180s, 5676.03/s  (0.194s, 5284.27/s)  LR: 3.915e-04  Data: 0.028 (0.037)
Train: 171 [ 850/1251 ( 68%)]  Loss: 4.064 (4.26)  Time: 0.173s, 5920.56/s  (0.194s, 5284.90/s)  LR: 3.915e-04  Data: 0.027 (0.037)
Train: 171 [ 900/1251 ( 72%)]  Loss: 4.091 (4.26)  Time: 0.186s, 5501.96/s  (0.194s, 5287.36/s)  LR: 3.915e-04  Data: 0.027 (0.036)
Train: 171 [ 950/1251 ( 76%)]  Loss: 4.038 (4.24)  Time: 0.309s, 3317.00/s  (0.194s, 5288.59/s)  LR: 3.915e-04  Data: 0.024 (0.036)
Train: 171 [1000/1251 ( 80%)]  Loss: 4.231 (4.24)  Time: 0.179s, 5733.40/s  (0.194s, 5288.32/s)  LR: 3.915e-04  Data: 0.025 (0.035)
Train: 171 [1050/1251 ( 84%)]  Loss: 4.275 (4.25)  Time: 0.164s, 6252.13/s  (0.193s, 5296.55/s)  LR: 3.915e-04  Data: 0.028 (0.035)
Train: 171 [1100/1251 ( 88%)]  Loss: 4.474 (4.26)  Time: 0.191s, 5363.12/s  (0.193s, 5293.58/s)  LR: 3.915e-04  Data: 0.030 (0.035)
Train: 171 [1150/1251 ( 92%)]  Loss: 3.996 (4.24)  Time: 0.165s, 6190.60/s  (0.193s, 5301.65/s)  LR: 3.915e-04  Data: 0.031 (0.035)
Train: 171 [1200/1251 ( 96%)]  Loss: 4.071 (4.24)  Time: 0.169s, 6049.64/s  (0.193s, 5298.22/s)  LR: 3.915e-04  Data: 0.030 (0.034)
Train: 171 [1250/1251 (100%)]  Loss: 4.281 (4.24)  Time: 0.114s, 9001.69/s  (0.193s, 5309.89/s)  LR: 3.915e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.921 (1.921)  Loss:  1.0457 (1.0457)  Acc@1: 82.3242 (82.3242)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2054 (1.7025)  Acc@1: 80.0707 (65.9300)  Acc@5: 93.1604 (87.0340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-166.pth.tar', 65.78000000244141)

Train: 172 [   0/1251 (  0%)]  Loss: 3.993 (3.99)  Time: 1.682s,  608.97/s  (1.682s,  608.97/s)  LR: 3.864e-04  Data: 1.551 (1.551)
Train: 172 [  50/1251 (  4%)]  Loss: 4.424 (4.21)  Time: 0.178s, 5760.46/s  (0.225s, 4552.25/s)  LR: 3.864e-04  Data: 0.029 (0.081)
Train: 172 [ 100/1251 (  8%)]  Loss: 3.673 (4.03)  Time: 0.181s, 5667.05/s  (0.208s, 4928.88/s)  LR: 3.864e-04  Data: 0.028 (0.058)
Train: 172 [ 150/1251 ( 12%)]  Loss: 4.145 (4.06)  Time: 0.169s, 6067.43/s  (0.202s, 5079.87/s)  LR: 3.864e-04  Data: 0.031 (0.048)
Train: 172 [ 200/1251 ( 16%)]  Loss: 4.265 (4.10)  Time: 0.157s, 6525.80/s  (0.198s, 5168.45/s)  LR: 3.864e-04  Data: 0.028 (0.043)
Train: 172 [ 250/1251 ( 20%)]  Loss: 4.313 (4.14)  Time: 0.171s, 5988.13/s  (0.196s, 5222.57/s)  LR: 3.864e-04  Data: 0.025 (0.040)
Train: 172 [ 300/1251 ( 24%)]  Loss: 3.618 (4.06)  Time: 0.175s, 5849.25/s  (0.196s, 5223.55/s)  LR: 3.864e-04  Data: 0.020 (0.038)
Train: 172 [ 350/1251 ( 28%)]  Loss: 4.072 (4.06)  Time: 0.195s, 5254.33/s  (0.195s, 5237.94/s)  LR: 3.864e-04  Data: 0.028 (0.037)
Train: 172 [ 400/1251 ( 32%)]  Loss: 4.418 (4.10)  Time: 0.163s, 6274.02/s  (0.194s, 5269.75/s)  LR: 3.864e-04  Data: 0.031 (0.036)
Train: 172 [ 450/1251 ( 36%)]  Loss: 4.307 (4.12)  Time: 0.161s, 6363.89/s  (0.194s, 5278.15/s)  LR: 3.864e-04  Data: 0.028 (0.035)
Train: 172 [ 500/1251 ( 40%)]  Loss: 4.030 (4.11)  Time: 0.158s, 6467.85/s  (0.193s, 5301.51/s)  LR: 3.864e-04  Data: 0.029 (0.034)
Train: 172 [ 550/1251 ( 44%)]  Loss: 4.376 (4.14)  Time: 0.194s, 5290.70/s  (0.193s, 5298.85/s)  LR: 3.864e-04  Data: 0.026 (0.034)
Train: 172 [ 600/1251 ( 48%)]  Loss: 4.235 (4.14)  Time: 0.197s, 5186.58/s  (0.193s, 5300.29/s)  LR: 3.864e-04  Data: 0.026 (0.033)
Train: 172 [ 650/1251 ( 52%)]  Loss: 3.963 (4.13)  Time: 0.163s, 6283.43/s  (0.193s, 5310.92/s)  LR: 3.864e-04  Data: 0.025 (0.033)
Train: 172 [ 700/1251 ( 56%)]  Loss: 4.086 (4.13)  Time: 0.182s, 5641.46/s  (0.193s, 5303.84/s)  LR: 3.864e-04  Data: 0.034 (0.033)
Train: 172 [ 750/1251 ( 60%)]  Loss: 4.351 (4.14)  Time: 0.173s, 5931.36/s  (0.193s, 5312.88/s)  LR: 3.864e-04  Data: 0.024 (0.032)
Train: 172 [ 800/1251 ( 64%)]  Loss: 4.710 (4.18)  Time: 0.170s, 6012.17/s  (0.193s, 5319.07/s)  LR: 3.864e-04  Data: 0.031 (0.032)
Train: 172 [ 850/1251 ( 68%)]  Loss: 4.416 (4.19)  Time: 0.169s, 6046.41/s  (0.193s, 5298.73/s)  LR: 3.864e-04  Data: 0.036 (0.032)
Train: 172 [ 900/1251 ( 72%)]  Loss: 3.848 (4.17)  Time: 0.200s, 5122.13/s  (0.193s, 5306.52/s)  LR: 3.864e-04  Data: 0.023 (0.032)
Train: 172 [ 950/1251 ( 76%)]  Loss: 4.235 (4.17)  Time: 0.167s, 6132.93/s  (0.193s, 5308.35/s)  LR: 3.864e-04  Data: 0.025 (0.032)
Train: 172 [1000/1251 ( 80%)]  Loss: 3.818 (4.16)  Time: 0.166s, 6159.88/s  (0.193s, 5312.50/s)  LR: 3.864e-04  Data: 0.020 (0.031)
Train: 172 [1050/1251 ( 84%)]  Loss: 4.128 (4.16)  Time: 0.175s, 5849.67/s  (0.193s, 5303.37/s)  LR: 3.864e-04  Data: 0.026 (0.031)
Train: 172 [1100/1251 ( 88%)]  Loss: 4.358 (4.16)  Time: 0.161s, 6341.37/s  (0.193s, 5300.04/s)  LR: 3.864e-04  Data: 0.027 (0.031)
Train: 172 [1150/1251 ( 92%)]  Loss: 4.016 (4.16)  Time: 0.201s, 5094.96/s  (0.194s, 5288.46/s)  LR: 3.864e-04  Data: 0.038 (0.031)
Train: 172 [1200/1251 ( 96%)]  Loss: 4.552 (4.17)  Time: 0.165s, 6193.13/s  (0.193s, 5292.95/s)  LR: 3.864e-04  Data: 0.025 (0.031)
Train: 172 [1250/1251 (100%)]  Loss: 3.924 (4.16)  Time: 0.114s, 9016.05/s  (0.193s, 5304.68/s)  LR: 3.864e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.806 (1.806)  Loss:  1.1183 (1.1183)  Acc@1: 80.8594 (80.8594)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.0987 (1.7570)  Acc@1: 81.0142 (66.3920)  Acc@5: 94.8113 (87.1920)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-159.pth.tar', 65.78200010742188)

Train: 173 [   0/1251 (  0%)]  Loss: 4.387 (4.39)  Time: 1.760s,  581.68/s  (1.760s,  581.68/s)  LR: 3.814e-04  Data: 1.628 (1.628)
Train: 173 [  50/1251 (  4%)]  Loss: 4.101 (4.24)  Time: 0.182s, 5636.98/s  (0.227s, 4505.33/s)  LR: 3.814e-04  Data: 0.019 (0.072)
Train: 173 [ 100/1251 (  8%)]  Loss: 4.067 (4.18)  Time: 0.175s, 5864.73/s  (0.211s, 4854.43/s)  LR: 3.814e-04  Data: 0.027 (0.049)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 173 [ 150/1251 ( 12%)]  Loss: 4.227 (4.20)  Time: 0.175s, 5840.85/s  (0.201s, 5094.56/s)  LR: 3.814e-04  Data: 0.025 (0.042)
Train: 173 [ 200/1251 ( 16%)]  Loss: 4.426 (4.24)  Time: 0.173s, 5931.09/s  (0.199s, 5155.83/s)  LR: 3.814e-04  Data: 0.027 (0.039)
Train: 173 [ 250/1251 ( 20%)]  Loss: 4.320 (4.25)  Time: 0.161s, 6345.56/s  (0.197s, 5193.30/s)  LR: 3.814e-04  Data: 0.035 (0.037)
Train: 173 [ 300/1251 ( 24%)]  Loss: 4.019 (4.22)  Time: 0.185s, 5545.80/s  (0.195s, 5238.65/s)  LR: 3.814e-04  Data: 0.031 (0.035)
Train: 173 [ 350/1251 ( 28%)]  Loss: 4.387 (4.24)  Time: 0.175s, 5854.05/s  (0.195s, 5250.32/s)  LR: 3.814e-04  Data: 0.032 (0.034)
Train: 173 [ 400/1251 ( 32%)]  Loss: 3.802 (4.19)  Time: 0.174s, 5877.67/s  (0.194s, 5283.94/s)  LR: 3.814e-04  Data: 0.029 (0.033)
Train: 173 [ 450/1251 ( 36%)]  Loss: 3.695 (4.14)  Time: 0.164s, 6250.67/s  (0.194s, 5272.07/s)  LR: 3.814e-04  Data: 0.035 (0.033)
Train: 173 [ 500/1251 ( 40%)]  Loss: 3.806 (4.11)  Time: 0.191s, 5361.27/s  (0.194s, 5283.82/s)  LR: 3.814e-04  Data: 0.022 (0.032)
Train: 173 [ 550/1251 ( 44%)]  Loss: 3.905 (4.10)  Time: 0.174s, 5882.50/s  (0.193s, 5293.30/s)  LR: 3.814e-04  Data: 0.026 (0.032)
Train: 173 [ 600/1251 ( 48%)]  Loss: 4.323 (4.11)  Time: 0.160s, 6406.85/s  (0.193s, 5303.81/s)  LR: 3.814e-04  Data: 0.029 (0.032)
Train: 173 [ 650/1251 ( 52%)]  Loss: 4.487 (4.14)  Time: 0.173s, 5910.78/s  (0.193s, 5303.19/s)  LR: 3.814e-04  Data: 0.031 (0.032)
Train: 173 [ 700/1251 ( 56%)]  Loss: 4.252 (4.15)  Time: 0.169s, 6055.31/s  (0.193s, 5300.01/s)  LR: 3.814e-04  Data: 0.019 (0.032)
Train: 173 [ 750/1251 ( 60%)]  Loss: 3.858 (4.13)  Time: 0.180s, 5702.86/s  (0.193s, 5300.02/s)  LR: 3.814e-04  Data: 0.020 (0.032)
Train: 173 [ 800/1251 ( 64%)]  Loss: 4.026 (4.12)  Time: 0.167s, 6124.12/s  (0.194s, 5288.33/s)  LR: 3.814e-04  Data: 0.022 (0.032)
Train: 173 [ 850/1251 ( 68%)]  Loss: 3.952 (4.11)  Time: 0.161s, 6375.25/s  (0.193s, 5293.61/s)  LR: 3.814e-04  Data: 0.025 (0.031)
Train: 173 [ 900/1251 ( 72%)]  Loss: 4.187 (4.12)  Time: 0.163s, 6266.18/s  (0.193s, 5297.88/s)  LR: 3.814e-04  Data: 0.031 (0.031)
Train: 173 [ 950/1251 ( 76%)]  Loss: 4.039 (4.11)  Time: 0.184s, 5561.38/s  (0.194s, 5291.27/s)  LR: 3.814e-04  Data: 0.037 (0.031)
Train: 173 [1000/1251 ( 80%)]  Loss: 4.260 (4.12)  Time: 0.176s, 5823.73/s  (0.193s, 5309.45/s)  LR: 3.814e-04  Data: 0.035 (0.031)
Train: 173 [1050/1251 ( 84%)]  Loss: 4.043 (4.12)  Time: 0.184s, 5560.15/s  (0.193s, 5304.88/s)  LR: 3.814e-04  Data: 0.031 (0.031)
Train: 173 [1100/1251 ( 88%)]  Loss: 4.511 (4.13)  Time: 0.177s, 5789.39/s  (0.193s, 5303.21/s)  LR: 3.814e-04  Data: 0.029 (0.031)
Train: 173 [1150/1251 ( 92%)]  Loss: 4.119 (4.13)  Time: 0.168s, 6078.77/s  (0.193s, 5298.89/s)  LR: 3.814e-04  Data: 0.026 (0.031)
Train: 173 [1200/1251 ( 96%)]  Loss: 4.208 (4.14)  Time: 0.165s, 6199.70/s  (0.193s, 5292.88/s)  LR: 3.814e-04  Data: 0.033 (0.031)
Train: 173 [1250/1251 (100%)]  Loss: 4.169 (4.14)  Time: 0.114s, 8990.84/s  (0.193s, 5303.21/s)  LR: 3.814e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.712 (1.712)  Loss:  1.0770 (1.0770)  Acc@1: 81.4453 (81.4453)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.1681 (1.7878)  Acc@1: 81.2500 (66.1920)  Acc@5: 93.3962 (87.0180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-160.pth.tar', 65.80800002441406)

Train: 174 [   0/1251 (  0%)]  Loss: 4.097 (4.10)  Time: 1.949s,  525.39/s  (1.949s,  525.39/s)  LR: 3.763e-04  Data: 1.824 (1.824)
Train: 174 [  50/1251 (  4%)]  Loss: 4.341 (4.22)  Time: 0.175s, 5857.15/s  (0.229s, 4479.05/s)  LR: 3.763e-04  Data: 0.028 (0.086)
Train: 174 [ 100/1251 (  8%)]  Loss: 4.053 (4.16)  Time: 0.175s, 5861.95/s  (0.208s, 4927.73/s)  LR: 3.763e-04  Data: 0.025 (0.061)
Train: 174 [ 150/1251 ( 12%)]  Loss: 4.189 (4.17)  Time: 0.176s, 5820.57/s  (0.201s, 5106.85/s)  LR: 3.763e-04  Data: 0.026 (0.052)
Train: 174 [ 200/1251 ( 16%)]  Loss: 3.938 (4.12)  Time: 0.198s, 5184.42/s  (0.199s, 5158.22/s)  LR: 3.763e-04  Data: 0.022 (0.046)
Train: 174 [ 250/1251 ( 20%)]  Loss: 3.867 (4.08)  Time: 0.161s, 6373.04/s  (0.197s, 5207.11/s)  LR: 3.763e-04  Data: 0.039 (0.043)
Train: 174 [ 300/1251 ( 24%)]  Loss: 3.795 (4.04)  Time: 0.168s, 6085.26/s  (0.196s, 5234.51/s)  LR: 3.763e-04  Data: 0.028 (0.040)
Train: 174 [ 350/1251 ( 28%)]  Loss: 4.424 (4.09)  Time: 0.231s, 4437.92/s  (0.194s, 5288.62/s)  LR: 3.763e-04  Data: 0.022 (0.038)
Train: 174 [ 400/1251 ( 32%)]  Loss: 4.269 (4.11)  Time: 0.192s, 5345.68/s  (0.194s, 5274.98/s)  LR: 3.763e-04  Data: 0.021 (0.037)
Train: 174 [ 450/1251 ( 36%)]  Loss: 4.210 (4.12)  Time: 0.164s, 6229.36/s  (0.193s, 5310.57/s)  LR: 3.763e-04  Data: 0.031 (0.036)
Train: 174 [ 500/1251 ( 40%)]  Loss: 4.548 (4.16)  Time: 0.234s, 4382.07/s  (0.193s, 5312.94/s)  LR: 3.763e-04  Data: 0.027 (0.035)
Train: 174 [ 550/1251 ( 44%)]  Loss: 4.644 (4.20)  Time: 0.181s, 5670.80/s  (0.192s, 5328.90/s)  LR: 3.763e-04  Data: 0.029 (0.034)
Train: 174 [ 600/1251 ( 48%)]  Loss: 4.249 (4.20)  Time: 0.208s, 4916.45/s  (0.192s, 5324.39/s)  LR: 3.763e-04  Data: 0.020 (0.034)
Train: 174 [ 650/1251 ( 52%)]  Loss: 4.150 (4.20)  Time: 0.188s, 5437.50/s  (0.193s, 5317.94/s)  LR: 3.763e-04  Data: 0.028 (0.034)
Train: 174 [ 700/1251 ( 56%)]  Loss: 3.805 (4.17)  Time: 0.179s, 5734.60/s  (0.193s, 5307.49/s)  LR: 3.763e-04  Data: 0.039 (0.033)
Train: 174 [ 750/1251 ( 60%)]  Loss: 4.313 (4.18)  Time: 0.202s, 5080.75/s  (0.193s, 5301.06/s)  LR: 3.763e-04  Data: 0.026 (0.033)
Train: 174 [ 800/1251 ( 64%)]  Loss: 4.024 (4.17)  Time: 0.346s, 2955.91/s  (0.193s, 5315.24/s)  LR: 3.763e-04  Data: 0.021 (0.033)
Train: 174 [ 850/1251 ( 68%)]  Loss: 4.161 (4.17)  Time: 0.275s, 3724.19/s  (0.192s, 5321.24/s)  LR: 3.763e-04  Data: 0.025 (0.032)
Train: 174 [ 900/1251 ( 72%)]  Loss: 4.218 (4.17)  Time: 0.177s, 5789.84/s  (0.193s, 5308.89/s)  LR: 3.763e-04  Data: 0.032 (0.032)
Train: 174 [ 950/1251 ( 76%)]  Loss: 3.975 (4.16)  Time: 0.165s, 6223.61/s  (0.193s, 5295.91/s)  LR: 3.763e-04  Data: 0.045 (0.032)
Train: 174 [1000/1251 ( 80%)]  Loss: 4.205 (4.17)  Time: 0.185s, 5533.13/s  (0.193s, 5298.93/s)  LR: 3.763e-04  Data: 0.027 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 174 [1050/1251 ( 84%)]  Loss: 4.322 (4.17)  Time: 0.155s, 6599.97/s  (0.193s, 5299.22/s)  LR: 3.763e-04  Data: 0.024 (0.031)
Train: 174 [1100/1251 ( 88%)]  Loss: 4.385 (4.18)  Time: 0.172s, 5958.10/s  (0.193s, 5306.78/s)  LR: 3.763e-04  Data: 0.029 (0.031)
Train: 174 [1150/1251 ( 92%)]  Loss: 4.164 (4.18)  Time: 0.167s, 6141.22/s  (0.193s, 5296.80/s)  LR: 3.763e-04  Data: 0.026 (0.031)
Train: 174 [1200/1251 ( 96%)]  Loss: 4.250 (4.18)  Time: 0.184s, 5558.25/s  (0.193s, 5293.70/s)  LR: 3.763e-04  Data: 0.025 (0.031)
Train: 174 [1250/1251 (100%)]  Loss: 3.790 (4.17)  Time: 0.113s, 9044.34/s  (0.193s, 5302.13/s)  LR: 3.763e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.890 (1.890)  Loss:  1.0246 (1.0246)  Acc@1: 82.1289 (82.1289)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.0802 (1.7168)  Acc@1: 80.3066 (66.0180)  Acc@5: 94.2217 (87.0340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-174.pth.tar', 66.01799997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-161.pth.tar', 65.85800000732422)

Train: 175 [   0/1251 (  0%)]  Loss: 3.593 (3.59)  Time: 1.726s,  593.13/s  (1.726s,  593.13/s)  LR: 3.712e-04  Data: 1.600 (1.600)
Train: 175 [  50/1251 (  4%)]  Loss: 4.154 (3.87)  Time: 0.164s, 6235.62/s  (0.221s, 4623.19/s)  LR: 3.712e-04  Data: 0.031 (0.060)
Train: 175 [ 100/1251 (  8%)]  Loss: 4.400 (4.05)  Time: 0.166s, 6183.06/s  (0.206s, 4977.55/s)  LR: 3.712e-04  Data: 0.034 (0.044)
Train: 175 [ 150/1251 ( 12%)]  Loss: 4.229 (4.09)  Time: 0.174s, 5872.41/s  (0.201s, 5087.40/s)  LR: 3.712e-04  Data: 0.030 (0.039)
Train: 175 [ 200/1251 ( 16%)]  Loss: 4.395 (4.15)  Time: 0.251s, 4084.29/s  (0.199s, 5156.33/s)  LR: 3.712e-04  Data: 0.025 (0.037)
Train: 175 [ 250/1251 ( 20%)]  Loss: 4.282 (4.18)  Time: 0.148s, 6925.14/s  (0.195s, 5241.15/s)  LR: 3.712e-04  Data: 0.026 (0.035)
Train: 175 [ 300/1251 ( 24%)]  Loss: 4.186 (4.18)  Time: 0.179s, 5713.44/s  (0.194s, 5266.69/s)  LR: 3.712e-04  Data: 0.036 (0.034)
Train: 175 [ 350/1251 ( 28%)]  Loss: 4.303 (4.19)  Time: 0.163s, 6275.76/s  (0.194s, 5285.04/s)  LR: 3.712e-04  Data: 0.030 (0.033)
Train: 175 [ 400/1251 ( 32%)]  Loss: 4.116 (4.18)  Time: 0.177s, 5776.27/s  (0.194s, 5286.65/s)  LR: 3.712e-04  Data: 0.021 (0.033)
Train: 175 [ 450/1251 ( 36%)]  Loss: 4.236 (4.19)  Time: 0.182s, 5617.44/s  (0.194s, 5285.49/s)  LR: 3.712e-04  Data: 0.028 (0.032)
Train: 175 [ 500/1251 ( 40%)]  Loss: 4.356 (4.20)  Time: 0.241s, 4244.08/s  (0.193s, 5304.94/s)  LR: 3.712e-04  Data: 0.036 (0.032)
Train: 175 [ 550/1251 ( 44%)]  Loss: 4.060 (4.19)  Time: 0.162s, 6324.09/s  (0.193s, 5305.48/s)  LR: 3.712e-04  Data: 0.023 (0.031)
Train: 175 [ 600/1251 ( 48%)]  Loss: 4.293 (4.20)  Time: 0.193s, 5297.80/s  (0.193s, 5306.92/s)  LR: 3.712e-04  Data: 0.029 (0.032)
Train: 175 [ 650/1251 ( 52%)]  Loss: 4.153 (4.20)  Time: 0.175s, 5866.71/s  (0.193s, 5302.98/s)  LR: 3.712e-04  Data: 0.030 (0.033)
Train: 175 [ 700/1251 ( 56%)]  Loss: 4.266 (4.20)  Time: 0.512s, 1998.23/s  (0.193s, 5293.67/s)  LR: 3.712e-04  Data: 0.382 (0.034)
Train: 175 [ 750/1251 ( 60%)]  Loss: 4.381 (4.21)  Time: 0.152s, 6722.38/s  (0.193s, 5313.21/s)  LR: 3.712e-04  Data: 0.024 (0.034)
Train: 175 [ 800/1251 ( 64%)]  Loss: 4.007 (4.20)  Time: 0.161s, 6344.60/s  (0.192s, 5328.77/s)  LR: 3.712e-04  Data: 0.029 (0.035)
Train: 175 [ 850/1251 ( 68%)]  Loss: 4.214 (4.20)  Time: 0.175s, 5868.04/s  (0.193s, 5314.82/s)  LR: 3.712e-04  Data: 0.032 (0.036)
Train: 175 [ 900/1251 ( 72%)]  Loss: 4.232 (4.20)  Time: 0.417s, 2455.77/s  (0.193s, 5309.86/s)  LR: 3.712e-04  Data: 0.288 (0.036)
Train: 175 [ 950/1251 ( 76%)]  Loss: 4.266 (4.21)  Time: 0.168s, 6079.05/s  (0.193s, 5307.22/s)  LR: 3.712e-04  Data: 0.028 (0.037)
Train: 175 [1000/1251 ( 80%)]  Loss: 4.292 (4.21)  Time: 0.164s, 6239.88/s  (0.193s, 5311.86/s)  LR: 3.712e-04  Data: 0.029 (0.037)
Train: 175 [1050/1251 ( 84%)]  Loss: 3.901 (4.20)  Time: 0.210s, 4880.22/s  (0.193s, 5307.44/s)  LR: 3.712e-04  Data: 0.027 (0.037)
Train: 175 [1100/1251 ( 88%)]  Loss: 4.256 (4.20)  Time: 0.252s, 4057.90/s  (0.193s, 5305.95/s)  LR: 3.712e-04  Data: 0.127 (0.037)
Train: 175 [1150/1251 ( 92%)]  Loss: 4.088 (4.19)  Time: 0.169s, 6049.62/s  (0.193s, 5309.56/s)  LR: 3.712e-04  Data: 0.024 (0.037)
Train: 175 [1200/1251 ( 96%)]  Loss: 4.181 (4.19)  Time: 0.164s, 6243.44/s  (0.193s, 5301.86/s)  LR: 3.712e-04  Data: 0.026 (0.037)
Train: 175 [1250/1251 (100%)]  Loss: 4.298 (4.20)  Time: 0.114s, 8980.42/s  (0.193s, 5314.12/s)  LR: 3.712e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.812 (1.812)  Loss:  1.0883 (1.0883)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1089 (1.7257)  Acc@1: 81.6038 (66.7500)  Acc@5: 94.5755 (87.5000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-174.pth.tar', 66.01799997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-158.pth.tar', 65.87800013916015)

Train: 176 [   0/1251 (  0%)]  Loss: 4.449 (4.45)  Time: 1.885s,  543.23/s  (1.885s,  543.23/s)  LR: 3.662e-04  Data: 1.757 (1.757)
Train: 176 [  50/1251 (  4%)]  Loss: 3.985 (4.22)  Time: 0.181s, 5660.13/s  (0.222s, 4611.19/s)  LR: 3.662e-04  Data: 0.031 (0.078)
Train: 176 [ 100/1251 (  8%)]  Loss: 4.032 (4.16)  Time: 0.175s, 5844.77/s  (0.206s, 4958.95/s)  LR: 3.662e-04  Data: 0.022 (0.062)
Train: 176 [ 150/1251 ( 12%)]  Loss: 4.244 (4.18)  Time: 0.152s, 6749.55/s  (0.202s, 5075.13/s)  LR: 3.662e-04  Data: 0.029 (0.057)
Train: 176 [ 200/1251 ( 16%)]  Loss: 4.142 (4.17)  Time: 0.202s, 5061.20/s  (0.200s, 5127.31/s)  LR: 3.662e-04  Data: 0.025 (0.053)
Train: 176 [ 250/1251 ( 20%)]  Loss: 3.915 (4.13)  Time: 0.177s, 5797.14/s  (0.195s, 5243.62/s)  LR: 3.662e-04  Data: 0.020 (0.049)
Train: 176 [ 300/1251 ( 24%)]  Loss: 4.321 (4.16)  Time: 0.200s, 5128.51/s  (0.194s, 5269.32/s)  LR: 3.662e-04  Data: 0.027 (0.048)
Train: 176 [ 350/1251 ( 28%)]  Loss: 3.981 (4.13)  Time: 0.154s, 6635.50/s  (0.194s, 5274.49/s)  LR: 3.662e-04  Data: 0.025 (0.045)
Train: 176 [ 400/1251 ( 32%)]  Loss: 4.093 (4.13)  Time: 0.183s, 5594.85/s  (0.194s, 5285.72/s)  LR: 3.662e-04  Data: 0.031 (0.044)
Train: 176 [ 450/1251 ( 36%)]  Loss: 4.247 (4.14)  Time: 0.186s, 5507.03/s  (0.194s, 5271.88/s)  LR: 3.662e-04  Data: 0.023 (0.044)
Train: 176 [ 500/1251 ( 40%)]  Loss: 3.845 (4.11)  Time: 0.168s, 6083.50/s  (0.194s, 5285.95/s)  LR: 3.662e-04  Data: 0.019 (0.042)
Train: 176 [ 550/1251 ( 44%)]  Loss: 3.995 (4.10)  Time: 0.170s, 6018.51/s  (0.194s, 5284.72/s)  LR: 3.662e-04  Data: 0.029 (0.041)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 176 [ 600/1251 ( 48%)]  Loss: 3.986 (4.09)  Time: 0.174s, 5877.18/s  (0.193s, 5297.02/s)  LR: 3.662e-04  Data: 0.024 (0.040)
Train: 176 [ 650/1251 ( 52%)]  Loss: 3.854 (4.08)  Time: 0.167s, 6118.07/s  (0.193s, 5294.66/s)  LR: 3.662e-04  Data: 0.020 (0.039)
Train: 176 [ 700/1251 ( 56%)]  Loss: 4.111 (4.08)  Time: 0.186s, 5501.10/s  (0.194s, 5291.56/s)  LR: 3.662e-04  Data: 0.028 (0.038)
Train: 176 [ 750/1251 ( 60%)]  Loss: 3.604 (4.05)  Time: 0.311s, 3297.88/s  (0.194s, 5287.10/s)  LR: 3.662e-04  Data: 0.020 (0.038)
Train: 176 [ 800/1251 ( 64%)]  Loss: 3.837 (4.04)  Time: 0.174s, 5886.14/s  (0.194s, 5281.98/s)  LR: 3.662e-04  Data: 0.031 (0.037)
Train: 176 [ 850/1251 ( 68%)]  Loss: 4.487 (4.06)  Time: 0.167s, 6134.03/s  (0.194s, 5284.44/s)  LR: 3.662e-04  Data: 0.034 (0.036)
Train: 176 [ 900/1251 ( 72%)]  Loss: 3.664 (4.04)  Time: 0.166s, 6154.60/s  (0.194s, 5289.29/s)  LR: 3.662e-04  Data: 0.031 (0.036)
Train: 176 [ 950/1251 ( 76%)]  Loss: 4.194 (4.05)  Time: 0.370s, 2770.28/s  (0.194s, 5288.65/s)  LR: 3.662e-04  Data: 0.024 (0.036)
Train: 176 [1000/1251 ( 80%)]  Loss: 4.094 (4.05)  Time: 0.166s, 6183.18/s  (0.194s, 5289.63/s)  LR: 3.662e-04  Data: 0.035 (0.035)
Train: 176 [1050/1251 ( 84%)]  Loss: 3.773 (4.04)  Time: 0.168s, 6112.48/s  (0.194s, 5291.07/s)  LR: 3.662e-04  Data: 0.026 (0.035)
Train: 176 [1100/1251 ( 88%)]  Loss: 4.223 (4.05)  Time: 0.189s, 5417.59/s  (0.194s, 5286.21/s)  LR: 3.662e-04  Data: 0.028 (0.035)
Train: 176 [1150/1251 ( 92%)]  Loss: 4.578 (4.07)  Time: 0.299s, 3429.82/s  (0.194s, 5284.58/s)  LR: 3.662e-04  Data: 0.020 (0.034)
Train: 176 [1200/1251 ( 96%)]  Loss: 4.130 (4.07)  Time: 0.152s, 6718.01/s  (0.194s, 5284.71/s)  LR: 3.662e-04  Data: 0.028 (0.034)
Train: 176 [1250/1251 (100%)]  Loss: 3.931 (4.07)  Time: 0.114s, 9002.90/s  (0.193s, 5297.80/s)  LR: 3.662e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.912 (1.912)  Loss:  1.0128 (1.0128)  Acc@1: 82.0312 (82.0312)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.1296 (1.6771)  Acc@1: 78.8915 (66.5800)  Acc@5: 93.8679 (87.2680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-174.pth.tar', 66.01799997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-171.pth.tar', 65.92999987548828)

Train: 177 [   0/1251 (  0%)]  Loss: 4.537 (4.54)  Time: 1.817s,  563.53/s  (1.817s,  563.53/s)  LR: 3.611e-04  Data: 1.689 (1.689)
Train: 177 [  50/1251 (  4%)]  Loss: 4.292 (4.41)  Time: 0.165s, 6215.62/s  (0.220s, 4649.07/s)  LR: 3.611e-04  Data: 0.023 (0.077)
Train: 177 [ 100/1251 (  8%)]  Loss: 4.121 (4.32)  Time: 0.188s, 5453.48/s  (0.208s, 4926.73/s)  LR: 3.611e-04  Data: 0.027 (0.056)
Train: 177 [ 150/1251 ( 12%)]  Loss: 3.876 (4.21)  Time: 0.157s, 6516.21/s  (0.199s, 5147.37/s)  LR: 3.611e-04  Data: 0.026 (0.050)
Train: 177 [ 200/1251 ( 16%)]  Loss: 4.000 (4.17)  Time: 0.156s, 6565.87/s  (0.198s, 5174.37/s)  LR: 3.611e-04  Data: 0.028 (0.045)
Train: 177 [ 250/1251 ( 20%)]  Loss: 3.816 (4.11)  Time: 0.195s, 5262.20/s  (0.196s, 5218.33/s)  LR: 3.611e-04  Data: 0.029 (0.042)
Train: 177 [ 300/1251 ( 24%)]  Loss: 3.860 (4.07)  Time: 0.164s, 6243.68/s  (0.195s, 5250.96/s)  LR: 3.611e-04  Data: 0.024 (0.040)
Train: 177 [ 350/1251 ( 28%)]  Loss: 3.674 (4.02)  Time: 0.179s, 5705.46/s  (0.195s, 5256.84/s)  LR: 3.611e-04  Data: 0.027 (0.038)
Train: 177 [ 400/1251 ( 32%)]  Loss: 4.459 (4.07)  Time: 0.182s, 5632.71/s  (0.194s, 5288.42/s)  LR: 3.611e-04  Data: 0.034 (0.037)
Train: 177 [ 450/1251 ( 36%)]  Loss: 3.998 (4.06)  Time: 0.166s, 6185.61/s  (0.193s, 5308.61/s)  LR: 3.611e-04  Data: 0.022 (0.036)
Train: 177 [ 500/1251 ( 40%)]  Loss: 3.708 (4.03)  Time: 0.171s, 5994.87/s  (0.193s, 5307.67/s)  LR: 3.611e-04  Data: 0.032 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 177 [ 550/1251 ( 44%)]  Loss: 3.982 (4.03)  Time: 0.162s, 6310.69/s  (0.193s, 5306.21/s)  LR: 3.611e-04  Data: 0.020 (0.037)
Train: 177 [ 600/1251 ( 48%)]  Loss: 4.111 (4.03)  Time: 0.186s, 5509.39/s  (0.193s, 5306.70/s)  LR: 3.611e-04  Data: 0.027 (0.036)
Train: 177 [ 650/1251 ( 52%)]  Loss: 4.294 (4.05)  Time: 0.169s, 6063.55/s  (0.193s, 5306.31/s)  LR: 3.611e-04  Data: 0.030 (0.037)
Train: 177 [ 700/1251 ( 56%)]  Loss: 4.212 (4.06)  Time: 0.149s, 6891.71/s  (0.193s, 5316.20/s)  LR: 3.611e-04  Data: 0.025 (0.037)
Train: 177 [ 750/1251 ( 60%)]  Loss: 3.786 (4.05)  Time: 0.188s, 5451.62/s  (0.192s, 5321.27/s)  LR: 3.611e-04  Data: 0.020 (0.036)
Train: 177 [ 800/1251 ( 64%)]  Loss: 4.306 (4.06)  Time: 0.187s, 5471.96/s  (0.193s, 5314.17/s)  LR: 3.611e-04  Data: 0.020 (0.037)
Train: 177 [ 850/1251 ( 68%)]  Loss: 3.612 (4.04)  Time: 0.189s, 5405.92/s  (0.193s, 5317.73/s)  LR: 3.611e-04  Data: 0.035 (0.038)
Train: 177 [ 900/1251 ( 72%)]  Loss: 4.397 (4.05)  Time: 0.259s, 3958.26/s  (0.193s, 5312.73/s)  LR: 3.611e-04  Data: 0.141 (0.038)
Train: 177 [ 950/1251 ( 76%)]  Loss: 4.122 (4.06)  Time: 0.162s, 6307.63/s  (0.193s, 5313.78/s)  LR: 3.611e-04  Data: 0.021 (0.037)
Train: 177 [1000/1251 ( 80%)]  Loss: 4.092 (4.06)  Time: 0.230s, 4447.27/s  (0.193s, 5317.16/s)  LR: 3.611e-04  Data: 0.025 (0.037)
Train: 177 [1050/1251 ( 84%)]  Loss: 4.216 (4.07)  Time: 0.159s, 6422.73/s  (0.193s, 5309.24/s)  LR: 3.611e-04  Data: 0.025 (0.036)
Train: 177 [1100/1251 ( 88%)]  Loss: 4.354 (4.08)  Time: 0.166s, 6158.80/s  (0.193s, 5312.11/s)  LR: 3.611e-04  Data: 0.031 (0.036)
Train: 177 [1150/1251 ( 92%)]  Loss: 4.353 (4.09)  Time: 0.172s, 5956.53/s  (0.193s, 5304.04/s)  LR: 3.611e-04  Data: 0.020 (0.036)
Train: 177 [1200/1251 ( 96%)]  Loss: 4.039 (4.09)  Time: 0.156s, 6570.56/s  (0.193s, 5299.42/s)  LR: 3.611e-04  Data: 0.022 (0.035)
Train: 177 [1250/1251 (100%)]  Loss: 4.344 (4.10)  Time: 0.121s, 8486.33/s  (0.193s, 5308.24/s)  LR: 3.611e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.836 (1.836)  Loss:  1.1104 (1.1104)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1177 (1.7494)  Acc@1: 80.3066 (65.9220)  Acc@5: 93.7500 (87.0340)
Train: 178 [   0/1251 (  0%)]  Loss: 4.219 (4.22)  Time: 1.739s,  588.97/s  (1.739s,  588.97/s)  LR: 3.561e-04  Data: 1.609 (1.609)
Train: 178 [  50/1251 (  4%)]  Loss: 4.347 (4.28)  Time: 0.168s, 6113.14/s  (0.220s, 4655.77/s)  LR: 3.561e-04  Data: 0.025 (0.068)
Train: 178 [ 100/1251 (  8%)]  Loss: 4.402 (4.32)  Time: 0.183s, 5590.57/s  (0.206s, 4965.65/s)  LR: 3.561e-04  Data: 0.025 (0.048)
Train: 178 [ 150/1251 ( 12%)]  Loss: 3.990 (4.24)  Time: 0.180s, 5701.85/s  (0.202s, 5073.63/s)  LR: 3.561e-04  Data: 0.025 (0.042)
Train: 178 [ 200/1251 ( 16%)]  Loss: 3.840 (4.16)  Time: 0.178s, 5759.60/s  (0.199s, 5136.49/s)  LR: 3.561e-04  Data: 0.035 (0.038)
Train: 178 [ 250/1251 ( 20%)]  Loss: 3.799 (4.10)  Time: 0.179s, 5730.70/s  (0.197s, 5210.50/s)  LR: 3.561e-04  Data: 0.023 (0.036)
Train: 178 [ 300/1251 ( 24%)]  Loss: 4.304 (4.13)  Time: 0.163s, 6291.08/s  (0.195s, 5247.88/s)  LR: 3.561e-04  Data: 0.027 (0.035)
Train: 178 [ 350/1251 ( 28%)]  Loss: 3.664 (4.07)  Time: 0.158s, 6478.23/s  (0.194s, 5277.41/s)  LR: 3.561e-04  Data: 0.023 (0.036)
Train: 178 [ 400/1251 ( 32%)]  Loss: 4.477 (4.12)  Time: 0.301s, 3399.40/s  (0.194s, 5272.42/s)  LR: 3.561e-04  Data: 0.026 (0.035)
Train: 178 [ 450/1251 ( 36%)]  Loss: 4.342 (4.14)  Time: 0.173s, 5929.70/s  (0.193s, 5298.76/s)  LR: 3.561e-04  Data: 0.029 (0.035)
Train: 178 [ 500/1251 ( 40%)]  Loss: 4.309 (4.15)  Time: 0.177s, 5783.76/s  (0.193s, 5310.42/s)  LR: 3.561e-04  Data: 0.020 (0.035)
Train: 178 [ 550/1251 ( 44%)]  Loss: 4.202 (4.16)  Time: 0.181s, 5663.14/s  (0.193s, 5301.33/s)  LR: 3.561e-04  Data: 0.022 (0.036)
Train: 178 [ 600/1251 ( 48%)]  Loss: 4.163 (4.16)  Time: 0.161s, 6343.07/s  (0.193s, 5295.86/s)  LR: 3.561e-04  Data: 0.032 (0.037)
Train: 178 [ 650/1251 ( 52%)]  Loss: 4.565 (4.19)  Time: 0.148s, 6896.30/s  (0.193s, 5312.96/s)  LR: 3.561e-04  Data: 0.024 (0.037)
Train: 178 [ 700/1251 ( 56%)]  Loss: 3.787 (4.16)  Time: 0.177s, 5784.20/s  (0.193s, 5313.52/s)  LR: 3.561e-04  Data: 0.033 (0.038)
Train: 178 [ 750/1251 ( 60%)]  Loss: 4.226 (4.16)  Time: 0.194s, 5272.64/s  (0.192s, 5320.58/s)  LR: 3.561e-04  Data: 0.024 (0.038)
Train: 178 [ 800/1251 ( 64%)]  Loss: 4.699 (4.20)  Time: 0.162s, 6328.42/s  (0.193s, 5315.82/s)  LR: 3.561e-04  Data: 0.032 (0.038)
Train: 178 [ 850/1251 ( 68%)]  Loss: 4.095 (4.19)  Time: 0.164s, 6249.83/s  (0.193s, 5306.56/s)  LR: 3.561e-04  Data: 0.024 (0.039)
Train: 178 [ 900/1251 ( 72%)]  Loss: 4.144 (4.19)  Time: 0.181s, 5668.15/s  (0.193s, 5303.70/s)  LR: 3.561e-04  Data: 0.025 (0.040)
Train: 178 [ 950/1251 ( 76%)]  Loss: 4.032 (4.18)  Time: 0.172s, 5960.83/s  (0.193s, 5304.59/s)  LR: 3.561e-04  Data: 0.028 (0.040)
Train: 178 [1000/1251 ( 80%)]  Loss: 4.039 (4.17)  Time: 0.205s, 5001.03/s  (0.193s, 5295.94/s)  LR: 3.561e-04  Data: 0.025 (0.041)
Train: 178 [1050/1251 ( 84%)]  Loss: 4.124 (4.17)  Time: 0.171s, 5984.83/s  (0.193s, 5292.76/s)  LR: 3.561e-04  Data: 0.025 (0.041)
Train: 178 [1100/1251 ( 88%)]  Loss: 4.425 (4.18)  Time: 0.165s, 6206.02/s  (0.194s, 5291.63/s)  LR: 3.561e-04  Data: 0.033 (0.041)
Train: 178 [1150/1251 ( 92%)]  Loss: 3.829 (4.17)  Time: 0.186s, 5514.58/s  (0.193s, 5299.95/s)  LR: 3.561e-04  Data: 0.041 (0.041)
Train: 178 [1200/1251 ( 96%)]  Loss: 4.173 (4.17)  Time: 0.174s, 5879.22/s  (0.193s, 5299.21/s)  LR: 3.561e-04  Data: 0.031 (0.042)
Train: 178 [1250/1251 (100%)]  Loss: 4.047 (4.16)  Time: 0.114s, 9020.12/s  (0.193s, 5306.19/s)  LR: 3.561e-04  Data: 0.000 (0.042)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.730 (1.730)  Loss:  1.1258 (1.1258)  Acc@1: 81.4453 (81.4453)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1370 (1.7627)  Acc@1: 80.7783 (66.2700)  Acc@5: 94.3396 (87.2440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-174.pth.tar', 66.01799997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-164.pth.tar', 65.98599998046875)

Train: 179 [   0/1251 (  0%)]  Loss: 4.169 (4.17)  Time: 1.847s,  554.38/s  (1.847s,  554.38/s)  LR: 3.511e-04  Data: 1.701 (1.701)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 179 [  50/1251 (  4%)]  Loss: 4.529 (4.35)  Time: 0.156s, 6573.68/s  (0.221s, 4635.81/s)  LR: 3.511e-04  Data: 0.021 (0.075)
Train: 179 [ 100/1251 (  8%)]  Loss: 3.633 (4.11)  Time: 0.178s, 5748.31/s  (0.204s, 5014.50/s)  LR: 3.511e-04  Data: 0.020 (0.056)
Train: 179 [ 150/1251 ( 12%)]  Loss: 3.998 (4.08)  Time: 0.166s, 6182.94/s  (0.199s, 5143.81/s)  LR: 3.511e-04  Data: 0.025 (0.046)
Train: 179 [ 200/1251 ( 16%)]  Loss: 4.396 (4.15)  Time: 0.176s, 5824.77/s  (0.198s, 5159.85/s)  LR: 3.511e-04  Data: 0.029 (0.042)
Train: 179 [ 250/1251 ( 20%)]  Loss: 3.976 (4.12)  Time: 0.158s, 6488.89/s  (0.197s, 5185.06/s)  LR: 3.511e-04  Data: 0.032 (0.039)
Train: 179 [ 300/1251 ( 24%)]  Loss: 3.736 (4.06)  Time: 0.172s, 5953.90/s  (0.195s, 5258.43/s)  LR: 3.511e-04  Data: 0.025 (0.037)
Train: 179 [ 350/1251 ( 28%)]  Loss: 4.194 (4.08)  Time: 0.171s, 5998.16/s  (0.194s, 5285.59/s)  LR: 3.511e-04  Data: 0.028 (0.036)
Train: 179 [ 400/1251 ( 32%)]  Loss: 4.414 (4.12)  Time: 0.172s, 5957.26/s  (0.194s, 5280.50/s)  LR: 3.511e-04  Data: 0.027 (0.035)
Train: 179 [ 450/1251 ( 36%)]  Loss: 4.258 (4.13)  Time: 0.173s, 5924.33/s  (0.193s, 5296.33/s)  LR: 3.511e-04  Data: 0.033 (0.034)
Train: 179 [ 500/1251 ( 40%)]  Loss: 4.475 (4.16)  Time: 0.178s, 5754.67/s  (0.193s, 5294.66/s)  LR: 3.511e-04  Data: 0.029 (0.034)
Train: 179 [ 550/1251 ( 44%)]  Loss: 3.916 (4.14)  Time: 0.184s, 5577.25/s  (0.193s, 5311.46/s)  LR: 3.511e-04  Data: 0.020 (0.033)
Train: 179 [ 600/1251 ( 48%)]  Loss: 4.280 (4.15)  Time: 0.193s, 5312.25/s  (0.193s, 5298.90/s)  LR: 3.511e-04  Data: 0.022 (0.032)
Train: 179 [ 650/1251 ( 52%)]  Loss: 4.647 (4.19)  Time: 0.150s, 6809.82/s  (0.193s, 5304.78/s)  LR: 3.511e-04  Data: 0.024 (0.032)
Train: 179 [ 700/1251 ( 56%)]  Loss: 4.257 (4.19)  Time: 0.160s, 6395.68/s  (0.192s, 5319.81/s)  LR: 3.511e-04  Data: 0.024 (0.032)
Train: 179 [ 750/1251 ( 60%)]  Loss: 4.241 (4.20)  Time: 0.169s, 6058.32/s  (0.193s, 5310.07/s)  LR: 3.511e-04  Data: 0.034 (0.032)
Train: 179 [ 800/1251 ( 64%)]  Loss: 3.810 (4.17)  Time: 0.165s, 6200.69/s  (0.193s, 5310.69/s)  LR: 3.511e-04  Data: 0.028 (0.031)
Train: 179 [ 850/1251 ( 68%)]  Loss: 3.909 (4.16)  Time: 0.159s, 6424.87/s  (0.193s, 5317.47/s)  LR: 3.511e-04  Data: 0.021 (0.031)
Train: 179 [ 900/1251 ( 72%)]  Loss: 4.292 (4.16)  Time: 0.153s, 6702.67/s  (0.193s, 5314.15/s)  LR: 3.511e-04  Data: 0.032 (0.031)
Train: 179 [ 950/1251 ( 76%)]  Loss: 4.786 (4.20)  Time: 0.175s, 5858.92/s  (0.193s, 5318.58/s)  LR: 3.511e-04  Data: 0.032 (0.031)
Train: 179 [1000/1251 ( 80%)]  Loss: 4.122 (4.19)  Time: 0.171s, 5981.46/s  (0.193s, 5316.59/s)  LR: 3.511e-04  Data: 0.020 (0.031)
Train: 179 [1050/1251 ( 84%)]  Loss: 3.749 (4.17)  Time: 0.161s, 6360.42/s  (0.193s, 5311.50/s)  LR: 3.511e-04  Data: 0.027 (0.031)
Train: 179 [1100/1251 ( 88%)]  Loss: 4.001 (4.16)  Time: 0.151s, 6775.86/s  (0.193s, 5306.60/s)  LR: 3.511e-04  Data: 0.028 (0.031)
Train: 179 [1150/1251 ( 92%)]  Loss: 4.101 (4.16)  Time: 0.172s, 5963.34/s  (0.193s, 5302.28/s)  LR: 3.511e-04  Data: 0.030 (0.031)
Train: 179 [1200/1251 ( 96%)]  Loss: 3.976 (4.15)  Time: 0.185s, 5527.32/s  (0.193s, 5304.11/s)  LR: 3.511e-04  Data: 0.027 (0.030)
Train: 179 [1250/1251 (100%)]  Loss: 4.074 (4.15)  Time: 0.113s, 9062.95/s  (0.192s, 5321.91/s)  LR: 3.511e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.887 (1.887)  Loss:  1.0601 (1.0601)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1484 (1.7266)  Acc@1: 80.6604 (66.1580)  Acc@5: 93.2783 (87.0440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-179.pth.tar', 66.15800000244141)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-174.pth.tar', 66.01799997802735)

Train: 180 [   0/1251 (  0%)]  Loss: 3.752 (3.75)  Time: 1.804s,  567.61/s  (1.804s,  567.61/s)  LR: 3.461e-04  Data: 1.679 (1.679)
Train: 180 [  50/1251 (  4%)]  Loss: 4.072 (3.91)  Time: 0.205s, 5004.93/s  (0.229s, 4463.06/s)  LR: 3.461e-04  Data: 0.040 (0.080)
Train: 180 [ 100/1251 (  8%)]  Loss: 3.660 (3.83)  Time: 0.174s, 5899.66/s  (0.211s, 4861.30/s)  LR: 3.461e-04  Data: 0.032 (0.062)
Train: 180 [ 150/1251 ( 12%)]  Loss: 4.214 (3.92)  Time: 0.171s, 5994.89/s  (0.201s, 5086.25/s)  LR: 3.461e-04  Data: 0.025 (0.053)
Train: 180 [ 200/1251 ( 16%)]  Loss: 3.965 (3.93)  Time: 0.168s, 6112.30/s  (0.200s, 5128.65/s)  LR: 3.461e-04  Data: 0.034 (0.052)
Train: 180 [ 250/1251 ( 20%)]  Loss: 4.117 (3.96)  Time: 0.169s, 6072.69/s  (0.198s, 5175.04/s)  LR: 3.461e-04  Data: 0.021 (0.050)
Train: 180 [ 300/1251 ( 24%)]  Loss: 4.142 (3.99)  Time: 0.173s, 5909.67/s  (0.196s, 5234.79/s)  LR: 3.461e-04  Data: 0.026 (0.048)
Train: 180 [ 350/1251 ( 28%)]  Loss: 3.824 (3.97)  Time: 0.179s, 5731.51/s  (0.195s, 5252.12/s)  LR: 3.461e-04  Data: 0.020 (0.045)
Train: 180 [ 400/1251 ( 32%)]  Loss: 4.122 (3.99)  Time: 0.162s, 6336.84/s  (0.195s, 5260.13/s)  LR: 3.461e-04  Data: 0.039 (0.043)
Train: 180 [ 450/1251 ( 36%)]  Loss: 4.462 (4.03)  Time: 0.173s, 5917.58/s  (0.194s, 5269.03/s)  LR: 3.461e-04  Data: 0.024 (0.041)
Train: 180 [ 500/1251 ( 40%)]  Loss: 4.313 (4.06)  Time: 0.254s, 4029.21/s  (0.194s, 5280.92/s)  LR: 3.461e-04  Data: 0.025 (0.040)
Train: 180 [ 550/1251 ( 44%)]  Loss: 4.231 (4.07)  Time: 0.188s, 5451.16/s  (0.193s, 5293.30/s)  LR: 3.461e-04  Data: 0.036 (0.039)
Train: 180 [ 600/1251 ( 48%)]  Loss: 4.240 (4.09)  Time: 0.541s, 1893.57/s  (0.194s, 5281.70/s)  LR: 3.461e-04  Data: 0.030 (0.038)
Train: 180 [ 650/1251 ( 52%)]  Loss: 4.217 (4.10)  Time: 0.164s, 6248.04/s  (0.193s, 5295.87/s)  LR: 3.461e-04  Data: 0.021 (0.037)
Train: 180 [ 700/1251 ( 56%)]  Loss: 4.380 (4.11)  Time: 0.174s, 5897.53/s  (0.193s, 5297.66/s)  LR: 3.461e-04  Data: 0.025 (0.037)
Train: 180 [ 750/1251 ( 60%)]  Loss: 4.322 (4.13)  Time: 0.186s, 5511.71/s  (0.194s, 5286.61/s)  LR: 3.461e-04  Data: 0.026 (0.036)
Train: 180 [ 800/1251 ( 64%)]  Loss: 3.909 (4.11)  Time: 0.265s, 3867.60/s  (0.193s, 5294.26/s)  LR: 3.461e-04  Data: 0.024 (0.036)
Train: 180 [ 850/1251 ( 68%)]  Loss: 4.014 (4.11)  Time: 0.182s, 5633.40/s  (0.193s, 5293.86/s)  LR: 3.461e-04  Data: 0.027 (0.035)
Train: 180 [ 900/1251 ( 72%)]  Loss: 3.673 (4.09)  Time: 0.180s, 5704.25/s  (0.194s, 5287.73/s)  LR: 3.461e-04  Data: 0.031 (0.035)
Train: 180 [ 950/1251 ( 76%)]  Loss: 4.269 (4.10)  Time: 0.176s, 5814.66/s  (0.194s, 5290.16/s)  LR: 3.461e-04  Data: 0.028 (0.035)
Train: 180 [1000/1251 ( 80%)]  Loss: 4.004 (4.09)  Time: 0.360s, 2841.45/s  (0.194s, 5287.69/s)  LR: 3.461e-04  Data: 0.031 (0.034)
Train: 180 [1050/1251 ( 84%)]  Loss: 3.839 (4.08)  Time: 0.158s, 6466.93/s  (0.193s, 5295.18/s)  LR: 3.461e-04  Data: 0.028 (0.034)
Train: 180 [1100/1251 ( 88%)]  Loss: 4.058 (4.08)  Time: 0.161s, 6362.26/s  (0.194s, 5286.94/s)  LR: 3.461e-04  Data: 0.034 (0.034)
Train: 180 [1150/1251 ( 92%)]  Loss: 4.138 (4.08)  Time: 0.162s, 6315.17/s  (0.194s, 5286.95/s)  LR: 3.461e-04  Data: 0.025 (0.034)
Train: 180 [1200/1251 ( 96%)]  Loss: 4.070 (4.08)  Time: 0.275s, 3727.53/s  (0.194s, 5281.73/s)  LR: 3.461e-04  Data: 0.027 (0.034)
Train: 180 [1250/1251 (100%)]  Loss: 4.252 (4.09)  Time: 0.114s, 9016.47/s  (0.193s, 5294.25/s)  LR: 3.461e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.785 (1.785)  Loss:  1.0215 (1.0215)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1380 (1.6682)  Acc@1: 81.0142 (66.7300)  Acc@5: 94.4576 (87.2660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-179.pth.tar', 66.15800000244141)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-169.pth.tar', 66.1120000366211)

Train: 181 [   0/1251 (  0%)]  Loss: 4.001 (4.00)  Time: 1.693s,  604.98/s  (1.693s,  604.98/s)  LR: 3.412e-04  Data: 1.518 (1.518)
Train: 181 [  50/1251 (  4%)]  Loss: 4.064 (4.03)  Time: 0.172s, 5962.11/s  (0.225s, 4543.70/s)  LR: 3.412e-04  Data: 0.031 (0.068)
Train: 181 [ 100/1251 (  8%)]  Loss: 3.925 (4.00)  Time: 0.188s, 5452.20/s  (0.205s, 4994.55/s)  LR: 3.412e-04  Data: 0.026 (0.051)
Train: 181 [ 150/1251 ( 12%)]  Loss: 4.075 (4.02)  Time: 0.172s, 5945.16/s  (0.202s, 5079.63/s)  LR: 3.412e-04  Data: 0.028 (0.044)
Train: 181 [ 200/1251 ( 16%)]  Loss: 4.488 (4.11)  Time: 0.169s, 6046.42/s  (0.199s, 5154.96/s)  LR: 3.412e-04  Data: 0.036 (0.041)
Train: 181 [ 250/1251 ( 20%)]  Loss: 3.877 (4.07)  Time: 0.165s, 6222.21/s  (0.197s, 5194.61/s)  LR: 3.412e-04  Data: 0.028 (0.038)
Train: 181 [ 300/1251 ( 24%)]  Loss: 4.132 (4.08)  Time: 0.177s, 5799.34/s  (0.195s, 5244.91/s)  LR: 3.412e-04  Data: 0.030 (0.036)
Train: 181 [ 350/1251 ( 28%)]  Loss: 4.183 (4.09)  Time: 0.168s, 6091.71/s  (0.195s, 5258.73/s)  LR: 3.412e-04  Data: 0.023 (0.035)
Train: 181 [ 400/1251 ( 32%)]  Loss: 4.094 (4.09)  Time: 0.173s, 5921.37/s  (0.194s, 5286.15/s)  LR: 3.412e-04  Data: 0.026 (0.034)
Train: 181 [ 450/1251 ( 36%)]  Loss: 4.292 (4.11)  Time: 0.238s, 4303.29/s  (0.193s, 5295.87/s)  LR: 3.412e-04  Data: 0.023 (0.034)
Train: 181 [ 500/1251 ( 40%)]  Loss: 4.153 (4.12)  Time: 0.176s, 5810.61/s  (0.193s, 5297.27/s)  LR: 3.412e-04  Data: 0.039 (0.033)
Train: 181 [ 550/1251 ( 44%)]  Loss: 3.828 (4.09)  Time: 0.178s, 5749.68/s  (0.193s, 5302.30/s)  LR: 3.412e-04  Data: 0.035 (0.033)
Train: 181 [ 600/1251 ( 48%)]  Loss: 3.999 (4.09)  Time: 0.176s, 5818.66/s  (0.193s, 5296.87/s)  LR: 3.412e-04  Data: 0.026 (0.032)
Train: 181 [ 650/1251 ( 52%)]  Loss: 3.814 (4.07)  Time: 0.177s, 5799.89/s  (0.193s, 5306.52/s)  LR: 3.412e-04  Data: 0.044 (0.032)
Train: 181 [ 700/1251 ( 56%)]  Loss: 4.463 (4.09)  Time: 0.301s, 3404.28/s  (0.193s, 5310.42/s)  LR: 3.412e-04  Data: 0.027 (0.032)
Train: 181 [ 750/1251 ( 60%)]  Loss: 3.772 (4.07)  Time: 0.179s, 5717.02/s  (0.193s, 5311.56/s)  LR: 3.412e-04  Data: 0.028 (0.032)
Train: 181 [ 800/1251 ( 64%)]  Loss: 3.945 (4.06)  Time: 0.184s, 5553.30/s  (0.193s, 5297.15/s)  LR: 3.412e-04  Data: 0.055 (0.032)
Train: 181 [ 850/1251 ( 68%)]  Loss: 4.451 (4.09)  Time: 0.167s, 6129.39/s  (0.194s, 5291.72/s)  LR: 3.412e-04  Data: 0.028 (0.032)
Train: 181 [ 900/1251 ( 72%)]  Loss: 4.387 (4.10)  Time: 0.170s, 6014.37/s  (0.193s, 5298.48/s)  LR: 3.412e-04  Data: 0.031 (0.031)
Train: 181 [ 950/1251 ( 76%)]  Loss: 4.638 (4.13)  Time: 0.174s, 5894.85/s  (0.194s, 5289.91/s)  LR: 3.412e-04  Data: 0.021 (0.031)
Train: 181 [1000/1251 ( 80%)]  Loss: 3.987 (4.12)  Time: 0.187s, 5479.38/s  (0.194s, 5288.40/s)  LR: 3.412e-04  Data: 0.024 (0.031)
Train: 181 [1050/1251 ( 84%)]  Loss: 4.146 (4.12)  Time: 0.174s, 5874.68/s  (0.193s, 5300.68/s)  LR: 3.412e-04  Data: 0.043 (0.031)
Train: 181 [1100/1251 ( 88%)]  Loss: 4.448 (4.14)  Time: 0.177s, 5782.11/s  (0.193s, 5299.14/s)  LR: 3.412e-04  Data: 0.037 (0.031)
Train: 181 [1150/1251 ( 92%)]  Loss: 4.139 (4.14)  Time: 0.183s, 5601.55/s  (0.193s, 5292.37/s)  LR: 3.412e-04  Data: 0.024 (0.031)
Train: 181 [1200/1251 ( 96%)]  Loss: 4.311 (4.14)  Time: 0.194s, 5279.00/s  (0.194s, 5287.05/s)  LR: 3.412e-04  Data: 0.030 (0.031)
Train: 181 [1250/1251 (100%)]  Loss: 4.473 (4.16)  Time: 0.114s, 9008.26/s  (0.193s, 5297.69/s)  LR: 3.412e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.793 (1.793)  Loss:  1.0580 (1.0580)  Acc@1: 81.7383 (81.7383)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.1229 (1.6924)  Acc@1: 79.9528 (66.5880)  Acc@5: 93.9859 (87.4280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-179.pth.tar', 66.15800000244141)

Train: 182 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 1.651s,  620.38/s  (1.651s,  620.38/s)  LR: 3.362e-04  Data: 1.508 (1.508)
Train: 182 [  50/1251 (  4%)]  Loss: 4.168 (4.34)  Time: 0.165s, 6221.43/s  (0.232s, 4421.76/s)  LR: 3.362e-04  Data: 0.024 (0.065)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 182 [ 100/1251 (  8%)]  Loss: 4.149 (4.28)  Time: 0.184s, 5575.03/s  (0.208s, 4918.86/s)  LR: 3.362e-04  Data: 0.027 (0.047)
Train: 182 [ 150/1251 ( 12%)]  Loss: 4.257 (4.27)  Time: 0.162s, 6322.44/s  (0.201s, 5091.68/s)  LR: 3.362e-04  Data: 0.031 (0.041)
Train: 182 [ 200/1251 ( 16%)]  Loss: 3.986 (4.21)  Time: 0.174s, 5870.06/s  (0.198s, 5165.33/s)  LR: 3.362e-04  Data: 0.020 (0.038)
Train: 182 [ 250/1251 ( 20%)]  Loss: 4.129 (4.20)  Time: 0.182s, 5625.74/s  (0.195s, 5244.64/s)  LR: 3.362e-04  Data: 0.026 (0.036)
Train: 182 [ 300/1251 ( 24%)]  Loss: 4.799 (4.29)  Time: 0.196s, 5229.39/s  (0.196s, 5233.71/s)  LR: 3.362e-04  Data: 0.069 (0.035)
Train: 182 [ 350/1251 ( 28%)]  Loss: 3.838 (4.23)  Time: 0.164s, 6233.43/s  (0.195s, 5248.06/s)  LR: 3.362e-04  Data: 0.024 (0.034)
Train: 182 [ 400/1251 ( 32%)]  Loss: 4.417 (4.25)  Time: 0.162s, 6339.34/s  (0.194s, 5288.06/s)  LR: 3.362e-04  Data: 0.031 (0.033)
Train: 182 [ 450/1251 ( 36%)]  Loss: 4.442 (4.27)  Time: 0.166s, 6161.75/s  (0.194s, 5286.15/s)  LR: 3.362e-04  Data: 0.032 (0.033)
Train: 182 [ 500/1251 ( 40%)]  Loss: 4.195 (4.26)  Time: 0.159s, 6444.42/s  (0.193s, 5299.24/s)  LR: 3.362e-04  Data: 0.026 (0.032)
Train: 182 [ 550/1251 ( 44%)]  Loss: 4.313 (4.27)  Time: 0.160s, 6381.75/s  (0.193s, 5319.09/s)  LR: 3.362e-04  Data: 0.025 (0.032)
Train: 182 [ 600/1251 ( 48%)]  Loss: 3.739 (4.23)  Time: 0.403s, 2541.65/s  (0.193s, 5314.59/s)  LR: 3.362e-04  Data: 0.027 (0.032)
Train: 182 [ 650/1251 ( 52%)]  Loss: 4.372 (4.24)  Time: 0.171s, 5971.49/s  (0.193s, 5318.54/s)  LR: 3.362e-04  Data: 0.021 (0.032)
Train: 182 [ 700/1251 ( 56%)]  Loss: 4.224 (4.24)  Time: 0.158s, 6491.78/s  (0.192s, 5322.93/s)  LR: 3.362e-04  Data: 0.034 (0.031)
Train: 182 [ 750/1251 ( 60%)]  Loss: 4.429 (4.25)  Time: 0.168s, 6113.10/s  (0.193s, 5316.71/s)  LR: 3.362e-04  Data: 0.024 (0.031)
Train: 182 [ 800/1251 ( 64%)]  Loss: 3.696 (4.22)  Time: 0.240s, 4271.78/s  (0.192s, 5327.09/s)  LR: 3.362e-04  Data: 0.024 (0.031)
Train: 182 [ 850/1251 ( 68%)]  Loss: 4.197 (4.21)  Time: 0.157s, 6540.63/s  (0.192s, 5339.44/s)  LR: 3.362e-04  Data: 0.026 (0.031)
Train: 182 [ 900/1251 ( 72%)]  Loss: 4.494 (4.23)  Time: 0.177s, 5785.16/s  (0.192s, 5328.55/s)  LR: 3.362e-04  Data: 0.034 (0.031)
Train: 182 [ 950/1251 ( 76%)]  Loss: 4.327 (4.23)  Time: 0.179s, 5714.72/s  (0.192s, 5322.22/s)  LR: 3.362e-04  Data: 0.021 (0.031)
Train: 182 [1000/1251 ( 80%)]  Loss: 4.206 (4.23)  Time: 0.291s, 3523.49/s  (0.193s, 5313.61/s)  LR: 3.362e-04  Data: 0.030 (0.031)
Train: 182 [1050/1251 ( 84%)]  Loss: 4.172 (4.23)  Time: 0.171s, 5998.55/s  (0.193s, 5317.50/s)  LR: 3.362e-04  Data: 0.025 (0.031)
Train: 182 [1100/1251 ( 88%)]  Loss: 4.022 (4.22)  Time: 0.268s, 3826.58/s  (0.193s, 5313.50/s)  LR: 3.362e-04  Data: 0.034 (0.030)
Train: 182 [1150/1251 ( 92%)]  Loss: 3.986 (4.21)  Time: 0.189s, 5412.25/s  (0.193s, 5307.07/s)  LR: 3.362e-04  Data: 0.024 (0.030)
Train: 182 [1200/1251 ( 96%)]  Loss: 4.083 (4.21)  Time: 0.587s, 1743.38/s  (0.193s, 5295.49/s)  LR: 3.362e-04  Data: 0.024 (0.030)
Train: 182 [1250/1251 (100%)]  Loss: 3.867 (4.19)  Time: 0.113s, 9022.47/s  (0.193s, 5315.99/s)  LR: 3.362e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.818 (1.818)  Loss:  1.0912 (1.0912)  Acc@1: 82.4219 (82.4219)  Acc@5: 94.3359 (94.3359)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1066 (1.7073)  Acc@1: 79.9528 (66.3660)  Acc@5: 92.9245 (87.1680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-182.pth.tar', 66.36600008300782)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-165.pth.tar', 66.1860000024414)

Train: 183 [   0/1251 (  0%)]  Loss: 4.313 (4.31)  Time: 2.014s,  508.32/s  (2.014s,  508.32/s)  LR: 3.313e-04  Data: 1.743 (1.743)
Train: 183 [  50/1251 (  4%)]  Loss: 4.008 (4.16)  Time: 0.184s, 5553.21/s  (0.228s, 4499.56/s)  LR: 3.313e-04  Data: 0.027 (0.066)
Train: 183 [ 100/1251 (  8%)]  Loss: 4.177 (4.17)  Time: 0.170s, 6032.43/s  (0.207s, 4945.05/s)  LR: 3.313e-04  Data: 0.030 (0.052)
Train: 183 [ 150/1251 ( 12%)]  Loss: 4.342 (4.21)  Time: 0.178s, 5738.95/s  (0.201s, 5097.59/s)  LR: 3.313e-04  Data: 0.027 (0.049)
Train: 183 [ 200/1251 ( 16%)]  Loss: 4.009 (4.17)  Time: 0.166s, 6171.60/s  (0.199s, 5147.38/s)  LR: 3.313e-04  Data: 0.031 (0.050)
Train: 183 [ 250/1251 ( 20%)]  Loss: 3.928 (4.13)  Time: 0.254s, 4032.36/s  (0.197s, 5196.66/s)  LR: 3.313e-04  Data: 0.116 (0.047)
Train: 183 [ 300/1251 ( 24%)]  Loss: 4.442 (4.17)  Time: 0.175s, 5858.23/s  (0.196s, 5216.46/s)  LR: 3.313e-04  Data: 0.029 (0.045)
Train: 183 [ 350/1251 ( 28%)]  Loss: 4.337 (4.19)  Time: 0.170s, 6013.86/s  (0.195s, 5246.96/s)  LR: 3.313e-04  Data: 0.031 (0.043)
Train: 183 [ 400/1251 ( 32%)]  Loss: 3.922 (4.16)  Time: 0.184s, 5577.58/s  (0.195s, 5248.93/s)  LR: 3.313e-04  Data: 0.026 (0.041)
Train: 183 [ 450/1251 ( 36%)]  Loss: 3.692 (4.12)  Time: 0.176s, 5829.95/s  (0.194s, 5265.38/s)  LR: 3.313e-04  Data: 0.032 (0.039)
Train: 183 [ 500/1251 ( 40%)]  Loss: 4.155 (4.12)  Time: 0.176s, 5801.70/s  (0.193s, 5292.43/s)  LR: 3.313e-04  Data: 0.020 (0.038)
Train: 183 [ 550/1251 ( 44%)]  Loss: 3.659 (4.08)  Time: 0.163s, 6264.65/s  (0.193s, 5307.67/s)  LR: 3.313e-04  Data: 0.027 (0.038)
Train: 183 [ 600/1251 ( 48%)]  Loss: 4.481 (4.11)  Time: 0.264s, 3883.32/s  (0.193s, 5302.77/s)  LR: 3.313e-04  Data: 0.031 (0.038)
Train: 183 [ 650/1251 ( 52%)]  Loss: 3.855 (4.09)  Time: 0.181s, 5673.11/s  (0.193s, 5311.15/s)  LR: 3.313e-04  Data: 0.025 (0.037)
Train: 183 [ 700/1251 ( 56%)]  Loss: 4.108 (4.10)  Time: 0.162s, 6338.55/s  (0.193s, 5311.98/s)  LR: 3.313e-04  Data: 0.032 (0.037)
Train: 183 [ 750/1251 ( 60%)]  Loss: 3.876 (4.08)  Time: 0.149s, 6881.22/s  (0.193s, 5303.98/s)  LR: 3.313e-04  Data: 0.030 (0.036)
Train: 183 [ 800/1251 ( 64%)]  Loss: 4.001 (4.08)  Time: 0.183s, 5587.09/s  (0.193s, 5312.67/s)  LR: 3.313e-04  Data: 0.029 (0.036)
Train: 183 [ 850/1251 ( 68%)]  Loss: 4.263 (4.09)  Time: 0.172s, 5966.45/s  (0.193s, 5306.92/s)  LR: 3.313e-04  Data: 0.021 (0.035)
Train: 183 [ 900/1251 ( 72%)]  Loss: 4.477 (4.11)  Time: 0.195s, 5262.81/s  (0.193s, 5302.82/s)  LR: 3.313e-04  Data: 0.026 (0.035)
Train: 183 [ 950/1251 ( 76%)]  Loss: 3.733 (4.09)  Time: 0.191s, 5347.94/s  (0.193s, 5295.18/s)  LR: 3.313e-04  Data: 0.025 (0.034)
Train: 183 [1000/1251 ( 80%)]  Loss: 4.207 (4.09)  Time: 0.153s, 6698.07/s  (0.193s, 5304.87/s)  LR: 3.313e-04  Data: 0.025 (0.034)
Train: 183 [1050/1251 ( 84%)]  Loss: 3.990 (4.09)  Time: 0.184s, 5573.47/s  (0.193s, 5309.51/s)  LR: 3.313e-04  Data: 0.022 (0.034)
Train: 183 [1100/1251 ( 88%)]  Loss: 3.939 (4.08)  Time: 0.159s, 6434.18/s  (0.193s, 5293.66/s)  LR: 3.313e-04  Data: 0.035 (0.033)
Train: 183 [1150/1251 ( 92%)]  Loss: 3.969 (4.08)  Time: 0.155s, 6590.62/s  (0.194s, 5289.73/s)  LR: 3.313e-04  Data: 0.030 (0.033)
Train: 183 [1200/1251 ( 96%)]  Loss: 4.420 (4.09)  Time: 0.177s, 5777.45/s  (0.193s, 5293.52/s)  LR: 3.313e-04  Data: 0.037 (0.033)
Train: 183 [1250/1251 (100%)]  Loss: 3.950 (4.09)  Time: 0.114s, 8969.09/s  (0.193s, 5312.32/s)  LR: 3.313e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.809 (1.809)  Loss:  1.0204 (1.0204)  Acc@1: 82.2266 (82.2266)  Acc@5: 95.2148 (95.2148)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.1755 (1.6729)  Acc@1: 80.3066 (66.6440)  Acc@5: 93.8679 (87.5280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-182.pth.tar', 66.36600008300782)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-173.pth.tar', 66.192)

Train: 184 [   0/1251 (  0%)]  Loss: 3.982 (3.98)  Time: 1.714s,  597.29/s  (1.714s,  597.29/s)  LR: 3.264e-04  Data: 1.597 (1.597)
Train: 184 [  50/1251 (  4%)]  Loss: 3.715 (3.85)  Time: 0.168s, 6105.72/s  (0.222s, 4607.91/s)  LR: 3.264e-04  Data: 0.033 (0.064)
Train: 184 [ 100/1251 (  8%)]  Loss: 4.203 (3.97)  Time: 0.158s, 6469.82/s  (0.208s, 4920.20/s)  LR: 3.264e-04  Data: 0.022 (0.053)
Train: 184 [ 150/1251 ( 12%)]  Loss: 4.104 (4.00)  Time: 0.184s, 5551.00/s  (0.202s, 5079.78/s)  LR: 3.264e-04  Data: 0.027 (0.047)
Train: 184 [ 200/1251 ( 16%)]  Loss: 3.961 (3.99)  Time: 0.176s, 5810.72/s  (0.197s, 5204.93/s)  LR: 3.264e-04  Data: 0.021 (0.045)
Train: 184 [ 250/1251 ( 20%)]  Loss: 3.827 (3.97)  Time: 0.172s, 5961.61/s  (0.197s, 5187.96/s)  LR: 3.264e-04  Data: 0.030 (0.043)
Train: 184 [ 300/1251 ( 24%)]  Loss: 3.985 (3.97)  Time: 0.170s, 6028.62/s  (0.195s, 5238.54/s)  LR: 3.264e-04  Data: 0.026 (0.040)
Train: 184 [ 350/1251 ( 28%)]  Loss: 4.057 (3.98)  Time: 0.171s, 6005.00/s  (0.194s, 5273.01/s)  LR: 3.264e-04  Data: 0.034 (0.039)
Train: 184 [ 400/1251 ( 32%)]  Loss: 4.443 (4.03)  Time: 0.184s, 5565.73/s  (0.195s, 5263.17/s)  LR: 3.264e-04  Data: 0.019 (0.037)
Train: 184 [ 450/1251 ( 36%)]  Loss: 4.085 (4.04)  Time: 0.179s, 5706.77/s  (0.194s, 5281.70/s)  LR: 3.264e-04  Data: 0.027 (0.036)
Train: 184 [ 500/1251 ( 40%)]  Loss: 4.014 (4.03)  Time: 0.174s, 5901.70/s  (0.193s, 5294.69/s)  LR: 3.264e-04  Data: 0.026 (0.036)
Train: 184 [ 550/1251 ( 44%)]  Loss: 4.040 (4.03)  Time: 0.165s, 6221.43/s  (0.193s, 5295.27/s)  LR: 3.264e-04  Data: 0.026 (0.035)
Train: 184 [ 600/1251 ( 48%)]  Loss: 4.175 (4.05)  Time: 0.188s, 5438.59/s  (0.193s, 5295.60/s)  LR: 3.264e-04  Data: 0.029 (0.034)
Train: 184 [ 650/1251 ( 52%)]  Loss: 3.990 (4.04)  Time: 0.161s, 6344.47/s  (0.193s, 5300.05/s)  LR: 3.264e-04  Data: 0.026 (0.034)
Train: 184 [ 700/1251 ( 56%)]  Loss: 4.039 (4.04)  Time: 0.482s, 2126.33/s  (0.193s, 5295.98/s)  LR: 3.264e-04  Data: 0.020 (0.033)
Train: 184 [ 750/1251 ( 60%)]  Loss: 3.933 (4.03)  Time: 0.165s, 6205.05/s  (0.193s, 5301.10/s)  LR: 3.264e-04  Data: 0.027 (0.033)
Train: 184 [ 800/1251 ( 64%)]  Loss: 4.023 (4.03)  Time: 0.175s, 5839.98/s  (0.193s, 5298.36/s)  LR: 3.264e-04  Data: 0.024 (0.033)
Train: 184 [ 850/1251 ( 68%)]  Loss: 4.145 (4.04)  Time: 0.282s, 3630.97/s  (0.193s, 5295.29/s)  LR: 3.264e-04  Data: 0.027 (0.032)
Train: 184 [ 900/1251 ( 72%)]  Loss: 4.031 (4.04)  Time: 0.192s, 5344.33/s  (0.194s, 5287.07/s)  LR: 3.264e-04  Data: 0.021 (0.032)
Train: 184 [ 950/1251 ( 76%)]  Loss: 4.171 (4.05)  Time: 0.170s, 6022.55/s  (0.194s, 5288.29/s)  LR: 3.264e-04  Data: 0.031 (0.032)
Train: 184 [1000/1251 ( 80%)]  Loss: 3.867 (4.04)  Time: 0.163s, 6270.13/s  (0.194s, 5286.59/s)  LR: 3.264e-04  Data: 0.026 (0.032)
Train: 184 [1050/1251 ( 84%)]  Loss: 4.124 (4.04)  Time: 0.161s, 6356.63/s  (0.193s, 5296.11/s)  LR: 3.264e-04  Data: 0.025 (0.032)
Train: 184 [1100/1251 ( 88%)]  Loss: 3.594 (4.02)  Time: 0.159s, 6445.40/s  (0.194s, 5281.69/s)  LR: 3.264e-04  Data: 0.025 (0.032)
Train: 184 [1150/1251 ( 92%)]  Loss: 4.562 (4.04)  Time: 0.188s, 5460.90/s  (0.194s, 5278.78/s)  LR: 3.264e-04  Data: 0.027 (0.031)
Train: 184 [1200/1251 ( 96%)]  Loss: 4.061 (4.05)  Time: 0.153s, 6678.35/s  (0.194s, 5284.54/s)  LR: 3.264e-04  Data: 0.020 (0.031)
Train: 184 [1250/1251 (100%)]  Loss: 4.276 (4.05)  Time: 0.114s, 8982.37/s  (0.193s, 5296.64/s)  LR: 3.264e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.853 (1.853)  Loss:  1.0181 (1.0181)  Acc@1: 83.5938 (83.5938)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1102 (1.6934)  Acc@1: 81.6038 (67.0700)  Acc@5: 94.9292 (87.5500)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-182.pth.tar', 66.36600008300782)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-178.pth.tar', 66.27000005371094)

Train: 185 [   0/1251 (  0%)]  Loss: 4.273 (4.27)  Time: 1.929s,  530.78/s  (1.929s,  530.78/s)  LR: 3.215e-04  Data: 1.797 (1.797)
Train: 185 [  50/1251 (  4%)]  Loss: 3.705 (3.99)  Time: 0.175s, 5848.90/s  (0.227s, 4516.48/s)  LR: 3.215e-04  Data: 0.025 (0.063)
Train: 185 [ 100/1251 (  8%)]  Loss: 3.985 (3.99)  Time: 0.162s, 6325.75/s  (0.208s, 4917.48/s)  LR: 3.215e-04  Data: 0.028 (0.046)
Train: 185 [ 150/1251 ( 12%)]  Loss: 4.302 (4.07)  Time: 0.175s, 5840.08/s  (0.203s, 5045.41/s)  LR: 3.215e-04  Data: 0.029 (0.040)
Train: 185 [ 200/1251 ( 16%)]  Loss: 4.102 (4.07)  Time: 0.176s, 5815.18/s  (0.200s, 5121.59/s)  LR: 3.215e-04  Data: 0.034 (0.037)
Train: 185 [ 250/1251 ( 20%)]  Loss: 3.797 (4.03)  Time: 0.175s, 5850.48/s  (0.197s, 5204.81/s)  LR: 3.215e-04  Data: 0.024 (0.036)
Train: 185 [ 300/1251 ( 24%)]  Loss: 4.136 (4.04)  Time: 0.167s, 6142.56/s  (0.195s, 5248.97/s)  LR: 3.215e-04  Data: 0.024 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 185 [ 350/1251 ( 28%)]  Loss: 4.098 (4.05)  Time: 0.200s, 5121.14/s  (0.194s, 5283.25/s)  LR: 3.215e-04  Data: 0.025 (0.033)
Train: 185 [ 400/1251 ( 32%)]  Loss: 4.330 (4.08)  Time: 0.173s, 5919.72/s  (0.194s, 5289.79/s)  LR: 3.215e-04  Data: 0.027 (0.033)
Train: 185 [ 450/1251 ( 36%)]  Loss: 4.274 (4.10)  Time: 0.209s, 4900.84/s  (0.193s, 5304.52/s)  LR: 3.215e-04  Data: 0.029 (0.032)
Train: 185 [ 500/1251 ( 40%)]  Loss: 3.864 (4.08)  Time: 0.155s, 6612.30/s  (0.193s, 5309.53/s)  LR: 3.215e-04  Data: 0.022 (0.032)
Train: 185 [ 550/1251 ( 44%)]  Loss: 4.042 (4.08)  Time: 0.161s, 6342.60/s  (0.193s, 5316.95/s)  LR: 3.215e-04  Data: 0.024 (0.031)
Train: 185 [ 600/1251 ( 48%)]  Loss: 4.134 (4.08)  Time: 0.164s, 6258.06/s  (0.193s, 5309.41/s)  LR: 3.215e-04  Data: 0.026 (0.031)
Train: 185 [ 650/1251 ( 52%)]  Loss: 4.280 (4.09)  Time: 0.166s, 6171.52/s  (0.193s, 5318.66/s)  LR: 3.215e-04  Data: 0.030 (0.031)
Train: 185 [ 700/1251 ( 56%)]  Loss: 4.179 (4.10)  Time: 0.178s, 5762.32/s  (0.192s, 5333.78/s)  LR: 3.215e-04  Data: 0.025 (0.030)
Train: 185 [ 750/1251 ( 60%)]  Loss: 4.107 (4.10)  Time: 0.174s, 5868.90/s  (0.192s, 5324.35/s)  LR: 3.215e-04  Data: 0.026 (0.030)
Train: 185 [ 800/1251 ( 64%)]  Loss: 4.073 (4.10)  Time: 0.167s, 6138.69/s  (0.192s, 5324.26/s)  LR: 3.215e-04  Data: 0.032 (0.030)
Train: 185 [ 850/1251 ( 68%)]  Loss: 4.128 (4.10)  Time: 0.173s, 5905.02/s  (0.192s, 5328.31/s)  LR: 3.215e-04  Data: 0.037 (0.030)
Train: 185 [ 900/1251 ( 72%)]  Loss: 4.038 (4.10)  Time: 0.159s, 6438.12/s  (0.192s, 5322.13/s)  LR: 3.215e-04  Data: 0.025 (0.030)
Train: 185 [ 950/1251 ( 76%)]  Loss: 3.934 (4.09)  Time: 0.173s, 5935.97/s  (0.193s, 5313.92/s)  LR: 3.215e-04  Data: 0.032 (0.030)
Train: 185 [1000/1251 ( 80%)]  Loss: 4.190 (4.09)  Time: 0.151s, 6760.42/s  (0.193s, 5312.25/s)  LR: 3.215e-04  Data: 0.030 (0.030)
Train: 185 [1050/1251 ( 84%)]  Loss: 4.068 (4.09)  Time: 0.158s, 6466.20/s  (0.193s, 5313.62/s)  LR: 3.215e-04  Data: 0.030 (0.030)
Train: 185 [1100/1251 ( 88%)]  Loss: 4.209 (4.10)  Time: 0.187s, 5472.68/s  (0.193s, 5309.54/s)  LR: 3.215e-04  Data: 0.026 (0.030)
Train: 185 [1150/1251 ( 92%)]  Loss: 4.167 (4.10)  Time: 0.177s, 5798.27/s  (0.193s, 5307.20/s)  LR: 3.215e-04  Data: 0.023 (0.029)
Train: 185 [1200/1251 ( 96%)]  Loss: 4.304 (4.11)  Time: 0.161s, 6356.70/s  (0.193s, 5302.43/s)  LR: 3.215e-04  Data: 0.022 (0.029)
Train: 185 [1250/1251 (100%)]  Loss: 4.102 (4.11)  Time: 0.113s, 9098.45/s  (0.193s, 5318.48/s)  LR: 3.215e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.844 (1.844)  Loss:  1.0534 (1.0534)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.1642 (1.7251)  Acc@1: 81.8396 (66.8300)  Acc@5: 93.3962 (87.4980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-182.pth.tar', 66.36600008300782)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-168.pth.tar', 66.321999921875)

Train: 186 [   0/1251 (  0%)]  Loss: 4.010 (4.01)  Time: 1.828s,  560.32/s  (1.828s,  560.32/s)  LR: 3.166e-04  Data: 1.588 (1.588)
Train: 186 [  50/1251 (  4%)]  Loss: 4.412 (4.21)  Time: 0.167s, 6119.36/s  (0.224s, 4576.83/s)  LR: 3.166e-04  Data: 0.030 (0.062)
Train: 186 [ 100/1251 (  8%)]  Loss: 4.323 (4.25)  Time: 0.173s, 5903.35/s  (0.204s, 5010.18/s)  LR: 3.166e-04  Data: 0.028 (0.047)
Train: 186 [ 150/1251 ( 12%)]  Loss: 4.070 (4.20)  Time: 0.161s, 6365.19/s  (0.201s, 5090.48/s)  LR: 3.166e-04  Data: 0.028 (0.044)
Train: 186 [ 200/1251 ( 16%)]  Loss: 4.096 (4.18)  Time: 0.163s, 6279.57/s  (0.197s, 5208.85/s)  LR: 3.166e-04  Data: 0.026 (0.040)
Train: 186 [ 250/1251 ( 20%)]  Loss: 4.195 (4.18)  Time: 0.185s, 5545.43/s  (0.196s, 5235.95/s)  LR: 3.166e-04  Data: 0.032 (0.038)
Train: 186 [ 300/1251 ( 24%)]  Loss: 4.399 (4.21)  Time: 0.181s, 5646.35/s  (0.195s, 5254.39/s)  LR: 3.166e-04  Data: 0.024 (0.036)
Train: 186 [ 350/1251 ( 28%)]  Loss: 4.231 (4.22)  Time: 0.188s, 5458.04/s  (0.194s, 5282.35/s)  LR: 3.166e-04  Data: 0.026 (0.035)
Train: 186 [ 400/1251 ( 32%)]  Loss: 4.150 (4.21)  Time: 0.187s, 5480.72/s  (0.194s, 5279.41/s)  LR: 3.166e-04  Data: 0.030 (0.034)
Train: 186 [ 450/1251 ( 36%)]  Loss: 4.280 (4.22)  Time: 0.193s, 5312.17/s  (0.192s, 5321.31/s)  LR: 3.166e-04  Data: 0.028 (0.033)
Train: 186 [ 500/1251 ( 40%)]  Loss: 4.351 (4.23)  Time: 0.167s, 6118.98/s  (0.193s, 5313.29/s)  LR: 3.166e-04  Data: 0.024 (0.033)
Train: 186 [ 550/1251 ( 44%)]  Loss: 3.878 (4.20)  Time: 0.180s, 5683.26/s  (0.192s, 5319.86/s)  LR: 3.166e-04  Data: 0.025 (0.033)
Train: 186 [ 600/1251 ( 48%)]  Loss: 4.350 (4.21)  Time: 0.171s, 5993.59/s  (0.193s, 5316.80/s)  LR: 3.166e-04  Data: 0.025 (0.034)
Train: 186 [ 650/1251 ( 52%)]  Loss: 4.084 (4.20)  Time: 0.165s, 6192.74/s  (0.194s, 5286.78/s)  LR: 3.166e-04  Data: 0.022 (0.034)
Train: 186 [ 700/1251 ( 56%)]  Loss: 4.571 (4.23)  Time: 0.188s, 5440.67/s  (0.193s, 5301.25/s)  LR: 3.166e-04  Data: 0.022 (0.033)
Train: 186 [ 750/1251 ( 60%)]  Loss: 3.969 (4.21)  Time: 0.178s, 5753.46/s  (0.193s, 5311.23/s)  LR: 3.166e-04  Data: 0.031 (0.033)
Train: 186 [ 800/1251 ( 64%)]  Loss: 4.141 (4.21)  Time: 0.177s, 5790.58/s  (0.193s, 5304.32/s)  LR: 3.166e-04  Data: 0.024 (0.033)
Train: 186 [ 850/1251 ( 68%)]  Loss: 4.104 (4.20)  Time: 0.166s, 6154.66/s  (0.193s, 5304.65/s)  LR: 3.166e-04  Data: 0.031 (0.033)
Train: 186 [ 900/1251 ( 72%)]  Loss: 3.966 (4.19)  Time: 0.193s, 5301.62/s  (0.193s, 5307.59/s)  LR: 3.166e-04  Data: 0.039 (0.032)
Train: 186 [ 950/1251 ( 76%)]  Loss: 4.051 (4.18)  Time: 0.176s, 5806.69/s  (0.193s, 5314.14/s)  LR: 3.166e-04  Data: 0.030 (0.032)
Train: 186 [1000/1251 ( 80%)]  Loss: 4.562 (4.20)  Time: 0.196s, 5220.92/s  (0.193s, 5310.32/s)  LR: 3.166e-04  Data: 0.023 (0.032)
Train: 186 [1050/1251 ( 84%)]  Loss: 3.855 (4.18)  Time: 0.162s, 6307.50/s  (0.193s, 5312.44/s)  LR: 3.166e-04  Data: 0.023 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 186 [1100/1251 ( 88%)]  Loss: 4.397 (4.19)  Time: 0.349s, 2937.93/s  (0.193s, 5307.92/s)  LR: 3.166e-04  Data: 0.032 (0.032)
Train: 186 [1150/1251 ( 92%)]  Loss: 3.966 (4.18)  Time: 0.178s, 5761.74/s  (0.193s, 5307.80/s)  LR: 3.166e-04  Data: 0.025 (0.031)
Train: 186 [1200/1251 ( 96%)]  Loss: 3.709 (4.16)  Time: 0.187s, 5483.32/s  (0.193s, 5302.76/s)  LR: 3.166e-04  Data: 0.027 (0.031)
Train: 186 [1250/1251 (100%)]  Loss: 4.512 (4.18)  Time: 0.114s, 9013.78/s  (0.193s, 5310.62/s)  LR: 3.166e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.745 (1.745)  Loss:  1.0375 (1.0375)  Acc@1: 82.9102 (82.9102)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0789 (1.6595)  Acc@1: 81.9576 (66.8540)  Acc@5: 95.0472 (87.5760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-182.pth.tar', 66.36600008300782)

Train: 187 [   0/1251 (  0%)]  Loss: 4.111 (4.11)  Time: 1.888s,  542.28/s  (1.888s,  542.28/s)  LR: 3.118e-04  Data: 1.761 (1.761)
Train: 187 [  50/1251 (  4%)]  Loss: 4.206 (4.16)  Time: 0.164s, 6232.90/s  (0.222s, 4605.64/s)  LR: 3.118e-04  Data: 0.022 (0.078)
Train: 187 [ 100/1251 (  8%)]  Loss: 4.212 (4.18)  Time: 0.187s, 5463.79/s  (0.208s, 4924.55/s)  LR: 3.118e-04  Data: 0.031 (0.064)
Train: 187 [ 150/1251 ( 12%)]  Loss: 4.162 (4.17)  Time: 0.156s, 6581.42/s  (0.200s, 5111.50/s)  LR: 3.118e-04  Data: 0.031 (0.055)
Train: 187 [ 200/1251 ( 16%)]  Loss: 4.337 (4.21)  Time: 0.206s, 4981.42/s  (0.199s, 5155.58/s)  LR: 3.118e-04  Data: 0.064 (0.053)
Train: 187 [ 250/1251 ( 20%)]  Loss: 3.996 (4.17)  Time: 0.187s, 5487.72/s  (0.197s, 5190.58/s)  LR: 3.118e-04  Data: 0.027 (0.048)
Train: 187 [ 300/1251 ( 24%)]  Loss: 4.274 (4.19)  Time: 0.156s, 6579.40/s  (0.195s, 5241.08/s)  LR: 3.118e-04  Data: 0.023 (0.045)
Train: 187 [ 350/1251 ( 28%)]  Loss: 4.128 (4.18)  Time: 0.167s, 6149.40/s  (0.194s, 5277.92/s)  LR: 3.118e-04  Data: 0.029 (0.042)
Train: 187 [ 400/1251 ( 32%)]  Loss: 3.963 (4.15)  Time: 0.159s, 6436.43/s  (0.194s, 5275.18/s)  LR: 3.118e-04  Data: 0.028 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 187 [ 450/1251 ( 36%)]  Loss: 3.503 (4.09)  Time: 0.178s, 5763.36/s  (0.193s, 5305.78/s)  LR: 3.118e-04  Data: 0.036 (0.039)
Train: 187 [ 500/1251 ( 40%)]  Loss: 4.266 (4.11)  Time: 0.176s, 5809.86/s  (0.193s, 5315.09/s)  LR: 3.118e-04  Data: 0.029 (0.038)
Train: 187 [ 550/1251 ( 44%)]  Loss: 3.954 (4.09)  Time: 0.175s, 5855.99/s  (0.193s, 5305.59/s)  LR: 3.118e-04  Data: 0.028 (0.037)
Train: 187 [ 600/1251 ( 48%)]  Loss: 4.119 (4.09)  Time: 0.155s, 6627.78/s  (0.194s, 5285.56/s)  LR: 3.118e-04  Data: 0.023 (0.036)
Train: 187 [ 650/1251 ( 52%)]  Loss: 4.563 (4.13)  Time: 0.162s, 6310.26/s  (0.193s, 5299.20/s)  LR: 3.118e-04  Data: 0.029 (0.036)
Train: 187 [ 700/1251 ( 56%)]  Loss: 4.227 (4.13)  Time: 0.179s, 5734.77/s  (0.193s, 5305.13/s)  LR: 3.118e-04  Data: 0.030 (0.035)
Train: 187 [ 750/1251 ( 60%)]  Loss: 4.327 (4.15)  Time: 0.187s, 5471.71/s  (0.193s, 5296.48/s)  LR: 3.118e-04  Data: 0.032 (0.035)
Train: 187 [ 800/1251 ( 64%)]  Loss: 4.454 (4.16)  Time: 0.177s, 5794.11/s  (0.193s, 5305.89/s)  LR: 3.118e-04  Data: 0.027 (0.034)
Train: 187 [ 850/1251 ( 68%)]  Loss: 4.072 (4.16)  Time: 0.170s, 6006.82/s  (0.193s, 5301.72/s)  LR: 3.118e-04  Data: 0.026 (0.034)
Train: 187 [ 900/1251 ( 72%)]  Loss: 4.211 (4.16)  Time: 0.167s, 6140.78/s  (0.193s, 5301.05/s)  LR: 3.118e-04  Data: 0.025 (0.034)
Train: 187 [ 950/1251 ( 76%)]  Loss: 3.660 (4.14)  Time: 0.172s, 5937.05/s  (0.193s, 5308.40/s)  LR: 3.118e-04  Data: 0.034 (0.034)
Train: 187 [1000/1251 ( 80%)]  Loss: 4.449 (4.15)  Time: 0.170s, 6025.90/s  (0.193s, 5311.26/s)  LR: 3.118e-04  Data: 0.029 (0.033)
Train: 187 [1050/1251 ( 84%)]  Loss: 3.831 (4.14)  Time: 0.189s, 5414.39/s  (0.193s, 5311.68/s)  LR: 3.118e-04  Data: 0.028 (0.033)
Train: 187 [1100/1251 ( 88%)]  Loss: 4.118 (4.14)  Time: 0.168s, 6091.02/s  (0.193s, 5302.56/s)  LR: 3.118e-04  Data: 0.027 (0.033)
Train: 187 [1150/1251 ( 92%)]  Loss: 3.982 (4.13)  Time: 0.167s, 6118.67/s  (0.193s, 5299.19/s)  LR: 3.118e-04  Data: 0.027 (0.033)
Train: 187 [1200/1251 ( 96%)]  Loss: 4.233 (4.13)  Time: 0.193s, 5315.63/s  (0.193s, 5295.35/s)  LR: 3.118e-04  Data: 0.026 (0.033)
Train: 187 [1250/1251 (100%)]  Loss: 4.056 (4.13)  Time: 0.114s, 9007.03/s  (0.193s, 5310.55/s)  LR: 3.118e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.872 (1.872)  Loss:  1.0621 (1.0621)  Acc@1: 83.6914 (83.6914)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1481 (1.7166)  Acc@1: 80.7783 (67.0360)  Acc@5: 94.1038 (87.5940)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-172.pth.tar', 66.39200002685547)

Train: 188 [   0/1251 (  0%)]  Loss: 4.363 (4.36)  Time: 1.684s,  608.22/s  (1.684s,  608.22/s)  LR: 3.069e-04  Data: 1.562 (1.562)
Train: 188 [  50/1251 (  4%)]  Loss: 3.987 (4.17)  Time: 0.159s, 6458.10/s  (0.223s, 4591.48/s)  LR: 3.069e-04  Data: 0.031 (0.060)
Train: 188 [ 100/1251 (  8%)]  Loss: 4.416 (4.26)  Time: 0.166s, 6166.93/s  (0.207s, 4951.85/s)  LR: 3.069e-04  Data: 0.032 (0.044)
Train: 188 [ 150/1251 ( 12%)]  Loss: 4.191 (4.24)  Time: 0.185s, 5525.95/s  (0.201s, 5103.02/s)  LR: 3.069e-04  Data: 0.027 (0.040)
Train: 188 [ 200/1251 ( 16%)]  Loss: 4.471 (4.29)  Time: 0.173s, 5905.03/s  (0.198s, 5162.54/s)  LR: 3.069e-04  Data: 0.037 (0.038)
Train: 188 [ 250/1251 ( 20%)]  Loss: 4.003 (4.24)  Time: 0.168s, 6084.55/s  (0.197s, 5186.38/s)  LR: 3.069e-04  Data: 0.020 (0.036)
Train: 188 [ 300/1251 ( 24%)]  Loss: 4.151 (4.23)  Time: 0.178s, 5766.57/s  (0.195s, 5240.18/s)  LR: 3.069e-04  Data: 0.019 (0.035)
Train: 188 [ 350/1251 ( 28%)]  Loss: 4.024 (4.20)  Time: 0.170s, 6027.18/s  (0.194s, 5289.68/s)  LR: 3.069e-04  Data: 0.028 (0.034)
Train: 188 [ 400/1251 ( 32%)]  Loss: 4.177 (4.20)  Time: 0.168s, 6095.45/s  (0.194s, 5279.08/s)  LR: 3.069e-04  Data: 0.032 (0.033)
Train: 188 [ 450/1251 ( 36%)]  Loss: 4.638 (4.24)  Time: 0.169s, 6076.07/s  (0.193s, 5296.43/s)  LR: 3.069e-04  Data: 0.032 (0.033)
Train: 188 [ 500/1251 ( 40%)]  Loss: 3.763 (4.20)  Time: 0.195s, 5258.81/s  (0.193s, 5313.27/s)  LR: 3.069e-04  Data: 0.028 (0.032)
Train: 188 [ 550/1251 ( 44%)]  Loss: 4.184 (4.20)  Time: 0.182s, 5631.92/s  (0.194s, 5290.60/s)  LR: 3.069e-04  Data: 0.027 (0.032)
Train: 188 [ 600/1251 ( 48%)]  Loss: 3.970 (4.18)  Time: 0.171s, 5998.05/s  (0.194s, 5291.48/s)  LR: 3.069e-04  Data: 0.026 (0.032)
Train: 188 [ 650/1251 ( 52%)]  Loss: 3.874 (4.16)  Time: 0.160s, 6395.70/s  (0.193s, 5295.41/s)  LR: 3.069e-04  Data: 0.026 (0.031)
Train: 188 [ 700/1251 ( 56%)]  Loss: 4.326 (4.17)  Time: 0.179s, 5720.45/s  (0.193s, 5299.89/s)  LR: 3.069e-04  Data: 0.024 (0.031)
Train: 188 [ 750/1251 ( 60%)]  Loss: 4.130 (4.17)  Time: 0.170s, 6019.21/s  (0.193s, 5300.81/s)  LR: 3.069e-04  Data: 0.019 (0.031)
Train: 188 [ 800/1251 ( 64%)]  Loss: 4.369 (4.18)  Time: 0.285s, 3591.32/s  (0.193s, 5309.23/s)  LR: 3.069e-04  Data: 0.019 (0.031)
Train: 188 [ 850/1251 ( 68%)]  Loss: 3.958 (4.17)  Time: 0.170s, 6031.72/s  (0.193s, 5309.09/s)  LR: 3.069e-04  Data: 0.034 (0.031)
Train: 188 [ 900/1251 ( 72%)]  Loss: 3.940 (4.15)  Time: 0.416s, 2459.41/s  (0.194s, 5291.28/s)  LR: 3.069e-04  Data: 0.026 (0.031)
Train: 188 [ 950/1251 ( 76%)]  Loss: 3.613 (4.13)  Time: 0.166s, 6167.00/s  (0.193s, 5296.27/s)  LR: 3.069e-04  Data: 0.025 (0.031)
Train: 188 [1000/1251 ( 80%)]  Loss: 4.410 (4.14)  Time: 0.187s, 5468.08/s  (0.193s, 5297.59/s)  LR: 3.069e-04  Data: 0.031 (0.030)
Train: 188 [1050/1251 ( 84%)]  Loss: 3.816 (4.13)  Time: 0.177s, 5775.62/s  (0.193s, 5307.97/s)  LR: 3.069e-04  Data: 0.028 (0.030)
Train: 188 [1100/1251 ( 88%)]  Loss: 4.378 (4.14)  Time: 0.268s, 3827.51/s  (0.193s, 5306.03/s)  LR: 3.069e-04  Data: 0.023 (0.030)
Train: 188 [1150/1251 ( 92%)]  Loss: 4.128 (4.14)  Time: 0.174s, 5873.29/s  (0.193s, 5299.42/s)  LR: 3.069e-04  Data: 0.029 (0.030)
Train: 188 [1200/1251 ( 96%)]  Loss: 4.505 (4.15)  Time: 0.179s, 5722.24/s  (0.193s, 5300.59/s)  LR: 3.069e-04  Data: 0.033 (0.030)
Train: 188 [1250/1251 (100%)]  Loss: 4.329 (4.16)  Time: 0.114s, 8985.55/s  (0.193s, 5314.12/s)  LR: 3.069e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.912 (1.912)  Loss:  1.0962 (1.0962)  Acc@1: 81.4453 (81.4453)  Acc@5: 94.6289 (94.6289)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1710 (1.7120)  Acc@1: 82.1934 (67.0340)  Acc@5: 94.5755 (87.6380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-176.pth.tar', 66.58000000976563)

Train: 189 [   0/1251 (  0%)]  Loss: 4.298 (4.30)  Time: 2.090s,  490.06/s  (2.090s,  490.06/s)  LR: 3.021e-04  Data: 1.953 (1.953)
Train: 189 [  50/1251 (  4%)]  Loss: 4.009 (4.15)  Time: 0.159s, 6444.26/s  (0.222s, 4612.57/s)  LR: 3.021e-04  Data: 0.025 (0.077)
Train: 189 [ 100/1251 (  8%)]  Loss: 4.288 (4.20)  Time: 0.153s, 6698.78/s  (0.209s, 4888.84/s)  LR: 3.021e-04  Data: 0.030 (0.054)
Train: 189 [ 150/1251 ( 12%)]  Loss: 4.033 (4.16)  Time: 0.170s, 6019.64/s  (0.202s, 5065.89/s)  LR: 3.021e-04  Data: 0.029 (0.046)
Train: 189 [ 200/1251 ( 16%)]  Loss: 4.431 (4.21)  Time: 0.179s, 5704.77/s  (0.199s, 5158.32/s)  LR: 3.021e-04  Data: 0.020 (0.041)
Train: 189 [ 250/1251 ( 20%)]  Loss: 3.589 (4.11)  Time: 0.274s, 3740.14/s  (0.196s, 5222.37/s)  LR: 3.021e-04  Data: 0.025 (0.039)
Train: 189 [ 300/1251 ( 24%)]  Loss: 4.214 (4.12)  Time: 0.150s, 6808.56/s  (0.194s, 5272.89/s)  LR: 3.021e-04  Data: 0.027 (0.038)
Train: 189 [ 350/1251 ( 28%)]  Loss: 4.296 (4.14)  Time: 0.175s, 5840.17/s  (0.195s, 5239.62/s)  LR: 3.021e-04  Data: 0.028 (0.036)
Train: 189 [ 400/1251 ( 32%)]  Loss: 4.509 (4.19)  Time: 0.171s, 6001.06/s  (0.194s, 5265.44/s)  LR: 3.021e-04  Data: 0.026 (0.035)
Train: 189 [ 450/1251 ( 36%)]  Loss: 3.958 (4.16)  Time: 0.171s, 6004.87/s  (0.193s, 5307.03/s)  LR: 3.021e-04  Data: 0.020 (0.035)
Train: 189 [ 500/1251 ( 40%)]  Loss: 4.519 (4.19)  Time: 0.193s, 5308.04/s  (0.194s, 5279.81/s)  LR: 3.021e-04  Data: 0.031 (0.037)
Train: 189 [ 550/1251 ( 44%)]  Loss: 4.035 (4.18)  Time: 0.363s, 2824.33/s  (0.194s, 5281.11/s)  LR: 3.021e-04  Data: 0.229 (0.038)
Train: 189 [ 600/1251 ( 48%)]  Loss: 3.995 (4.17)  Time: 0.178s, 5767.41/s  (0.194s, 5283.38/s)  LR: 3.021e-04  Data: 0.022 (0.039)
Train: 189 [ 650/1251 ( 52%)]  Loss: 4.191 (4.17)  Time: 0.178s, 5749.22/s  (0.193s, 5297.62/s)  LR: 3.021e-04  Data: 0.032 (0.039)
Train: 189 [ 700/1251 ( 56%)]  Loss: 4.355 (4.18)  Time: 0.185s, 5529.61/s  (0.193s, 5305.47/s)  LR: 3.021e-04  Data: 0.028 (0.039)
Train: 189 [ 750/1251 ( 60%)]  Loss: 3.915 (4.16)  Time: 0.165s, 6193.41/s  (0.193s, 5298.21/s)  LR: 3.021e-04  Data: 0.030 (0.039)
Train: 189 [ 800/1251 ( 64%)]  Loss: 3.864 (4.15)  Time: 0.181s, 5644.08/s  (0.193s, 5301.67/s)  LR: 3.021e-04  Data: 0.028 (0.038)
Train: 189 [ 850/1251 ( 68%)]  Loss: 4.451 (4.16)  Time: 0.163s, 6287.08/s  (0.193s, 5308.91/s)  LR: 3.021e-04  Data: 0.022 (0.037)
Train: 189 [ 900/1251 ( 72%)]  Loss: 4.052 (4.16)  Time: 0.173s, 5921.04/s  (0.193s, 5302.10/s)  LR: 3.021e-04  Data: 0.025 (0.037)
Train: 189 [ 950/1251 ( 76%)]  Loss: 3.855 (4.14)  Time: 0.159s, 6438.51/s  (0.193s, 5296.83/s)  LR: 3.021e-04  Data: 0.036 (0.037)
Train: 189 [1000/1251 ( 80%)]  Loss: 4.498 (4.16)  Time: 0.156s, 6574.11/s  (0.193s, 5299.30/s)  LR: 3.021e-04  Data: 0.031 (0.036)
Train: 189 [1050/1251 ( 84%)]  Loss: 4.353 (4.17)  Time: 0.152s, 6741.09/s  (0.193s, 5294.43/s)  LR: 3.021e-04  Data: 0.025 (0.036)
Train: 189 [1100/1251 ( 88%)]  Loss: 4.288 (4.17)  Time: 0.170s, 6024.09/s  (0.194s, 5290.74/s)  LR: 3.021e-04  Data: 0.020 (0.035)
Train: 189 [1150/1251 ( 92%)]  Loss: 4.385 (4.18)  Time: 0.165s, 6205.17/s  (0.194s, 5291.09/s)  LR: 3.021e-04  Data: 0.029 (0.035)
Train: 189 [1200/1251 ( 96%)]  Loss: 4.246 (4.19)  Time: 0.166s, 6159.78/s  (0.194s, 5286.25/s)  LR: 3.021e-04  Data: 0.033 (0.035)
Train: 189 [1250/1251 (100%)]  Loss: 4.051 (4.18)  Time: 0.114s, 9000.94/s  (0.193s, 5299.99/s)  LR: 3.021e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.808 (1.808)  Loss:  0.9795 (0.9795)  Acc@1: 82.4219 (82.4219)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.1615 (1.7095)  Acc@1: 81.9576 (67.0300)  Acc@5: 94.5755 (87.5800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-181.pth.tar', 66.58799995361328)

Train: 190 [   0/1251 (  0%)]  Loss: 4.356 (4.36)  Time: 1.792s,  571.38/s  (1.792s,  571.38/s)  LR: 2.973e-04  Data: 1.667 (1.667)
Train: 190 [  50/1251 (  4%)]  Loss: 4.146 (4.25)  Time: 0.165s, 6210.02/s  (0.221s, 4638.90/s)  LR: 2.973e-04  Data: 0.030 (0.068)
Train: 190 [ 100/1251 (  8%)]  Loss: 3.927 (4.14)  Time: 0.164s, 6236.40/s  (0.206s, 4970.81/s)  LR: 2.973e-04  Data: 0.030 (0.049)
Train: 190 [ 150/1251 ( 12%)]  Loss: 4.072 (4.13)  Time: 0.182s, 5613.98/s  (0.200s, 5127.39/s)  LR: 2.973e-04  Data: 0.035 (0.044)
Train: 190 [ 200/1251 ( 16%)]  Loss: 3.913 (4.08)  Time: 0.170s, 6019.84/s  (0.198s, 5179.85/s)  LR: 2.973e-04  Data: 0.034 (0.042)
Train: 190 [ 250/1251 ( 20%)]  Loss: 3.898 (4.05)  Time: 0.170s, 6028.46/s  (0.197s, 5194.47/s)  LR: 2.973e-04  Data: 0.025 (0.040)
Train: 190 [ 300/1251 ( 24%)]  Loss: 4.113 (4.06)  Time: 0.184s, 5573.10/s  (0.195s, 5260.64/s)  LR: 2.973e-04  Data: 0.023 (0.038)
Train: 190 [ 350/1251 ( 28%)]  Loss: 3.993 (4.05)  Time: 0.171s, 5994.96/s  (0.194s, 5276.80/s)  LR: 2.973e-04  Data: 0.031 (0.037)
Train: 190 [ 400/1251 ( 32%)]  Loss: 4.427 (4.09)  Time: 0.375s, 2728.46/s  (0.195s, 5264.62/s)  LR: 2.973e-04  Data: 0.028 (0.036)
Train: 190 [ 450/1251 ( 36%)]  Loss: 4.294 (4.11)  Time: 0.156s, 6578.61/s  (0.194s, 5276.96/s)  LR: 2.973e-04  Data: 0.031 (0.035)
Train: 190 [ 500/1251 ( 40%)]  Loss: 3.807 (4.09)  Time: 0.166s, 6182.50/s  (0.193s, 5292.55/s)  LR: 2.973e-04  Data: 0.031 (0.034)
Train: 190 [ 550/1251 ( 44%)]  Loss: 3.723 (4.06)  Time: 0.176s, 5824.68/s  (0.193s, 5300.41/s)  LR: 2.973e-04  Data: 0.023 (0.034)
Train: 190 [ 600/1251 ( 48%)]  Loss: 4.144 (4.06)  Time: 0.181s, 5647.23/s  (0.193s, 5306.30/s)  LR: 2.973e-04  Data: 0.029 (0.033)
Train: 190 [ 650/1251 ( 52%)]  Loss: 3.877 (4.05)  Time: 0.186s, 5497.35/s  (0.193s, 5310.56/s)  LR: 2.973e-04  Data: 0.029 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 190 [ 700/1251 ( 56%)]  Loss: 3.953 (4.04)  Time: 0.182s, 5638.25/s  (0.193s, 5317.92/s)  LR: 2.973e-04  Data: 0.033 (0.033)
Train: 190 [ 750/1251 ( 60%)]  Loss: 4.088 (4.05)  Time: 0.170s, 6028.64/s  (0.193s, 5311.48/s)  LR: 2.973e-04  Data: 0.029 (0.032)
Train: 190 [ 800/1251 ( 64%)]  Loss: 4.143 (4.05)  Time: 0.582s, 1760.06/s  (0.193s, 5296.33/s)  LR: 2.973e-04  Data: 0.032 (0.032)
Train: 190 [ 850/1251 ( 68%)]  Loss: 3.982 (4.05)  Time: 0.160s, 6381.27/s  (0.193s, 5307.81/s)  LR: 2.973e-04  Data: 0.028 (0.032)
Train: 190 [ 900/1251 ( 72%)]  Loss: 4.600 (4.08)  Time: 0.171s, 5980.79/s  (0.193s, 5308.79/s)  LR: 2.973e-04  Data: 0.025 (0.032)
Train: 190 [ 950/1251 ( 76%)]  Loss: 4.100 (4.08)  Time: 0.162s, 6322.10/s  (0.193s, 5301.67/s)  LR: 2.973e-04  Data: 0.027 (0.032)
Train: 190 [1000/1251 ( 80%)]  Loss: 4.469 (4.10)  Time: 0.380s, 2695.58/s  (0.193s, 5303.84/s)  LR: 2.973e-04  Data: 0.021 (0.032)
Train: 190 [1050/1251 ( 84%)]  Loss: 4.290 (4.11)  Time: 0.159s, 6436.05/s  (0.193s, 5306.06/s)  LR: 2.973e-04  Data: 0.030 (0.031)
Train: 190 [1100/1251 ( 88%)]  Loss: 3.925 (4.10)  Time: 0.175s, 5836.83/s  (0.193s, 5303.64/s)  LR: 2.973e-04  Data: 0.026 (0.031)
Train: 190 [1150/1251 ( 92%)]  Loss: 3.792 (4.08)  Time: 0.177s, 5786.11/s  (0.193s, 5302.02/s)  LR: 2.973e-04  Data: 0.040 (0.032)
Train: 190 [1200/1251 ( 96%)]  Loss: 4.089 (4.08)  Time: 0.175s, 5845.95/s  (0.193s, 5298.54/s)  LR: 2.973e-04  Data: 0.024 (0.032)
Train: 190 [1250/1251 (100%)]  Loss: 4.049 (4.08)  Time: 0.114s, 8984.08/s  (0.193s, 5311.71/s)  LR: 2.973e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.892 (1.892)  Loss:  1.0202 (1.0202)  Acc@1: 83.3008 (83.3008)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.1267 (1.7089)  Acc@1: 81.3679 (67.4700)  Acc@5: 94.5755 (87.8560)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-183.pth.tar', 66.64399997802734)

Train: 191 [   0/1251 (  0%)]  Loss: 4.222 (4.22)  Time: 1.788s,  572.72/s  (1.788s,  572.72/s)  LR: 2.926e-04  Data: 1.641 (1.641)
Train: 191 [  50/1251 (  4%)]  Loss: 4.186 (4.20)  Time: 0.178s, 5747.51/s  (0.227s, 4518.75/s)  LR: 2.926e-04  Data: 0.029 (0.073)
Train: 191 [ 100/1251 (  8%)]  Loss: 4.340 (4.25)  Time: 0.175s, 5852.92/s  (0.209s, 4895.64/s)  LR: 2.926e-04  Data: 0.027 (0.051)
Train: 191 [ 150/1251 ( 12%)]  Loss: 3.861 (4.15)  Time: 0.156s, 6544.80/s  (0.202s, 5071.82/s)  LR: 2.926e-04  Data: 0.032 (0.044)
Train: 191 [ 200/1251 ( 16%)]  Loss: 3.925 (4.11)  Time: 0.349s, 2931.99/s  (0.198s, 5161.40/s)  LR: 2.926e-04  Data: 0.032 (0.040)
Train: 191 [ 250/1251 ( 20%)]  Loss: 4.334 (4.14)  Time: 0.175s, 5848.56/s  (0.197s, 5200.64/s)  LR: 2.926e-04  Data: 0.028 (0.038)
Train: 191 [ 300/1251 ( 24%)]  Loss: 4.086 (4.14)  Time: 0.168s, 6102.02/s  (0.197s, 5209.69/s)  LR: 2.926e-04  Data: 0.026 (0.036)
Train: 191 [ 350/1251 ( 28%)]  Loss: 3.899 (4.11)  Time: 0.166s, 6177.24/s  (0.196s, 5232.04/s)  LR: 2.926e-04  Data: 0.026 (0.035)
Train: 191 [ 400/1251 ( 32%)]  Loss: 4.602 (4.16)  Time: 0.178s, 5747.30/s  (0.195s, 5256.27/s)  LR: 2.926e-04  Data: 0.022 (0.034)
Train: 191 [ 450/1251 ( 36%)]  Loss: 4.207 (4.17)  Time: 0.178s, 5737.79/s  (0.194s, 5275.13/s)  LR: 2.926e-04  Data: 0.029 (0.034)
Train: 191 [ 500/1251 ( 40%)]  Loss: 4.048 (4.16)  Time: 0.171s, 6000.11/s  (0.194s, 5272.20/s)  LR: 2.926e-04  Data: 0.027 (0.033)
Train: 191 [ 550/1251 ( 44%)]  Loss: 3.724 (4.12)  Time: 0.156s, 6572.14/s  (0.193s, 5293.77/s)  LR: 2.926e-04  Data: 0.027 (0.033)
Train: 191 [ 600/1251 ( 48%)]  Loss: 4.139 (4.12)  Time: 0.169s, 6048.25/s  (0.193s, 5300.53/s)  LR: 2.926e-04  Data: 0.027 (0.032)
Train: 191 [ 650/1251 ( 52%)]  Loss: 4.467 (4.15)  Time: 0.174s, 5868.40/s  (0.193s, 5296.64/s)  LR: 2.926e-04  Data: 0.026 (0.032)
Train: 191 [ 700/1251 ( 56%)]  Loss: 4.575 (4.17)  Time: 0.157s, 6530.40/s  (0.193s, 5294.92/s)  LR: 2.926e-04  Data: 0.028 (0.032)
Train: 191 [ 750/1251 ( 60%)]  Loss: 4.097 (4.17)  Time: 0.174s, 5893.62/s  (0.193s, 5293.40/s)  LR: 2.926e-04  Data: 0.028 (0.032)
Train: 191 [ 800/1251 ( 64%)]  Loss: 3.897 (4.15)  Time: 0.178s, 5756.17/s  (0.193s, 5312.61/s)  LR: 2.926e-04  Data: 0.022 (0.031)
Train: 191 [ 850/1251 ( 68%)]  Loss: 3.957 (4.14)  Time: 0.173s, 5912.70/s  (0.193s, 5309.86/s)  LR: 2.926e-04  Data: 0.020 (0.031)
Train: 191 [ 900/1251 ( 72%)]  Loss: 4.290 (4.15)  Time: 0.162s, 6329.39/s  (0.193s, 5313.95/s)  LR: 2.926e-04  Data: 0.024 (0.031)
Train: 191 [ 950/1251 ( 76%)]  Loss: 3.925 (4.14)  Time: 0.173s, 5926.04/s  (0.193s, 5306.50/s)  LR: 2.926e-04  Data: 0.021 (0.031)
Train: 191 [1000/1251 ( 80%)]  Loss: 4.119 (4.14)  Time: 0.170s, 6009.88/s  (0.193s, 5300.33/s)  LR: 2.926e-04  Data: 0.024 (0.031)
Train: 191 [1050/1251 ( 84%)]  Loss: 4.318 (4.15)  Time: 0.273s, 3744.86/s  (0.193s, 5297.09/s)  LR: 2.926e-04  Data: 0.023 (0.030)
Train: 191 [1100/1251 ( 88%)]  Loss: 4.220 (4.15)  Time: 0.176s, 5831.43/s  (0.193s, 5301.27/s)  LR: 2.926e-04  Data: 0.029 (0.030)
Train: 191 [1150/1251 ( 92%)]  Loss: 3.737 (4.13)  Time: 0.167s, 6120.85/s  (0.193s, 5297.40/s)  LR: 2.926e-04  Data: 0.027 (0.031)
Train: 191 [1200/1251 ( 96%)]  Loss: 4.393 (4.14)  Time: 0.166s, 6184.98/s  (0.193s, 5298.44/s)  LR: 2.926e-04  Data: 0.024 (0.031)
Train: 191 [1250/1251 (100%)]  Loss: 3.627 (4.12)  Time: 0.114s, 9020.88/s  (0.193s, 5310.69/s)  LR: 2.926e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.779 (1.779)  Loss:  1.0331 (1.0331)  Acc@1: 82.2266 (82.2266)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1764 (1.7258)  Acc@1: 79.4811 (67.1300)  Acc@5: 93.5142 (87.7660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-180.pth.tar', 66.73000002685546)

Train: 192 [   0/1251 (  0%)]  Loss: 3.964 (3.96)  Time: 1.908s,  536.67/s  (1.908s,  536.67/s)  LR: 2.878e-04  Data: 1.786 (1.786)
Train: 192 [  50/1251 (  4%)]  Loss: 4.234 (4.10)  Time: 0.192s, 5325.27/s  (0.227s, 4506.09/s)  LR: 2.878e-04  Data: 0.029 (0.082)
Train: 192 [ 100/1251 (  8%)]  Loss: 3.963 (4.05)  Time: 0.168s, 6108.84/s  (0.209s, 4903.02/s)  LR: 2.878e-04  Data: 0.029 (0.061)
Train: 192 [ 150/1251 ( 12%)]  Loss: 3.608 (3.94)  Time: 0.159s, 6457.57/s  (0.200s, 5122.37/s)  LR: 2.878e-04  Data: 0.031 (0.053)
Train: 192 [ 200/1251 ( 16%)]  Loss: 4.412 (4.04)  Time: 0.171s, 6003.53/s  (0.198s, 5162.31/s)  LR: 2.878e-04  Data: 0.022 (0.053)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 192 [ 250/1251 ( 20%)]  Loss: 4.336 (4.09)  Time: 0.184s, 5574.21/s  (0.196s, 5225.61/s)  LR: 2.878e-04  Data: 0.027 (0.049)
Train: 192 [ 300/1251 ( 24%)]  Loss: 4.024 (4.08)  Time: 0.198s, 5180.43/s  (0.195s, 5243.50/s)  LR: 2.878e-04  Data: 0.024 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 192 [ 350/1251 ( 28%)]  Loss: 4.107 (4.08)  Time: 0.162s, 6332.40/s  (0.194s, 5279.53/s)  LR: 2.878e-04  Data: 0.030 (0.043)
Train: 192 [ 400/1251 ( 32%)]  Loss: 4.149 (4.09)  Time: 0.223s, 4594.42/s  (0.194s, 5291.71/s)  LR: 2.878e-04  Data: 0.091 (0.041)
Train: 192 [ 450/1251 ( 36%)]  Loss: 4.373 (4.12)  Time: 0.164s, 6237.32/s  (0.194s, 5286.12/s)  LR: 2.878e-04  Data: 0.031 (0.040)
Train: 192 [ 500/1251 ( 40%)]  Loss: 4.114 (4.12)  Time: 0.178s, 5748.60/s  (0.193s, 5297.48/s)  LR: 2.878e-04  Data: 0.026 (0.039)
Train: 192 [ 550/1251 ( 44%)]  Loss: 4.155 (4.12)  Time: 0.172s, 5962.54/s  (0.193s, 5305.64/s)  LR: 2.878e-04  Data: 0.029 (0.038)
Train: 192 [ 600/1251 ( 48%)]  Loss: 3.784 (4.09)  Time: 0.179s, 5718.21/s  (0.192s, 5320.52/s)  LR: 2.878e-04  Data: 0.032 (0.037)
Train: 192 [ 650/1251 ( 52%)]  Loss: 4.312 (4.11)  Time: 0.160s, 6420.00/s  (0.192s, 5325.71/s)  LR: 2.878e-04  Data: 0.030 (0.036)
Train: 192 [ 700/1251 ( 56%)]  Loss: 4.243 (4.12)  Time: 0.184s, 5559.61/s  (0.192s, 5331.90/s)  LR: 2.878e-04  Data: 0.026 (0.036)
Train: 192 [ 750/1251 ( 60%)]  Loss: 4.253 (4.13)  Time: 0.299s, 3419.98/s  (0.192s, 5336.51/s)  LR: 2.878e-04  Data: 0.021 (0.035)
Train: 192 [ 800/1251 ( 64%)]  Loss: 4.114 (4.13)  Time: 0.185s, 5531.89/s  (0.192s, 5320.20/s)  LR: 2.878e-04  Data: 0.024 (0.035)
Train: 192 [ 850/1251 ( 68%)]  Loss: 4.190 (4.13)  Time: 0.193s, 5296.09/s  (0.192s, 5323.65/s)  LR: 2.878e-04  Data: 0.030 (0.034)
Train: 192 [ 900/1251 ( 72%)]  Loss: 4.121 (4.13)  Time: 0.174s, 5890.92/s  (0.193s, 5317.74/s)  LR: 2.878e-04  Data: 0.031 (0.034)
Train: 192 [ 950/1251 ( 76%)]  Loss: 4.169 (4.13)  Time: 0.285s, 3595.66/s  (0.192s, 5324.86/s)  LR: 2.878e-04  Data: 0.027 (0.034)
Train: 192 [1000/1251 ( 80%)]  Loss: 4.154 (4.13)  Time: 0.281s, 3649.69/s  (0.192s, 5319.67/s)  LR: 2.878e-04  Data: 0.020 (0.033)
Train: 192 [1050/1251 ( 84%)]  Loss: 4.051 (4.13)  Time: 0.164s, 6259.12/s  (0.193s, 5312.52/s)  LR: 2.878e-04  Data: 0.033 (0.033)
Train: 192 [1100/1251 ( 88%)]  Loss: 3.949 (4.12)  Time: 0.186s, 5505.42/s  (0.193s, 5308.18/s)  LR: 2.878e-04  Data: 0.025 (0.033)
Train: 192 [1150/1251 ( 92%)]  Loss: 4.124 (4.12)  Time: 0.170s, 6033.54/s  (0.193s, 5303.98/s)  LR: 2.878e-04  Data: 0.027 (0.033)
Train: 192 [1200/1251 ( 96%)]  Loss: 3.554 (4.10)  Time: 0.319s, 3214.08/s  (0.193s, 5305.32/s)  LR: 2.878e-04  Data: 0.022 (0.032)
Train: 192 [1250/1251 (100%)]  Loss: 4.102 (4.10)  Time: 0.114s, 8976.09/s  (0.192s, 5322.89/s)  LR: 2.878e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.803 (1.803)  Loss:  1.1425 (1.1425)  Acc@1: 81.2500 (81.2500)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1371 (1.6920)  Acc@1: 81.3679 (67.1080)  Acc@5: 93.3962 (87.5280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-175.pth.tar', 66.75000002441406)

Train: 193 [   0/1251 (  0%)]  Loss: 4.095 (4.09)  Time: 1.762s,  581.14/s  (1.762s,  581.14/s)  LR: 2.831e-04  Data: 1.641 (1.641)
Train: 193 [  50/1251 (  4%)]  Loss: 4.112 (4.10)  Time: 0.168s, 6090.20/s  (0.220s, 4656.65/s)  LR: 2.831e-04  Data: 0.026 (0.076)
Train: 193 [ 100/1251 (  8%)]  Loss: 3.741 (3.98)  Time: 0.165s, 6187.75/s  (0.205s, 4995.61/s)  LR: 2.831e-04  Data: 0.027 (0.060)
Train: 193 [ 150/1251 ( 12%)]  Loss: 4.186 (4.03)  Time: 0.179s, 5735.89/s  (0.200s, 5126.28/s)  LR: 2.831e-04  Data: 0.025 (0.054)
Train: 193 [ 200/1251 ( 16%)]  Loss: 4.145 (4.06)  Time: 0.260s, 3942.96/s  (0.197s, 5191.80/s)  LR: 2.831e-04  Data: 0.028 (0.049)
Train: 193 [ 250/1251 ( 20%)]  Loss: 3.846 (4.02)  Time: 0.259s, 3948.87/s  (0.196s, 5212.59/s)  LR: 2.831e-04  Data: 0.028 (0.044)
Train: 193 [ 300/1251 ( 24%)]  Loss: 4.456 (4.08)  Time: 0.185s, 5522.67/s  (0.195s, 5259.01/s)  LR: 2.831e-04  Data: 0.025 (0.042)
Train: 193 [ 350/1251 ( 28%)]  Loss: 4.050 (4.08)  Time: 0.158s, 6468.04/s  (0.194s, 5275.11/s)  LR: 2.831e-04  Data: 0.026 (0.041)
Train: 193 [ 400/1251 ( 32%)]  Loss: 3.677 (4.03)  Time: 0.150s, 6825.36/s  (0.194s, 5290.26/s)  LR: 2.831e-04  Data: 0.026 (0.042)
Train: 193 [ 450/1251 ( 36%)]  Loss: 3.995 (4.03)  Time: 0.170s, 6012.84/s  (0.194s, 5286.09/s)  LR: 2.831e-04  Data: 0.029 (0.040)
Train: 193 [ 500/1251 ( 40%)]  Loss: 3.634 (3.99)  Time: 0.172s, 5952.67/s  (0.193s, 5311.82/s)  LR: 2.831e-04  Data: 0.024 (0.039)
Train: 193 [ 550/1251 ( 44%)]  Loss: 4.175 (4.01)  Time: 0.185s, 5541.54/s  (0.193s, 5316.29/s)  LR: 2.831e-04  Data: 0.023 (0.038)
Train: 193 [ 600/1251 ( 48%)]  Loss: 3.859 (4.00)  Time: 0.158s, 6488.53/s  (0.192s, 5327.80/s)  LR: 2.831e-04  Data: 0.023 (0.037)
Train: 193 [ 650/1251 ( 52%)]  Loss: 4.240 (4.01)  Time: 0.321s, 3192.35/s  (0.192s, 5332.65/s)  LR: 2.831e-04  Data: 0.025 (0.036)
Train: 193 [ 700/1251 ( 56%)]  Loss: 4.041 (4.02)  Time: 0.151s, 6803.62/s  (0.192s, 5337.71/s)  LR: 2.831e-04  Data: 0.029 (0.036)
Train: 193 [ 750/1251 ( 60%)]  Loss: 4.309 (4.03)  Time: 0.177s, 5791.06/s  (0.192s, 5339.77/s)  LR: 2.831e-04  Data: 0.023 (0.036)
Train: 193 [ 800/1251 ( 64%)]  Loss: 3.668 (4.01)  Time: 0.194s, 5275.10/s  (0.192s, 5338.36/s)  LR: 2.831e-04  Data: 0.025 (0.036)
Train: 193 [ 850/1251 ( 68%)]  Loss: 4.126 (4.02)  Time: 0.174s, 5893.33/s  (0.192s, 5334.08/s)  LR: 2.831e-04  Data: 0.025 (0.036)
Train: 193 [ 900/1251 ( 72%)]  Loss: 4.251 (4.03)  Time: 0.204s, 5016.83/s  (0.192s, 5322.38/s)  LR: 2.831e-04  Data: 0.025 (0.036)
Train: 193 [ 950/1251 ( 76%)]  Loss: 4.423 (4.05)  Time: 0.175s, 5851.71/s  (0.192s, 5331.20/s)  LR: 2.831e-04  Data: 0.021 (0.037)
Train: 193 [1000/1251 ( 80%)]  Loss: 3.523 (4.03)  Time: 0.182s, 5625.60/s  (0.192s, 5332.19/s)  LR: 2.831e-04  Data: 0.022 (0.037)
Train: 193 [1050/1251 ( 84%)]  Loss: 4.090 (4.03)  Time: 0.165s, 6216.78/s  (0.192s, 5330.73/s)  LR: 2.831e-04  Data: 0.024 (0.037)
Train: 193 [1100/1251 ( 88%)]  Loss: 4.023 (4.03)  Time: 0.179s, 5726.36/s  (0.193s, 5318.79/s)  LR: 2.831e-04  Data: 0.023 (0.038)
Train: 193 [1150/1251 ( 92%)]  Loss: 4.232 (4.04)  Time: 0.162s, 6304.06/s  (0.192s, 5320.08/s)  LR: 2.831e-04  Data: 0.029 (0.039)
Train: 193 [1200/1251 ( 96%)]  Loss: 4.372 (4.05)  Time: 0.174s, 5873.31/s  (0.192s, 5319.76/s)  LR: 2.831e-04  Data: 0.026 (0.039)
Train: 193 [1250/1251 (100%)]  Loss: 3.952 (4.05)  Time: 0.114s, 9004.69/s  (0.192s, 5326.96/s)  LR: 2.831e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.834 (1.834)  Loss:  1.1220 (1.1220)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.2362 (1.7544)  Acc@1: 80.7783 (67.0400)  Acc@5: 93.7500 (87.5320)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-185.pth.tar', 66.82999986816407)

Train: 194 [   0/1251 (  0%)]  Loss: 4.092 (4.09)  Time: 1.800s,  568.83/s  (1.800s,  568.83/s)  LR: 2.784e-04  Data: 1.671 (1.671)
Train: 194 [  50/1251 (  4%)]  Loss: 3.926 (4.01)  Time: 0.175s, 5846.92/s  (0.224s, 4571.28/s)  LR: 2.784e-04  Data: 0.027 (0.067)
Train: 194 [ 100/1251 (  8%)]  Loss: 4.409 (4.14)  Time: 0.187s, 5483.51/s  (0.210s, 4885.54/s)  LR: 2.784e-04  Data: 0.022 (0.053)
Train: 194 [ 150/1251 ( 12%)]  Loss: 4.152 (4.14)  Time: 0.170s, 6012.46/s  (0.203s, 5044.96/s)  LR: 2.784e-04  Data: 0.036 (0.045)
Train: 194 [ 200/1251 ( 16%)]  Loss: 4.506 (4.22)  Time: 0.181s, 5647.26/s  (0.199s, 5152.09/s)  LR: 2.784e-04  Data: 0.029 (0.041)
Train: 194 [ 250/1251 ( 20%)]  Loss: 4.391 (4.25)  Time: 0.203s, 5032.56/s  (0.198s, 5170.47/s)  LR: 2.784e-04  Data: 0.025 (0.039)
Train: 194 [ 300/1251 ( 24%)]  Loss: 4.218 (4.24)  Time: 0.197s, 5193.16/s  (0.196s, 5228.35/s)  LR: 2.784e-04  Data: 0.031 (0.037)
Train: 194 [ 350/1251 ( 28%)]  Loss: 3.938 (4.20)  Time: 0.173s, 5918.36/s  (0.194s, 5270.19/s)  LR: 2.784e-04  Data: 0.026 (0.036)
Train: 194 [ 400/1251 ( 32%)]  Loss: 4.246 (4.21)  Time: 0.286s, 3584.94/s  (0.193s, 5294.66/s)  LR: 2.784e-04  Data: 0.024 (0.035)
Train: 194 [ 450/1251 ( 36%)]  Loss: 4.385 (4.23)  Time: 0.161s, 6379.61/s  (0.193s, 5301.27/s)  LR: 2.784e-04  Data: 0.029 (0.034)
Train: 194 [ 500/1251 ( 40%)]  Loss: 4.499 (4.25)  Time: 0.164s, 6262.41/s  (0.193s, 5315.08/s)  LR: 2.784e-04  Data: 0.022 (0.033)
Train: 194 [ 550/1251 ( 44%)]  Loss: 4.602 (4.28)  Time: 0.196s, 5226.87/s  (0.193s, 5302.62/s)  LR: 2.784e-04  Data: 0.029 (0.033)
Train: 194 [ 600/1251 ( 48%)]  Loss: 4.486 (4.30)  Time: 0.206s, 4963.40/s  (0.193s, 5312.93/s)  LR: 2.784e-04  Data: 0.065 (0.033)
Train: 194 [ 650/1251 ( 52%)]  Loss: 3.880 (4.27)  Time: 0.177s, 5794.00/s  (0.193s, 5318.46/s)  LR: 2.784e-04  Data: 0.027 (0.032)
Train: 194 [ 700/1251 ( 56%)]  Loss: 3.972 (4.25)  Time: 0.161s, 6349.01/s  (0.193s, 5316.06/s)  LR: 2.784e-04  Data: 0.025 (0.032)
Train: 194 [ 750/1251 ( 60%)]  Loss: 4.338 (4.25)  Time: 0.188s, 5438.00/s  (0.193s, 5312.08/s)  LR: 2.784e-04  Data: 0.028 (0.032)
Train: 194 [ 800/1251 ( 64%)]  Loss: 4.065 (4.24)  Time: 0.194s, 5282.08/s  (0.193s, 5313.52/s)  LR: 2.784e-04  Data: 0.027 (0.032)
Train: 194 [ 850/1251 ( 68%)]  Loss: 4.267 (4.24)  Time: 0.211s, 4864.53/s  (0.193s, 5312.53/s)  LR: 2.784e-04  Data: 0.019 (0.031)
Train: 194 [ 900/1251 ( 72%)]  Loss: 4.243 (4.24)  Time: 0.169s, 6044.27/s  (0.193s, 5310.08/s)  LR: 2.784e-04  Data: 0.026 (0.031)
Train: 194 [ 950/1251 ( 76%)]  Loss: 4.144 (4.24)  Time: 0.187s, 5472.86/s  (0.193s, 5310.02/s)  LR: 2.784e-04  Data: 0.023 (0.031)
Train: 194 [1000/1251 ( 80%)]  Loss: 3.969 (4.23)  Time: 0.180s, 5700.39/s  (0.193s, 5305.16/s)  LR: 2.784e-04  Data: 0.022 (0.031)
Train: 194 [1050/1251 ( 84%)]  Loss: 3.908 (4.21)  Time: 0.162s, 6308.26/s  (0.193s, 5306.53/s)  LR: 2.784e-04  Data: 0.025 (0.031)
Train: 194 [1100/1251 ( 88%)]  Loss: 4.010 (4.20)  Time: 0.189s, 5414.94/s  (0.193s, 5304.25/s)  LR: 2.784e-04  Data: 0.026 (0.031)
Train: 194 [1150/1251 ( 92%)]  Loss: 4.138 (4.20)  Time: 0.173s, 5912.68/s  (0.193s, 5307.05/s)  LR: 2.784e-04  Data: 0.026 (0.031)
Train: 194 [1200/1251 ( 96%)]  Loss: 3.922 (4.19)  Time: 0.161s, 6357.04/s  (0.193s, 5307.26/s)  LR: 2.784e-04  Data: 0.028 (0.031)
Train: 194 [1250/1251 (100%)]  Loss: 4.228 (4.19)  Time: 0.113s, 9076.09/s  (0.193s, 5310.61/s)  LR: 2.784e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.805 (1.805)  Loss:  1.0615 (1.0615)  Acc@1: 82.9102 (82.9102)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2084 (1.7021)  Acc@1: 80.3066 (67.3360)  Acc@5: 94.6934 (87.7340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-186.pth.tar', 66.85400004882813)

Train: 195 [   0/1251 (  0%)]  Loss: 4.526 (4.53)  Time: 1.741s,  588.15/s  (1.741s,  588.15/s)  LR: 2.737e-04  Data: 1.615 (1.615)
Train: 195 [  50/1251 (  4%)]  Loss: 4.341 (4.43)  Time: 0.171s, 5972.50/s  (0.220s, 4653.37/s)  LR: 2.737e-04  Data: 0.025 (0.075)
Train: 195 [ 100/1251 (  8%)]  Loss: 4.140 (4.34)  Time: 0.167s, 6143.89/s  (0.206s, 4967.08/s)  LR: 2.737e-04  Data: 0.028 (0.061)
Train: 195 [ 150/1251 ( 12%)]  Loss: 4.275 (4.32)  Time: 0.190s, 5381.00/s  (0.202s, 5078.28/s)  LR: 2.737e-04  Data: 0.025 (0.051)
Train: 195 [ 200/1251 ( 16%)]  Loss: 4.298 (4.32)  Time: 0.215s, 4763.89/s  (0.199s, 5142.81/s)  LR: 2.737e-04  Data: 0.027 (0.045)
Train: 195 [ 250/1251 ( 20%)]  Loss: 4.115 (4.28)  Time: 0.164s, 6230.60/s  (0.197s, 5204.61/s)  LR: 2.737e-04  Data: 0.031 (0.042)
Train: 195 [ 300/1251 ( 24%)]  Loss: 4.172 (4.27)  Time: 0.178s, 5763.18/s  (0.194s, 5271.88/s)  LR: 2.737e-04  Data: 0.027 (0.040)
Train: 195 [ 350/1251 ( 28%)]  Loss: 4.021 (4.24)  Time: 0.183s, 5601.86/s  (0.194s, 5279.70/s)  LR: 2.737e-04  Data: 0.024 (0.038)
Train: 195 [ 400/1251 ( 32%)]  Loss: 4.048 (4.22)  Time: 0.187s, 5466.12/s  (0.194s, 5278.70/s)  LR: 2.737e-04  Data: 0.021 (0.038)
Train: 195 [ 450/1251 ( 36%)]  Loss: 4.246 (4.22)  Time: 0.158s, 6483.18/s  (0.194s, 5284.73/s)  LR: 2.737e-04  Data: 0.026 (0.039)
Train: 195 [ 500/1251 ( 40%)]  Loss: 4.197 (4.22)  Time: 0.176s, 5813.78/s  (0.193s, 5306.20/s)  LR: 2.737e-04  Data: 0.025 (0.038)
Train: 195 [ 550/1251 ( 44%)]  Loss: 3.924 (4.19)  Time: 0.167s, 6133.41/s  (0.193s, 5292.08/s)  LR: 2.737e-04  Data: 0.033 (0.040)
Train: 195 [ 600/1251 ( 48%)]  Loss: 4.384 (4.21)  Time: 0.183s, 5584.75/s  (0.193s, 5311.72/s)  LR: 2.737e-04  Data: 0.025 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 195 [ 650/1251 ( 52%)]  Loss: 4.086 (4.20)  Time: 0.180s, 5682.51/s  (0.192s, 5320.01/s)  LR: 2.737e-04  Data: 0.029 (0.040)
Train: 195 [ 700/1251 ( 56%)]  Loss: 3.957 (4.18)  Time: 0.153s, 6674.25/s  (0.193s, 5306.70/s)  LR: 2.737e-04  Data: 0.025 (0.040)
Train: 195 [ 750/1251 ( 60%)]  Loss: 4.156 (4.18)  Time: 0.173s, 5907.02/s  (0.193s, 5307.56/s)  LR: 2.737e-04  Data: 0.022 (0.041)
Train: 195 [ 800/1251 ( 64%)]  Loss: 4.275 (4.19)  Time: 0.168s, 6108.37/s  (0.193s, 5311.78/s)  LR: 2.737e-04  Data: 0.033 (0.041)
Train: 195 [ 850/1251 ( 68%)]  Loss: 4.316 (4.19)  Time: 0.173s, 5928.70/s  (0.193s, 5319.30/s)  LR: 2.737e-04  Data: 0.024 (0.041)
Train: 195 [ 900/1251 ( 72%)]  Loss: 4.110 (4.19)  Time: 0.165s, 6208.19/s  (0.193s, 5317.96/s)  LR: 2.737e-04  Data: 0.031 (0.040)
Train: 195 [ 950/1251 ( 76%)]  Loss: 3.877 (4.17)  Time: 0.178s, 5753.46/s  (0.192s, 5323.47/s)  LR: 2.737e-04  Data: 0.026 (0.040)
Train: 195 [1000/1251 ( 80%)]  Loss: 4.191 (4.17)  Time: 0.176s, 5821.97/s  (0.193s, 5314.49/s)  LR: 2.737e-04  Data: 0.033 (0.041)
Train: 195 [1050/1251 ( 84%)]  Loss: 3.864 (4.16)  Time: 0.163s, 6294.45/s  (0.193s, 5308.51/s)  LR: 2.737e-04  Data: 0.024 (0.041)
Train: 195 [1100/1251 ( 88%)]  Loss: 4.051 (4.16)  Time: 0.184s, 5563.36/s  (0.193s, 5310.57/s)  LR: 2.737e-04  Data: 0.032 (0.040)
Train: 195 [1150/1251 ( 92%)]  Loss: 4.278 (4.16)  Time: 0.191s, 5372.05/s  (0.193s, 5313.99/s)  LR: 2.737e-04  Data: 0.025 (0.039)
Train: 195 [1200/1251 ( 96%)]  Loss: 4.598 (4.18)  Time: 0.185s, 5531.60/s  (0.193s, 5315.14/s)  LR: 2.737e-04  Data: 0.028 (0.039)
Train: 195 [1250/1251 (100%)]  Loss: 4.419 (4.19)  Time: 0.113s, 9065.68/s  (0.193s, 5312.04/s)  LR: 2.737e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.925 (1.925)  Loss:  1.1237 (1.1237)  Acc@1: 82.1289 (82.1289)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1744 (1.7270)  Acc@1: 80.8962 (67.1400)  Acc@5: 93.5142 (87.6720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-189.pth.tar', 67.03000004882813)

Train: 196 [   0/1251 (  0%)]  Loss: 4.502 (4.50)  Time: 1.724s,  593.81/s  (1.724s,  593.81/s)  LR: 2.691e-04  Data: 1.598 (1.598)
Train: 196 [  50/1251 (  4%)]  Loss: 3.857 (4.18)  Time: 0.152s, 6740.56/s  (0.221s, 4639.34/s)  LR: 2.691e-04  Data: 0.030 (0.071)
Train: 196 [ 100/1251 (  8%)]  Loss: 4.163 (4.17)  Time: 0.175s, 5851.37/s  (0.205s, 4998.22/s)  LR: 2.691e-04  Data: 0.034 (0.051)
Train: 196 [ 150/1251 ( 12%)]  Loss: 4.081 (4.15)  Time: 0.168s, 6078.19/s  (0.200s, 5123.75/s)  LR: 2.691e-04  Data: 0.032 (0.044)
Train: 196 [ 200/1251 ( 16%)]  Loss: 4.190 (4.16)  Time: 0.190s, 5379.36/s  (0.197s, 5192.77/s)  LR: 2.691e-04  Data: 0.030 (0.040)
Train: 196 [ 250/1251 ( 20%)]  Loss: 4.478 (4.21)  Time: 0.177s, 5776.89/s  (0.195s, 5256.43/s)  LR: 2.691e-04  Data: 0.028 (0.038)
Train: 196 [ 300/1251 ( 24%)]  Loss: 3.887 (4.17)  Time: 0.184s, 5554.33/s  (0.194s, 5271.82/s)  LR: 2.691e-04  Data: 0.037 (0.038)
Train: 196 [ 350/1251 ( 28%)]  Loss: 4.178 (4.17)  Time: 0.174s, 5902.00/s  (0.195s, 5257.72/s)  LR: 2.691e-04  Data: 0.030 (0.036)
Train: 196 [ 400/1251 ( 32%)]  Loss: 3.761 (4.12)  Time: 0.319s, 3213.03/s  (0.194s, 5272.96/s)  LR: 2.691e-04  Data: 0.027 (0.036)
Train: 196 [ 450/1251 ( 36%)]  Loss: 3.881 (4.10)  Time: 0.175s, 5837.80/s  (0.194s, 5288.50/s)  LR: 2.691e-04  Data: 0.034 (0.035)
Train: 196 [ 500/1251 ( 40%)]  Loss: 4.567 (4.14)  Time: 0.199s, 5142.39/s  (0.194s, 5291.35/s)  LR: 2.691e-04  Data: 0.026 (0.034)
Train: 196 [ 550/1251 ( 44%)]  Loss: 3.889 (4.12)  Time: 0.168s, 6110.25/s  (0.193s, 5311.76/s)  LR: 2.691e-04  Data: 0.031 (0.034)
Train: 196 [ 600/1251 ( 48%)]  Loss: 4.252 (4.13)  Time: 0.468s, 2189.84/s  (0.193s, 5315.86/s)  LR: 2.691e-04  Data: 0.023 (0.033)
Train: 196 [ 650/1251 ( 52%)]  Loss: 4.205 (4.14)  Time: 0.171s, 5992.34/s  (0.193s, 5312.90/s)  LR: 2.691e-04  Data: 0.024 (0.033)
Train: 196 [ 700/1251 ( 56%)]  Loss: 3.995 (4.13)  Time: 0.192s, 5343.83/s  (0.192s, 5320.87/s)  LR: 2.691e-04  Data: 0.022 (0.033)
Train: 196 [ 750/1251 ( 60%)]  Loss: 4.297 (4.14)  Time: 0.179s, 5714.28/s  (0.193s, 5311.63/s)  LR: 2.691e-04  Data: 0.020 (0.032)
Train: 196 [ 800/1251 ( 64%)]  Loss: 3.820 (4.12)  Time: 0.218s, 4693.75/s  (0.193s, 5311.40/s)  LR: 2.691e-04  Data: 0.022 (0.032)
Train: 196 [ 850/1251 ( 68%)]  Loss: 4.401 (4.13)  Time: 0.279s, 3671.90/s  (0.193s, 5314.75/s)  LR: 2.691e-04  Data: 0.026 (0.032)
Train: 196 [ 900/1251 ( 72%)]  Loss: 4.453 (4.15)  Time: 0.161s, 6362.69/s  (0.192s, 5321.36/s)  LR: 2.691e-04  Data: 0.029 (0.032)
Train: 196 [ 950/1251 ( 76%)]  Loss: 4.103 (4.15)  Time: 0.180s, 5691.85/s  (0.193s, 5305.20/s)  LR: 2.691e-04  Data: 0.028 (0.031)
Train: 196 [1000/1251 ( 80%)]  Loss: 4.316 (4.16)  Time: 0.549s, 1865.36/s  (0.193s, 5308.33/s)  LR: 2.691e-04  Data: 0.027 (0.031)
Train: 196 [1050/1251 ( 84%)]  Loss: 3.969 (4.15)  Time: 0.167s, 6126.25/s  (0.193s, 5306.67/s)  LR: 2.691e-04  Data: 0.027 (0.031)
Train: 196 [1100/1251 ( 88%)]  Loss: 3.749 (4.13)  Time: 0.160s, 6389.11/s  (0.193s, 5304.70/s)  LR: 2.691e-04  Data: 0.032 (0.031)
Train: 196 [1150/1251 ( 92%)]  Loss: 3.873 (4.12)  Time: 0.175s, 5847.44/s  (0.193s, 5303.10/s)  LR: 2.691e-04  Data: 0.035 (0.031)
Train: 196 [1200/1251 ( 96%)]  Loss: 4.187 (4.12)  Time: 0.291s, 3524.18/s  (0.193s, 5297.20/s)  LR: 2.691e-04  Data: 0.032 (0.031)
Train: 196 [1250/1251 (100%)]  Loss: 4.005 (4.12)  Time: 0.114s, 8974.27/s  (0.193s, 5308.41/s)  LR: 2.691e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.869 (1.869)  Loss:  1.0704 (1.0704)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.1079 (1.6840)  Acc@1: 82.5472 (67.3040)  Acc@5: 94.9293 (87.9660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-188.pth.tar', 67.03400002197266)

Train: 197 [   0/1251 (  0%)]  Loss: 4.447 (4.45)  Time: 1.777s,  576.11/s  (1.777s,  576.11/s)  LR: 2.645e-04  Data: 1.642 (1.642)
Train: 197 [  50/1251 (  4%)]  Loss: 4.492 (4.47)  Time: 0.157s, 6530.92/s  (0.221s, 4635.25/s)  LR: 2.645e-04  Data: 0.027 (0.072)
Train: 197 [ 100/1251 (  8%)]  Loss: 4.314 (4.42)  Time: 0.164s, 6239.74/s  (0.206s, 4981.08/s)  LR: 2.645e-04  Data: 0.029 (0.054)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 197 [ 150/1251 ( 12%)]  Loss: 4.050 (4.33)  Time: 0.185s, 5524.58/s  (0.202s, 5065.20/s)  LR: 2.645e-04  Data: 0.042 (0.046)
Train: 197 [ 200/1251 ( 16%)]  Loss: 3.820 (4.22)  Time: 0.165s, 6211.12/s  (0.199s, 5134.65/s)  LR: 2.645e-04  Data: 0.027 (0.041)
Train: 197 [ 250/1251 ( 20%)]  Loss: 4.201 (4.22)  Time: 0.171s, 6004.18/s  (0.197s, 5188.99/s)  LR: 2.645e-04  Data: 0.036 (0.038)
Train: 197 [ 300/1251 ( 24%)]  Loss: 4.161 (4.21)  Time: 0.150s, 6815.64/s  (0.195s, 5255.66/s)  LR: 2.645e-04  Data: 0.032 (0.037)
Train: 197 [ 350/1251 ( 28%)]  Loss: 3.997 (4.19)  Time: 0.189s, 5431.86/s  (0.194s, 5287.10/s)  LR: 2.645e-04  Data: 0.025 (0.036)
Train: 197 [ 400/1251 ( 32%)]  Loss: 4.095 (4.18)  Time: 0.421s, 2430.79/s  (0.193s, 5304.32/s)  LR: 2.645e-04  Data: 0.027 (0.035)
Train: 197 [ 450/1251 ( 36%)]  Loss: 4.039 (4.16)  Time: 0.184s, 5575.50/s  (0.193s, 5308.29/s)  LR: 2.645e-04  Data: 0.025 (0.034)
Train: 197 [ 500/1251 ( 40%)]  Loss: 4.449 (4.19)  Time: 0.170s, 6027.65/s  (0.193s, 5308.38/s)  LR: 2.645e-04  Data: 0.022 (0.033)
Train: 197 [ 550/1251 ( 44%)]  Loss: 3.863 (4.16)  Time: 0.181s, 5649.34/s  (0.193s, 5318.08/s)  LR: 2.645e-04  Data: 0.029 (0.033)
Train: 197 [ 600/1251 ( 48%)]  Loss: 4.381 (4.18)  Time: 0.153s, 6712.48/s  (0.192s, 5320.91/s)  LR: 2.645e-04  Data: 0.026 (0.033)
Train: 197 [ 650/1251 ( 52%)]  Loss: 4.031 (4.17)  Time: 0.170s, 6033.67/s  (0.193s, 5316.41/s)  LR: 2.645e-04  Data: 0.026 (0.032)
Train: 197 [ 700/1251 ( 56%)]  Loss: 4.057 (4.16)  Time: 0.159s, 6451.46/s  (0.193s, 5306.86/s)  LR: 2.645e-04  Data: 0.028 (0.032)
Train: 197 [ 750/1251 ( 60%)]  Loss: 3.959 (4.15)  Time: 0.165s, 6218.91/s  (0.193s, 5304.00/s)  LR: 2.645e-04  Data: 0.030 (0.031)
Train: 197 [ 800/1251 ( 64%)]  Loss: 4.230 (4.15)  Time: 0.172s, 5946.93/s  (0.193s, 5308.09/s)  LR: 2.645e-04  Data: 0.030 (0.031)
Train: 197 [ 850/1251 ( 68%)]  Loss: 3.796 (4.13)  Time: 0.182s, 5621.16/s  (0.193s, 5317.48/s)  LR: 2.645e-04  Data: 0.024 (0.031)
Train: 197 [ 900/1251 ( 72%)]  Loss: 3.965 (4.12)  Time: 0.184s, 5570.12/s  (0.193s, 5319.37/s)  LR: 2.645e-04  Data: 0.029 (0.031)
Train: 197 [ 950/1251 ( 76%)]  Loss: 4.521 (4.14)  Time: 0.156s, 6554.60/s  (0.193s, 5313.01/s)  LR: 2.645e-04  Data: 0.029 (0.031)
Train: 197 [1000/1251 ( 80%)]  Loss: 4.256 (4.15)  Time: 0.178s, 5752.32/s  (0.193s, 5318.00/s)  LR: 2.645e-04  Data: 0.031 (0.031)
Train: 197 [1050/1251 ( 84%)]  Loss: 4.287 (4.16)  Time: 0.164s, 6232.27/s  (0.193s, 5317.97/s)  LR: 2.645e-04  Data: 0.030 (0.031)
Train: 197 [1100/1251 ( 88%)]  Loss: 4.274 (4.16)  Time: 0.170s, 6013.55/s  (0.193s, 5318.73/s)  LR: 2.645e-04  Data: 0.034 (0.031)
Train: 197 [1150/1251 ( 92%)]  Loss: 3.847 (4.15)  Time: 0.157s, 6523.52/s  (0.193s, 5312.07/s)  LR: 2.645e-04  Data: 0.033 (0.030)
Train: 197 [1200/1251 ( 96%)]  Loss: 3.991 (4.14)  Time: 0.161s, 6369.04/s  (0.193s, 5303.53/s)  LR: 2.645e-04  Data: 0.031 (0.030)
Train: 197 [1250/1251 (100%)]  Loss: 4.368 (4.15)  Time: 0.114s, 8979.58/s  (0.193s, 5314.15/s)  LR: 2.645e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.849 (1.849)  Loss:  1.0563 (1.0563)  Acc@1: 82.4219 (82.4219)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1360 (1.6814)  Acc@1: 80.7783 (67.4860)  Acc@5: 95.0472 (87.9800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-187.pth.tar', 67.03600005371094)

Train: 198 [   0/1251 (  0%)]  Loss: 4.284 (4.28)  Time: 1.773s,  577.52/s  (1.773s,  577.52/s)  LR: 2.599e-04  Data: 1.637 (1.637)
Train: 198 [  50/1251 (  4%)]  Loss: 4.348 (4.32)  Time: 0.195s, 5261.81/s  (0.219s, 4683.29/s)  LR: 2.599e-04  Data: 0.030 (0.069)
Train: 198 [ 100/1251 (  8%)]  Loss: 4.102 (4.24)  Time: 0.171s, 5972.18/s  (0.206s, 4965.98/s)  LR: 2.599e-04  Data: 0.030 (0.055)
Train: 198 [ 150/1251 ( 12%)]  Loss: 4.166 (4.22)  Time: 0.157s, 6511.42/s  (0.200s, 5119.15/s)  LR: 2.599e-04  Data: 0.027 (0.046)
Train: 198 [ 200/1251 ( 16%)]  Loss: 4.217 (4.22)  Time: 0.180s, 5683.60/s  (0.200s, 5131.64/s)  LR: 2.599e-04  Data: 0.025 (0.047)
Train: 198 [ 250/1251 ( 20%)]  Loss: 4.176 (4.22)  Time: 0.378s, 2708.43/s  (0.197s, 5205.90/s)  LR: 2.599e-04  Data: 0.029 (0.044)
Train: 198 [ 300/1251 ( 24%)]  Loss: 4.188 (4.21)  Time: 0.161s, 6358.02/s  (0.193s, 5294.17/s)  LR: 2.599e-04  Data: 0.032 (0.042)
Train: 198 [ 350/1251 ( 28%)]  Loss: 3.921 (4.18)  Time: 0.408s, 2511.01/s  (0.194s, 5271.90/s)  LR: 2.599e-04  Data: 0.028 (0.040)
Train: 198 [ 400/1251 ( 32%)]  Loss: 4.048 (4.16)  Time: 0.176s, 5812.60/s  (0.194s, 5290.39/s)  LR: 2.599e-04  Data: 0.026 (0.039)
Train: 198 [ 450/1251 ( 36%)]  Loss: 3.972 (4.14)  Time: 0.157s, 6525.22/s  (0.194s, 5288.22/s)  LR: 2.599e-04  Data: 0.029 (0.038)
Train: 198 [ 500/1251 ( 40%)]  Loss: 4.083 (4.14)  Time: 0.181s, 5665.50/s  (0.193s, 5307.65/s)  LR: 2.599e-04  Data: 0.038 (0.037)
Train: 198 [ 550/1251 ( 44%)]  Loss: 4.145 (4.14)  Time: 0.197s, 5187.09/s  (0.192s, 5320.51/s)  LR: 2.599e-04  Data: 0.030 (0.036)
Train: 198 [ 600/1251 ( 48%)]  Loss: 3.843 (4.11)  Time: 0.190s, 5396.57/s  (0.193s, 5319.27/s)  LR: 2.599e-04  Data: 0.029 (0.035)
Train: 198 [ 650/1251 ( 52%)]  Loss: 3.902 (4.10)  Time: 0.350s, 2928.86/s  (0.192s, 5329.15/s)  LR: 2.599e-04  Data: 0.023 (0.035)
Train: 198 [ 700/1251 ( 56%)]  Loss: 3.868 (4.08)  Time: 0.256s, 4004.22/s  (0.193s, 5316.95/s)  LR: 2.599e-04  Data: 0.036 (0.035)
Train: 198 [ 750/1251 ( 60%)]  Loss: 4.090 (4.08)  Time: 0.169s, 6057.09/s  (0.192s, 5325.69/s)  LR: 2.599e-04  Data: 0.020 (0.034)
Train: 198 [ 800/1251 ( 64%)]  Loss: 4.504 (4.11)  Time: 0.159s, 6450.05/s  (0.192s, 5323.26/s)  LR: 2.599e-04  Data: 0.025 (0.034)
Train: 198 [ 850/1251 ( 68%)]  Loss: 4.309 (4.12)  Time: 0.173s, 5927.94/s  (0.192s, 5326.89/s)  LR: 2.599e-04  Data: 0.028 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 198 [ 900/1251 ( 72%)]  Loss: 4.577 (4.14)  Time: 0.279s, 3675.42/s  (0.193s, 5313.20/s)  LR: 2.599e-04  Data: 0.149 (0.035)
Train: 198 [ 950/1251 ( 76%)]  Loss: 4.377 (4.16)  Time: 0.285s, 3594.17/s  (0.193s, 5302.09/s)  LR: 2.599e-04  Data: 0.021 (0.036)
Train: 198 [1000/1251 ( 80%)]  Loss: 4.409 (4.17)  Time: 0.160s, 6412.43/s  (0.193s, 5312.99/s)  LR: 2.599e-04  Data: 0.024 (0.036)
Train: 198 [1050/1251 ( 84%)]  Loss: 4.140 (4.17)  Time: 0.164s, 6250.93/s  (0.193s, 5310.42/s)  LR: 2.599e-04  Data: 0.032 (0.037)
Train: 198 [1100/1251 ( 88%)]  Loss: 3.875 (4.15)  Time: 0.373s, 2748.68/s  (0.193s, 5305.98/s)  LR: 2.599e-04  Data: 0.244 (0.037)
Train: 198 [1150/1251 ( 92%)]  Loss: 3.935 (4.14)  Time: 0.155s, 6600.78/s  (0.193s, 5308.37/s)  LR: 2.599e-04  Data: 0.025 (0.038)
Train: 198 [1200/1251 ( 96%)]  Loss: 4.486 (4.16)  Time: 0.166s, 6182.35/s  (0.193s, 5310.33/s)  LR: 2.599e-04  Data: 0.035 (0.038)
Train: 198 [1250/1251 (100%)]  Loss: 4.223 (4.16)  Time: 0.113s, 9031.41/s  (0.192s, 5322.31/s)  LR: 2.599e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.806 (1.806)  Loss:  1.1299 (1.1299)  Acc@1: 83.3984 (83.3984)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1381 (1.7290)  Acc@1: 82.4292 (67.5060)  Acc@5: 94.9293 (87.8480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-193.pth.tar', 67.04000005371094)

Train: 199 [   0/1251 (  0%)]  Loss: 4.150 (4.15)  Time: 1.930s,  530.46/s  (1.930s,  530.46/s)  LR: 2.553e-04  Data: 1.767 (1.767)
Train: 199 [  50/1251 (  4%)]  Loss: 3.898 (4.02)  Time: 0.167s, 6133.57/s  (0.223s, 4598.82/s)  LR: 2.553e-04  Data: 0.025 (0.078)
Train: 199 [ 100/1251 (  8%)]  Loss: 4.169 (4.07)  Time: 0.198s, 5183.81/s  (0.208s, 4924.09/s)  LR: 2.553e-04  Data: 0.030 (0.063)
Train: 199 [ 150/1251 ( 12%)]  Loss: 3.849 (4.02)  Time: 0.195s, 5254.28/s  (0.201s, 5091.48/s)  LR: 2.553e-04  Data: 0.023 (0.054)
Train: 199 [ 200/1251 ( 16%)]  Loss: 3.877 (3.99)  Time: 0.197s, 5193.86/s  (0.198s, 5178.47/s)  LR: 2.553e-04  Data: 0.020 (0.051)
Train: 199 [ 250/1251 ( 20%)]  Loss: 4.178 (4.02)  Time: 0.168s, 6098.61/s  (0.197s, 5208.19/s)  LR: 2.553e-04  Data: 0.026 (0.050)
Train: 199 [ 300/1251 ( 24%)]  Loss: 4.144 (4.04)  Time: 0.535s, 1912.76/s  (0.195s, 5260.92/s)  LR: 2.553e-04  Data: 0.404 (0.048)
Train: 199 [ 350/1251 ( 28%)]  Loss: 4.146 (4.05)  Time: 0.165s, 6201.61/s  (0.193s, 5317.86/s)  LR: 2.553e-04  Data: 0.023 (0.046)
Train: 199 [ 400/1251 ( 32%)]  Loss: 4.224 (4.07)  Time: 0.175s, 5845.43/s  (0.192s, 5329.11/s)  LR: 2.553e-04  Data: 0.028 (0.045)
Train: 199 [ 450/1251 ( 36%)]  Loss: 3.990 (4.06)  Time: 0.340s, 3011.27/s  (0.193s, 5313.09/s)  LR: 2.553e-04  Data: 0.030 (0.043)
Train: 199 [ 500/1251 ( 40%)]  Loss: 3.999 (4.06)  Time: 0.164s, 6252.78/s  (0.192s, 5341.97/s)  LR: 2.553e-04  Data: 0.025 (0.042)
Train: 199 [ 550/1251 ( 44%)]  Loss: 3.944 (4.05)  Time: 0.168s, 6109.29/s  (0.192s, 5340.72/s)  LR: 2.553e-04  Data: 0.039 (0.040)
Train: 199 [ 600/1251 ( 48%)]  Loss: 4.398 (4.07)  Time: 0.147s, 6971.70/s  (0.192s, 5346.73/s)  LR: 2.553e-04  Data: 0.023 (0.039)
Train: 199 [ 650/1251 ( 52%)]  Loss: 4.187 (4.08)  Time: 0.175s, 5861.58/s  (0.192s, 5341.16/s)  LR: 2.553e-04  Data: 0.024 (0.039)
Train: 199 [ 700/1251 ( 56%)]  Loss: 4.250 (4.09)  Time: 0.162s, 6325.00/s  (0.192s, 5337.99/s)  LR: 2.553e-04  Data: 0.020 (0.038)
Train: 199 [ 750/1251 ( 60%)]  Loss: 4.030 (4.09)  Time: 0.180s, 5692.28/s  (0.192s, 5337.23/s)  LR: 2.553e-04  Data: 0.029 (0.037)
Train: 199 [ 800/1251 ( 64%)]  Loss: 4.332 (4.10)  Time: 0.426s, 2406.49/s  (0.192s, 5335.95/s)  LR: 2.553e-04  Data: 0.275 (0.037)
Train: 199 [ 850/1251 ( 68%)]  Loss: 3.881 (4.09)  Time: 0.209s, 4896.90/s  (0.192s, 5333.91/s)  LR: 2.553e-04  Data: 0.026 (0.038)
Train: 199 [ 900/1251 ( 72%)]  Loss: 4.520 (4.11)  Time: 0.181s, 5642.48/s  (0.192s, 5344.51/s)  LR: 2.553e-04  Data: 0.021 (0.038)
Train: 199 [ 950/1251 ( 76%)]  Loss: 4.451 (4.13)  Time: 0.157s, 6513.78/s  (0.192s, 5346.69/s)  LR: 2.553e-04  Data: 0.024 (0.037)
Train: 199 [1000/1251 ( 80%)]  Loss: 4.179 (4.13)  Time: 0.262s, 3901.55/s  (0.192s, 5337.81/s)  LR: 2.553e-04  Data: 0.030 (0.037)
Train: 199 [1050/1251 ( 84%)]  Loss: 3.840 (4.12)  Time: 0.164s, 6253.61/s  (0.192s, 5331.89/s)  LR: 2.553e-04  Data: 0.038 (0.037)
Train: 199 [1100/1251 ( 88%)]  Loss: 4.027 (4.12)  Time: 0.175s, 5857.41/s  (0.192s, 5336.28/s)  LR: 2.553e-04  Data: 0.022 (0.036)
Train: 199 [1150/1251 ( 92%)]  Loss: 4.577 (4.13)  Time: 0.159s, 6438.25/s  (0.192s, 5332.72/s)  LR: 2.553e-04  Data: 0.022 (0.037)
Train: 199 [1200/1251 ( 96%)]  Loss: 4.290 (4.14)  Time: 0.174s, 5876.69/s  (0.192s, 5331.93/s)  LR: 2.553e-04  Data: 0.029 (0.037)
Train: 199 [1250/1251 (100%)]  Loss: 4.384 (4.15)  Time: 0.116s, 8846.96/s  (0.192s, 5344.89/s)  LR: 2.553e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.826 (1.826)  Loss:  1.0080 (1.0080)  Acc@1: 83.9844 (83.9844)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0918 (1.6788)  Acc@1: 81.0142 (67.4660)  Acc@5: 93.9858 (88.1040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-184.pth.tar', 67.07000002441406)

Train: 200 [   0/1251 (  0%)]  Loss: 4.381 (4.38)  Time: 1.897s,  539.81/s  (1.897s,  539.81/s)  LR: 2.507e-04  Data: 1.768 (1.768)
Train: 200 [  50/1251 (  4%)]  Loss: 3.710 (4.05)  Time: 0.160s, 6411.26/s  (0.221s, 4635.42/s)  LR: 2.507e-04  Data: 0.025 (0.074)
Train: 200 [ 100/1251 (  8%)]  Loss: 3.920 (4.00)  Time: 0.179s, 5711.66/s  (0.203s, 5034.43/s)  LR: 2.507e-04  Data: 0.034 (0.052)
Train: 200 [ 150/1251 ( 12%)]  Loss: 3.813 (3.96)  Time: 0.165s, 6210.33/s  (0.199s, 5138.70/s)  LR: 2.507e-04  Data: 0.031 (0.045)
Train: 200 [ 200/1251 ( 16%)]  Loss: 3.899 (3.94)  Time: 0.176s, 5808.94/s  (0.199s, 5149.64/s)  LR: 2.507e-04  Data: 0.028 (0.045)
Train: 200 [ 250/1251 ( 20%)]  Loss: 4.188 (3.99)  Time: 0.189s, 5418.64/s  (0.196s, 5220.78/s)  LR: 2.507e-04  Data: 0.029 (0.044)
Train: 200 [ 300/1251 ( 24%)]  Loss: 4.128 (4.01)  Time: 0.177s, 5769.35/s  (0.195s, 5248.71/s)  LR: 2.507e-04  Data: 0.030 (0.043)
Train: 200 [ 350/1251 ( 28%)]  Loss: 4.028 (4.01)  Time: 0.188s, 5440.89/s  (0.195s, 5263.83/s)  LR: 2.507e-04  Data: 0.044 (0.044)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 200 [ 400/1251 ( 32%)]  Loss: 4.172 (4.03)  Time: 0.177s, 5777.94/s  (0.194s, 5291.59/s)  LR: 2.507e-04  Data: 0.024 (0.044)
Train: 200 [ 450/1251 ( 36%)]  Loss: 4.389 (4.06)  Time: 0.178s, 5763.18/s  (0.193s, 5309.50/s)  LR: 2.507e-04  Data: 0.025 (0.044)
Train: 200 [ 500/1251 ( 40%)]  Loss: 3.871 (4.05)  Time: 0.164s, 6233.97/s  (0.192s, 5330.38/s)  LR: 2.507e-04  Data: 0.028 (0.043)
Train: 200 [ 550/1251 ( 44%)]  Loss: 4.054 (4.05)  Time: 0.181s, 5646.84/s  (0.193s, 5319.40/s)  LR: 2.507e-04  Data: 0.021 (0.041)
Train: 200 [ 600/1251 ( 48%)]  Loss: 4.554 (4.09)  Time: 0.195s, 5238.62/s  (0.192s, 5332.42/s)  LR: 2.507e-04  Data: 0.020 (0.040)
Train: 200 [ 650/1251 ( 52%)]  Loss: 4.109 (4.09)  Time: 0.191s, 5362.30/s  (0.192s, 5320.64/s)  LR: 2.507e-04  Data: 0.029 (0.039)
Train: 200 [ 700/1251 ( 56%)]  Loss: 4.191 (4.09)  Time: 0.181s, 5656.06/s  (0.192s, 5329.80/s)  LR: 2.507e-04  Data: 0.026 (0.038)
Train: 200 [ 750/1251 ( 60%)]  Loss: 3.782 (4.07)  Time: 0.161s, 6353.63/s  (0.192s, 5332.85/s)  LR: 2.507e-04  Data: 0.023 (0.038)
Train: 200 [ 800/1251 ( 64%)]  Loss: 3.889 (4.06)  Time: 0.177s, 5791.89/s  (0.192s, 5341.99/s)  LR: 2.507e-04  Data: 0.028 (0.037)
Train: 200 [ 850/1251 ( 68%)]  Loss: 4.290 (4.08)  Time: 0.193s, 5310.08/s  (0.192s, 5338.08/s)  LR: 2.507e-04  Data: 0.022 (0.037)
Train: 200 [ 900/1251 ( 72%)]  Loss: 4.295 (4.09)  Time: 0.173s, 5932.05/s  (0.192s, 5337.07/s)  LR: 2.507e-04  Data: 0.029 (0.037)
Train: 200 [ 950/1251 ( 76%)]  Loss: 3.828 (4.07)  Time: 0.171s, 5991.85/s  (0.192s, 5329.39/s)  LR: 2.507e-04  Data: 0.028 (0.037)
Train: 200 [1000/1251 ( 80%)]  Loss: 4.105 (4.08)  Time: 0.185s, 5526.97/s  (0.192s, 5324.08/s)  LR: 2.507e-04  Data: 0.032 (0.038)
Train: 200 [1050/1251 ( 84%)]  Loss: 3.968 (4.07)  Time: 0.163s, 6273.70/s  (0.193s, 5317.71/s)  LR: 2.507e-04  Data: 0.025 (0.038)
Train: 200 [1100/1251 ( 88%)]  Loss: 3.842 (4.06)  Time: 0.167s, 6115.43/s  (0.193s, 5317.79/s)  LR: 2.507e-04  Data: 0.031 (0.037)
Train: 200 [1150/1251 ( 92%)]  Loss: 4.248 (4.07)  Time: 0.169s, 6053.67/s  (0.193s, 5314.40/s)  LR: 2.507e-04  Data: 0.025 (0.037)
Train: 200 [1200/1251 ( 96%)]  Loss: 4.501 (4.09)  Time: 0.176s, 5818.52/s  (0.193s, 5318.62/s)  LR: 2.507e-04  Data: 0.035 (0.037)
Train: 200 [1250/1251 (100%)]  Loss: 4.010 (4.08)  Time: 0.124s, 8289.39/s  (0.192s, 5333.05/s)  LR: 2.507e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.812 (1.812)  Loss:  1.0130 (1.0130)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.1699 (1.7091)  Acc@1: 81.2500 (67.4600)  Acc@5: 94.2217 (87.8660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-192.pth.tar', 67.10800005126953)

Train: 201 [   0/1251 (  0%)]  Loss: 4.073 (4.07)  Time: 1.675s,  611.35/s  (1.675s,  611.35/s)  LR: 2.462e-04  Data: 1.531 (1.531)
Train: 201 [  50/1251 (  4%)]  Loss: 4.226 (4.15)  Time: 0.169s, 6070.22/s  (0.223s, 4586.92/s)  LR: 2.462e-04  Data: 0.023 (0.064)
Train: 201 [ 100/1251 (  8%)]  Loss: 3.686 (3.99)  Time: 0.166s, 6153.44/s  (0.210s, 4867.18/s)  LR: 2.462e-04  Data: 0.018 (0.046)
Train: 201 [ 150/1251 ( 12%)]  Loss: 4.125 (4.03)  Time: 0.176s, 5815.26/s  (0.203s, 5052.42/s)  LR: 2.462e-04  Data: 0.029 (0.040)
Train: 201 [ 200/1251 ( 16%)]  Loss: 4.159 (4.05)  Time: 0.173s, 5926.10/s  (0.198s, 5165.94/s)  LR: 2.462e-04  Data: 0.025 (0.037)
Train: 201 [ 250/1251 ( 20%)]  Loss: 4.237 (4.08)  Time: 0.180s, 5693.13/s  (0.197s, 5206.04/s)  LR: 2.462e-04  Data: 0.025 (0.036)
Train: 201 [ 300/1251 ( 24%)]  Loss: 4.036 (4.08)  Time: 0.175s, 5844.04/s  (0.194s, 5266.95/s)  LR: 2.462e-04  Data: 0.029 (0.034)
Train: 201 [ 350/1251 ( 28%)]  Loss: 4.382 (4.12)  Time: 0.281s, 3645.07/s  (0.195s, 5257.40/s)  LR: 2.462e-04  Data: 0.030 (0.034)
Train: 201 [ 400/1251 ( 32%)]  Loss: 3.805 (4.08)  Time: 0.314s, 3262.36/s  (0.194s, 5273.72/s)  LR: 2.462e-04  Data: 0.053 (0.033)
Train: 201 [ 450/1251 ( 36%)]  Loss: 4.154 (4.09)  Time: 0.163s, 6266.89/s  (0.194s, 5275.38/s)  LR: 2.462e-04  Data: 0.032 (0.033)
Train: 201 [ 500/1251 ( 40%)]  Loss: 4.246 (4.10)  Time: 0.170s, 6018.88/s  (0.193s, 5296.76/s)  LR: 2.462e-04  Data: 0.025 (0.032)
Train: 201 [ 550/1251 ( 44%)]  Loss: 4.353 (4.12)  Time: 0.184s, 5569.06/s  (0.194s, 5288.65/s)  LR: 2.462e-04  Data: 0.032 (0.032)
Train: 201 [ 600/1251 ( 48%)]  Loss: 3.819 (4.10)  Time: 0.179s, 5727.13/s  (0.194s, 5290.62/s)  LR: 2.462e-04  Data: 0.028 (0.032)
Train: 201 [ 650/1251 ( 52%)]  Loss: 4.038 (4.10)  Time: 0.184s, 5552.20/s  (0.193s, 5303.04/s)  LR: 2.462e-04  Data: 0.027 (0.032)
Train: 201 [ 700/1251 ( 56%)]  Loss: 4.259 (4.11)  Time: 0.173s, 5925.68/s  (0.193s, 5312.01/s)  LR: 2.462e-04  Data: 0.029 (0.031)
Train: 201 [ 750/1251 ( 60%)]  Loss: 3.941 (4.10)  Time: 0.157s, 6516.78/s  (0.193s, 5312.32/s)  LR: 2.462e-04  Data: 0.024 (0.031)
Train: 201 [ 800/1251 ( 64%)]  Loss: 3.859 (4.08)  Time: 0.164s, 6231.26/s  (0.193s, 5306.12/s)  LR: 2.462e-04  Data: 0.031 (0.031)
Train: 201 [ 850/1251 ( 68%)]  Loss: 4.133 (4.09)  Time: 0.159s, 6423.46/s  (0.193s, 5305.16/s)  LR: 2.462e-04  Data: 0.026 (0.031)
Train: 201 [ 900/1251 ( 72%)]  Loss: 4.367 (4.10)  Time: 0.162s, 6340.20/s  (0.193s, 5310.15/s)  LR: 2.462e-04  Data: 0.022 (0.031)
Train: 201 [ 950/1251 ( 76%)]  Loss: 4.071 (4.10)  Time: 0.171s, 5982.78/s  (0.193s, 5312.86/s)  LR: 2.462e-04  Data: 0.032 (0.031)
Train: 201 [1000/1251 ( 80%)]  Loss: 3.557 (4.07)  Time: 0.182s, 5630.59/s  (0.193s, 5303.75/s)  LR: 2.462e-04  Data: 0.029 (0.030)
Train: 201 [1050/1251 ( 84%)]  Loss: 4.150 (4.08)  Time: 0.277s, 3698.27/s  (0.193s, 5300.47/s)  LR: 2.462e-04  Data: 0.022 (0.030)
Train: 201 [1100/1251 ( 88%)]  Loss: 3.971 (4.07)  Time: 0.177s, 5799.58/s  (0.194s, 5291.79/s)  LR: 2.462e-04  Data: 0.028 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 201 [1150/1251 ( 92%)]  Loss: 3.871 (4.06)  Time: 0.169s, 6044.07/s  (0.193s, 5293.72/s)  LR: 2.462e-04  Data: 0.028 (0.030)
Train: 201 [1200/1251 ( 96%)]  Loss: 4.500 (4.08)  Time: 0.186s, 5512.26/s  (0.193s, 5298.74/s)  LR: 2.462e-04  Data: 0.031 (0.030)
Train: 201 [1250/1251 (100%)]  Loss: 3.953 (4.08)  Time: 0.114s, 9012.80/s  (0.193s, 5310.62/s)  LR: 2.462e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.843 (1.843)  Loss:  0.9919 (0.9919)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.1172 (95.1172)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1142 (1.6531)  Acc@1: 81.3679 (67.5760)  Acc@5: 94.3396 (87.9980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-191.pth.tar', 67.13000013671875)

Train: 202 [   0/1251 (  0%)]  Loss: 3.608 (3.61)  Time: 1.878s,  545.37/s  (1.878s,  545.37/s)  LR: 2.417e-04  Data: 1.748 (1.748)
Train: 202 [  50/1251 (  4%)]  Loss: 3.815 (3.71)  Time: 0.157s, 6539.35/s  (0.220s, 4646.72/s)  LR: 2.417e-04  Data: 0.031 (0.061)
Train: 202 [ 100/1251 (  8%)]  Loss: 4.254 (3.89)  Time: 0.175s, 5863.25/s  (0.205s, 4988.88/s)  LR: 2.417e-04  Data: 0.030 (0.045)
Train: 202 [ 150/1251 ( 12%)]  Loss: 3.955 (3.91)  Time: 0.160s, 6381.60/s  (0.199s, 5134.58/s)  LR: 2.417e-04  Data: 0.029 (0.040)
Train: 202 [ 200/1251 ( 16%)]  Loss: 3.925 (3.91)  Time: 0.164s, 6231.73/s  (0.197s, 5185.58/s)  LR: 2.417e-04  Data: 0.034 (0.037)
Train: 202 [ 250/1251 ( 20%)]  Loss: 4.247 (3.97)  Time: 0.156s, 6554.14/s  (0.196s, 5218.11/s)  LR: 2.417e-04  Data: 0.030 (0.036)
Train: 202 [ 300/1251 ( 24%)]  Loss: 4.271 (4.01)  Time: 0.186s, 5499.18/s  (0.195s, 5248.49/s)  LR: 2.417e-04  Data: 0.024 (0.035)
Train: 202 [ 350/1251 ( 28%)]  Loss: 4.368 (4.06)  Time: 0.168s, 6096.14/s  (0.195s, 5245.92/s)  LR: 2.417e-04  Data: 0.028 (0.034)
Train: 202 [ 400/1251 ( 32%)]  Loss: 3.641 (4.01)  Time: 0.160s, 6396.59/s  (0.195s, 5263.03/s)  LR: 2.417e-04  Data: 0.029 (0.033)
Train: 202 [ 450/1251 ( 36%)]  Loss: 3.686 (3.98)  Time: 0.173s, 5933.64/s  (0.194s, 5288.41/s)  LR: 2.417e-04  Data: 0.030 (0.033)
Train: 202 [ 500/1251 ( 40%)]  Loss: 4.618 (4.04)  Time: 0.149s, 6852.25/s  (0.193s, 5304.78/s)  LR: 2.417e-04  Data: 0.026 (0.032)
Train: 202 [ 550/1251 ( 44%)]  Loss: 3.725 (4.01)  Time: 0.165s, 6209.44/s  (0.192s, 5329.12/s)  LR: 2.417e-04  Data: 0.030 (0.032)
Train: 202 [ 600/1251 ( 48%)]  Loss: 4.229 (4.03)  Time: 0.519s, 1973.14/s  (0.193s, 5302.09/s)  LR: 2.417e-04  Data: 0.022 (0.031)
Train: 202 [ 650/1251 ( 52%)]  Loss: 4.118 (4.03)  Time: 0.169s, 6054.02/s  (0.192s, 5324.07/s)  LR: 2.417e-04  Data: 0.027 (0.031)
Train: 202 [ 700/1251 ( 56%)]  Loss: 4.118 (4.04)  Time: 0.175s, 5846.69/s  (0.192s, 5322.44/s)  LR: 2.417e-04  Data: 0.028 (0.031)
Train: 202 [ 750/1251 ( 60%)]  Loss: 4.102 (4.04)  Time: 0.193s, 5314.05/s  (0.192s, 5327.19/s)  LR: 2.417e-04  Data: 0.030 (0.031)
Train: 202 [ 800/1251 ( 64%)]  Loss: 4.428 (4.07)  Time: 0.232s, 4420.82/s  (0.192s, 5328.13/s)  LR: 2.417e-04  Data: 0.029 (0.031)
Train: 202 [ 850/1251 ( 68%)]  Loss: 4.185 (4.07)  Time: 0.168s, 6107.82/s  (0.192s, 5323.76/s)  LR: 2.417e-04  Data: 0.035 (0.031)
Train: 202 [ 900/1251 ( 72%)]  Loss: 4.205 (4.08)  Time: 0.173s, 5907.54/s  (0.192s, 5324.39/s)  LR: 2.417e-04  Data: 0.037 (0.031)
Train: 202 [ 950/1251 ( 76%)]  Loss: 4.044 (4.08)  Time: 0.188s, 5436.19/s  (0.193s, 5311.91/s)  LR: 2.417e-04  Data: 0.059 (0.030)
Train: 202 [1000/1251 ( 80%)]  Loss: 4.150 (4.08)  Time: 0.155s, 6593.23/s  (0.192s, 5323.65/s)  LR: 2.417e-04  Data: 0.036 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 202 [1050/1251 ( 84%)]  Loss: 4.441 (4.10)  Time: 0.164s, 6254.28/s  (0.193s, 5316.58/s)  LR: 2.417e-04  Data: 0.025 (0.030)
Train: 202 [1100/1251 ( 88%)]  Loss: 4.114 (4.10)  Time: 0.185s, 5520.73/s  (0.193s, 5316.76/s)  LR: 2.417e-04  Data: 0.032 (0.030)
Train: 202 [1150/1251 ( 92%)]  Loss: 4.035 (4.10)  Time: 0.396s, 2587.81/s  (0.193s, 5297.83/s)  LR: 2.417e-04  Data: 0.031 (0.030)
Train: 202 [1200/1251 ( 96%)]  Loss: 4.166 (4.10)  Time: 0.177s, 5776.66/s  (0.193s, 5299.26/s)  LR: 2.417e-04  Data: 0.036 (0.030)
Train: 202 [1250/1251 (100%)]  Loss: 4.469 (4.11)  Time: 0.113s, 9052.04/s  (0.193s, 5315.71/s)  LR: 2.417e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.798 (1.798)  Loss:  1.0779 (1.0779)  Acc@1: 82.8125 (82.8125)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1759 (1.7399)  Acc@1: 80.8962 (67.6300)  Acc@5: 94.9292 (87.8800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-195.pth.tar', 67.13999997558594)

Train: 203 [   0/1251 (  0%)]  Loss: 3.671 (3.67)  Time: 1.992s,  514.02/s  (1.992s,  514.02/s)  LR: 2.373e-04  Data: 1.861 (1.861)
Train: 203 [  50/1251 (  4%)]  Loss: 4.347 (4.01)  Time: 0.172s, 5967.74/s  (0.220s, 4654.20/s)  LR: 2.373e-04  Data: 0.035 (0.071)
Train: 203 [ 100/1251 (  8%)]  Loss: 3.943 (3.99)  Time: 0.185s, 5532.88/s  (0.204s, 5013.37/s)  LR: 2.373e-04  Data: 0.028 (0.055)
Train: 203 [ 150/1251 ( 12%)]  Loss: 3.785 (3.94)  Time: 0.187s, 5474.44/s  (0.200s, 5115.09/s)  LR: 2.373e-04  Data: 0.025 (0.048)
Train: 203 [ 200/1251 ( 16%)]  Loss: 4.039 (3.96)  Time: 0.176s, 5820.26/s  (0.200s, 5132.35/s)  LR: 2.373e-04  Data: 0.044 (0.043)
Train: 203 [ 250/1251 ( 20%)]  Loss: 4.096 (3.98)  Time: 0.189s, 5409.58/s  (0.196s, 5217.27/s)  LR: 2.373e-04  Data: 0.036 (0.040)
Train: 203 [ 300/1251 ( 24%)]  Loss: 4.049 (3.99)  Time: 0.186s, 5496.59/s  (0.196s, 5217.84/s)  LR: 2.373e-04  Data: 0.025 (0.038)
Train: 203 [ 350/1251 ( 28%)]  Loss: 4.073 (4.00)  Time: 0.170s, 6037.85/s  (0.195s, 5252.41/s)  LR: 2.373e-04  Data: 0.023 (0.037)
Train: 203 [ 400/1251 ( 32%)]  Loss: 3.864 (3.99)  Time: 0.157s, 6522.24/s  (0.194s, 5270.41/s)  LR: 2.373e-04  Data: 0.028 (0.036)
Train: 203 [ 450/1251 ( 36%)]  Loss: 4.453 (4.03)  Time: 0.277s, 3693.76/s  (0.194s, 5269.36/s)  LR: 2.373e-04  Data: 0.153 (0.036)
Train: 203 [ 500/1251 ( 40%)]  Loss: 4.514 (4.08)  Time: 0.173s, 5917.60/s  (0.194s, 5274.91/s)  LR: 2.373e-04  Data: 0.031 (0.035)
Train: 203 [ 550/1251 ( 44%)]  Loss: 3.903 (4.06)  Time: 0.161s, 6356.88/s  (0.194s, 5287.05/s)  LR: 2.373e-04  Data: 0.024 (0.035)
Train: 203 [ 600/1251 ( 48%)]  Loss: 4.272 (4.08)  Time: 0.176s, 5824.81/s  (0.193s, 5298.24/s)  LR: 2.373e-04  Data: 0.028 (0.034)
Train: 203 [ 650/1251 ( 52%)]  Loss: 4.029 (4.07)  Time: 0.168s, 6107.60/s  (0.193s, 5302.23/s)  LR: 2.373e-04  Data: 0.037 (0.034)
Train: 203 [ 700/1251 ( 56%)]  Loss: 4.080 (4.07)  Time: 0.198s, 5164.85/s  (0.193s, 5317.31/s)  LR: 2.373e-04  Data: 0.029 (0.033)
Train: 203 [ 750/1251 ( 60%)]  Loss: 4.059 (4.07)  Time: 0.163s, 6266.30/s  (0.193s, 5318.90/s)  LR: 2.373e-04  Data: 0.026 (0.033)
Train: 203 [ 800/1251 ( 64%)]  Loss: 4.564 (4.10)  Time: 0.196s, 5222.05/s  (0.193s, 5306.80/s)  LR: 2.373e-04  Data: 0.035 (0.033)
Train: 203 [ 850/1251 ( 68%)]  Loss: 3.890 (4.09)  Time: 0.190s, 5400.29/s  (0.193s, 5309.46/s)  LR: 2.373e-04  Data: 0.024 (0.032)
Train: 203 [ 900/1251 ( 72%)]  Loss: 4.219 (4.10)  Time: 0.181s, 5667.13/s  (0.193s, 5318.47/s)  LR: 2.373e-04  Data: 0.023 (0.032)
Train: 203 [ 950/1251 ( 76%)]  Loss: 3.963 (4.09)  Time: 0.175s, 5844.88/s  (0.193s, 5309.98/s)  LR: 2.373e-04  Data: 0.032 (0.032)
Train: 203 [1000/1251 ( 80%)]  Loss: 4.331 (4.10)  Time: 0.200s, 5129.32/s  (0.193s, 5307.49/s)  LR: 2.373e-04  Data: 0.028 (0.032)
Train: 203 [1050/1251 ( 84%)]  Loss: 4.203 (4.11)  Time: 0.155s, 6622.86/s  (0.193s, 5306.26/s)  LR: 2.373e-04  Data: 0.030 (0.032)
Train: 203 [1100/1251 ( 88%)]  Loss: 4.101 (4.11)  Time: 0.188s, 5453.47/s  (0.193s, 5308.38/s)  LR: 2.373e-04  Data: 0.027 (0.031)
Train: 203 [1150/1251 ( 92%)]  Loss: 4.323 (4.12)  Time: 0.159s, 6433.55/s  (0.193s, 5302.36/s)  LR: 2.373e-04  Data: 0.028 (0.031)
Train: 203 [1200/1251 ( 96%)]  Loss: 3.482 (4.09)  Time: 0.166s, 6161.72/s  (0.193s, 5310.52/s)  LR: 2.373e-04  Data: 0.036 (0.031)
Train: 203 [1250/1251 (100%)]  Loss: 3.990 (4.09)  Time: 0.113s, 9026.64/s  (0.192s, 5326.18/s)  LR: 2.373e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.859 (1.859)  Loss:  0.9919 (0.9919)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0221 (1.6286)  Acc@1: 82.6651 (67.9920)  Acc@5: 95.5189 (88.1160)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-196.pth.tar', 67.30400004638672)

Train: 204 [   0/1251 (  0%)]  Loss: 4.055 (4.05)  Time: 1.836s,  557.64/s  (1.836s,  557.64/s)  LR: 2.329e-04  Data: 1.704 (1.704)
Train: 204 [  50/1251 (  4%)]  Loss: 3.843 (3.95)  Time: 0.187s, 5479.74/s  (0.227s, 4512.87/s)  LR: 2.329e-04  Data: 0.027 (0.079)
Train: 204 [ 100/1251 (  8%)]  Loss: 4.115 (4.00)  Time: 0.163s, 6269.92/s  (0.209s, 4909.62/s)  LR: 2.329e-04  Data: 0.026 (0.063)
Train: 204 [ 150/1251 ( 12%)]  Loss: 4.124 (4.03)  Time: 0.191s, 5370.74/s  (0.203s, 5042.34/s)  LR: 2.329e-04  Data: 0.020 (0.058)
Train: 204 [ 200/1251 ( 16%)]  Loss: 4.267 (4.08)  Time: 0.167s, 6149.07/s  (0.199s, 5143.50/s)  LR: 2.329e-04  Data: 0.030 (0.054)
Train: 204 [ 250/1251 ( 20%)]  Loss: 4.148 (4.09)  Time: 0.179s, 5710.19/s  (0.198s, 5177.77/s)  LR: 2.329e-04  Data: 0.024 (0.052)
Train: 204 [ 300/1251 ( 24%)]  Loss: 4.234 (4.11)  Time: 0.179s, 5734.45/s  (0.196s, 5212.84/s)  LR: 2.329e-04  Data: 0.025 (0.051)
Train: 204 [ 350/1251 ( 28%)]  Loss: 3.955 (4.09)  Time: 0.158s, 6480.88/s  (0.195s, 5263.21/s)  LR: 2.329e-04  Data: 0.022 (0.049)
Train: 204 [ 400/1251 ( 32%)]  Loss: 4.160 (4.10)  Time: 0.165s, 6200.73/s  (0.194s, 5290.01/s)  LR: 2.329e-04  Data: 0.022 (0.048)
Train: 204 [ 450/1251 ( 36%)]  Loss: 4.164 (4.11)  Time: 0.169s, 6071.05/s  (0.193s, 5297.07/s)  LR: 2.329e-04  Data: 0.024 (0.048)
Train: 204 [ 500/1251 ( 40%)]  Loss: 3.795 (4.08)  Time: 0.181s, 5649.35/s  (0.193s, 5295.61/s)  LR: 2.329e-04  Data: 0.034 (0.048)
Train: 204 [ 550/1251 ( 44%)]  Loss: 4.237 (4.09)  Time: 0.294s, 3482.78/s  (0.193s, 5316.73/s)  LR: 2.329e-04  Data: 0.024 (0.047)
Train: 204 [ 600/1251 ( 48%)]  Loss: 4.297 (4.11)  Time: 0.189s, 5406.90/s  (0.193s, 5318.27/s)  LR: 2.329e-04  Data: 0.040 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 204 [ 650/1251 ( 52%)]  Loss: 4.018 (4.10)  Time: 0.162s, 6333.30/s  (0.192s, 5324.75/s)  LR: 2.329e-04  Data: 0.024 (0.044)
Train: 204 [ 700/1251 ( 56%)]  Loss: 4.296 (4.11)  Time: 0.160s, 6394.15/s  (0.192s, 5323.68/s)  LR: 2.329e-04  Data: 0.036 (0.043)
Train: 204 [ 750/1251 ( 60%)]  Loss: 4.310 (4.13)  Time: 0.277s, 3692.96/s  (0.192s, 5325.43/s)  LR: 2.329e-04  Data: 0.029 (0.042)
Train: 204 [ 800/1251 ( 64%)]  Loss: 4.240 (4.13)  Time: 0.205s, 4989.63/s  (0.192s, 5329.70/s)  LR: 2.329e-04  Data: 0.026 (0.041)
Train: 204 [ 850/1251 ( 68%)]  Loss: 4.279 (4.14)  Time: 0.178s, 5762.63/s  (0.192s, 5324.96/s)  LR: 2.329e-04  Data: 0.034 (0.040)
Train: 204 [ 900/1251 ( 72%)]  Loss: 4.279 (4.15)  Time: 0.192s, 5339.75/s  (0.192s, 5325.49/s)  LR: 2.329e-04  Data: 0.026 (0.039)
Train: 204 [ 950/1251 ( 76%)]  Loss: 3.862 (4.13)  Time: 0.178s, 5753.52/s  (0.192s, 5323.87/s)  LR: 2.329e-04  Data: 0.026 (0.039)
Train: 204 [1000/1251 ( 80%)]  Loss: 4.297 (4.14)  Time: 0.165s, 6198.51/s  (0.192s, 5330.38/s)  LR: 2.329e-04  Data: 0.024 (0.038)
Train: 204 [1050/1251 ( 84%)]  Loss: 3.946 (4.13)  Time: 0.159s, 6447.38/s  (0.192s, 5324.36/s)  LR: 2.329e-04  Data: 0.026 (0.038)
Train: 204 [1100/1251 ( 88%)]  Loss: 4.200 (4.14)  Time: 0.167s, 6125.39/s  (0.193s, 5312.66/s)  LR: 2.329e-04  Data: 0.029 (0.037)
Train: 204 [1150/1251 ( 92%)]  Loss: 3.912 (4.13)  Time: 0.167s, 6140.22/s  (0.193s, 5315.65/s)  LR: 2.329e-04  Data: 0.033 (0.037)
Train: 204 [1200/1251 ( 96%)]  Loss: 3.845 (4.12)  Time: 0.606s, 1689.84/s  (0.193s, 5306.76/s)  LR: 2.329e-04  Data: 0.025 (0.036)
Train: 204 [1250/1251 (100%)]  Loss: 4.157 (4.12)  Time: 0.114s, 9017.11/s  (0.192s, 5324.05/s)  LR: 2.329e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.829 (1.829)  Loss:  1.0824 (1.0824)  Acc@1: 83.3008 (83.3008)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1365 (1.7124)  Acc@1: 82.9009 (67.9460)  Acc@5: 95.1651 (88.1200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-194.pth.tar', 67.33599997802735)

Train: 205 [   0/1251 (  0%)]  Loss: 3.885 (3.88)  Time: 1.780s,  575.39/s  (1.780s,  575.39/s)  LR: 2.285e-04  Data: 1.648 (1.648)
Train: 205 [  50/1251 (  4%)]  Loss: 3.713 (3.80)  Time: 0.183s, 5581.14/s  (0.221s, 4630.36/s)  LR: 2.285e-04  Data: 0.031 (0.075)
Train: 205 [ 100/1251 (  8%)]  Loss: 3.998 (3.87)  Time: 0.170s, 6020.12/s  (0.204s, 5028.28/s)  LR: 2.285e-04  Data: 0.029 (0.058)
Train: 205 [ 150/1251 ( 12%)]  Loss: 4.151 (3.94)  Time: 0.162s, 6301.72/s  (0.199s, 5158.06/s)  LR: 2.285e-04  Data: 0.032 (0.050)
Train: 205 [ 200/1251 ( 16%)]  Loss: 3.814 (3.91)  Time: 0.231s, 4435.74/s  (0.199s, 5153.75/s)  LR: 2.285e-04  Data: 0.109 (0.050)
Train: 205 [ 250/1251 ( 20%)]  Loss: 4.395 (3.99)  Time: 0.184s, 5570.64/s  (0.198s, 5184.01/s)  LR: 2.285e-04  Data: 0.024 (0.045)
Train: 205 [ 300/1251 ( 24%)]  Loss: 3.907 (3.98)  Time: 0.168s, 6078.98/s  (0.195s, 5258.52/s)  LR: 2.285e-04  Data: 0.025 (0.042)
Train: 205 [ 350/1251 ( 28%)]  Loss: 4.103 (4.00)  Time: 0.176s, 5825.73/s  (0.195s, 5243.29/s)  LR: 2.285e-04  Data: 0.019 (0.044)
Train: 205 [ 400/1251 ( 32%)]  Loss: 3.757 (3.97)  Time: 0.168s, 6093.79/s  (0.193s, 5299.90/s)  LR: 2.285e-04  Data: 0.025 (0.042)
Train: 205 [ 450/1251 ( 36%)]  Loss: 4.188 (3.99)  Time: 0.168s, 6095.35/s  (0.194s, 5270.07/s)  LR: 2.285e-04  Data: 0.027 (0.040)
Train: 205 [ 500/1251 ( 40%)]  Loss: 3.835 (3.98)  Time: 0.177s, 5791.25/s  (0.193s, 5301.72/s)  LR: 2.285e-04  Data: 0.038 (0.039)
Train: 205 [ 550/1251 ( 44%)]  Loss: 3.930 (3.97)  Time: 0.162s, 6336.72/s  (0.193s, 5299.71/s)  LR: 2.285e-04  Data: 0.024 (0.038)
Train: 205 [ 600/1251 ( 48%)]  Loss: 4.436 (4.01)  Time: 0.175s, 5868.11/s  (0.193s, 5300.55/s)  LR: 2.285e-04  Data: 0.037 (0.037)
Train: 205 [ 650/1251 ( 52%)]  Loss: 3.831 (4.00)  Time: 0.163s, 6276.64/s  (0.193s, 5297.83/s)  LR: 2.285e-04  Data: 0.032 (0.037)
Train: 205 [ 700/1251 ( 56%)]  Loss: 3.758 (3.98)  Time: 0.165s, 6199.80/s  (0.193s, 5317.94/s)  LR: 2.285e-04  Data: 0.028 (0.036)
Train: 205 [ 750/1251 ( 60%)]  Loss: 3.959 (3.98)  Time: 0.168s, 6089.07/s  (0.193s, 5315.30/s)  LR: 2.285e-04  Data: 0.022 (0.036)
Train: 205 [ 800/1251 ( 64%)]  Loss: 4.286 (4.00)  Time: 0.149s, 6863.68/s  (0.193s, 5318.30/s)  LR: 2.285e-04  Data: 0.025 (0.035)
Train: 205 [ 850/1251 ( 68%)]  Loss: 4.613 (4.03)  Time: 0.191s, 5351.18/s  (0.193s, 5317.00/s)  LR: 2.285e-04  Data: 0.020 (0.035)
Train: 205 [ 900/1251 ( 72%)]  Loss: 4.293 (4.04)  Time: 0.206s, 4981.58/s  (0.193s, 5316.10/s)  LR: 2.285e-04  Data: 0.025 (0.034)
Train: 205 [ 950/1251 ( 76%)]  Loss: 4.412 (4.06)  Time: 0.157s, 6538.33/s  (0.193s, 5313.84/s)  LR: 2.285e-04  Data: 0.026 (0.034)
Train: 205 [1000/1251 ( 80%)]  Loss: 4.510 (4.08)  Time: 0.166s, 6165.82/s  (0.193s, 5314.55/s)  LR: 2.285e-04  Data: 0.022 (0.034)
Train: 205 [1050/1251 ( 84%)]  Loss: 4.054 (4.08)  Time: 0.180s, 5684.67/s  (0.193s, 5305.86/s)  LR: 2.285e-04  Data: 0.026 (0.033)
Train: 205 [1100/1251 ( 88%)]  Loss: 3.957 (4.08)  Time: 0.167s, 6145.18/s  (0.193s, 5301.62/s)  LR: 2.285e-04  Data: 0.026 (0.033)
Train: 205 [1150/1251 ( 92%)]  Loss: 3.874 (4.07)  Time: 0.171s, 5981.93/s  (0.193s, 5300.33/s)  LR: 2.285e-04  Data: 0.032 (0.033)
Train: 205 [1200/1251 ( 96%)]  Loss: 4.208 (4.07)  Time: 0.158s, 6485.32/s  (0.193s, 5298.48/s)  LR: 2.285e-04  Data: 0.025 (0.033)
Train: 205 [1250/1251 (100%)]  Loss: 3.986 (4.07)  Time: 0.113s, 9038.95/s  (0.193s, 5308.42/s)  LR: 2.285e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.870 (1.870)  Loss:  1.0595 (1.0595)  Acc@1: 82.4219 (82.4219)  Acc@5: 95.0195 (95.0195)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.0700 (1.6381)  Acc@1: 82.1934 (67.7140)  Acc@5: 95.0472 (87.8740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-200.pth.tar', 67.46)

Train: 206 [   0/1251 (  0%)]  Loss: 4.220 (4.22)  Time: 1.764s,  580.48/s  (1.764s,  580.48/s)  LR: 2.241e-04  Data: 1.642 (1.642)
Train: 206 [  50/1251 (  4%)]  Loss: 3.610 (3.91)  Time: 0.159s, 6442.77/s  (0.227s, 4511.66/s)  LR: 2.241e-04  Data: 0.030 (0.085)
Train: 206 [ 100/1251 (  8%)]  Loss: 4.078 (3.97)  Time: 0.168s, 6099.84/s  (0.205s, 4993.46/s)  LR: 2.241e-04  Data: 0.034 (0.061)
Train: 206 [ 150/1251 ( 12%)]  Loss: 4.114 (4.01)  Time: 0.201s, 5086.87/s  (0.199s, 5145.68/s)  LR: 2.241e-04  Data: 0.024 (0.051)
Train: 206 [ 200/1251 ( 16%)]  Loss: 3.679 (3.94)  Time: 0.263s, 3887.78/s  (0.198s, 5165.90/s)  LR: 2.241e-04  Data: 0.028 (0.045)
Train: 206 [ 250/1251 ( 20%)]  Loss: 3.819 (3.92)  Time: 0.166s, 6179.45/s  (0.195s, 5254.03/s)  LR: 2.241e-04  Data: 0.035 (0.042)
Train: 206 [ 300/1251 ( 24%)]  Loss: 4.001 (3.93)  Time: 0.377s, 2717.34/s  (0.196s, 5233.72/s)  LR: 2.241e-04  Data: 0.254 (0.044)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 206 [ 350/1251 ( 28%)]  Loss: 3.476 (3.87)  Time: 0.157s, 6536.45/s  (0.195s, 5260.65/s)  LR: 2.241e-04  Data: 0.024 (0.044)
Train: 206 [ 400/1251 ( 32%)]  Loss: 4.253 (3.92)  Time: 0.194s, 5280.93/s  (0.194s, 5272.82/s)  LR: 2.241e-04  Data: 0.024 (0.043)
Train: 206 [ 450/1251 ( 36%)]  Loss: 4.100 (3.93)  Time: 0.158s, 6499.91/s  (0.193s, 5300.74/s)  LR: 2.241e-04  Data: 0.020 (0.043)
Train: 206 [ 500/1251 ( 40%)]  Loss: 4.565 (3.99)  Time: 0.163s, 6263.88/s  (0.193s, 5303.63/s)  LR: 2.241e-04  Data: 0.030 (0.043)
Train: 206 [ 550/1251 ( 44%)]  Loss: 3.636 (3.96)  Time: 0.169s, 6056.91/s  (0.193s, 5305.88/s)  LR: 2.241e-04  Data: 0.029 (0.043)
Train: 206 [ 600/1251 ( 48%)]  Loss: 4.280 (3.99)  Time: 0.174s, 5900.13/s  (0.192s, 5321.02/s)  LR: 2.241e-04  Data: 0.025 (0.042)
Train: 206 [ 650/1251 ( 52%)]  Loss: 4.109 (4.00)  Time: 0.172s, 5954.45/s  (0.193s, 5314.18/s)  LR: 2.241e-04  Data: 0.022 (0.042)
Train: 206 [ 700/1251 ( 56%)]  Loss: 4.289 (4.02)  Time: 0.162s, 6313.52/s  (0.193s, 5307.31/s)  LR: 2.241e-04  Data: 0.022 (0.042)
Train: 206 [ 750/1251 ( 60%)]  Loss: 4.207 (4.03)  Time: 0.168s, 6111.21/s  (0.193s, 5311.53/s)  LR: 2.241e-04  Data: 0.027 (0.043)
Train: 206 [ 800/1251 ( 64%)]  Loss: 4.544 (4.06)  Time: 0.179s, 5735.16/s  (0.193s, 5314.72/s)  LR: 2.241e-04  Data: 0.027 (0.043)
Train: 206 [ 850/1251 ( 68%)]  Loss: 3.954 (4.05)  Time: 0.175s, 5837.44/s  (0.193s, 5314.44/s)  LR: 2.241e-04  Data: 0.026 (0.043)
Train: 206 [ 900/1251 ( 72%)]  Loss: 4.551 (4.08)  Time: 0.174s, 5886.57/s  (0.193s, 5306.01/s)  LR: 2.241e-04  Data: 0.023 (0.042)
Train: 206 [ 950/1251 ( 76%)]  Loss: 4.470 (4.10)  Time: 0.268s, 3822.79/s  (0.193s, 5299.95/s)  LR: 2.241e-04  Data: 0.033 (0.042)
Train: 206 [1000/1251 ( 80%)]  Loss: 4.013 (4.09)  Time: 0.165s, 6198.51/s  (0.193s, 5297.55/s)  LR: 2.241e-04  Data: 0.034 (0.041)
Train: 206 [1050/1251 ( 84%)]  Loss: 4.248 (4.10)  Time: 0.187s, 5469.41/s  (0.193s, 5293.06/s)  LR: 2.241e-04  Data: 0.024 (0.040)
Train: 206 [1100/1251 ( 88%)]  Loss: 4.101 (4.10)  Time: 0.169s, 6068.05/s  (0.193s, 5299.14/s)  LR: 2.241e-04  Data: 0.029 (0.040)
Train: 206 [1150/1251 ( 92%)]  Loss: 3.811 (4.09)  Time: 0.396s, 2588.41/s  (0.193s, 5295.86/s)  LR: 2.241e-04  Data: 0.035 (0.039)
Train: 206 [1200/1251 ( 96%)]  Loss: 3.869 (4.08)  Time: 0.171s, 5976.21/s  (0.194s, 5291.91/s)  LR: 2.241e-04  Data: 0.028 (0.039)
Train: 206 [1250/1251 (100%)]  Loss: 3.917 (4.07)  Time: 0.113s, 9032.40/s  (0.193s, 5305.86/s)  LR: 2.241e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.809 (1.809)  Loss:  1.0019 (1.0019)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0907 (1.6470)  Acc@1: 81.6038 (67.7300)  Acc@5: 94.8113 (88.3680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-199.pth.tar', 67.46600002685547)

Train: 207 [   0/1251 (  0%)]  Loss: 4.316 (4.32)  Time: 1.976s,  518.23/s  (1.976s,  518.23/s)  LR: 2.197e-04  Data: 1.850 (1.850)
Train: 207 [  50/1251 (  4%)]  Loss: 4.223 (4.27)  Time: 0.160s, 6406.38/s  (0.229s, 4481.22/s)  LR: 2.197e-04  Data: 0.030 (0.067)
Train: 207 [ 100/1251 (  8%)]  Loss: 3.777 (4.11)  Time: 0.183s, 5602.44/s  (0.211s, 4849.15/s)  LR: 2.197e-04  Data: 0.031 (0.049)
Train: 207 [ 150/1251 ( 12%)]  Loss: 4.351 (4.17)  Time: 0.177s, 5781.00/s  (0.202s, 5070.06/s)  LR: 2.197e-04  Data: 0.023 (0.042)
Train: 207 [ 200/1251 ( 16%)]  Loss: 4.038 (4.14)  Time: 0.162s, 6319.19/s  (0.200s, 5112.56/s)  LR: 2.197e-04  Data: 0.028 (0.039)
Train: 207 [ 250/1251 ( 20%)]  Loss: 4.355 (4.18)  Time: 0.168s, 6095.03/s  (0.198s, 5173.77/s)  LR: 2.197e-04  Data: 0.037 (0.037)
Train: 207 [ 300/1251 ( 24%)]  Loss: 4.187 (4.18)  Time: 0.178s, 5738.11/s  (0.197s, 5201.97/s)  LR: 2.197e-04  Data: 0.028 (0.035)
Train: 207 [ 350/1251 ( 28%)]  Loss: 4.205 (4.18)  Time: 0.173s, 5918.22/s  (0.196s, 5235.46/s)  LR: 2.197e-04  Data: 0.024 (0.035)
Train: 207 [ 400/1251 ( 32%)]  Loss: 4.078 (4.17)  Time: 0.161s, 6373.67/s  (0.195s, 5251.84/s)  LR: 2.197e-04  Data: 0.030 (0.034)
Train: 207 [ 450/1251 ( 36%)]  Loss: 4.053 (4.16)  Time: 0.165s, 6210.70/s  (0.194s, 5269.88/s)  LR: 2.197e-04  Data: 0.040 (0.033)
Train: 207 [ 500/1251 ( 40%)]  Loss: 4.195 (4.16)  Time: 0.163s, 6285.21/s  (0.194s, 5274.15/s)  LR: 2.197e-04  Data: 0.027 (0.033)
Train: 207 [ 550/1251 ( 44%)]  Loss: 4.503 (4.19)  Time: 0.171s, 6000.10/s  (0.194s, 5268.85/s)  LR: 2.197e-04  Data: 0.029 (0.033)
Train: 207 [ 600/1251 ( 48%)]  Loss: 4.249 (4.19)  Time: 0.180s, 5688.56/s  (0.194s, 5284.79/s)  LR: 2.197e-04  Data: 0.028 (0.032)
Train: 207 [ 650/1251 ( 52%)]  Loss: 4.385 (4.21)  Time: 0.190s, 5398.73/s  (0.193s, 5295.35/s)  LR: 2.197e-04  Data: 0.025 (0.032)
Train: 207 [ 700/1251 ( 56%)]  Loss: 3.698 (4.17)  Time: 0.157s, 6506.34/s  (0.194s, 5290.30/s)  LR: 2.197e-04  Data: 0.026 (0.032)
Train: 207 [ 750/1251 ( 60%)]  Loss: 4.256 (4.18)  Time: 0.194s, 5285.45/s  (0.194s, 5290.19/s)  LR: 2.197e-04  Data: 0.036 (0.031)
Train: 207 [ 800/1251 ( 64%)]  Loss: 4.283 (4.19)  Time: 0.176s, 5815.44/s  (0.194s, 5280.90/s)  LR: 2.197e-04  Data: 0.027 (0.031)
Train: 207 [ 850/1251 ( 68%)]  Loss: 4.354 (4.19)  Time: 0.254s, 4031.35/s  (0.194s, 5288.43/s)  LR: 2.197e-04  Data: 0.029 (0.031)
Train: 207 [ 900/1251 ( 72%)]  Loss: 3.890 (4.18)  Time: 0.160s, 6392.28/s  (0.193s, 5293.73/s)  LR: 2.197e-04  Data: 0.026 (0.031)
Train: 207 [ 950/1251 ( 76%)]  Loss: 4.027 (4.17)  Time: 0.187s, 5476.42/s  (0.193s, 5296.71/s)  LR: 2.197e-04  Data: 0.019 (0.031)
Train: 207 [1000/1251 ( 80%)]  Loss: 3.941 (4.16)  Time: 0.170s, 6009.85/s  (0.194s, 5289.41/s)  LR: 2.197e-04  Data: 0.027 (0.031)
Train: 207 [1050/1251 ( 84%)]  Loss: 4.177 (4.16)  Time: 0.161s, 6360.08/s  (0.194s, 5285.35/s)  LR: 2.197e-04  Data: 0.034 (0.031)
Train: 207 [1100/1251 ( 88%)]  Loss: 4.062 (4.16)  Time: 0.156s, 6543.87/s  (0.194s, 5279.90/s)  LR: 2.197e-04  Data: 0.028 (0.031)
Train: 207 [1150/1251 ( 92%)]  Loss: 3.919 (4.15)  Time: 0.153s, 6688.49/s  (0.194s, 5280.81/s)  LR: 2.197e-04  Data: 0.023 (0.031)
Train: 207 [1200/1251 ( 96%)]  Loss: 4.471 (4.16)  Time: 0.165s, 6198.28/s  (0.194s, 5279.07/s)  LR: 2.197e-04  Data: 0.024 (0.031)
Train: 207 [1250/1251 (100%)]  Loss: 4.101 (4.16)  Time: 0.114s, 9012.42/s  (0.193s, 5293.31/s)  LR: 2.197e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.865 (1.865)  Loss:  1.0028 (1.0028)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.1851 (1.6560)  Acc@1: 81.7217 (68.1160)  Acc@5: 94.1038 (88.2260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-190.pth.tar', 67.47000005126954)

Train: 208 [   0/1251 (  0%)]  Loss: 4.238 (4.24)  Time: 1.765s,  580.26/s  (1.765s,  580.26/s)  LR: 2.154e-04  Data: 1.634 (1.634)
Train: 208 [  50/1251 (  4%)]  Loss: 4.289 (4.26)  Time: 0.165s, 6219.93/s  (0.228s, 4482.61/s)  LR: 2.154e-04  Data: 0.021 (0.076)
Train: 208 [ 100/1251 (  8%)]  Loss: 4.304 (4.28)  Time: 0.170s, 6040.02/s  (0.207s, 4942.80/s)  LR: 2.154e-04  Data: 0.030 (0.052)
Train: 208 [ 150/1251 ( 12%)]  Loss: 3.602 (4.11)  Time: 0.170s, 6034.34/s  (0.202s, 5078.03/s)  LR: 2.154e-04  Data: 0.028 (0.049)
Train: 208 [ 200/1251 ( 16%)]  Loss: 4.096 (4.11)  Time: 0.174s, 5884.22/s  (0.199s, 5158.42/s)  LR: 2.154e-04  Data: 0.027 (0.048)
Train: 208 [ 250/1251 ( 20%)]  Loss: 4.308 (4.14)  Time: 0.158s, 6472.57/s  (0.197s, 5210.94/s)  LR: 2.154e-04  Data: 0.027 (0.047)
Train: 208 [ 300/1251 ( 24%)]  Loss: 4.016 (4.12)  Time: 0.171s, 5981.73/s  (0.195s, 5247.69/s)  LR: 2.154e-04  Data: 0.032 (0.047)
Train: 208 [ 350/1251 ( 28%)]  Loss: 4.327 (4.15)  Time: 0.165s, 6224.57/s  (0.194s, 5267.27/s)  LR: 2.154e-04  Data: 0.029 (0.047)
Train: 208 [ 400/1251 ( 32%)]  Loss: 4.457 (4.18)  Time: 0.187s, 5461.50/s  (0.194s, 5278.33/s)  LR: 2.154e-04  Data: 0.026 (0.047)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 208 [ 450/1251 ( 36%)]  Loss: 3.539 (4.12)  Time: 0.177s, 5793.94/s  (0.193s, 5302.04/s)  LR: 2.154e-04  Data: 0.020 (0.046)
Train: 208 [ 500/1251 ( 40%)]  Loss: 3.880 (4.10)  Time: 0.170s, 6030.14/s  (0.193s, 5314.26/s)  LR: 2.154e-04  Data: 0.027 (0.046)
Train: 208 [ 550/1251 ( 44%)]  Loss: 4.193 (4.10)  Time: 0.201s, 5102.91/s  (0.193s, 5302.53/s)  LR: 2.154e-04  Data: 0.028 (0.044)
Train: 208 [ 600/1251 ( 48%)]  Loss: 4.294 (4.12)  Time: 0.166s, 6168.64/s  (0.193s, 5309.34/s)  LR: 2.154e-04  Data: 0.032 (0.043)
Train: 208 [ 650/1251 ( 52%)]  Loss: 4.093 (4.12)  Time: 0.345s, 2967.53/s  (0.193s, 5304.06/s)  LR: 2.154e-04  Data: 0.022 (0.041)
Train: 208 [ 700/1251 ( 56%)]  Loss: 3.766 (4.09)  Time: 0.178s, 5737.62/s  (0.193s, 5310.73/s)  LR: 2.154e-04  Data: 0.023 (0.041)
Train: 208 [ 750/1251 ( 60%)]  Loss: 3.841 (4.08)  Time: 0.171s, 5981.52/s  (0.193s, 5310.24/s)  LR: 2.154e-04  Data: 0.025 (0.040)
Train: 208 [ 800/1251 ( 64%)]  Loss: 4.345 (4.09)  Time: 0.166s, 6154.49/s  (0.193s, 5307.76/s)  LR: 2.154e-04  Data: 0.028 (0.039)
Train: 208 [ 850/1251 ( 68%)]  Loss: 3.854 (4.08)  Time: 0.319s, 3208.23/s  (0.193s, 5308.88/s)  LR: 2.154e-04  Data: 0.025 (0.038)
Train: 208 [ 900/1251 ( 72%)]  Loss: 4.239 (4.09)  Time: 0.310s, 3300.67/s  (0.193s, 5304.32/s)  LR: 2.154e-04  Data: 0.028 (0.038)
Train: 208 [ 950/1251 ( 76%)]  Loss: 4.119 (4.09)  Time: 0.165s, 6197.85/s  (0.193s, 5299.47/s)  LR: 2.154e-04  Data: 0.025 (0.037)
Train: 208 [1000/1251 ( 80%)]  Loss: 3.717 (4.07)  Time: 0.176s, 5821.88/s  (0.193s, 5294.46/s)  LR: 2.154e-04  Data: 0.026 (0.037)
Train: 208 [1050/1251 ( 84%)]  Loss: 4.338 (4.08)  Time: 0.166s, 6182.78/s  (0.193s, 5295.82/s)  LR: 2.154e-04  Data: 0.027 (0.037)
Train: 208 [1100/1251 ( 88%)]  Loss: 4.100 (4.09)  Time: 0.211s, 4863.48/s  (0.193s, 5296.52/s)  LR: 2.154e-04  Data: 0.030 (0.036)
Train: 208 [1150/1251 ( 92%)]  Loss: 3.877 (4.08)  Time: 0.165s, 6222.48/s  (0.193s, 5294.52/s)  LR: 2.154e-04  Data: 0.030 (0.036)
Train: 208 [1200/1251 ( 96%)]  Loss: 3.911 (4.07)  Time: 0.187s, 5468.70/s  (0.194s, 5291.72/s)  LR: 2.154e-04  Data: 0.036 (0.036)
Train: 208 [1250/1251 (100%)]  Loss: 4.032 (4.07)  Time: 0.113s, 9061.25/s  (0.193s, 5300.42/s)  LR: 2.154e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.757 (1.757)  Loss:  0.9646 (0.9646)  Acc@1: 82.9102 (82.9102)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.1679 (1.6738)  Acc@1: 81.2500 (67.8680)  Acc@5: 94.8113 (88.0140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-197.pth.tar', 67.48600005371094)

Train: 209 [   0/1251 (  0%)]  Loss: 4.082 (4.08)  Time: 1.619s,  632.68/s  (1.619s,  632.68/s)  LR: 2.111e-04  Data: 1.422 (1.422)
Train: 209 [  50/1251 (  4%)]  Loss: 4.417 (4.25)  Time: 0.174s, 5871.58/s  (0.232s, 4412.76/s)  LR: 2.111e-04  Data: 0.024 (0.056)
Train: 209 [ 100/1251 (  8%)]  Loss: 3.827 (4.11)  Time: 0.167s, 6138.67/s  (0.208s, 4918.91/s)  LR: 2.111e-04  Data: 0.031 (0.042)
Train: 209 [ 150/1251 ( 12%)]  Loss: 4.139 (4.12)  Time: 0.184s, 5552.35/s  (0.202s, 5065.66/s)  LR: 2.111e-04  Data: 0.038 (0.038)
Train: 209 [ 200/1251 ( 16%)]  Loss: 4.020 (4.10)  Time: 0.173s, 5931.78/s  (0.197s, 5194.20/s)  LR: 2.111e-04  Data: 0.029 (0.035)
Train: 209 [ 250/1251 ( 20%)]  Loss: 4.428 (4.15)  Time: 0.183s, 5585.57/s  (0.197s, 5191.76/s)  LR: 2.111e-04  Data: 0.029 (0.034)
Train: 209 [ 300/1251 ( 24%)]  Loss: 4.041 (4.14)  Time: 0.167s, 6117.06/s  (0.196s, 5227.47/s)  LR: 2.111e-04  Data: 0.026 (0.033)
Train: 209 [ 350/1251 ( 28%)]  Loss: 3.814 (4.10)  Time: 0.172s, 5941.80/s  (0.195s, 5243.68/s)  LR: 2.111e-04  Data: 0.024 (0.032)
Train: 209 [ 400/1251 ( 32%)]  Loss: 4.326 (4.12)  Time: 0.167s, 6123.38/s  (0.195s, 5238.81/s)  LR: 2.111e-04  Data: 0.023 (0.032)
Train: 209 [ 450/1251 ( 36%)]  Loss: 4.381 (4.15)  Time: 0.176s, 5828.77/s  (0.195s, 5251.87/s)  LR: 2.111e-04  Data: 0.032 (0.031)
Train: 209 [ 500/1251 ( 40%)]  Loss: 3.892 (4.12)  Time: 0.293s, 3499.41/s  (0.194s, 5283.31/s)  LR: 2.111e-04  Data: 0.027 (0.031)
Train: 209 [ 550/1251 ( 44%)]  Loss: 4.355 (4.14)  Time: 0.179s, 5706.96/s  (0.194s, 5284.75/s)  LR: 2.111e-04  Data: 0.034 (0.031)
Train: 209 [ 600/1251 ( 48%)]  Loss: 3.978 (4.13)  Time: 0.193s, 5300.51/s  (0.193s, 5298.92/s)  LR: 2.111e-04  Data: 0.030 (0.031)
Train: 209 [ 650/1251 ( 52%)]  Loss: 3.850 (4.11)  Time: 0.171s, 5985.85/s  (0.193s, 5300.59/s)  LR: 2.111e-04  Data: 0.024 (0.031)
Train: 209 [ 700/1251 ( 56%)]  Loss: 4.091 (4.11)  Time: 0.191s, 5352.02/s  (0.193s, 5301.78/s)  LR: 2.111e-04  Data: 0.032 (0.031)
Train: 209 [ 750/1251 ( 60%)]  Loss: 4.024 (4.10)  Time: 0.167s, 6144.49/s  (0.193s, 5296.09/s)  LR: 2.111e-04  Data: 0.037 (0.030)
Train: 209 [ 800/1251 ( 64%)]  Loss: 4.284 (4.11)  Time: 0.173s, 5914.40/s  (0.193s, 5304.11/s)  LR: 2.111e-04  Data: 0.023 (0.030)
Train: 209 [ 850/1251 ( 68%)]  Loss: 4.127 (4.12)  Time: 0.172s, 5942.27/s  (0.193s, 5300.60/s)  LR: 2.111e-04  Data: 0.041 (0.030)
Train: 209 [ 900/1251 ( 72%)]  Loss: 3.828 (4.10)  Time: 0.178s, 5766.43/s  (0.193s, 5294.25/s)  LR: 2.111e-04  Data: 0.031 (0.030)
Train: 209 [ 950/1251 ( 76%)]  Loss: 4.215 (4.11)  Time: 0.179s, 5714.59/s  (0.193s, 5299.27/s)  LR: 2.111e-04  Data: 0.032 (0.030)
Train: 209 [1000/1251 ( 80%)]  Loss: 4.078 (4.10)  Time: 0.197s, 5208.94/s  (0.194s, 5290.49/s)  LR: 2.111e-04  Data: 0.030 (0.030)
Train: 209 [1050/1251 ( 84%)]  Loss: 4.232 (4.11)  Time: 0.177s, 5793.50/s  (0.194s, 5290.55/s)  LR: 2.111e-04  Data: 0.028 (0.030)
Train: 209 [1100/1251 ( 88%)]  Loss: 3.992 (4.11)  Time: 0.235s, 4356.18/s  (0.193s, 5294.07/s)  LR: 2.111e-04  Data: 0.028 (0.030)
Train: 209 [1150/1251 ( 92%)]  Loss: 4.271 (4.11)  Time: 0.174s, 5876.21/s  (0.194s, 5291.61/s)  LR: 2.111e-04  Data: 0.025 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 209 [1200/1251 ( 96%)]  Loss: 4.235 (4.12)  Time: 0.165s, 6208.26/s  (0.194s, 5291.17/s)  LR: 2.111e-04  Data: 0.025 (0.029)
Train: 209 [1250/1251 (100%)]  Loss: 3.976 (4.11)  Time: 0.114s, 8981.44/s  (0.193s, 5304.42/s)  LR: 2.111e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.740 (1.740)  Loss:  0.9923 (0.9923)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.4102 (95.4102)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0737 (1.6261)  Acc@1: 81.8396 (67.6880)  Acc@5: 94.9292 (88.1580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-209.pth.tar', 67.68799986816406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-198.pth.tar', 67.5059999951172)

Train: 210 [   0/1251 (  0%)]  Loss: 4.413 (4.41)  Time: 1.725s,  593.65/s  (1.725s,  593.65/s)  LR: 2.069e-04  Data: 1.584 (1.584)
Train: 210 [  50/1251 (  4%)]  Loss: 4.034 (4.22)  Time: 0.166s, 6162.25/s  (0.219s, 4678.13/s)  LR: 2.069e-04  Data: 0.035 (0.073)
Train: 210 [ 100/1251 (  8%)]  Loss: 4.230 (4.23)  Time: 0.166s, 6184.45/s  (0.202s, 5066.94/s)  LR: 2.069e-04  Data: 0.029 (0.054)
Train: 210 [ 150/1251 ( 12%)]  Loss: 3.810 (4.12)  Time: 0.174s, 5896.25/s  (0.197s, 5195.05/s)  LR: 2.069e-04  Data: 0.033 (0.047)
Train: 210 [ 200/1251 ( 16%)]  Loss: 4.086 (4.11)  Time: 0.204s, 5008.42/s  (0.195s, 5238.20/s)  LR: 2.069e-04  Data: 0.080 (0.044)
Train: 210 [ 250/1251 ( 20%)]  Loss: 4.163 (4.12)  Time: 0.174s, 5875.03/s  (0.195s, 5259.58/s)  LR: 2.069e-04  Data: 0.027 (0.041)
Train: 210 [ 300/1251 ( 24%)]  Loss: 3.710 (4.06)  Time: 0.193s, 5311.74/s  (0.193s, 5315.37/s)  LR: 2.069e-04  Data: 0.028 (0.040)
Train: 210 [ 350/1251 ( 28%)]  Loss: 4.188 (4.08)  Time: 0.499s, 2051.45/s  (0.193s, 5316.79/s)  LR: 2.069e-04  Data: 0.031 (0.039)
Train: 210 [ 400/1251 ( 32%)]  Loss: 3.793 (4.05)  Time: 0.166s, 6183.86/s  (0.193s, 5311.80/s)  LR: 2.069e-04  Data: 0.024 (0.037)
Train: 210 [ 450/1251 ( 36%)]  Loss: 4.301 (4.07)  Time: 0.173s, 5919.63/s  (0.192s, 5322.12/s)  LR: 2.069e-04  Data: 0.023 (0.036)
Train: 210 [ 500/1251 ( 40%)]  Loss: 4.251 (4.09)  Time: 0.178s, 5744.98/s  (0.192s, 5328.09/s)  LR: 2.069e-04  Data: 0.034 (0.035)
Train: 210 [ 550/1251 ( 44%)]  Loss: 3.892 (4.07)  Time: 0.239s, 4284.08/s  (0.192s, 5337.87/s)  LR: 2.069e-04  Data: 0.026 (0.035)
Train: 210 [ 600/1251 ( 48%)]  Loss: 3.979 (4.07)  Time: 0.161s, 6351.16/s  (0.192s, 5332.11/s)  LR: 2.069e-04  Data: 0.034 (0.034)
Train: 210 [ 650/1251 ( 52%)]  Loss: 4.032 (4.06)  Time: 0.157s, 6520.06/s  (0.192s, 5345.79/s)  LR: 2.069e-04  Data: 0.035 (0.034)
Train: 210 [ 700/1251 ( 56%)]  Loss: 4.201 (4.07)  Time: 0.167s, 6146.46/s  (0.192s, 5327.42/s)  LR: 2.069e-04  Data: 0.025 (0.034)
Train: 210 [ 750/1251 ( 60%)]  Loss: 4.254 (4.08)  Time: 0.172s, 5948.11/s  (0.192s, 5326.62/s)  LR: 2.069e-04  Data: 0.022 (0.033)
Train: 210 [ 800/1251 ( 64%)]  Loss: 4.307 (4.10)  Time: 0.191s, 5351.87/s  (0.192s, 5321.92/s)  LR: 2.069e-04  Data: 0.063 (0.033)
Train: 210 [ 850/1251 ( 68%)]  Loss: 4.380 (4.11)  Time: 0.155s, 6602.14/s  (0.192s, 5334.88/s)  LR: 2.069e-04  Data: 0.026 (0.033)
Train: 210 [ 900/1251 ( 72%)]  Loss: 4.512 (4.13)  Time: 0.173s, 5933.66/s  (0.192s, 5331.88/s)  LR: 2.069e-04  Data: 0.025 (0.033)
Train: 210 [ 950/1251 ( 76%)]  Loss: 4.011 (4.13)  Time: 0.178s, 5760.80/s  (0.192s, 5332.84/s)  LR: 2.069e-04  Data: 0.023 (0.032)
Train: 210 [1000/1251 ( 80%)]  Loss: 4.394 (4.14)  Time: 0.170s, 6033.25/s  (0.192s, 5324.69/s)  LR: 2.069e-04  Data: 0.029 (0.032)
Train: 210 [1050/1251 ( 84%)]  Loss: 3.880 (4.13)  Time: 0.177s, 5791.79/s  (0.192s, 5325.84/s)  LR: 2.069e-04  Data: 0.029 (0.032)
Train: 210 [1100/1251 ( 88%)]  Loss: 4.422 (4.14)  Time: 0.165s, 6217.67/s  (0.193s, 5318.59/s)  LR: 2.069e-04  Data: 0.020 (0.032)
Train: 210 [1150/1251 ( 92%)]  Loss: 4.037 (4.14)  Time: 0.177s, 5798.38/s  (0.192s, 5320.09/s)  LR: 2.069e-04  Data: 0.034 (0.032)
Train: 210 [1200/1251 ( 96%)]  Loss: 4.143 (4.14)  Time: 0.172s, 5964.01/s  (0.193s, 5316.78/s)  LR: 2.069e-04  Data: 0.037 (0.031)
Train: 210 [1250/1251 (100%)]  Loss: 3.983 (4.13)  Time: 0.114s, 9019.00/s  (0.192s, 5326.01/s)  LR: 2.069e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.754 (1.754)  Loss:  1.0274 (1.0274)  Acc@1: 82.8125 (82.8125)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0569 (1.6387)  Acc@1: 83.1368 (68.0140)  Acc@5: 94.8113 (88.1180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-209.pth.tar', 67.68799986816406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-201.pth.tar', 67.57600005126953)

Train: 211 [   0/1251 (  0%)]  Loss: 3.950 (3.95)  Time: 2.046s,  500.37/s  (2.046s,  500.37/s)  LR: 2.027e-04  Data: 1.925 (1.925)
Train: 211 [  50/1251 (  4%)]  Loss: 4.344 (4.15)  Time: 0.160s, 6407.41/s  (0.227s, 4509.67/s)  LR: 2.027e-04  Data: 0.031 (0.083)
Train: 211 [ 100/1251 (  8%)]  Loss: 3.981 (4.09)  Time: 0.168s, 6077.67/s  (0.207s, 4940.00/s)  LR: 2.027e-04  Data: 0.031 (0.063)
Train: 211 [ 150/1251 ( 12%)]  Loss: 4.124 (4.10)  Time: 0.180s, 5674.06/s  (0.200s, 5129.36/s)  LR: 2.027e-04  Data: 0.022 (0.055)
Train: 211 [ 200/1251 ( 16%)]  Loss: 3.931 (4.07)  Time: 0.178s, 5756.21/s  (0.198s, 5171.88/s)  LR: 2.027e-04  Data: 0.020 (0.050)
Train: 211 [ 250/1251 ( 20%)]  Loss: 4.259 (4.10)  Time: 0.172s, 5958.64/s  (0.196s, 5233.44/s)  LR: 2.027e-04  Data: 0.024 (0.045)
Train: 211 [ 300/1251 ( 24%)]  Loss: 4.051 (4.09)  Time: 0.148s, 6940.22/s  (0.195s, 5257.70/s)  LR: 2.027e-04  Data: 0.027 (0.045)
Train: 211 [ 350/1251 ( 28%)]  Loss: 4.366 (4.13)  Time: 0.159s, 6431.94/s  (0.194s, 5290.46/s)  LR: 2.027e-04  Data: 0.035 (0.045)
Train: 211 [ 400/1251 ( 32%)]  Loss: 3.810 (4.09)  Time: 0.191s, 5350.68/s  (0.193s, 5299.62/s)  LR: 2.027e-04  Data: 0.027 (0.045)
Train: 211 [ 450/1251 ( 36%)]  Loss: 4.198 (4.10)  Time: 0.168s, 6087.46/s  (0.193s, 5300.22/s)  LR: 2.027e-04  Data: 0.025 (0.045)
Train: 211 [ 500/1251 ( 40%)]  Loss: 4.420 (4.13)  Time: 0.184s, 5563.58/s  (0.193s, 5319.18/s)  LR: 2.027e-04  Data: 0.030 (0.044)
Train: 211 [ 550/1251 ( 44%)]  Loss: 3.522 (4.08)  Time: 0.161s, 6342.96/s  (0.193s, 5297.26/s)  LR: 2.027e-04  Data: 0.020 (0.045)
Train: 211 [ 600/1251 ( 48%)]  Loss: 3.891 (4.07)  Time: 0.180s, 5685.30/s  (0.193s, 5309.48/s)  LR: 2.027e-04  Data: 0.030 (0.045)
Train: 211 [ 650/1251 ( 52%)]  Loss: 4.036 (4.06)  Time: 0.170s, 6020.94/s  (0.192s, 5326.36/s)  LR: 2.027e-04  Data: 0.021 (0.045)
Train: 211 [ 700/1251 ( 56%)]  Loss: 4.103 (4.07)  Time: 0.163s, 6288.85/s  (0.192s, 5321.88/s)  LR: 2.027e-04  Data: 0.033 (0.045)
Train: 211 [ 750/1251 ( 60%)]  Loss: 4.185 (4.07)  Time: 0.163s, 6264.80/s  (0.193s, 5314.45/s)  LR: 2.027e-04  Data: 0.024 (0.046)
Train: 211 [ 800/1251 ( 64%)]  Loss: 4.013 (4.07)  Time: 0.179s, 5706.52/s  (0.193s, 5313.80/s)  LR: 2.027e-04  Data: 0.026 (0.046)
Train: 211 [ 850/1251 ( 68%)]  Loss: 4.199 (4.08)  Time: 0.199s, 5150.10/s  (0.193s, 5316.56/s)  LR: 2.027e-04  Data: 0.034 (0.046)
Train: 211 [ 900/1251 ( 72%)]  Loss: 4.440 (4.10)  Time: 0.169s, 6069.84/s  (0.193s, 5306.73/s)  LR: 2.027e-04  Data: 0.027 (0.046)
Train: 211 [ 950/1251 ( 76%)]  Loss: 4.030 (4.09)  Time: 0.172s, 5963.98/s  (0.193s, 5307.78/s)  LR: 2.027e-04  Data: 0.030 (0.046)
Train: 211 [1000/1251 ( 80%)]  Loss: 3.959 (4.09)  Time: 0.174s, 5891.30/s  (0.193s, 5314.21/s)  LR: 2.027e-04  Data: 0.025 (0.046)
Train: 211 [1050/1251 ( 84%)]  Loss: 4.136 (4.09)  Time: 0.163s, 6286.19/s  (0.193s, 5313.49/s)  LR: 2.027e-04  Data: 0.027 (0.046)
Train: 211 [1100/1251 ( 88%)]  Loss: 3.807 (4.08)  Time: 0.181s, 5657.43/s  (0.193s, 5312.64/s)  LR: 2.027e-04  Data: 0.021 (0.046)
Train: 211 [1150/1251 ( 92%)]  Loss: 4.619 (4.10)  Time: 0.170s, 6023.75/s  (0.193s, 5313.09/s)  LR: 2.027e-04  Data: 0.020 (0.046)
Train: 211 [1200/1251 ( 96%)]  Loss: 3.994 (4.09)  Time: 0.172s, 5955.07/s  (0.193s, 5302.48/s)  LR: 2.027e-04  Data: 0.021 (0.046)
Train: 211 [1250/1251 (100%)]  Loss: 4.001 (4.09)  Time: 0.113s, 9043.12/s  (0.193s, 5318.04/s)  LR: 2.027e-04  Data: 0.000 (0.045)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.889 (1.889)  Loss:  0.9960 (0.9960)  Acc@1: 83.1055 (83.1055)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0721 (1.6736)  Acc@1: 83.2547 (67.8640)  Acc@5: 95.0472 (88.0140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-211.pth.tar', 67.86400009521485)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-209.pth.tar', 67.68799986816406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-202.pth.tar', 67.63000010498047)

Train: 212 [   0/1251 (  0%)]  Loss: 4.363 (4.36)  Time: 1.915s,  534.80/s  (1.915s,  534.80/s)  LR: 1.985e-04  Data: 1.782 (1.782)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 212 [  50/1251 (  4%)]  Loss: 4.234 (4.30)  Time: 0.181s, 5671.02/s  (0.226s, 4527.56/s)  LR: 1.985e-04  Data: 0.028 (0.083)
Train: 212 [ 100/1251 (  8%)]  Loss: 4.349 (4.32)  Time: 0.165s, 6200.62/s  (0.205s, 4986.79/s)  LR: 1.985e-04  Data: 0.026 (0.060)
Train: 212 [ 150/1251 ( 12%)]  Loss: 4.206 (4.29)  Time: 0.168s, 6108.90/s  (0.199s, 5142.76/s)  LR: 1.985e-04  Data: 0.026 (0.053)
Train: 212 [ 200/1251 ( 16%)]  Loss: 3.414 (4.11)  Time: 0.160s, 6381.43/s  (0.197s, 5185.80/s)  LR: 1.985e-04  Data: 0.028 (0.051)
Train: 212 [ 250/1251 ( 20%)]  Loss: 3.946 (4.09)  Time: 0.303s, 3377.24/s  (0.196s, 5235.80/s)  LR: 1.985e-04  Data: 0.181 (0.048)
Train: 212 [ 300/1251 ( 24%)]  Loss: 3.921 (4.06)  Time: 0.189s, 5409.94/s  (0.195s, 5254.58/s)  LR: 1.985e-04  Data: 0.036 (0.047)
Train: 212 [ 350/1251 ( 28%)]  Loss: 4.175 (4.08)  Time: 0.159s, 6444.88/s  (0.194s, 5283.24/s)  LR: 1.985e-04  Data: 0.029 (0.046)
Train: 212 [ 400/1251 ( 32%)]  Loss: 4.218 (4.09)  Time: 0.153s, 6682.44/s  (0.194s, 5286.48/s)  LR: 1.985e-04  Data: 0.032 (0.044)
Train: 212 [ 450/1251 ( 36%)]  Loss: 4.106 (4.09)  Time: 0.172s, 5953.24/s  (0.193s, 5314.94/s)  LR: 1.985e-04  Data: 0.035 (0.042)
Train: 212 [ 500/1251 ( 40%)]  Loss: 4.261 (4.11)  Time: 0.251s, 4080.08/s  (0.193s, 5312.82/s)  LR: 1.985e-04  Data: 0.024 (0.041)
Train: 212 [ 550/1251 ( 44%)]  Loss: 4.004 (4.10)  Time: 0.178s, 5759.93/s  (0.193s, 5313.12/s)  LR: 1.985e-04  Data: 0.028 (0.040)
Train: 212 [ 600/1251 ( 48%)]  Loss: 3.939 (4.09)  Time: 0.170s, 6023.14/s  (0.193s, 5312.39/s)  LR: 1.985e-04  Data: 0.030 (0.039)
Train: 212 [ 650/1251 ( 52%)]  Loss: 3.905 (4.07)  Time: 0.205s, 4997.75/s  (0.193s, 5312.24/s)  LR: 1.985e-04  Data: 0.024 (0.038)
Train: 212 [ 700/1251 ( 56%)]  Loss: 4.249 (4.09)  Time: 0.180s, 5677.21/s  (0.193s, 5317.97/s)  LR: 1.985e-04  Data: 0.025 (0.037)
Train: 212 [ 750/1251 ( 60%)]  Loss: 3.957 (4.08)  Time: 0.168s, 6102.66/s  (0.192s, 5330.95/s)  LR: 1.985e-04  Data: 0.025 (0.037)
Train: 212 [ 800/1251 ( 64%)]  Loss: 4.097 (4.08)  Time: 0.162s, 6305.41/s  (0.192s, 5332.78/s)  LR: 1.985e-04  Data: 0.025 (0.036)
Train: 212 [ 850/1251 ( 68%)]  Loss: 4.388 (4.10)  Time: 0.196s, 5219.78/s  (0.193s, 5316.93/s)  LR: 1.985e-04  Data: 0.020 (0.036)
Train: 212 [ 900/1251 ( 72%)]  Loss: 4.150 (4.10)  Time: 0.168s, 6102.93/s  (0.192s, 5330.14/s)  LR: 1.985e-04  Data: 0.020 (0.035)
Train: 212 [ 950/1251 ( 76%)]  Loss: 4.010 (4.09)  Time: 0.189s, 5405.25/s  (0.192s, 5332.60/s)  LR: 1.985e-04  Data: 0.021 (0.035)
Train: 212 [1000/1251 ( 80%)]  Loss: 4.448 (4.11)  Time: 0.170s, 6010.39/s  (0.192s, 5334.86/s)  LR: 1.985e-04  Data: 0.028 (0.035)
Train: 212 [1050/1251 ( 84%)]  Loss: 3.939 (4.10)  Time: 0.562s, 1822.60/s  (0.192s, 5323.36/s)  LR: 1.985e-04  Data: 0.048 (0.034)
Train: 212 [1100/1251 ( 88%)]  Loss: 3.958 (4.10)  Time: 0.176s, 5808.45/s  (0.192s, 5324.18/s)  LR: 1.985e-04  Data: 0.023 (0.034)
Train: 212 [1150/1251 ( 92%)]  Loss: 4.103 (4.10)  Time: 0.174s, 5897.74/s  (0.192s, 5324.12/s)  LR: 1.985e-04  Data: 0.026 (0.034)
Train: 212 [1200/1251 ( 96%)]  Loss: 4.150 (4.10)  Time: 0.171s, 5998.22/s  (0.192s, 5324.03/s)  LR: 1.985e-04  Data: 0.032 (0.034)
Train: 212 [1250/1251 (100%)]  Loss: 4.057 (4.10)  Time: 0.113s, 9035.57/s  (0.192s, 5337.53/s)  LR: 1.985e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.828 (1.828)  Loss:  1.0979 (1.0979)  Acc@1: 83.8867 (83.8867)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.0887 (1.6607)  Acc@1: 81.8396 (68.3300)  Acc@5: 95.1651 (88.3920)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-211.pth.tar', 67.86400009521485)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-209.pth.tar', 67.68799986816406)

Train: 213 [   0/1251 (  0%)]  Loss: 3.986 (3.99)  Time: 1.682s,  608.97/s  (1.682s,  608.97/s)  LR: 1.944e-04  Data: 1.553 (1.553)
Train: 213 [  50/1251 (  4%)]  Loss: 4.016 (4.00)  Time: 0.164s, 6259.00/s  (0.224s, 4581.41/s)  LR: 1.944e-04  Data: 0.028 (0.068)
Train: 213 [ 100/1251 (  8%)]  Loss: 4.210 (4.07)  Time: 0.194s, 5265.04/s  (0.207s, 4948.68/s)  LR: 1.944e-04  Data: 0.027 (0.048)
Train: 213 [ 150/1251 ( 12%)]  Loss: 3.925 (4.03)  Time: 0.181s, 5661.66/s  (0.202s, 5077.79/s)  LR: 1.944e-04  Data: 0.030 (0.042)
Train: 213 [ 200/1251 ( 16%)]  Loss: 4.182 (4.06)  Time: 0.176s, 5818.49/s  (0.198s, 5166.45/s)  LR: 1.944e-04  Data: 0.038 (0.038)
Train: 213 [ 250/1251 ( 20%)]  Loss: 3.670 (4.00)  Time: 0.166s, 6179.64/s  (0.197s, 5205.31/s)  LR: 1.944e-04  Data: 0.025 (0.036)
Train: 213 [ 300/1251 ( 24%)]  Loss: 4.351 (4.05)  Time: 0.165s, 6204.56/s  (0.196s, 5218.92/s)  LR: 1.944e-04  Data: 0.033 (0.035)
Train: 213 [ 350/1251 ( 28%)]  Loss: 4.341 (4.09)  Time: 0.163s, 6283.25/s  (0.195s, 5257.12/s)  LR: 1.944e-04  Data: 0.027 (0.034)
Train: 213 [ 400/1251 ( 32%)]  Loss: 4.424 (4.12)  Time: 0.169s, 6054.56/s  (0.194s, 5278.62/s)  LR: 1.944e-04  Data: 0.033 (0.033)
Train: 213 [ 450/1251 ( 36%)]  Loss: 4.177 (4.13)  Time: 0.386s, 2649.45/s  (0.194s, 5280.64/s)  LR: 1.944e-04  Data: 0.033 (0.033)
Train: 213 [ 500/1251 ( 40%)]  Loss: 4.202 (4.14)  Time: 0.172s, 5954.74/s  (0.193s, 5299.47/s)  LR: 1.944e-04  Data: 0.026 (0.032)
Train: 213 [ 550/1251 ( 44%)]  Loss: 4.322 (4.15)  Time: 0.197s, 5206.99/s  (0.193s, 5303.08/s)  LR: 1.944e-04  Data: 0.037 (0.032)
Train: 213 [ 600/1251 ( 48%)]  Loss: 4.295 (4.16)  Time: 0.166s, 6164.60/s  (0.193s, 5305.39/s)  LR: 1.944e-04  Data: 0.032 (0.032)
Train: 213 [ 650/1251 ( 52%)]  Loss: 4.558 (4.19)  Time: 0.360s, 2847.18/s  (0.193s, 5309.04/s)  LR: 1.944e-04  Data: 0.029 (0.031)
Train: 213 [ 700/1251 ( 56%)]  Loss: 4.191 (4.19)  Time: 0.154s, 6633.76/s  (0.193s, 5308.55/s)  LR: 1.944e-04  Data: 0.027 (0.031)
Train: 213 [ 750/1251 ( 60%)]  Loss: 3.980 (4.18)  Time: 0.173s, 5927.09/s  (0.193s, 5302.60/s)  LR: 1.944e-04  Data: 0.025 (0.031)
Train: 213 [ 800/1251 ( 64%)]  Loss: 4.028 (4.17)  Time: 0.154s, 6661.70/s  (0.193s, 5309.39/s)  LR: 1.944e-04  Data: 0.028 (0.031)
Train: 213 [ 850/1251 ( 68%)]  Loss: 4.410 (4.18)  Time: 0.165s, 6188.64/s  (0.193s, 5309.31/s)  LR: 1.944e-04  Data: 0.026 (0.030)
Train: 213 [ 900/1251 ( 72%)]  Loss: 4.219 (4.18)  Time: 0.170s, 6032.05/s  (0.193s, 5304.23/s)  LR: 1.944e-04  Data: 0.037 (0.030)
Train: 213 [ 950/1251 ( 76%)]  Loss: 4.152 (4.18)  Time: 0.173s, 5930.79/s  (0.193s, 5310.78/s)  LR: 1.944e-04  Data: 0.036 (0.030)
Train: 213 [1000/1251 ( 80%)]  Loss: 4.153 (4.18)  Time: 0.154s, 6631.88/s  (0.193s, 5300.31/s)  LR: 1.944e-04  Data: 0.025 (0.030)
Train: 213 [1050/1251 ( 84%)]  Loss: 4.149 (4.18)  Time: 0.177s, 5790.53/s  (0.193s, 5296.76/s)  LR: 1.944e-04  Data: 0.025 (0.030)
Train: 213 [1100/1251 ( 88%)]  Loss: 4.140 (4.18)  Time: 0.171s, 6004.15/s  (0.193s, 5297.36/s)  LR: 1.944e-04  Data: 0.027 (0.030)
Train: 213 [1150/1251 ( 92%)]  Loss: 4.566 (4.19)  Time: 0.180s, 5685.42/s  (0.194s, 5287.99/s)  LR: 1.944e-04  Data: 0.032 (0.030)
Train: 213 [1200/1251 ( 96%)]  Loss: 4.121 (4.19)  Time: 0.182s, 5629.07/s  (0.194s, 5290.85/s)  LR: 1.944e-04  Data: 0.028 (0.030)
Train: 213 [1250/1251 (100%)]  Loss: 4.001 (4.18)  Time: 0.114s, 9015.73/s  (0.193s, 5302.27/s)  LR: 1.944e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  0.9804 (0.9804)  Acc@1: 83.3984 (83.3984)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1171 (1.6218)  Acc@1: 82.0755 (68.2520)  Acc@5: 94.9292 (88.3580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-211.pth.tar', 67.86400009521485)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-205.pth.tar', 67.71400002197265)

Train: 214 [   0/1251 (  0%)]  Loss: 3.858 (3.86)  Time: 1.636s,  625.97/s  (1.636s,  625.97/s)  LR: 1.902e-04  Data: 1.510 (1.510)
Train: 214 [  50/1251 (  4%)]  Loss: 4.111 (3.98)  Time: 0.154s, 6630.76/s  (0.232s, 4420.13/s)  LR: 1.902e-04  Data: 0.024 (0.087)
Train: 214 [ 100/1251 (  8%)]  Loss: 4.106 (4.02)  Time: 0.190s, 5393.02/s  (0.209s, 4899.38/s)  LR: 1.902e-04  Data: 0.029 (0.063)
Train: 214 [ 150/1251 ( 12%)]  Loss: 3.778 (3.96)  Time: 0.174s, 5886.70/s  (0.199s, 5144.25/s)  LR: 1.902e-04  Data: 0.020 (0.054)
Train: 214 [ 200/1251 ( 16%)]  Loss: 4.123 (3.99)  Time: 0.153s, 6713.95/s  (0.197s, 5193.04/s)  LR: 1.902e-04  Data: 0.019 (0.050)
Train: 214 [ 250/1251 ( 20%)]  Loss: 3.740 (3.95)  Time: 0.165s, 6207.28/s  (0.196s, 5230.12/s)  LR: 1.902e-04  Data: 0.026 (0.047)
Train: 214 [ 300/1251 ( 24%)]  Loss: 3.910 (3.95)  Time: 0.549s, 1865.38/s  (0.197s, 5210.92/s)  LR: 1.902e-04  Data: 0.410 (0.047)
Train: 214 [ 350/1251 ( 28%)]  Loss: 4.233 (3.98)  Time: 0.154s, 6657.06/s  (0.196s, 5237.23/s)  LR: 1.902e-04  Data: 0.028 (0.047)
Train: 214 [ 400/1251 ( 32%)]  Loss: 4.194 (4.01)  Time: 0.152s, 6756.84/s  (0.195s, 5244.81/s)  LR: 1.902e-04  Data: 0.025 (0.047)
Train: 214 [ 450/1251 ( 36%)]  Loss: 4.290 (4.03)  Time: 0.156s, 6543.94/s  (0.194s, 5265.48/s)  LR: 1.902e-04  Data: 0.022 (0.046)
Train: 214 [ 500/1251 ( 40%)]  Loss: 4.130 (4.04)  Time: 0.440s, 2329.52/s  (0.195s, 5263.45/s)  LR: 1.902e-04  Data: 0.310 (0.046)
Train: 214 [ 550/1251 ( 44%)]  Loss: 3.968 (4.04)  Time: 0.185s, 5538.25/s  (0.194s, 5282.93/s)  LR: 1.902e-04  Data: 0.022 (0.046)
Train: 214 [ 600/1251 ( 48%)]  Loss: 3.954 (4.03)  Time: 0.185s, 5539.75/s  (0.193s, 5300.32/s)  LR: 1.902e-04  Data: 0.024 (0.045)
Train: 214 [ 650/1251 ( 52%)]  Loss: 3.708 (4.01)  Time: 0.160s, 6394.38/s  (0.193s, 5301.61/s)  LR: 1.902e-04  Data: 0.032 (0.045)
Train: 214 [ 700/1251 ( 56%)]  Loss: 3.681 (3.99)  Time: 0.213s, 4813.46/s  (0.193s, 5296.11/s)  LR: 1.902e-04  Data: 0.082 (0.045)
Train: 214 [ 750/1251 ( 60%)]  Loss: 4.337 (4.01)  Time: 0.315s, 3249.05/s  (0.193s, 5294.87/s)  LR: 1.902e-04  Data: 0.021 (0.044)
Train: 214 [ 800/1251 ( 64%)]  Loss: 3.880 (4.00)  Time: 0.176s, 5823.86/s  (0.193s, 5292.60/s)  LR: 1.902e-04  Data: 0.024 (0.043)
Train: 214 [ 850/1251 ( 68%)]  Loss: 3.784 (3.99)  Time: 0.167s, 6130.76/s  (0.193s, 5302.50/s)  LR: 1.902e-04  Data: 0.030 (0.043)
Train: 214 [ 900/1251 ( 72%)]  Loss: 3.720 (3.97)  Time: 0.173s, 5919.32/s  (0.193s, 5308.07/s)  LR: 1.902e-04  Data: 0.028 (0.042)
Train: 214 [ 950/1251 ( 76%)]  Loss: 4.183 (3.98)  Time: 0.251s, 4075.98/s  (0.193s, 5293.31/s)  LR: 1.902e-04  Data: 0.026 (0.041)
Train: 214 [1000/1251 ( 80%)]  Loss: 4.176 (3.99)  Time: 0.183s, 5591.59/s  (0.194s, 5289.89/s)  LR: 1.902e-04  Data: 0.031 (0.041)
Train: 214 [1050/1251 ( 84%)]  Loss: 3.830 (3.99)  Time: 0.164s, 6246.02/s  (0.193s, 5296.85/s)  LR: 1.902e-04  Data: 0.031 (0.040)
Train: 214 [1100/1251 ( 88%)]  Loss: 3.866 (3.98)  Time: 0.466s, 2195.50/s  (0.194s, 5287.96/s)  LR: 1.902e-04  Data: 0.026 (0.039)
Train: 214 [1150/1251 ( 92%)]  Loss: 3.662 (3.97)  Time: 0.193s, 5294.84/s  (0.194s, 5283.56/s)  LR: 1.902e-04  Data: 0.022 (0.039)
Train: 214 [1200/1251 ( 96%)]  Loss: 4.001 (3.97)  Time: 0.167s, 6138.13/s  (0.194s, 5289.56/s)  LR: 1.902e-04  Data: 0.023 (0.039)
Train: 214 [1250/1251 (100%)]  Loss: 3.783 (3.96)  Time: 0.113s, 9031.69/s  (0.193s, 5293.67/s)  LR: 1.902e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.852 (1.852)  Loss:  1.0736 (1.0736)  Acc@1: 82.6172 (82.6172)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1381 (1.6706)  Acc@1: 81.7217 (68.1520)  Acc@5: 93.6321 (88.2760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-211.pth.tar', 67.86400009521485)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-206.pth.tar', 67.73000002441407)

Train: 215 [   0/1251 (  0%)]  Loss: 4.571 (4.57)  Time: 1.656s,  618.23/s  (1.656s,  618.23/s)  LR: 1.862e-04  Data: 1.520 (1.520)
Train: 215 [  50/1251 (  4%)]  Loss: 4.343 (4.46)  Time: 0.178s, 5762.10/s  (0.224s, 4576.12/s)  LR: 1.862e-04  Data: 0.030 (0.063)
Train: 215 [ 100/1251 (  8%)]  Loss: 3.813 (4.24)  Time: 0.177s, 5771.82/s  (0.205s, 4987.77/s)  LR: 1.862e-04  Data: 0.024 (0.047)
Train: 215 [ 150/1251 ( 12%)]  Loss: 4.331 (4.26)  Time: 0.175s, 5865.90/s  (0.200s, 5111.35/s)  LR: 1.862e-04  Data: 0.030 (0.041)
Train: 215 [ 200/1251 ( 16%)]  Loss: 3.948 (4.20)  Time: 0.172s, 5950.38/s  (0.198s, 5179.04/s)  LR: 1.862e-04  Data: 0.026 (0.038)
Train: 215 [ 250/1251 ( 20%)]  Loss: 3.948 (4.16)  Time: 0.165s, 6216.80/s  (0.196s, 5224.27/s)  LR: 1.862e-04  Data: 0.029 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 215 [ 300/1251 ( 24%)]  Loss: 4.179 (4.16)  Time: 0.183s, 5600.72/s  (0.195s, 5252.50/s)  LR: 1.862e-04  Data: 0.023 (0.035)
Train: 215 [ 350/1251 ( 28%)]  Loss: 4.191 (4.17)  Time: 0.185s, 5545.87/s  (0.195s, 5240.72/s)  LR: 1.862e-04  Data: 0.027 (0.034)
Train: 215 [ 400/1251 ( 32%)]  Loss: 3.980 (4.14)  Time: 0.169s, 6045.23/s  (0.194s, 5274.08/s)  LR: 1.862e-04  Data: 0.032 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 215 [ 450/1251 ( 36%)]  Loss: 3.934 (4.12)  Time: 0.170s, 6021.18/s  (0.194s, 5276.91/s)  LR: 1.862e-04  Data: 0.028 (0.033)
Train: 215 [ 500/1251 ( 40%)]  Loss: 4.173 (4.13)  Time: 0.175s, 5851.82/s  (0.194s, 5281.58/s)  LR: 1.862e-04  Data: 0.021 (0.032)
Train: 215 [ 550/1251 ( 44%)]  Loss: 4.150 (4.13)  Time: 0.171s, 5991.01/s  (0.194s, 5286.51/s)  LR: 1.862e-04  Data: 0.027 (0.032)
Train: 215 [ 600/1251 ( 48%)]  Loss: 4.172 (4.13)  Time: 0.177s, 5790.28/s  (0.193s, 5296.89/s)  LR: 1.862e-04  Data: 0.029 (0.031)
Train: 215 [ 650/1251 ( 52%)]  Loss: 3.888 (4.12)  Time: 0.151s, 6783.22/s  (0.194s, 5291.58/s)  LR: 1.862e-04  Data: 0.027 (0.031)
Train: 215 [ 700/1251 ( 56%)]  Loss: 3.981 (4.11)  Time: 0.167s, 6130.34/s  (0.193s, 5309.60/s)  LR: 1.862e-04  Data: 0.029 (0.031)
Train: 215 [ 750/1251 ( 60%)]  Loss: 3.852 (4.09)  Time: 0.177s, 5790.14/s  (0.193s, 5303.35/s)  LR: 1.862e-04  Data: 0.028 (0.031)
Train: 215 [ 800/1251 ( 64%)]  Loss: 4.351 (4.11)  Time: 0.160s, 6398.16/s  (0.193s, 5297.38/s)  LR: 1.862e-04  Data: 0.025 (0.031)
Train: 215 [ 850/1251 ( 68%)]  Loss: 3.861 (4.09)  Time: 0.269s, 3811.72/s  (0.194s, 5291.93/s)  LR: 1.862e-04  Data: 0.026 (0.031)
Train: 215 [ 900/1251 ( 72%)]  Loss: 4.415 (4.11)  Time: 0.175s, 5852.02/s  (0.193s, 5308.45/s)  LR: 1.862e-04  Data: 0.026 (0.031)
Train: 215 [ 950/1251 ( 76%)]  Loss: 4.111 (4.11)  Time: 0.161s, 6365.57/s  (0.193s, 5312.33/s)  LR: 1.862e-04  Data: 0.026 (0.030)
Train: 215 [1000/1251 ( 80%)]  Loss: 4.164 (4.11)  Time: 0.244s, 4199.48/s  (0.193s, 5310.28/s)  LR: 1.862e-04  Data: 0.025 (0.030)
Train: 215 [1050/1251 ( 84%)]  Loss: 3.961 (4.11)  Time: 0.172s, 5951.77/s  (0.193s, 5300.10/s)  LR: 1.862e-04  Data: 0.037 (0.030)
Train: 215 [1100/1251 ( 88%)]  Loss: 4.230 (4.11)  Time: 0.175s, 5861.14/s  (0.193s, 5294.74/s)  LR: 1.862e-04  Data: 0.025 (0.030)
Train: 215 [1150/1251 ( 92%)]  Loss: 4.168 (4.11)  Time: 0.193s, 5298.55/s  (0.193s, 5293.33/s)  LR: 1.862e-04  Data: 0.031 (0.030)
Train: 215 [1200/1251 ( 96%)]  Loss: 4.132 (4.11)  Time: 0.658s, 1556.47/s  (0.194s, 5286.14/s)  LR: 1.862e-04  Data: 0.024 (0.030)
Train: 215 [1250/1251 (100%)]  Loss: 4.378 (4.12)  Time: 0.113s, 9027.52/s  (0.193s, 5307.66/s)  LR: 1.862e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.770 (1.770)  Loss:  0.9700 (0.9700)  Acc@1: 83.3008 (83.3008)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.0575 (1.6018)  Acc@1: 79.9528 (68.0460)  Acc@5: 93.9858 (88.2000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-211.pth.tar', 67.86400009521485)

Train: 216 [   0/1251 (  0%)]  Loss: 4.133 (4.13)  Time: 1.708s,  599.39/s  (1.708s,  599.39/s)  LR: 1.821e-04  Data: 1.480 (1.480)
Train: 216 [  50/1251 (  4%)]  Loss: 4.171 (4.15)  Time: 0.179s, 5716.81/s  (0.222s, 4622.37/s)  LR: 1.821e-04  Data: 0.025 (0.057)
Train: 216 [ 100/1251 (  8%)]  Loss: 4.453 (4.25)  Time: 0.173s, 5914.21/s  (0.206s, 4967.92/s)  LR: 1.821e-04  Data: 0.026 (0.043)
Train: 216 [ 150/1251 ( 12%)]  Loss: 4.012 (4.19)  Time: 0.176s, 5806.78/s  (0.199s, 5133.14/s)  LR: 1.821e-04  Data: 0.026 (0.038)
Train: 216 [ 200/1251 ( 16%)]  Loss: 3.983 (4.15)  Time: 0.164s, 6226.77/s  (0.197s, 5207.74/s)  LR: 1.821e-04  Data: 0.027 (0.036)
Train: 216 [ 250/1251 ( 20%)]  Loss: 4.259 (4.17)  Time: 0.170s, 6018.17/s  (0.195s, 5258.94/s)  LR: 1.821e-04  Data: 0.024 (0.034)
Train: 216 [ 300/1251 ( 24%)]  Loss: 4.301 (4.19)  Time: 0.508s, 2015.64/s  (0.194s, 5269.93/s)  LR: 1.821e-04  Data: 0.028 (0.033)
Train: 216 [ 350/1251 ( 28%)]  Loss: 4.354 (4.21)  Time: 0.186s, 5493.85/s  (0.194s, 5274.87/s)  LR: 1.821e-04  Data: 0.024 (0.033)
Train: 216 [ 400/1251 ( 32%)]  Loss: 3.921 (4.18)  Time: 0.194s, 5265.38/s  (0.194s, 5272.81/s)  LR: 1.821e-04  Data: 0.025 (0.032)
Train: 216 [ 450/1251 ( 36%)]  Loss: 4.052 (4.16)  Time: 0.180s, 5682.57/s  (0.193s, 5298.42/s)  LR: 1.821e-04  Data: 0.027 (0.032)
Train: 216 [ 500/1251 ( 40%)]  Loss: 4.330 (4.18)  Time: 0.160s, 6387.80/s  (0.193s, 5314.91/s)  LR: 1.821e-04  Data: 0.033 (0.031)
Train: 216 [ 550/1251 ( 44%)]  Loss: 3.960 (4.16)  Time: 0.178s, 5761.54/s  (0.192s, 5331.66/s)  LR: 1.821e-04  Data: 0.040 (0.031)
Train: 216 [ 600/1251 ( 48%)]  Loss: 3.742 (4.13)  Time: 0.192s, 5344.43/s  (0.192s, 5334.56/s)  LR: 1.821e-04  Data: 0.032 (0.031)
Train: 216 [ 650/1251 ( 52%)]  Loss: 4.216 (4.13)  Time: 0.276s, 3710.44/s  (0.192s, 5331.91/s)  LR: 1.821e-04  Data: 0.023 (0.031)
Train: 216 [ 700/1251 ( 56%)]  Loss: 4.045 (4.13)  Time: 0.160s, 6405.24/s  (0.192s, 5330.96/s)  LR: 1.821e-04  Data: 0.027 (0.031)
Train: 216 [ 750/1251 ( 60%)]  Loss: 4.186 (4.13)  Time: 0.161s, 6356.24/s  (0.192s, 5329.85/s)  LR: 1.821e-04  Data: 0.032 (0.030)
Train: 216 [ 800/1251 ( 64%)]  Loss: 4.149 (4.13)  Time: 0.173s, 5929.61/s  (0.192s, 5324.27/s)  LR: 1.821e-04  Data: 0.028 (0.030)
Train: 216 [ 850/1251 ( 68%)]  Loss: 4.032 (4.13)  Time: 0.192s, 5346.39/s  (0.192s, 5328.42/s)  LR: 1.821e-04  Data: 0.031 (0.030)
Train: 216 [ 900/1251 ( 72%)]  Loss: 3.609 (4.10)  Time: 0.180s, 5683.63/s  (0.192s, 5320.44/s)  LR: 1.821e-04  Data: 0.024 (0.030)
Train: 216 [ 950/1251 ( 76%)]  Loss: 4.028 (4.10)  Time: 0.174s, 5872.58/s  (0.192s, 5331.30/s)  LR: 1.821e-04  Data: 0.025 (0.030)
Train: 216 [1000/1251 ( 80%)]  Loss: 3.940 (4.09)  Time: 0.259s, 3955.90/s  (0.192s, 5326.23/s)  LR: 1.821e-04  Data: 0.136 (0.031)
Train: 216 [1050/1251 ( 84%)]  Loss: 4.350 (4.10)  Time: 0.166s, 6163.69/s  (0.192s, 5324.86/s)  LR: 1.821e-04  Data: 0.024 (0.031)
Train: 216 [1100/1251 ( 88%)]  Loss: 4.205 (4.11)  Time: 0.177s, 5784.84/s  (0.192s, 5323.70/s)  LR: 1.821e-04  Data: 0.023 (0.031)
Train: 216 [1150/1251 ( 92%)]  Loss: 4.305 (4.11)  Time: 0.160s, 6404.89/s  (0.193s, 5317.42/s)  LR: 1.821e-04  Data: 0.028 (0.031)
Train: 216 [1200/1251 ( 96%)]  Loss: 4.399 (4.13)  Time: 0.180s, 5697.99/s  (0.193s, 5317.62/s)  LR: 1.821e-04  Data: 0.048 (0.031)
Train: 216 [1250/1251 (100%)]  Loss: 4.261 (4.13)  Time: 0.113s, 9062.87/s  (0.192s, 5337.60/s)  LR: 1.821e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.890 (1.890)  Loss:  1.0274 (1.0274)  Acc@1: 81.9336 (81.9336)  Acc@5: 94.9219 (94.9219)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0546 (1.6422)  Acc@1: 82.1934 (68.2520)  Acc@5: 95.4009 (88.2340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-208.pth.tar', 67.868)

Train: 217 [   0/1251 (  0%)]  Loss: 4.528 (4.53)  Time: 1.764s,  580.45/s  (1.764s,  580.45/s)  LR: 1.781e-04  Data: 1.644 (1.644)
Train: 217 [  50/1251 (  4%)]  Loss: 3.819 (4.17)  Time: 0.151s, 6760.45/s  (0.224s, 4562.12/s)  LR: 1.781e-04  Data: 0.031 (0.083)
Train: 217 [ 100/1251 (  8%)]  Loss: 4.190 (4.18)  Time: 0.156s, 6565.37/s  (0.210s, 4881.82/s)  LR: 1.781e-04  Data: 0.025 (0.066)
Train: 217 [ 150/1251 ( 12%)]  Loss: 3.565 (4.03)  Time: 0.173s, 5914.08/s  (0.203s, 5053.03/s)  LR: 1.781e-04  Data: 0.026 (0.058)
Train: 217 [ 200/1251 ( 16%)]  Loss: 4.000 (4.02)  Time: 0.168s, 6081.11/s  (0.199s, 5145.01/s)  LR: 1.781e-04  Data: 0.026 (0.055)
Train: 217 [ 250/1251 ( 20%)]  Loss: 3.867 (3.99)  Time: 0.181s, 5672.31/s  (0.198s, 5183.70/s)  LR: 1.781e-04  Data: 0.025 (0.050)
Train: 217 [ 300/1251 ( 24%)]  Loss: 4.048 (4.00)  Time: 0.163s, 6266.47/s  (0.196s, 5221.32/s)  LR: 1.781e-04  Data: 0.025 (0.046)
Train: 217 [ 350/1251 ( 28%)]  Loss: 3.977 (4.00)  Time: 0.172s, 5943.63/s  (0.194s, 5266.68/s)  LR: 1.781e-04  Data: 0.023 (0.044)
Train: 217 [ 400/1251 ( 32%)]  Loss: 3.924 (3.99)  Time: 0.168s, 6109.49/s  (0.193s, 5293.75/s)  LR: 1.781e-04  Data: 0.030 (0.041)
Train: 217 [ 450/1251 ( 36%)]  Loss: 4.109 (4.00)  Time: 0.172s, 5949.50/s  (0.194s, 5287.76/s)  LR: 1.781e-04  Data: 0.028 (0.040)
Train: 217 [ 500/1251 ( 40%)]  Loss: 4.341 (4.03)  Time: 0.193s, 5315.92/s  (0.193s, 5313.62/s)  LR: 1.781e-04  Data: 0.033 (0.039)
Train: 217 [ 550/1251 ( 44%)]  Loss: 4.079 (4.04)  Time: 0.300s, 3408.06/s  (0.193s, 5310.06/s)  LR: 1.781e-04  Data: 0.025 (0.038)
Train: 217 [ 600/1251 ( 48%)]  Loss: 3.947 (4.03)  Time: 0.175s, 5847.06/s  (0.193s, 5315.98/s)  LR: 1.781e-04  Data: 0.027 (0.037)
Train: 217 [ 650/1251 ( 52%)]  Loss: 3.506 (3.99)  Time: 0.180s, 5697.42/s  (0.193s, 5308.56/s)  LR: 1.781e-04  Data: 0.026 (0.036)
Train: 217 [ 700/1251 ( 56%)]  Loss: 4.319 (4.01)  Time: 0.178s, 5762.69/s  (0.193s, 5317.73/s)  LR: 1.781e-04  Data: 0.035 (0.036)
Train: 217 [ 750/1251 ( 60%)]  Loss: 4.128 (4.02)  Time: 0.345s, 2971.49/s  (0.193s, 5306.39/s)  LR: 1.781e-04  Data: 0.034 (0.035)
Train: 217 [ 800/1251 ( 64%)]  Loss: 4.200 (4.03)  Time: 0.171s, 5972.38/s  (0.193s, 5308.61/s)  LR: 1.781e-04  Data: 0.025 (0.035)
Train: 217 [ 850/1251 ( 68%)]  Loss: 4.390 (4.05)  Time: 0.182s, 5625.45/s  (0.192s, 5319.79/s)  LR: 1.781e-04  Data: 0.026 (0.035)
Train: 217 [ 900/1251 ( 72%)]  Loss: 4.025 (4.05)  Time: 0.170s, 6026.40/s  (0.193s, 5312.93/s)  LR: 1.781e-04  Data: 0.029 (0.034)
Train: 217 [ 950/1251 ( 76%)]  Loss: 3.970 (4.05)  Time: 0.215s, 4758.04/s  (0.193s, 5318.32/s)  LR: 1.781e-04  Data: 0.031 (0.034)
Train: 217 [1000/1251 ( 80%)]  Loss: 4.279 (4.06)  Time: 0.175s, 5851.85/s  (0.193s, 5312.01/s)  LR: 1.781e-04  Data: 0.028 (0.034)
Train: 217 [1050/1251 ( 84%)]  Loss: 4.387 (4.07)  Time: 0.158s, 6472.38/s  (0.193s, 5311.02/s)  LR: 1.781e-04  Data: 0.027 (0.034)
Train: 217 [1100/1251 ( 88%)]  Loss: 4.141 (4.08)  Time: 0.162s, 6311.35/s  (0.193s, 5314.85/s)  LR: 1.781e-04  Data: 0.028 (0.033)
Train: 217 [1150/1251 ( 92%)]  Loss: 3.897 (4.07)  Time: 0.169s, 6046.69/s  (0.193s, 5315.47/s)  LR: 1.781e-04  Data: 0.022 (0.033)
Train: 217 [1200/1251 ( 96%)]  Loss: 4.147 (4.07)  Time: 0.163s, 6267.80/s  (0.193s, 5306.06/s)  LR: 1.781e-04  Data: 0.025 (0.033)
Train: 217 [1250/1251 (100%)]  Loss: 3.605 (4.05)  Time: 0.114s, 9020.22/s  (0.192s, 5320.23/s)  LR: 1.781e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.689 (1.689)  Loss:  1.0516 (1.0516)  Acc@1: 83.3984 (83.3984)  Acc@5: 95.3125 (95.3125)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.1087 (1.6442)  Acc@1: 81.8396 (68.2460)  Acc@5: 94.9292 (88.3660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-204.pth.tar', 67.94599994140626)

Train: 218 [   0/1251 (  0%)]  Loss: 3.962 (3.96)  Time: 1.672s,  612.50/s  (1.672s,  612.50/s)  LR: 1.741e-04  Data: 1.551 (1.551)
Train: 218 [  50/1251 (  4%)]  Loss: 4.210 (4.09)  Time: 0.174s, 5898.10/s  (0.223s, 4601.50/s)  LR: 1.741e-04  Data: 0.023 (0.075)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 218 [ 100/1251 (  8%)]  Loss: 4.104 (4.09)  Time: 0.175s, 5848.25/s  (0.207s, 4939.13/s)  LR: 1.741e-04  Data: 0.022 (0.052)
Train: 218 [ 150/1251 ( 12%)]  Loss: 3.798 (4.02)  Time: 0.176s, 5825.56/s  (0.203s, 5045.37/s)  LR: 1.741e-04  Data: 0.020 (0.044)
Train: 218 [ 200/1251 ( 16%)]  Loss: 3.777 (3.97)  Time: 0.225s, 4559.08/s  (0.201s, 5097.11/s)  LR: 1.741e-04  Data: 0.026 (0.040)
Train: 218 [ 250/1251 ( 20%)]  Loss: 4.128 (4.00)  Time: 0.177s, 5780.52/s  (0.199s, 5143.84/s)  LR: 1.741e-04  Data: 0.026 (0.038)
Train: 218 [ 300/1251 ( 24%)]  Loss: 3.994 (4.00)  Time: 0.162s, 6327.96/s  (0.197s, 5205.84/s)  LR: 1.741e-04  Data: 0.028 (0.036)
Train: 218 [ 350/1251 ( 28%)]  Loss: 3.617 (3.95)  Time: 0.157s, 6516.80/s  (0.195s, 5253.97/s)  LR: 1.741e-04  Data: 0.027 (0.035)
Train: 218 [ 400/1251 ( 32%)]  Loss: 3.994 (3.95)  Time: 0.197s, 5194.73/s  (0.194s, 5267.10/s)  LR: 1.741e-04  Data: 0.034 (0.034)
Train: 218 [ 450/1251 ( 36%)]  Loss: 4.286 (3.99)  Time: 0.175s, 5836.05/s  (0.194s, 5271.23/s)  LR: 1.741e-04  Data: 0.028 (0.034)
Train: 218 [ 500/1251 ( 40%)]  Loss: 3.973 (3.99)  Time: 0.158s, 6460.74/s  (0.194s, 5282.37/s)  LR: 1.741e-04  Data: 0.029 (0.033)
Train: 218 [ 550/1251 ( 44%)]  Loss: 4.334 (4.01)  Time: 0.166s, 6162.33/s  (0.193s, 5293.95/s)  LR: 1.741e-04  Data: 0.030 (0.033)
Train: 218 [ 600/1251 ( 48%)]  Loss: 3.833 (4.00)  Time: 0.167s, 6126.64/s  (0.193s, 5296.03/s)  LR: 1.741e-04  Data: 0.022 (0.032)
Train: 218 [ 650/1251 ( 52%)]  Loss: 4.248 (4.02)  Time: 0.173s, 5910.64/s  (0.193s, 5300.06/s)  LR: 1.741e-04  Data: 0.026 (0.032)
Train: 218 [ 700/1251 ( 56%)]  Loss: 4.137 (4.03)  Time: 0.181s, 5660.34/s  (0.193s, 5297.01/s)  LR: 1.741e-04  Data: 0.026 (0.032)
Train: 218 [ 750/1251 ( 60%)]  Loss: 3.956 (4.02)  Time: 0.157s, 6528.39/s  (0.193s, 5306.53/s)  LR: 1.741e-04  Data: 0.028 (0.032)
Train: 218 [ 800/1251 ( 64%)]  Loss: 3.950 (4.02)  Time: 0.176s, 5821.55/s  (0.193s, 5305.07/s)  LR: 1.741e-04  Data: 0.022 (0.031)
Train: 218 [ 850/1251 ( 68%)]  Loss: 4.134 (4.02)  Time: 0.190s, 5393.34/s  (0.193s, 5313.66/s)  LR: 1.741e-04  Data: 0.029 (0.031)
Train: 218 [ 900/1251 ( 72%)]  Loss: 4.060 (4.03)  Time: 0.173s, 5921.76/s  (0.193s, 5302.92/s)  LR: 1.741e-04  Data: 0.023 (0.031)
Train: 218 [ 950/1251 ( 76%)]  Loss: 4.361 (4.04)  Time: 0.155s, 6603.24/s  (0.193s, 5303.75/s)  LR: 1.741e-04  Data: 0.027 (0.031)
Train: 218 [1000/1251 ( 80%)]  Loss: 3.991 (4.04)  Time: 0.274s, 3739.79/s  (0.193s, 5296.43/s)  LR: 1.741e-04  Data: 0.034 (0.031)
Train: 218 [1050/1251 ( 84%)]  Loss: 4.415 (4.06)  Time: 0.195s, 5259.60/s  (0.193s, 5298.18/s)  LR: 1.741e-04  Data: 0.019 (0.031)
Train: 218 [1100/1251 ( 88%)]  Loss: 3.854 (4.05)  Time: 0.163s, 6266.72/s  (0.193s, 5293.65/s)  LR: 1.741e-04  Data: 0.025 (0.031)
Train: 218 [1150/1251 ( 92%)]  Loss: 4.016 (4.05)  Time: 0.159s, 6436.39/s  (0.193s, 5293.77/s)  LR: 1.741e-04  Data: 0.026 (0.031)
Train: 218 [1200/1251 ( 96%)]  Loss: 4.256 (4.06)  Time: 0.177s, 5772.87/s  (0.194s, 5291.72/s)  LR: 1.741e-04  Data: 0.029 (0.030)
Train: 218 [1250/1251 (100%)]  Loss: 3.949 (4.05)  Time: 0.113s, 9024.82/s  (0.193s, 5304.67/s)  LR: 1.741e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.798 (1.798)  Loss:  1.0229 (1.0229)  Acc@1: 83.2031 (83.2031)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1824 (1.6598)  Acc@1: 81.3679 (68.4100)  Acc@5: 95.4009 (88.4780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-203.pth.tar', 67.99200009765624)

Train: 219 [   0/1251 (  0%)]  Loss: 4.001 (4.00)  Time: 1.809s,  565.98/s  (1.809s,  565.98/s)  LR: 1.702e-04  Data: 1.690 (1.690)
Train: 219 [  50/1251 (  4%)]  Loss: 4.384 (4.19)  Time: 0.173s, 5925.38/s  (0.224s, 4562.46/s)  LR: 1.702e-04  Data: 0.022 (0.067)
Train: 219 [ 100/1251 (  8%)]  Loss: 4.155 (4.18)  Time: 0.158s, 6467.92/s  (0.203s, 5042.43/s)  LR: 1.702e-04  Data: 0.024 (0.048)
Train: 219 [ 150/1251 ( 12%)]  Loss: 4.055 (4.15)  Time: 0.186s, 5499.27/s  (0.199s, 5150.16/s)  LR: 1.702e-04  Data: 0.041 (0.047)
Train: 219 [ 200/1251 ( 16%)]  Loss: 4.201 (4.16)  Time: 0.181s, 5645.42/s  (0.197s, 5186.65/s)  LR: 1.702e-04  Data: 0.027 (0.046)
Train: 219 [ 250/1251 ( 20%)]  Loss: 4.371 (4.19)  Time: 0.176s, 5832.01/s  (0.196s, 5227.85/s)  LR: 1.702e-04  Data: 0.024 (0.043)
Train: 219 [ 300/1251 ( 24%)]  Loss: 4.300 (4.21)  Time: 0.181s, 5651.98/s  (0.194s, 5282.62/s)  LR: 1.702e-04  Data: 0.028 (0.041)
Train: 219 [ 350/1251 ( 28%)]  Loss: 4.063 (4.19)  Time: 0.201s, 5101.82/s  (0.193s, 5293.31/s)  LR: 1.702e-04  Data: 0.028 (0.042)
Train: 219 [ 400/1251 ( 32%)]  Loss: 4.247 (4.20)  Time: 0.195s, 5260.90/s  (0.193s, 5314.85/s)  LR: 1.702e-04  Data: 0.028 (0.041)
Train: 219 [ 450/1251 ( 36%)]  Loss: 4.068 (4.18)  Time: 0.166s, 6185.03/s  (0.193s, 5319.18/s)  LR: 1.702e-04  Data: 0.027 (0.040)
Train: 219 [ 500/1251 ( 40%)]  Loss: 4.182 (4.18)  Time: 0.178s, 5738.42/s  (0.192s, 5327.44/s)  LR: 1.702e-04  Data: 0.032 (0.039)
Train: 219 [ 550/1251 ( 44%)]  Loss: 4.144 (4.18)  Time: 0.232s, 4409.66/s  (0.192s, 5321.96/s)  LR: 1.702e-04  Data: 0.031 (0.038)
Train: 219 [ 600/1251 ( 48%)]  Loss: 3.715 (4.15)  Time: 0.176s, 5807.98/s  (0.192s, 5331.99/s)  LR: 1.702e-04  Data: 0.030 (0.038)
Train: 219 [ 650/1251 ( 52%)]  Loss: 4.191 (4.15)  Time: 0.182s, 5629.42/s  (0.192s, 5327.11/s)  LR: 1.702e-04  Data: 0.026 (0.037)
Train: 219 [ 700/1251 ( 56%)]  Loss: 3.799 (4.13)  Time: 0.187s, 5470.30/s  (0.192s, 5344.06/s)  LR: 1.702e-04  Data: 0.025 (0.036)
Train: 219 [ 750/1251 ( 60%)]  Loss: 4.162 (4.13)  Time: 0.290s, 3531.63/s  (0.192s, 5329.65/s)  LR: 1.702e-04  Data: 0.041 (0.036)
Train: 219 [ 800/1251 ( 64%)]  Loss: 4.242 (4.13)  Time: 0.162s, 6324.90/s  (0.192s, 5330.20/s)  LR: 1.702e-04  Data: 0.025 (0.035)
Train: 219 [ 850/1251 ( 68%)]  Loss: 3.618 (4.11)  Time: 0.158s, 6486.42/s  (0.193s, 5319.25/s)  LR: 1.702e-04  Data: 0.030 (0.035)
Train: 219 [ 900/1251 ( 72%)]  Loss: 4.306 (4.12)  Time: 0.171s, 5980.81/s  (0.192s, 5319.93/s)  LR: 1.702e-04  Data: 0.019 (0.034)
Train: 219 [ 950/1251 ( 76%)]  Loss: 4.203 (4.12)  Time: 0.313s, 3273.85/s  (0.193s, 5315.46/s)  LR: 1.702e-04  Data: 0.032 (0.034)
Train: 219 [1000/1251 ( 80%)]  Loss: 3.673 (4.10)  Time: 0.181s, 5644.18/s  (0.192s, 5326.57/s)  LR: 1.702e-04  Data: 0.038 (0.034)
Train: 219 [1050/1251 ( 84%)]  Loss: 3.910 (4.09)  Time: 0.212s, 4833.89/s  (0.193s, 5316.35/s)  LR: 1.702e-04  Data: 0.025 (0.034)
Train: 219 [1100/1251 ( 88%)]  Loss: 4.051 (4.09)  Time: 0.164s, 6240.97/s  (0.192s, 5321.58/s)  LR: 1.702e-04  Data: 0.031 (0.033)
Train: 219 [1150/1251 ( 92%)]  Loss: 3.849 (4.08)  Time: 0.557s, 1839.41/s  (0.193s, 5306.16/s)  LR: 1.702e-04  Data: 0.023 (0.033)
Train: 219 [1200/1251 ( 96%)]  Loss: 4.124 (4.08)  Time: 0.159s, 6422.72/s  (0.193s, 5311.42/s)  LR: 1.702e-04  Data: 0.033 (0.033)
Train: 219 [1250/1251 (100%)]  Loss: 4.099 (4.08)  Time: 0.114s, 9012.32/s  (0.192s, 5321.93/s)  LR: 1.702e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.850 (1.850)  Loss:  0.9812 (0.9812)  Acc@1: 82.7148 (82.7148)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.0498 (1.6237)  Acc@1: 81.7217 (68.4960)  Acc@5: 95.1651 (88.5960)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-210.pth.tar', 68.01400004394532)

Train: 220 [   0/1251 (  0%)]  Loss: 3.837 (3.84)  Time: 1.688s,  606.50/s  (1.688s,  606.50/s)  LR: 1.663e-04  Data: 1.538 (1.538)
Train: 220 [  50/1251 (  4%)]  Loss: 3.994 (3.92)  Time: 0.178s, 5754.36/s  (0.222s, 4614.65/s)  LR: 1.663e-04  Data: 0.024 (0.076)
Train: 220 [ 100/1251 (  8%)]  Loss: 4.355 (4.06)  Time: 0.321s, 3191.33/s  (0.208s, 4920.95/s)  LR: 1.663e-04  Data: 0.191 (0.062)
Train: 220 [ 150/1251 ( 12%)]  Loss: 4.415 (4.15)  Time: 0.187s, 5466.37/s  (0.203s, 5048.83/s)  LR: 1.663e-04  Data: 0.020 (0.057)
Train: 220 [ 200/1251 ( 16%)]  Loss: 4.378 (4.20)  Time: 0.175s, 5853.14/s  (0.201s, 5105.85/s)  LR: 1.663e-04  Data: 0.024 (0.054)
Train: 220 [ 250/1251 ( 20%)]  Loss: 4.202 (4.20)  Time: 0.163s, 6279.44/s  (0.196s, 5224.45/s)  LR: 1.663e-04  Data: 0.025 (0.050)
Train: 220 [ 300/1251 ( 24%)]  Loss: 3.766 (4.14)  Time: 0.416s, 2464.03/s  (0.196s, 5236.20/s)  LR: 1.663e-04  Data: 0.289 (0.050)
Train: 220 [ 350/1251 ( 28%)]  Loss: 4.496 (4.18)  Time: 0.185s, 5521.76/s  (0.195s, 5249.25/s)  LR: 1.663e-04  Data: 0.030 (0.050)
Train: 220 [ 400/1251 ( 32%)]  Loss: 3.899 (4.15)  Time: 0.189s, 5425.60/s  (0.194s, 5273.52/s)  LR: 1.663e-04  Data: 0.023 (0.048)
Train: 220 [ 450/1251 ( 36%)]  Loss: 4.093 (4.14)  Time: 0.167s, 6148.44/s  (0.193s, 5304.48/s)  LR: 1.663e-04  Data: 0.029 (0.047)
Train: 220 [ 500/1251 ( 40%)]  Loss: 3.761 (4.11)  Time: 0.187s, 5464.00/s  (0.193s, 5308.37/s)  LR: 1.663e-04  Data: 0.033 (0.046)
Train: 220 [ 550/1251 ( 44%)]  Loss: 4.015 (4.10)  Time: 0.290s, 3526.82/s  (0.193s, 5305.91/s)  LR: 1.663e-04  Data: 0.033 (0.044)
Train: 220 [ 600/1251 ( 48%)]  Loss: 4.074 (4.10)  Time: 0.160s, 6380.88/s  (0.192s, 5323.07/s)  LR: 1.663e-04  Data: 0.027 (0.043)
Train: 220 [ 650/1251 ( 52%)]  Loss: 3.808 (4.08)  Time: 0.196s, 5227.95/s  (0.193s, 5315.05/s)  LR: 1.663e-04  Data: 0.035 (0.042)
Train: 220 [ 700/1251 ( 56%)]  Loss: 3.984 (4.07)  Time: 0.177s, 5778.41/s  (0.193s, 5307.13/s)  LR: 1.663e-04  Data: 0.038 (0.041)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 220 [ 750/1251 ( 60%)]  Loss: 4.087 (4.07)  Time: 0.177s, 5796.60/s  (0.193s, 5316.08/s)  LR: 1.663e-04  Data: 0.024 (0.040)
Train: 220 [ 800/1251 ( 64%)]  Loss: 3.853 (4.06)  Time: 0.175s, 5862.08/s  (0.193s, 5318.62/s)  LR: 1.663e-04  Data: 0.030 (0.039)
Train: 220 [ 850/1251 ( 68%)]  Loss: 4.425 (4.08)  Time: 0.182s, 5626.05/s  (0.193s, 5308.68/s)  LR: 1.663e-04  Data: 0.029 (0.039)
Train: 220 [ 900/1251 ( 72%)]  Loss: 4.140 (4.08)  Time: 0.191s, 5369.70/s  (0.193s, 5309.20/s)  LR: 1.663e-04  Data: 0.029 (0.038)
Train: 220 [ 950/1251 ( 76%)]  Loss: 4.171 (4.09)  Time: 0.175s, 5848.85/s  (0.193s, 5315.88/s)  LR: 1.663e-04  Data: 0.023 (0.038)
Train: 220 [1000/1251 ( 80%)]  Loss: 4.265 (4.10)  Time: 0.162s, 6309.68/s  (0.193s, 5315.44/s)  LR: 1.663e-04  Data: 0.030 (0.037)
Train: 220 [1050/1251 ( 84%)]  Loss: 3.925 (4.09)  Time: 0.186s, 5505.58/s  (0.193s, 5310.95/s)  LR: 1.663e-04  Data: 0.057 (0.037)
Train: 220 [1100/1251 ( 88%)]  Loss: 4.297 (4.10)  Time: 0.174s, 5877.57/s  (0.193s, 5301.46/s)  LR: 1.663e-04  Data: 0.045 (0.037)
Train: 220 [1150/1251 ( 92%)]  Loss: 3.997 (4.09)  Time: 0.158s, 6486.14/s  (0.193s, 5292.14/s)  LR: 1.663e-04  Data: 0.028 (0.036)
Train: 220 [1200/1251 ( 96%)]  Loss: 3.810 (4.08)  Time: 0.169s, 6055.20/s  (0.193s, 5295.82/s)  LR: 1.663e-04  Data: 0.034 (0.036)
Train: 220 [1250/1251 (100%)]  Loss: 3.812 (4.07)  Time: 0.113s, 9037.57/s  (0.193s, 5310.08/s)  LR: 1.663e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.787 (1.787)  Loss:  0.9825 (0.9825)  Acc@1: 84.3750 (84.3750)  Acc@5: 96.0938 (96.0938)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.0602 (1.6196)  Acc@1: 82.6651 (68.7520)  Acc@5: 94.9292 (88.6440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-215.pth.tar', 68.04600008300781)

Train: 221 [   0/1251 (  0%)]  Loss: 4.287 (4.29)  Time: 1.773s,  577.68/s  (1.773s,  577.68/s)  LR: 1.624e-04  Data: 1.649 (1.649)
Train: 221 [  50/1251 (  4%)]  Loss: 4.174 (4.23)  Time: 0.182s, 5629.10/s  (0.222s, 4602.28/s)  LR: 1.624e-04  Data: 0.021 (0.076)
Train: 221 [ 100/1251 (  8%)]  Loss: 4.015 (4.16)  Time: 0.187s, 5485.26/s  (0.208s, 4924.06/s)  LR: 1.624e-04  Data: 0.026 (0.061)
Train: 221 [ 150/1251 ( 12%)]  Loss: 4.141 (4.15)  Time: 0.174s, 5868.69/s  (0.199s, 5135.90/s)  LR: 1.624e-04  Data: 0.028 (0.052)
Train: 221 [ 200/1251 ( 16%)]  Loss: 4.233 (4.17)  Time: 0.164s, 6236.03/s  (0.197s, 5198.40/s)  LR: 1.624e-04  Data: 0.031 (0.050)
Train: 221 [ 250/1251 ( 20%)]  Loss: 3.920 (4.13)  Time: 0.171s, 5971.96/s  (0.196s, 5222.93/s)  LR: 1.624e-04  Data: 0.028 (0.047)
Train: 221 [ 300/1251 ( 24%)]  Loss: 4.162 (4.13)  Time: 0.172s, 5948.53/s  (0.196s, 5236.30/s)  LR: 1.624e-04  Data: 0.026 (0.044)
Train: 221 [ 350/1251 ( 28%)]  Loss: 3.800 (4.09)  Time: 0.164s, 6235.76/s  (0.193s, 5297.46/s)  LR: 1.624e-04  Data: 0.025 (0.041)
Train: 221 [ 400/1251 ( 32%)]  Loss: 4.199 (4.10)  Time: 0.177s, 5791.39/s  (0.193s, 5302.91/s)  LR: 1.624e-04  Data: 0.026 (0.040)
Train: 221 [ 450/1251 ( 36%)]  Loss: 3.815 (4.07)  Time: 0.181s, 5652.71/s  (0.194s, 5276.58/s)  LR: 1.624e-04  Data: 0.028 (0.038)
Train: 221 [ 500/1251 ( 40%)]  Loss: 4.028 (4.07)  Time: 0.162s, 6335.40/s  (0.193s, 5306.21/s)  LR: 1.624e-04  Data: 0.025 (0.038)
Train: 221 [ 550/1251 ( 44%)]  Loss: 4.172 (4.08)  Time: 0.173s, 5902.47/s  (0.193s, 5316.58/s)  LR: 1.624e-04  Data: 0.027 (0.037)
Train: 221 [ 600/1251 ( 48%)]  Loss: 4.019 (4.07)  Time: 0.174s, 5879.96/s  (0.192s, 5326.96/s)  LR: 1.624e-04  Data: 0.033 (0.036)
Train: 221 [ 650/1251 ( 52%)]  Loss: 4.017 (4.07)  Time: 0.176s, 5807.97/s  (0.193s, 5313.89/s)  LR: 1.624e-04  Data: 0.022 (0.035)
Train: 221 [ 700/1251 ( 56%)]  Loss: 4.064 (4.07)  Time: 0.163s, 6273.62/s  (0.192s, 5333.55/s)  LR: 1.624e-04  Data: 0.029 (0.035)
Train: 221 [ 750/1251 ( 60%)]  Loss: 4.100 (4.07)  Time: 0.179s, 5731.35/s  (0.192s, 5322.11/s)  LR: 1.624e-04  Data: 0.037 (0.034)
Train: 221 [ 800/1251 ( 64%)]  Loss: 3.901 (4.06)  Time: 0.171s, 5973.77/s  (0.192s, 5319.80/s)  LR: 1.624e-04  Data: 0.029 (0.034)
Train: 221 [ 850/1251 ( 68%)]  Loss: 4.179 (4.07)  Time: 0.186s, 5493.09/s  (0.193s, 5318.49/s)  LR: 1.624e-04  Data: 0.026 (0.034)
Train: 221 [ 900/1251 ( 72%)]  Loss: 3.999 (4.06)  Time: 0.164s, 6226.72/s  (0.192s, 5321.69/s)  LR: 1.624e-04  Data: 0.032 (0.033)
Train: 221 [ 950/1251 ( 76%)]  Loss: 4.343 (4.08)  Time: 0.181s, 5646.27/s  (0.192s, 5324.98/s)  LR: 1.624e-04  Data: 0.037 (0.033)
Train: 221 [1000/1251 ( 80%)]  Loss: 3.779 (4.06)  Time: 0.175s, 5850.80/s  (0.193s, 5317.38/s)  LR: 1.624e-04  Data: 0.031 (0.033)
Train: 221 [1050/1251 ( 84%)]  Loss: 4.199 (4.07)  Time: 0.165s, 6195.81/s  (0.193s, 5313.99/s)  LR: 1.624e-04  Data: 0.023 (0.033)
Train: 221 [1100/1251 ( 88%)]  Loss: 4.095 (4.07)  Time: 0.160s, 6410.65/s  (0.193s, 5307.72/s)  LR: 1.624e-04  Data: 0.024 (0.032)
Train: 221 [1150/1251 ( 92%)]  Loss: 3.536 (4.05)  Time: 0.178s, 5760.58/s  (0.193s, 5299.76/s)  LR: 1.624e-04  Data: 0.027 (0.032)
Train: 221 [1200/1251 ( 96%)]  Loss: 4.405 (4.06)  Time: 0.165s, 6193.73/s  (0.193s, 5305.53/s)  LR: 1.624e-04  Data: 0.032 (0.032)
Train: 221 [1250/1251 (100%)]  Loss: 4.137 (4.07)  Time: 0.113s, 9031.18/s  (0.193s, 5316.22/s)  LR: 1.624e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.790 (1.790)  Loss:  1.0161 (1.0161)  Acc@1: 83.4961 (83.4961)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.0605 (1.6001)  Acc@1: 81.1321 (68.8400)  Acc@5: 95.1651 (88.5200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-207.pth.tar', 68.11599994628907)

Train: 222 [   0/1251 (  0%)]  Loss: 4.488 (4.49)  Time: 1.944s,  526.85/s  (1.944s,  526.85/s)  LR: 1.586e-04  Data: 1.805 (1.805)
Train: 222 [  50/1251 (  4%)]  Loss: 3.805 (4.15)  Time: 0.156s, 6577.08/s  (0.220s, 4654.50/s)  LR: 1.586e-04  Data: 0.023 (0.068)
Train: 222 [ 100/1251 (  8%)]  Loss: 3.858 (4.05)  Time: 0.175s, 5863.28/s  (0.205s, 4997.96/s)  LR: 1.586e-04  Data: 0.033 (0.049)
Train: 222 [ 150/1251 ( 12%)]  Loss: 4.014 (4.04)  Time: 0.163s, 6285.96/s  (0.198s, 5179.21/s)  LR: 1.586e-04  Data: 0.026 (0.042)
Train: 222 [ 200/1251 ( 16%)]  Loss: 4.029 (4.04)  Time: 0.190s, 5378.89/s  (0.199s, 5145.91/s)  LR: 1.586e-04  Data: 0.025 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 222 [ 250/1251 ( 20%)]  Loss: 4.375 (4.09)  Time: 0.207s, 4951.34/s  (0.196s, 5222.06/s)  LR: 1.586e-04  Data: 0.033 (0.037)
Train: 222 [ 300/1251 ( 24%)]  Loss: 3.898 (4.07)  Time: 0.165s, 6197.02/s  (0.195s, 5262.89/s)  LR: 1.586e-04  Data: 0.025 (0.035)
Train: 222 [ 350/1251 ( 28%)]  Loss: 4.143 (4.08)  Time: 0.173s, 5904.18/s  (0.193s, 5305.62/s)  LR: 1.586e-04  Data: 0.030 (0.035)
Train: 222 [ 400/1251 ( 32%)]  Loss: 3.968 (4.06)  Time: 0.179s, 5732.65/s  (0.193s, 5305.30/s)  LR: 1.586e-04  Data: 0.024 (0.034)
Train: 222 [ 450/1251 ( 36%)]  Loss: 4.158 (4.07)  Time: 0.191s, 5371.57/s  (0.193s, 5303.40/s)  LR: 1.586e-04  Data: 0.028 (0.033)
Train: 222 [ 500/1251 ( 40%)]  Loss: 3.706 (4.04)  Time: 0.174s, 5892.12/s  (0.193s, 5304.16/s)  LR: 1.586e-04  Data: 0.022 (0.032)
Train: 222 [ 550/1251 ( 44%)]  Loss: 4.008 (4.04)  Time: 0.185s, 5530.43/s  (0.193s, 5311.19/s)  LR: 1.586e-04  Data: 0.027 (0.032)
Train: 222 [ 600/1251 ( 48%)]  Loss: 4.068 (4.04)  Time: 0.291s, 3514.48/s  (0.192s, 5319.93/s)  LR: 1.586e-04  Data: 0.025 (0.032)
Train: 222 [ 650/1251 ( 52%)]  Loss: 4.279 (4.06)  Time: 0.178s, 5765.23/s  (0.193s, 5298.47/s)  LR: 1.586e-04  Data: 0.050 (0.032)
Train: 222 [ 700/1251 ( 56%)]  Loss: 4.131 (4.06)  Time: 0.174s, 5897.48/s  (0.193s, 5308.46/s)  LR: 1.586e-04  Data: 0.030 (0.031)
Train: 222 [ 750/1251 ( 60%)]  Loss: 4.236 (4.07)  Time: 0.157s, 6509.14/s  (0.193s, 5312.95/s)  LR: 1.586e-04  Data: 0.034 (0.031)
Train: 222 [ 800/1251 ( 64%)]  Loss: 3.982 (4.07)  Time: 0.176s, 5827.49/s  (0.193s, 5304.25/s)  LR: 1.586e-04  Data: 0.025 (0.031)
Train: 222 [ 850/1251 ( 68%)]  Loss: 3.897 (4.06)  Time: 0.161s, 6345.56/s  (0.193s, 5310.65/s)  LR: 1.586e-04  Data: 0.024 (0.031)
Train: 222 [ 900/1251 ( 72%)]  Loss: 4.051 (4.06)  Time: 0.192s, 5321.76/s  (0.193s, 5306.44/s)  LR: 1.586e-04  Data: 0.026 (0.031)
Train: 222 [ 950/1251 ( 76%)]  Loss: 3.993 (4.05)  Time: 0.169s, 6045.43/s  (0.192s, 5321.28/s)  LR: 1.586e-04  Data: 0.034 (0.031)
Train: 222 [1000/1251 ( 80%)]  Loss: 4.092 (4.06)  Time: 0.168s, 6087.22/s  (0.192s, 5328.19/s)  LR: 1.586e-04  Data: 0.023 (0.030)
Train: 222 [1050/1251 ( 84%)]  Loss: 4.319 (4.07)  Time: 0.190s, 5386.11/s  (0.193s, 5305.15/s)  LR: 1.586e-04  Data: 0.028 (0.030)
Train: 222 [1100/1251 ( 88%)]  Loss: 4.130 (4.07)  Time: 0.181s, 5650.34/s  (0.193s, 5297.75/s)  LR: 1.586e-04  Data: 0.027 (0.030)
Train: 222 [1150/1251 ( 92%)]  Loss: 4.321 (4.08)  Time: 0.187s, 5468.99/s  (0.193s, 5318.33/s)  LR: 1.586e-04  Data: 0.025 (0.030)
Train: 222 [1200/1251 ( 96%)]  Loss: 3.974 (4.08)  Time: 0.169s, 6052.70/s  (0.193s, 5305.27/s)  LR: 1.586e-04  Data: 0.021 (0.031)
Train: 222 [1250/1251 (100%)]  Loss: 4.050 (4.08)  Time: 0.113s, 9032.22/s  (0.193s, 5316.83/s)  LR: 1.586e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.847 (1.847)  Loss:  1.0251 (1.0251)  Acc@1: 83.7891 (83.7891)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.1706 (1.6706)  Acc@1: 83.3726 (68.7580)  Acc@5: 95.4009 (88.6920)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-214.pth.tar', 68.15199994628907)

Train: 223 [   0/1251 (  0%)]  Loss: 3.740 (3.74)  Time: 1.708s,  599.37/s  (1.708s,  599.37/s)  LR: 1.548e-04  Data: 1.581 (1.581)
Train: 223 [  50/1251 (  4%)]  Loss: 4.243 (3.99)  Time: 0.210s, 4876.76/s  (0.230s, 4451.72/s)  LR: 1.548e-04  Data: 0.022 (0.082)
Train: 223 [ 100/1251 (  8%)]  Loss: 4.339 (4.11)  Time: 0.154s, 6631.48/s  (0.209s, 4892.20/s)  LR: 1.548e-04  Data: 0.027 (0.063)
Train: 223 [ 150/1251 ( 12%)]  Loss: 4.118 (4.11)  Time: 0.161s, 6368.30/s  (0.204s, 5029.49/s)  LR: 1.548e-04  Data: 0.026 (0.058)
Train: 223 [ 200/1251 ( 16%)]  Loss: 4.070 (4.10)  Time: 0.276s, 3716.06/s  (0.199s, 5156.00/s)  LR: 1.548e-04  Data: 0.150 (0.052)
Train: 223 [ 250/1251 ( 20%)]  Loss: 4.258 (4.13)  Time: 0.182s, 5623.42/s  (0.198s, 5171.90/s)  LR: 1.548e-04  Data: 0.022 (0.052)
Train: 223 [ 300/1251 ( 24%)]  Loss: 3.740 (4.07)  Time: 0.160s, 6383.70/s  (0.196s, 5234.36/s)  LR: 1.548e-04  Data: 0.029 (0.049)
Train: 223 [ 350/1251 ( 28%)]  Loss: 4.161 (4.08)  Time: 0.163s, 6267.01/s  (0.195s, 5258.17/s)  LR: 1.548e-04  Data: 0.025 (0.049)
Train: 223 [ 400/1251 ( 32%)]  Loss: 4.148 (4.09)  Time: 0.177s, 5772.82/s  (0.195s, 5254.66/s)  LR: 1.548e-04  Data: 0.035 (0.049)
Train: 223 [ 450/1251 ( 36%)]  Loss: 4.284 (4.11)  Time: 0.188s, 5439.02/s  (0.194s, 5268.48/s)  LR: 1.548e-04  Data: 0.019 (0.049)
Train: 223 [ 500/1251 ( 40%)]  Loss: 3.688 (4.07)  Time: 0.165s, 6208.76/s  (0.193s, 5293.87/s)  LR: 1.548e-04  Data: 0.029 (0.048)
Train: 223 [ 550/1251 ( 44%)]  Loss: 3.635 (4.04)  Time: 0.169s, 6057.12/s  (0.193s, 5309.96/s)  LR: 1.548e-04  Data: 0.029 (0.047)
Train: 223 [ 600/1251 ( 48%)]  Loss: 3.897 (4.02)  Time: 0.162s, 6325.07/s  (0.193s, 5296.10/s)  LR: 1.548e-04  Data: 0.033 (0.048)
Train: 223 [ 650/1251 ( 52%)]  Loss: 3.871 (4.01)  Time: 0.161s, 6354.72/s  (0.193s, 5303.30/s)  LR: 1.548e-04  Data: 0.020 (0.048)
Train: 223 [ 700/1251 ( 56%)]  Loss: 3.809 (4.00)  Time: 0.186s, 5511.03/s  (0.193s, 5304.36/s)  LR: 1.548e-04  Data: 0.026 (0.047)
Train: 223 [ 750/1251 ( 60%)]  Loss: 4.059 (4.00)  Time: 0.173s, 5929.97/s  (0.193s, 5317.52/s)  LR: 1.548e-04  Data: 0.025 (0.047)
Train: 223 [ 800/1251 ( 64%)]  Loss: 4.373 (4.03)  Time: 0.166s, 6170.24/s  (0.193s, 5312.97/s)  LR: 1.548e-04  Data: 0.027 (0.046)
Train: 223 [ 850/1251 ( 68%)]  Loss: 3.783 (4.01)  Time: 0.165s, 6188.95/s  (0.193s, 5305.23/s)  LR: 1.548e-04  Data: 0.024 (0.045)
Train: 223 [ 900/1251 ( 72%)]  Loss: 3.889 (4.01)  Time: 0.162s, 6322.02/s  (0.193s, 5306.17/s)  LR: 1.548e-04  Data: 0.029 (0.044)
Train: 223 [ 950/1251 ( 76%)]  Loss: 3.977 (4.00)  Time: 0.159s, 6440.06/s  (0.193s, 5310.01/s)  LR: 1.548e-04  Data: 0.023 (0.043)
Train: 223 [1000/1251 ( 80%)]  Loss: 4.060 (4.01)  Time: 0.171s, 5987.57/s  (0.193s, 5313.56/s)  LR: 1.548e-04  Data: 0.040 (0.042)
Train: 223 [1050/1251 ( 84%)]  Loss: 4.113 (4.01)  Time: 0.381s, 2688.03/s  (0.193s, 5308.42/s)  LR: 1.548e-04  Data: 0.031 (0.042)
Train: 223 [1100/1251 ( 88%)]  Loss: 3.659 (4.00)  Time: 0.192s, 5338.76/s  (0.193s, 5311.33/s)  LR: 1.548e-04  Data: 0.031 (0.041)
Train: 223 [1150/1251 ( 92%)]  Loss: 3.930 (3.99)  Time: 0.172s, 5956.93/s  (0.193s, 5301.08/s)  LR: 1.548e-04  Data: 0.030 (0.040)
Train: 223 [1200/1251 ( 96%)]  Loss: 3.983 (3.99)  Time: 0.181s, 5648.54/s  (0.193s, 5310.13/s)  LR: 1.548e-04  Data: 0.023 (0.040)
Train: 223 [1250/1251 (100%)]  Loss: 3.654 (3.98)  Time: 0.113s, 9035.00/s  (0.192s, 5324.64/s)  LR: 1.548e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.802 (1.802)  Loss:  1.0178 (1.0178)  Acc@1: 84.1797 (84.1797)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0812 (1.6308)  Acc@1: 82.4292 (68.7040)  Acc@5: 95.0472 (88.6380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-217.pth.tar', 68.24600012695312)

Train: 224 [   0/1251 (  0%)]  Loss: 4.148 (4.15)  Time: 1.817s,  563.49/s  (1.817s,  563.49/s)  LR: 1.510e-04  Data: 1.630 (1.630)
Train: 224 [  50/1251 (  4%)]  Loss: 4.073 (4.11)  Time: 0.166s, 6179.44/s  (0.229s, 4470.11/s)  LR: 1.510e-04  Data: 0.021 (0.064)
Train: 224 [ 100/1251 (  8%)]  Loss: 3.684 (3.97)  Time: 0.201s, 5094.93/s  (0.208s, 4925.71/s)  LR: 1.510e-04  Data: 0.023 (0.046)
Train: 224 [ 150/1251 ( 12%)]  Loss: 3.884 (3.95)  Time: 0.163s, 6273.00/s  (0.201s, 5096.10/s)  LR: 1.510e-04  Data: 0.025 (0.040)
Train: 224 [ 200/1251 ( 16%)]  Loss: 4.100 (3.98)  Time: 0.161s, 6378.15/s  (0.198s, 5184.76/s)  LR: 1.510e-04  Data: 0.035 (0.037)
Train: 224 [ 250/1251 ( 20%)]  Loss: 3.741 (3.94)  Time: 0.171s, 5980.06/s  (0.195s, 5242.01/s)  LR: 1.510e-04  Data: 0.025 (0.036)
Train: 224 [ 300/1251 ( 24%)]  Loss: 4.327 (3.99)  Time: 0.174s, 5895.36/s  (0.195s, 5247.90/s)  LR: 1.510e-04  Data: 0.027 (0.036)
Train: 224 [ 350/1251 ( 28%)]  Loss: 4.192 (4.02)  Time: 0.173s, 5926.53/s  (0.194s, 5267.95/s)  LR: 1.510e-04  Data: 0.024 (0.035)
Train: 224 [ 400/1251 ( 32%)]  Loss: 4.152 (4.03)  Time: 0.224s, 4562.94/s  (0.194s, 5269.34/s)  LR: 1.510e-04  Data: 0.026 (0.034)
Train: 224 [ 450/1251 ( 36%)]  Loss: 3.867 (4.02)  Time: 0.181s, 5653.82/s  (0.193s, 5303.98/s)  LR: 1.510e-04  Data: 0.025 (0.034)
Train: 224 [ 500/1251 ( 40%)]  Loss: 3.901 (4.01)  Time: 0.163s, 6296.42/s  (0.193s, 5312.66/s)  LR: 1.510e-04  Data: 0.029 (0.033)
Train: 224 [ 550/1251 ( 44%)]  Loss: 3.850 (3.99)  Time: 0.177s, 5785.49/s  (0.192s, 5321.80/s)  LR: 1.510e-04  Data: 0.026 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 224 [ 600/1251 ( 48%)]  Loss: 4.235 (4.01)  Time: 0.247s, 4144.31/s  (0.192s, 5331.58/s)  LR: 1.510e-04  Data: 0.032 (0.032)
Train: 224 [ 650/1251 ( 52%)]  Loss: 4.020 (4.01)  Time: 0.186s, 5510.68/s  (0.192s, 5326.54/s)  LR: 1.510e-04  Data: 0.027 (0.032)
Train: 224 [ 700/1251 ( 56%)]  Loss: 4.022 (4.01)  Time: 0.197s, 5210.64/s  (0.192s, 5335.49/s)  LR: 1.510e-04  Data: 0.026 (0.032)
Train: 224 [ 750/1251 ( 60%)]  Loss: 3.611 (3.99)  Time: 0.169s, 6042.02/s  (0.192s, 5332.15/s)  LR: 1.510e-04  Data: 0.027 (0.032)
Train: 224 [ 800/1251 ( 64%)]  Loss: 3.555 (3.96)  Time: 0.175s, 5846.12/s  (0.192s, 5337.63/s)  LR: 1.510e-04  Data: 0.035 (0.031)
Train: 224 [ 850/1251 ( 68%)]  Loss: 4.179 (3.97)  Time: 0.167s, 6129.15/s  (0.192s, 5345.80/s)  LR: 1.510e-04  Data: 0.025 (0.031)
Train: 224 [ 900/1251 ( 72%)]  Loss: 3.887 (3.97)  Time: 0.174s, 5886.96/s  (0.191s, 5350.99/s)  LR: 1.510e-04  Data: 0.028 (0.031)
Train: 224 [ 950/1251 ( 76%)]  Loss: 4.062 (3.97)  Time: 0.422s, 2426.82/s  (0.192s, 5339.32/s)  LR: 1.510e-04  Data: 0.032 (0.031)
Train: 224 [1000/1251 ( 80%)]  Loss: 4.275 (3.99)  Time: 0.180s, 5683.96/s  (0.192s, 5340.75/s)  LR: 1.510e-04  Data: 0.020 (0.031)
Train: 224 [1050/1251 ( 84%)]  Loss: 3.550 (3.97)  Time: 0.173s, 5922.43/s  (0.192s, 5336.26/s)  LR: 1.510e-04  Data: 0.027 (0.030)
Train: 224 [1100/1251 ( 88%)]  Loss: 3.984 (3.97)  Time: 0.401s, 2552.85/s  (0.192s, 5327.03/s)  LR: 1.510e-04  Data: 0.023 (0.030)
Train: 224 [1150/1251 ( 92%)]  Loss: 4.161 (3.98)  Time: 0.166s, 6164.35/s  (0.192s, 5324.90/s)  LR: 1.510e-04  Data: 0.027 (0.030)
Train: 224 [1200/1251 ( 96%)]  Loss: 3.844 (3.97)  Time: 0.173s, 5918.29/s  (0.193s, 5318.26/s)  LR: 1.510e-04  Data: 0.030 (0.030)
Train: 224 [1250/1251 (100%)]  Loss: 3.883 (3.97)  Time: 0.114s, 9020.41/s  (0.192s, 5331.12/s)  LR: 1.510e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.754 (1.754)  Loss:  0.9417 (0.9417)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.0698 (1.5910)  Acc@1: 81.9576 (68.8120)  Acc@5: 94.9292 (88.7400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-213.pth.tar', 68.25199997070312)

Train: 225 [   0/1251 (  0%)]  Loss: 4.077 (4.08)  Time: 1.788s,  572.86/s  (1.788s,  572.86/s)  LR: 1.473e-04  Data: 1.668 (1.668)
Train: 225 [  50/1251 (  4%)]  Loss: 4.080 (4.08)  Time: 0.177s, 5800.73/s  (0.220s, 4654.37/s)  LR: 1.473e-04  Data: 0.021 (0.073)
Train: 225 [ 100/1251 (  8%)]  Loss: 4.215 (4.12)  Time: 0.162s, 6339.65/s  (0.206s, 4981.16/s)  LR: 1.473e-04  Data: 0.034 (0.053)
Train: 225 [ 150/1251 ( 12%)]  Loss: 3.735 (4.03)  Time: 0.182s, 5628.32/s  (0.200s, 5127.57/s)  LR: 1.473e-04  Data: 0.023 (0.045)
Train: 225 [ 200/1251 ( 16%)]  Loss: 3.861 (3.99)  Time: 0.195s, 5242.20/s  (0.197s, 5190.97/s)  LR: 1.473e-04  Data: 0.026 (0.041)
Train: 225 [ 250/1251 ( 20%)]  Loss: 3.943 (3.99)  Time: 0.177s, 5790.00/s  (0.197s, 5190.24/s)  LR: 1.473e-04  Data: 0.020 (0.038)
Train: 225 [ 300/1251 ( 24%)]  Loss: 4.518 (4.06)  Time: 0.172s, 5953.01/s  (0.195s, 5238.28/s)  LR: 1.473e-04  Data: 0.022 (0.036)
Train: 225 [ 350/1251 ( 28%)]  Loss: 3.859 (4.04)  Time: 0.176s, 5819.53/s  (0.195s, 5260.44/s)  LR: 1.473e-04  Data: 0.036 (0.035)
Train: 225 [ 400/1251 ( 32%)]  Loss: 4.227 (4.06)  Time: 0.186s, 5500.67/s  (0.194s, 5279.41/s)  LR: 1.473e-04  Data: 0.023 (0.034)
Train: 225 [ 450/1251 ( 36%)]  Loss: 3.850 (4.04)  Time: 0.175s, 5849.60/s  (0.193s, 5310.78/s)  LR: 1.473e-04  Data: 0.037 (0.033)
Train: 225 [ 500/1251 ( 40%)]  Loss: 4.009 (4.03)  Time: 0.173s, 5925.05/s  (0.193s, 5293.54/s)  LR: 1.473e-04  Data: 0.029 (0.033)
Train: 225 [ 550/1251 ( 44%)]  Loss: 3.886 (4.02)  Time: 0.177s, 5782.47/s  (0.193s, 5307.17/s)  LR: 1.473e-04  Data: 0.032 (0.032)
Train: 225 [ 600/1251 ( 48%)]  Loss: 4.356 (4.05)  Time: 0.180s, 5682.32/s  (0.193s, 5302.03/s)  LR: 1.473e-04  Data: 0.025 (0.032)
Train: 225 [ 650/1251 ( 52%)]  Loss: 3.849 (4.03)  Time: 0.193s, 5309.44/s  (0.193s, 5297.44/s)  LR: 1.473e-04  Data: 0.067 (0.032)
Train: 225 [ 700/1251 ( 56%)]  Loss: 3.774 (4.02)  Time: 0.181s, 5642.46/s  (0.193s, 5296.12/s)  LR: 1.473e-04  Data: 0.021 (0.032)
Train: 225 [ 750/1251 ( 60%)]  Loss: 4.215 (4.03)  Time: 0.178s, 5765.06/s  (0.193s, 5301.79/s)  LR: 1.473e-04  Data: 0.025 (0.031)
Train: 225 [ 800/1251 ( 64%)]  Loss: 4.275 (4.04)  Time: 0.193s, 5302.44/s  (0.194s, 5290.47/s)  LR: 1.473e-04  Data: 0.051 (0.031)
Train: 225 [ 850/1251 ( 68%)]  Loss: 3.963 (4.04)  Time: 0.145s, 7076.72/s  (0.193s, 5302.82/s)  LR: 1.473e-04  Data: 0.022 (0.031)
Train: 225 [ 900/1251 ( 72%)]  Loss: 4.128 (4.04)  Time: 0.325s, 3154.15/s  (0.193s, 5297.92/s)  LR: 1.473e-04  Data: 0.023 (0.031)
Train: 225 [ 950/1251 ( 76%)]  Loss: 4.226 (4.05)  Time: 0.177s, 5789.14/s  (0.193s, 5303.35/s)  LR: 1.473e-04  Data: 0.024 (0.030)
Train: 225 [1000/1251 ( 80%)]  Loss: 3.776 (4.04)  Time: 0.177s, 5794.93/s  (0.193s, 5302.85/s)  LR: 1.473e-04  Data: 0.028 (0.030)
Train: 225 [1050/1251 ( 84%)]  Loss: 3.811 (4.03)  Time: 0.174s, 5885.52/s  (0.193s, 5313.82/s)  LR: 1.473e-04  Data: 0.028 (0.030)
Train: 225 [1100/1251 ( 88%)]  Loss: 4.127 (4.03)  Time: 0.175s, 5852.94/s  (0.193s, 5316.21/s)  LR: 1.473e-04  Data: 0.030 (0.030)
Train: 225 [1150/1251 ( 92%)]  Loss: 3.690 (4.02)  Time: 0.173s, 5923.99/s  (0.193s, 5307.55/s)  LR: 1.473e-04  Data: 0.023 (0.030)
Train: 225 [1200/1251 ( 96%)]  Loss: 4.001 (4.02)  Time: 0.167s, 6135.05/s  (0.193s, 5305.67/s)  LR: 1.473e-04  Data: 0.024 (0.030)
Train: 225 [1250/1251 (100%)]  Loss: 4.070 (4.02)  Time: 0.114s, 9018.83/s  (0.193s, 5317.36/s)  LR: 1.473e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.818 (1.818)  Loss:  0.9504 (0.9504)  Acc@1: 84.1797 (84.1797)  Acc@5: 95.6055 (95.6055)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0474 (1.5977)  Acc@1: 82.5472 (68.8200)  Acc@5: 95.5189 (88.8100)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-216.pth.tar', 68.25200002197266)

Train: 226 [   0/1251 (  0%)]  Loss: 3.913 (3.91)  Time: 1.669s,  613.45/s  (1.669s,  613.45/s)  LR: 1.436e-04  Data: 1.547 (1.547)
Train: 226 [  50/1251 (  4%)]  Loss: 4.152 (4.03)  Time: 0.175s, 5845.28/s  (0.226s, 4540.77/s)  LR: 1.436e-04  Data: 0.027 (0.080)
Train: 226 [ 100/1251 (  8%)]  Loss: 4.139 (4.07)  Time: 0.163s, 6298.95/s  (0.210s, 4875.95/s)  LR: 1.436e-04  Data: 0.026 (0.063)
Train: 226 [ 150/1251 ( 12%)]  Loss: 4.082 (4.07)  Time: 0.163s, 6296.70/s  (0.201s, 5094.25/s)  LR: 1.436e-04  Data: 0.028 (0.056)
Train: 226 [ 200/1251 ( 16%)]  Loss: 3.762 (4.01)  Time: 0.182s, 5637.34/s  (0.198s, 5171.26/s)  LR: 1.436e-04  Data: 0.056 (0.053)
Train: 226 [ 250/1251 ( 20%)]  Loss: 4.428 (4.08)  Time: 0.182s, 5620.78/s  (0.196s, 5235.93/s)  LR: 1.436e-04  Data: 0.034 (0.049)
Train: 226 [ 300/1251 ( 24%)]  Loss: 4.095 (4.08)  Time: 0.163s, 6268.88/s  (0.195s, 5261.42/s)  LR: 1.436e-04  Data: 0.025 (0.049)
Train: 226 [ 350/1251 ( 28%)]  Loss: 3.950 (4.07)  Time: 0.159s, 6421.67/s  (0.193s, 5304.96/s)  LR: 1.436e-04  Data: 0.022 (0.048)
Train: 226 [ 400/1251 ( 32%)]  Loss: 4.481 (4.11)  Time: 0.190s, 5385.75/s  (0.193s, 5319.21/s)  LR: 1.436e-04  Data: 0.039 (0.046)
Train: 226 [ 450/1251 ( 36%)]  Loss: 3.872 (4.09)  Time: 0.296s, 3458.02/s  (0.193s, 5313.01/s)  LR: 1.436e-04  Data: 0.026 (0.045)
Train: 226 [ 500/1251 ( 40%)]  Loss: 4.021 (4.08)  Time: 0.206s, 4967.58/s  (0.193s, 5316.56/s)  LR: 1.436e-04  Data: 0.024 (0.043)
Train: 226 [ 550/1251 ( 44%)]  Loss: 3.632 (4.04)  Time: 0.214s, 4781.17/s  (0.192s, 5321.21/s)  LR: 1.436e-04  Data: 0.025 (0.043)
Train: 226 [ 600/1251 ( 48%)]  Loss: 4.137 (4.05)  Time: 0.188s, 5454.67/s  (0.192s, 5323.19/s)  LR: 1.436e-04  Data: 0.042 (0.042)
Train: 226 [ 650/1251 ( 52%)]  Loss: 4.305 (4.07)  Time: 0.183s, 5597.08/s  (0.192s, 5334.76/s)  LR: 1.436e-04  Data: 0.022 (0.041)
Train: 226 [ 700/1251 ( 56%)]  Loss: 3.993 (4.06)  Time: 0.173s, 5931.03/s  (0.192s, 5335.34/s)  LR: 1.436e-04  Data: 0.024 (0.040)
Train: 226 [ 750/1251 ( 60%)]  Loss: 4.025 (4.06)  Time: 0.177s, 5792.73/s  (0.192s, 5328.86/s)  LR: 1.436e-04  Data: 0.023 (0.039)
Train: 226 [ 800/1251 ( 64%)]  Loss: 3.617 (4.04)  Time: 0.186s, 5502.49/s  (0.192s, 5330.29/s)  LR: 1.436e-04  Data: 0.029 (0.038)
Train: 226 [ 850/1251 ( 68%)]  Loss: 4.042 (4.04)  Time: 0.173s, 5932.40/s  (0.192s, 5332.53/s)  LR: 1.436e-04  Data: 0.028 (0.038)
Train: 226 [ 900/1251 ( 72%)]  Loss: 3.955 (4.03)  Time: 0.169s, 6056.64/s  (0.192s, 5334.83/s)  LR: 1.436e-04  Data: 0.030 (0.037)
Train: 226 [ 950/1251 ( 76%)]  Loss: 3.823 (4.02)  Time: 0.159s, 6422.14/s  (0.192s, 5326.61/s)  LR: 1.436e-04  Data: 0.027 (0.036)
Train: 226 [1000/1251 ( 80%)]  Loss: 4.168 (4.03)  Time: 0.179s, 5710.13/s  (0.192s, 5327.60/s)  LR: 1.436e-04  Data: 0.033 (0.036)
Train: 226 [1050/1251 ( 84%)]  Loss: 4.295 (4.04)  Time: 0.475s, 2156.92/s  (0.193s, 5318.63/s)  LR: 1.436e-04  Data: 0.027 (0.036)
Train: 226 [1100/1251 ( 88%)]  Loss: 4.221 (4.05)  Time: 0.161s, 6350.12/s  (0.192s, 5322.61/s)  LR: 1.436e-04  Data: 0.034 (0.035)
Train: 226 [1150/1251 ( 92%)]  Loss: 4.168 (4.05)  Time: 0.160s, 6397.32/s  (0.192s, 5321.74/s)  LR: 1.436e-04  Data: 0.022 (0.035)
Train: 226 [1200/1251 ( 96%)]  Loss: 4.112 (4.06)  Time: 0.191s, 5359.32/s  (0.192s, 5325.20/s)  LR: 1.436e-04  Data: 0.025 (0.035)
Train: 226 [1250/1251 (100%)]  Loss: 4.026 (4.05)  Time: 0.114s, 8991.44/s  (0.192s, 5335.07/s)  LR: 1.436e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.838 (1.838)  Loss:  1.0058 (1.0058)  Acc@1: 83.0078 (83.0078)  Acc@5: 95.7031 (95.7031)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0877 (1.6334)  Acc@1: 82.6651 (68.7000)  Acc@5: 95.4009 (88.6760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-226.pth.tar', 68.70000009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-212.pth.tar', 68.33000012695312)

Train: 227 [   0/1251 (  0%)]  Loss: 4.019 (4.02)  Time: 1.857s,  551.28/s  (1.857s,  551.28/s)  LR: 1.400e-04  Data: 1.539 (1.539)
Train: 227 [  50/1251 (  4%)]  Loss: 3.871 (3.94)  Time: 0.180s, 5693.07/s  (0.223s, 4586.37/s)  LR: 1.400e-04  Data: 0.037 (0.058)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 227 [ 100/1251 (  8%)]  Loss: 3.936 (3.94)  Time: 0.186s, 5515.43/s  (0.208s, 4920.38/s)  LR: 1.400e-04  Data: 0.026 (0.043)
Train: 227 [ 150/1251 ( 12%)]  Loss: 4.037 (3.97)  Time: 0.182s, 5641.25/s  (0.201s, 5103.71/s)  LR: 1.400e-04  Data: 0.034 (0.038)
Train: 227 [ 200/1251 ( 16%)]  Loss: 3.974 (3.97)  Time: 0.161s, 6341.94/s  (0.196s, 5218.19/s)  LR: 1.400e-04  Data: 0.029 (0.036)
Train: 227 [ 250/1251 ( 20%)]  Loss: 3.855 (3.95)  Time: 0.166s, 6155.15/s  (0.195s, 5251.14/s)  LR: 1.400e-04  Data: 0.026 (0.034)
Train: 227 [ 300/1251 ( 24%)]  Loss: 4.330 (4.00)  Time: 0.177s, 5788.41/s  (0.194s, 5283.71/s)  LR: 1.400e-04  Data: 0.030 (0.033)
Train: 227 [ 350/1251 ( 28%)]  Loss: 4.061 (4.01)  Time: 0.168s, 6111.41/s  (0.193s, 5298.55/s)  LR: 1.400e-04  Data: 0.036 (0.032)
Train: 227 [ 400/1251 ( 32%)]  Loss: 3.794 (3.99)  Time: 0.171s, 5993.65/s  (0.193s, 5318.43/s)  LR: 1.400e-04  Data: 0.032 (0.033)
Train: 227 [ 450/1251 ( 36%)]  Loss: 3.712 (3.96)  Time: 0.159s, 6457.27/s  (0.192s, 5322.23/s)  LR: 1.400e-04  Data: 0.025 (0.033)
Train: 227 [ 500/1251 ( 40%)]  Loss: 4.019 (3.96)  Time: 0.153s, 6698.90/s  (0.192s, 5320.78/s)  LR: 1.400e-04  Data: 0.023 (0.033)
Train: 227 [ 550/1251 ( 44%)]  Loss: 4.085 (3.97)  Time: 0.170s, 6030.75/s  (0.193s, 5314.45/s)  LR: 1.400e-04  Data: 0.027 (0.032)
Train: 227 [ 600/1251 ( 48%)]  Loss: 4.174 (3.99)  Time: 0.169s, 6043.90/s  (0.192s, 5344.81/s)  LR: 1.400e-04  Data: 0.035 (0.032)
Train: 227 [ 650/1251 ( 52%)]  Loss: 3.866 (3.98)  Time: 0.204s, 5009.43/s  (0.192s, 5329.92/s)  LR: 1.400e-04  Data: 0.022 (0.032)
Train: 227 [ 700/1251 ( 56%)]  Loss: 4.342 (4.00)  Time: 0.178s, 5745.61/s  (0.192s, 5334.58/s)  LR: 1.400e-04  Data: 0.024 (0.031)
Train: 227 [ 750/1251 ( 60%)]  Loss: 4.232 (4.02)  Time: 0.175s, 5854.07/s  (0.192s, 5342.21/s)  LR: 1.400e-04  Data: 0.028 (0.031)
Train: 227 [ 800/1251 ( 64%)]  Loss: 3.617 (4.00)  Time: 0.177s, 5799.03/s  (0.192s, 5346.43/s)  LR: 1.400e-04  Data: 0.031 (0.031)
Train: 227 [ 850/1251 ( 68%)]  Loss: 4.028 (4.00)  Time: 0.288s, 3552.40/s  (0.192s, 5333.03/s)  LR: 1.400e-04  Data: 0.032 (0.031)
Train: 227 [ 900/1251 ( 72%)]  Loss: 4.268 (4.01)  Time: 0.168s, 6105.12/s  (0.192s, 5345.33/s)  LR: 1.400e-04  Data: 0.027 (0.031)
Train: 227 [ 950/1251 ( 76%)]  Loss: 4.082 (4.02)  Time: 0.162s, 6315.28/s  (0.192s, 5344.21/s)  LR: 1.400e-04  Data: 0.034 (0.031)
Train: 227 [1000/1251 ( 80%)]  Loss: 4.550 (4.04)  Time: 0.170s, 6036.23/s  (0.192s, 5344.38/s)  LR: 1.400e-04  Data: 0.024 (0.031)
Train: 227 [1050/1251 ( 84%)]  Loss: 4.361 (4.06)  Time: 0.156s, 6575.77/s  (0.192s, 5341.23/s)  LR: 1.400e-04  Data: 0.024 (0.031)
Train: 227 [1100/1251 ( 88%)]  Loss: 4.008 (4.05)  Time: 0.175s, 5862.15/s  (0.192s, 5344.37/s)  LR: 1.400e-04  Data: 0.024 (0.032)
Train: 227 [1150/1251 ( 92%)]  Loss: 4.343 (4.07)  Time: 0.174s, 5878.41/s  (0.192s, 5341.56/s)  LR: 1.400e-04  Data: 0.037 (0.032)
Train: 227 [1200/1251 ( 96%)]  Loss: 4.085 (4.07)  Time: 0.169s, 6055.85/s  (0.192s, 5335.61/s)  LR: 1.400e-04  Data: 0.024 (0.032)
Train: 227 [1250/1251 (100%)]  Loss: 4.099 (4.07)  Time: 0.113s, 9069.28/s  (0.191s, 5349.12/s)  LR: 1.400e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.908 (1.908)  Loss:  1.0110 (1.0110)  Acc@1: 84.4727 (84.4727)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.0800 (1.6586)  Acc@1: 82.5472 (68.9700)  Acc@5: 94.4576 (88.7480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-227.pth.tar', 68.9699999169922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-226.pth.tar', 68.70000009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-218.pth.tar', 68.409999921875)

Train: 228 [   0/1251 (  0%)]  Loss: 4.476 (4.48)  Time: 1.815s,  564.16/s  (1.815s,  564.16/s)  LR: 1.364e-04  Data: 1.688 (1.688)
Train: 228 [  50/1251 (  4%)]  Loss: 4.061 (4.27)  Time: 0.180s, 5695.79/s  (0.226s, 4533.37/s)  LR: 1.364e-04  Data: 0.026 (0.076)
Train: 228 [ 100/1251 (  8%)]  Loss: 3.960 (4.17)  Time: 0.188s, 5446.87/s  (0.208s, 4911.92/s)  LR: 1.364e-04  Data: 0.031 (0.055)
Train: 228 [ 150/1251 ( 12%)]  Loss: 4.183 (4.17)  Time: 0.181s, 5658.58/s  (0.200s, 5114.00/s)  LR: 1.364e-04  Data: 0.021 (0.046)
Train: 228 [ 200/1251 ( 16%)]  Loss: 3.501 (4.04)  Time: 0.183s, 5599.84/s  (0.199s, 5145.04/s)  LR: 1.364e-04  Data: 0.028 (0.041)
Train: 228 [ 250/1251 ( 20%)]  Loss: 4.240 (4.07)  Time: 0.192s, 5346.54/s  (0.197s, 5197.47/s)  LR: 1.364e-04  Data: 0.040 (0.038)
Train: 228 [ 300/1251 ( 24%)]  Loss: 4.158 (4.08)  Time: 0.175s, 5854.49/s  (0.196s, 5229.97/s)  LR: 1.364e-04  Data: 0.028 (0.036)
Train: 228 [ 350/1251 ( 28%)]  Loss: 4.233 (4.10)  Time: 0.172s, 5938.80/s  (0.194s, 5275.94/s)  LR: 1.364e-04  Data: 0.028 (0.035)
Train: 228 [ 400/1251 ( 32%)]  Loss: 4.407 (4.14)  Time: 0.178s, 5757.65/s  (0.194s, 5276.39/s)  LR: 1.364e-04  Data: 0.025 (0.034)
Train: 228 [ 450/1251 ( 36%)]  Loss: 4.107 (4.13)  Time: 0.181s, 5648.81/s  (0.193s, 5297.70/s)  LR: 1.364e-04  Data: 0.026 (0.034)
Train: 228 [ 500/1251 ( 40%)]  Loss: 4.122 (4.13)  Time: 0.296s, 3457.97/s  (0.194s, 5287.81/s)  LR: 1.364e-04  Data: 0.046 (0.033)
Train: 228 [ 550/1251 ( 44%)]  Loss: 4.111 (4.13)  Time: 0.196s, 5214.76/s  (0.193s, 5295.90/s)  LR: 1.364e-04  Data: 0.052 (0.033)
Train: 228 [ 600/1251 ( 48%)]  Loss: 4.049 (4.12)  Time: 0.190s, 5382.36/s  (0.193s, 5316.66/s)  LR: 1.364e-04  Data: 0.034 (0.033)
Train: 228 [ 650/1251 ( 52%)]  Loss: 4.189 (4.13)  Time: 0.157s, 6516.02/s  (0.192s, 5321.63/s)  LR: 1.364e-04  Data: 0.037 (0.033)
Train: 228 [ 700/1251 ( 56%)]  Loss: 4.047 (4.12)  Time: 0.176s, 5833.38/s  (0.192s, 5333.83/s)  LR: 1.364e-04  Data: 0.024 (0.032)
Train: 228 [ 750/1251 ( 60%)]  Loss: 3.747 (4.10)  Time: 0.162s, 6309.93/s  (0.192s, 5332.32/s)  LR: 1.364e-04  Data: 0.027 (0.032)
Train: 228 [ 800/1251 ( 64%)]  Loss: 4.025 (4.10)  Time: 0.173s, 5923.51/s  (0.192s, 5334.15/s)  LR: 1.364e-04  Data: 0.034 (0.032)
Train: 228 [ 850/1251 ( 68%)]  Loss: 4.123 (4.10)  Time: 0.192s, 5339.32/s  (0.193s, 5318.70/s)  LR: 1.364e-04  Data: 0.030 (0.032)
Train: 228 [ 900/1251 ( 72%)]  Loss: 4.042 (4.09)  Time: 0.173s, 5928.04/s  (0.192s, 5322.83/s)  LR: 1.364e-04  Data: 0.031 (0.031)
Train: 228 [ 950/1251 ( 76%)]  Loss: 4.464 (4.11)  Time: 0.178s, 5740.02/s  (0.192s, 5324.75/s)  LR: 1.364e-04  Data: 0.024 (0.031)
Train: 228 [1000/1251 ( 80%)]  Loss: 3.906 (4.10)  Time: 0.176s, 5834.13/s  (0.192s, 5331.20/s)  LR: 1.364e-04  Data: 0.027 (0.031)
Train: 228 [1050/1251 ( 84%)]  Loss: 4.206 (4.11)  Time: 0.198s, 5168.54/s  (0.192s, 5320.09/s)  LR: 1.364e-04  Data: 0.031 (0.031)
Train: 228 [1100/1251 ( 88%)]  Loss: 4.234 (4.11)  Time: 0.178s, 5746.04/s  (0.193s, 5315.54/s)  LR: 1.364e-04  Data: 0.026 (0.031)
Train: 228 [1150/1251 ( 92%)]  Loss: 3.797 (4.10)  Time: 0.217s, 4718.39/s  (0.193s, 5305.01/s)  LR: 1.364e-04  Data: 0.027 (0.031)
Train: 228 [1200/1251 ( 96%)]  Loss: 3.896 (4.09)  Time: 0.166s, 6153.20/s  (0.193s, 5295.72/s)  LR: 1.364e-04  Data: 0.023 (0.031)
Train: 228 [1250/1251 (100%)]  Loss: 3.900 (4.08)  Time: 0.113s, 9081.86/s  (0.193s, 5313.78/s)  LR: 1.364e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.779 (1.779)  Loss:  1.0416 (1.0416)  Acc@1: 84.0820 (84.0820)  Acc@5: 96.2891 (96.2891)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.0988 (1.6192)  Acc@1: 81.2500 (68.8300)  Acc@5: 95.4009 (88.7480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-227.pth.tar', 68.9699999169922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-228.pth.tar', 68.83)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-226.pth.tar', 68.70000009765624)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-219.pth.tar', 68.49599994628906)

Train: 229 [   0/1251 (  0%)]  Loss: 3.634 (3.63)  Time: 1.925s,  532.00/s  (1.925s,  532.00/s)  LR: 1.328e-04  Data: 1.603 (1.603)
Train: 229 [  50/1251 (  4%)]  Loss: 4.021 (3.83)  Time: 0.159s, 6442.46/s  (0.222s, 4622.97/s)  LR: 1.328e-04  Data: 0.022 (0.065)
Train: 229 [ 100/1251 (  8%)]  Loss: 4.444 (4.03)  Time: 0.179s, 5722.36/s  (0.206s, 4975.41/s)  LR: 1.328e-04  Data: 0.031 (0.054)
Train: 229 [ 150/1251 ( 12%)]  Loss: 3.669 (3.94)  Time: 0.180s, 5696.08/s  (0.200s, 5128.89/s)  LR: 1.328e-04  Data: 0.027 (0.047)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Train: 229 [ 200/1251 ( 16%)]  Loss: 3.979 (3.95)  Time: 0.145s, 7083.09/s  (0.197s, 5187.79/s)  LR: 1.328e-04  Data: 0.031 (0.044)

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Train: 229 [ 250/1251 ( 20%)]  Loss: 3.770 (3.92)  Time: 0.185s, 5532.29/s  (0.195s, 5251.21/s)  LR: 1.328e-04  Data: 0.024 (0.040)
Train: 229 [ 300/1251 ( 24%)]  Loss: 4.243 (3.97)  Time: 0.166s, 6160.80/s  (0.194s, 5272.85/s)  LR: 1.328e-04  Data: 0.024 (0.038)
Train: 229 [ 350/1251 ( 28%)]  Loss: 3.749 (3.94)  Time: 0.170s, 6033.05/s  (0.193s, 5297.74/s)  LR: 1.328e-04  Data: 0.027 (0.037)
Train: 229 [ 400/1251 ( 32%)]  Loss: 4.194 (3.97)  Time: 0.184s, 5554.48/s  (0.194s, 5287.54/s)  LR: 1.328e-04  Data: 0.029 (0.036)
Train: 229 [ 450/1251 ( 36%)]  Loss: 3.820 (3.95)  Time: 0.343s, 2986.72/s  (0.193s, 5307.76/s)  LR: 1.328e-04  Data: 0.034 (0.035)
Train: 229 [ 500/1251 ( 40%)]  Loss: 3.986 (3.96)  Time: 0.159s, 6420.18/s  (0.192s, 5320.30/s)  LR: 1.328e-04  Data: 0.032 (0.034)
Train: 229 [ 550/1251 ( 44%)]  Loss: 4.251 (3.98)  Time: 0.177s, 5790.51/s  (0.192s, 5326.80/s)  LR: 1.328e-04  Data: 0.020 (0.034)
Train: 229 [ 600/1251 ( 48%)]  Loss: 4.003 (3.98)  Time: 0.332s, 3082.03/s  (0.192s, 5327.71/s)  LR: 1.328e-04  Data: 0.028 (0.034)
Train: 229 [ 650/1251 ( 52%)]  Loss: 4.235 (4.00)  Time: 0.166s, 6153.24/s  (0.192s, 5319.77/s)  LR: 1.328e-04  Data: 0.039 (0.034)
Train: 229 [ 700/1251 ( 56%)]  Loss: 3.970 (4.00)  Time: 0.159s, 6429.67/s  (0.192s, 5327.90/s)  LR: 1.328e-04  Data: 0.032 (0.033)
Train: 229 [ 750/1251 ( 60%)]  Loss: 4.133 (4.01)  Time: 0.160s, 6401.16/s  (0.192s, 5332.46/s)  LR: 1.328e-04  Data: 0.026 (0.033)
Train: 229 [ 800/1251 ( 64%)]  Loss: 3.886 (4.00)  Time: 0.271s, 3771.84/s  (0.192s, 5322.74/s)  LR: 1.328e-04  Data: 0.026 (0.033)
Train: 229 [ 850/1251 ( 68%)]  Loss: 3.753 (3.99)  Time: 0.164s, 6230.27/s  (0.192s, 5319.75/s)  LR: 1.328e-04  Data: 0.029 (0.032)
Train: 229 [ 900/1251 ( 72%)]  Loss: 3.658 (3.97)  Time: 0.174s, 5890.00/s  (0.192s, 5334.22/s)  LR: 1.328e-04  Data: 0.026 (0.032)
Train: 229 [ 950/1251 ( 76%)]  Loss: 3.963 (3.97)  Time: 0.381s, 2689.57/s  (0.192s, 5326.83/s)  LR: 1.328e-04  Data: 0.027 (0.032)
Train: 229 [1000/1251 ( 80%)]  Loss: 4.007 (3.97)  Time: 0.159s, 6421.40/s  (0.192s, 5327.70/s)  LR: 1.328e-04  Data: 0.030 (0.032)
Train: 229 [1050/1251 ( 84%)]  Loss: 3.944 (3.97)  Time: 0.193s, 5299.22/s  (0.192s, 5322.16/s)  LR: 1.328e-04  Data: 0.036 (0.032)
Train: 229 [1100/1251 ( 88%)]  Loss: 4.003 (3.97)  Time: 0.166s, 6171.75/s  (0.193s, 5319.35/s)  LR: 1.328e-04  Data: 0.028 (0.031)
Train: 229 [1150/1251 ( 92%)]  Loss: 4.126 (3.98)  Time: 0.202s, 5067.50/s  (0.193s, 5316.46/s)  LR: 1.328e-04  Data: 0.027 (0.031)
Train: 229 [1200/1251 ( 96%)]  Loss: 3.585 (3.96)  Time: 0.181s, 5661.85/s  (0.193s, 5310.57/s)  LR: 1.328e-04  Data: 0.021 (0.031)
Train: 229 [1250/1251 (100%)]  Loss: 4.299 (3.97)  Time: 0.114s, 9017.05/s  (0.192s, 5325.79/s)  LR: 1.328e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.904 (1.904)  Loss:  0.9909 (0.9909)  Acc@1: 84.8633 (84.8633)  Acc@5: 95.8008 (95.8008)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.0578 (1.6151)  Acc@1: 83.1368 (69.3380)  Acc@5: 95.5189 (88.9960)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-229.pth.tar', 69.33800017333985)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-227.pth.tar', 68.9699999169922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-228.pth.tar', 68.83)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-226.pth.tar', 68.70000009765624)

Train: 230 [   0/1251 (  0%)]  Loss: 3.879 (3.88)  Time: 1.646s,  622.02/s  (1.646s,  622.02/s)  LR: 1.293e-04  Data: 1.525 (1.525)
Train: 230 [  50/1251 (  4%)]  Loss: 4.224 (4.05)  Time: 0.162s, 6323.60/s  (0.223s, 4590.30/s)  LR: 1.293e-04  Data: 0.028 (0.078)
Train: 230 [ 100/1251 (  8%)]  Loss: 4.069 (4.06)  Time: 0.164s, 6234.91/s  (0.207s, 4950.30/s)  LR: 1.293e-04  Data: 0.025 (0.053)
Train: 230 [ 150/1251 ( 12%)]  Loss: 3.615 (3.95)  Time: 0.170s, 6006.10/s  (0.201s, 5105.01/s)  LR: 1.293e-04  Data: 0.028 (0.045)
Train: 230 [ 200/1251 ( 16%)]  Loss: 4.143 (3.99)  Time: 0.179s, 5726.61/s  (0.198s, 5171.87/s)  LR: 1.293e-04  Data: 0.044 (0.041)
Train: 230 [ 250/1251 ( 20%)]  Loss: 4.163 (4.02)  Time: 0.173s, 5919.50/s  (0.197s, 5194.30/s)  LR: 1.293e-04  Data: 0.031 (0.038)
Train: 230 [ 300/1251 ( 24%)]  Loss: 4.502 (4.09)  Time: 0.171s, 6000.36/s  (0.195s, 5242.12/s)  LR: 1.293e-04  Data: 0.026 (0.037)
Train: 230 [ 350/1251 ( 28%)]  Loss: 3.948 (4.07)  Time: 0.157s, 6522.43/s  (0.193s, 5308.40/s)  LR: 1.293e-04  Data: 0.024 (0.036)
Train: 230 [ 400/1251 ( 32%)]  Loss: 4.112 (4.07)  Time: 0.183s, 5602.31/s  (0.193s, 5307.08/s)  LR: 1.293e-04  Data: 0.027 (0.035)
Train: 230 [ 450/1251 ( 36%)]  Loss: 4.324 (4.10)  Time: 0.170s, 6010.87/s  (0.193s, 5301.58/s)  LR: 1.293e-04  Data: 0.025 (0.034)
Train: 230 [ 500/1251 ( 40%)]  Loss: 4.019 (4.09)  Time: 0.164s, 6229.93/s  (0.193s, 5318.70/s)  LR: 1.293e-04  Data: 0.025 (0.034)
Train: 230 [ 550/1251 ( 44%)]  Loss: 4.084 (4.09)  Time: 0.185s, 5526.01/s  (0.192s, 5328.23/s)  LR: 1.293e-04  Data: 0.030 (0.033)
Train: 230 [ 600/1251 ( 48%)]  Loss: 3.670 (4.06)  Time: 0.214s, 4792.53/s  (0.192s, 5334.47/s)  LR: 1.293e-04  Data: 0.019 (0.033)
Train: 230 [ 650/1251 ( 52%)]  Loss: 3.933 (4.05)  Time: 0.252s, 4055.94/s  (0.192s, 5339.04/s)  LR: 1.293e-04  Data: 0.021 (0.032)
Train: 230 [ 700/1251 ( 56%)]  Loss: 4.276 (4.06)  Time: 0.179s, 5708.17/s  (0.192s, 5329.18/s)  LR: 1.293e-04  Data: 0.031 (0.032)
Train: 230 [ 750/1251 ( 60%)]  Loss: 4.107 (4.07)  Time: 0.184s, 5569.39/s  (0.192s, 5329.17/s)  LR: 1.293e-04  Data: 0.026 (0.032)
Train: 230 [ 800/1251 ( 64%)]  Loss: 3.924 (4.06)  Time: 0.180s, 5691.53/s  (0.191s, 5347.46/s)  LR: 1.293e-04  Data: 0.034 (0.031)
Train: 230 [ 850/1251 ( 68%)]  Loss: 4.145 (4.06)  Time: 0.171s, 5985.01/s  (0.192s, 5346.25/s)  LR: 1.293e-04  Data: 0.028 (0.031)
Train: 230 [ 900/1251 ( 72%)]  Loss: 4.062 (4.06)  Time: 0.166s, 6173.36/s  (0.192s, 5341.25/s)  LR: 1.293e-04  Data: 0.024 (0.031)
Train: 230 [ 950/1251 ( 76%)]  Loss: 4.420 (4.08)  Time: 0.182s, 5630.62/s  (0.192s, 5330.92/s)  LR: 1.293e-04  Data: 0.019 (0.031)
Train: 230 [1000/1251 ( 80%)]  Loss: 4.407 (4.10)  Time: 0.178s, 5737.56/s  (0.192s, 5332.09/s)  LR: 1.293e-04  Data: 0.031 (0.031)
Train: 230 [1050/1251 ( 84%)]  Loss: 3.697 (4.08)  Time: 0.161s, 6349.36/s  (0.192s, 5334.72/s)  LR: 1.293e-04  Data: 0.031 (0.031)
Train: 230 [1100/1251 ( 88%)]  Loss: 3.902 (4.07)  Time: 0.168s, 6108.07/s  (0.192s, 5332.46/s)  LR: 1.293e-04  Data: 0.024 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Train: 230 [1150/1251 ( 92%)]  Loss: 4.408 (4.08)  Time: 0.162s, 6315.97/s  (0.192s, 5329.21/s)  LR: 1.293e-04  Data: 0.029 (0.031)
Train: 230 [1200/1251 ( 96%)]  Loss: 4.216 (4.09)  Time: 0.168s, 6079.36/s  (0.192s, 5324.40/s)  LR: 1.293e-04  Data: 0.033 (0.031)
Train: 230 [1250/1251 (100%)]  Loss: 4.332 (4.10)  Time: 0.114s, 9012.38/s  (0.192s, 5335.21/s)  LR: 1.293e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.716 (1.716)  Loss:  0.9628 (0.9628)  Acc@1: 84.3750 (84.3750)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.0315 (1.6026)  Acc@1: 83.6085 (69.1200)  Acc@5: 94.5755 (88.8780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-229.pth.tar', 69.33800017333985)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-230.pth.tar', 69.11999999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-227.pth.tar', 68.9699999169922)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-221.pth.tar', 68.840000078125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-228.pth.tar', 68.83)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-225.pth.tar', 68.82000004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-224.pth.tar', 68.81200004882812)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-222.pth.tar', 68.75800001708984)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-220.pth.tar', 68.75200009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-223.pth.tar', 68.70399999511719)

Train: 231 [   0/1251 (  0%)]  Loss: 3.732 (3.73)  Time: 1.705s,  600.59/s  (1.705s,  600.59/s)  LR: 1.258e-04  Data: 1.558 (1.558)
Train: 231 [  50/1251 (  4%)]  Loss: 3.597 (3.66)  Time: 0.176s, 5815.89/s  (0.223s, 4587.75/s)  LR: 1.258e-04  Data: 0.038 (0.072)
Train: 231 [ 100/1251 (  8%)]  Loss: 3.732 (3.69)  Time: 0.169s, 6045.33/s  (0.208s, 4922.81/s)  LR: 1.258e-04  Data: 0.026 (0.050)
Train: 231 [ 150/1251 ( 12%)]  Loss: 4.260 (3.83)  Time: 0.168s, 6100.62/s  (0.202s, 5067.28/s)  LR: 1.258e-04  Data: 0.033 (0.043)
Train: 231 [ 200/1251 ( 16%)]  Loss: 4.272 (3.92)  Time: 0.166s, 6183.48/s  (0.199s, 5151.17/s)  LR: 1.258e-04  Data: 0.030 (0.039)
Train: 231 [ 250/1251 ( 20%)]  Loss: 4.498 (4.02)  Time: 0.183s, 5598.81/s  (0.196s, 5223.97/s)  LR: 1.258e-04  Data: 0.020 (0.037)
Train: 231 [ 300/1251 ( 24%)]  Loss: 4.409 (4.07)  Time: 0.191s, 5359.70/s  (0.196s, 5235.47/s)  LR: 1.258e-04  Data: 0.033 (0.035)
Train: 231 [ 350/1251 ( 28%)]  Loss: 3.715 (4.03)  Time: 0.185s, 5523.64/s  (0.194s, 5268.65/s)  LR: 1.258e-04  Data: 0.063 (0.034)
Train: 231 [ 400/1251 ( 32%)]  Loss: 4.333 (4.06)  Time: 0.164s, 6246.21/s  (0.193s, 5297.32/s)  LR: 1.258e-04  Data: 0.026 (0.034)
Train: 231 [ 450/1251 ( 36%)]  Loss: 3.977 (4.05)  Time: 0.176s, 5814.53/s  (0.193s, 5296.15/s)  LR: 1.258e-04  Data: 0.021 (0.033)
Train: 231 [ 500/1251 ( 40%)]  Loss: 3.799 (4.03)  Time: 0.180s, 5685.07/s  (0.193s, 5318.48/s)  LR: 1.258e-04  Data: 0.028 (0.033)
Train: 231 [ 550/1251 ( 44%)]  Loss: 4.378 (4.06)  Time: 0.171s, 5997.91/s  (0.193s, 5296.18/s)  LR: 1.258e-04  Data: 0.028 (0.032)
Train: 231 [ 600/1251 ( 48%)]  Loss: 3.775 (4.04)  Time: 0.168s, 6077.90/s  (0.193s, 5303.47/s)  LR: 1.258e-04  Data: 0.032 (0.032)
Train: 231 [ 650/1251 ( 52%)]  Loss: 3.771 (4.02)  Time: 0.199s, 5152.60/s  (0.193s, 5306.56/s)  LR: 1.258e-04  Data: 0.025 (0.032)
Train: 231 [ 700/1251 ( 56%)]  Loss: 3.674 (3.99)  Time: 0.200s, 5108.88/s  (0.193s, 5308.58/s)  LR: 1.258e-04  Data: 0.020 (0.032)
Train: 231 [ 750/1251 ( 60%)]  Loss: 3.705 (3.98)  Time: 0.175s, 5867.71/s  (0.192s, 5320.59/s)  LR: 1.258e-04  Data: 0.028 (0.031)
Train: 231 [ 800/1251 ( 64%)]  Loss: 3.956 (3.98)  Time: 0.177s, 5799.46/s  (0.192s, 5321.34/s)  LR: 1.258e-04  Data: 0.026 (0.031)
Train: 231 [ 850/1251 ( 68%)]  Loss: 3.876 (3.97)  Time: 0.173s, 5902.75/s  (0.192s, 5320.30/s)  LR: 1.258e-04  Data: 0.026 (0.031)
Train: 231 [ 900/1251 ( 72%)]  Loss: 3.896 (3.97)  Time: 0.320s, 3198.05/s  (0.193s, 5318.49/s)  LR: 1.258e-04  Data: 0.035 (0.031)
Train: 231 [ 950/1251 ( 76%)]  Loss: 3.653 (3.95)  Time: 0.185s, 5544.99/s  (0.193s, 5305.88/s)  LR: 1.258e-04  Data: 0.053 (0.031)
Train: 231 [1000/1251 ( 80%)]  Loss: 4.160 (3.96)  Time: 0.175s, 5839.03/s  (0.193s, 5307.71/s)  LR: 1.258e-04  Data: 0.032 (0.031)
Train: 231 [1050/1251 ( 84%)]  Loss: 3.775 (3.95)  Time: 0.169s, 6047.22/s  (0.193s, 5314.33/s)  LR: 1.258e-04  Data: 0.025 (0.031)
Train: 231 [1100/1251 ( 88%)]  Loss: 4.239 (3.96)  Time: 0.421s, 2434.89/s  (0.193s, 5308.80/s)  LR: 1.258e-04  Data: 0.284 (0.031)
slurmstepd: error: *** JOB 33409 ON g0007 CANCELLED AT 2022-02-24T07:55:43 ***
