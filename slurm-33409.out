Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 4, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 7, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 5, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 6, total 8.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 8.
Model mobilenetv2_100 created, param count:3504872
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Using NVIDIA APEX AMP. Training in mixed precision.
Using NVIDIA APEX DistributedDataParallel.
Scheduled epochs: 310
Train: 0 [   0/1251 (  0%)]  Loss: 6.958 (6.96)  Time: 5.024s,  203.82/s  (5.024s,  203.82/s)  LR: 1.000e-06  Data: 2.365 (2.365)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 0 [  50/1251 (  4%)]  Loss: 6.961 (6.96)  Time: 0.701s, 1459.96/s  (0.295s, 3471.02/s)  LR: 1.000e-06  Data: 0.022 (0.076)
Train: 0 [ 100/1251 (  8%)]  Loss: 6.943 (6.95)  Time: 0.162s, 6302.55/s  (0.262s, 3907.61/s)  LR: 1.000e-06  Data: 0.019 (0.056)
Train: 0 [ 150/1251 ( 12%)]  Loss: 6.953 (6.95)  Time: 0.158s, 6486.76/s  (0.254s, 4034.61/s)  LR: 1.000e-06  Data: 0.020 (0.059)
Train: 0 [ 200/1251 ( 16%)]  Loss: 6.933 (6.95)  Time: 0.145s, 7064.95/s  (0.252s, 4065.06/s)  LR: 1.000e-06  Data: 0.023 (0.068)
Train: 0 [ 250/1251 ( 20%)]  Loss: 6.942 (6.95)  Time: 0.155s, 6614.26/s  (0.249s, 4105.54/s)  LR: 1.000e-06  Data: 0.020 (0.070)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 0 [ 300/1251 ( 24%)]  Loss: 6.936 (6.95)  Time: 0.146s, 7032.92/s  (0.255s, 4013.04/s)  LR: 1.000e-06  Data: 0.017 (0.065)
Train: 0 [ 350/1251 ( 28%)]  Loss: 6.933 (6.94)  Time: 0.145s, 7045.49/s  (0.252s, 4061.09/s)  LR: 1.000e-06  Data: 0.023 (0.059)
Train: 0 [ 400/1251 ( 32%)]  Loss: 6.940 (6.94)  Time: 0.152s, 6724.01/s  (0.249s, 4117.32/s)  LR: 1.000e-06  Data: 0.026 (0.055)
Train: 0 [ 450/1251 ( 36%)]  Loss: 6.929 (6.94)  Time: 0.159s, 6426.58/s  (0.247s, 4149.65/s)  LR: 1.000e-06  Data: 0.021 (0.052)
Train: 0 [ 500/1251 ( 40%)]  Loss: 6.934 (6.94)  Time: 0.161s, 6351.98/s  (0.247s, 4143.22/s)  LR: 1.000e-06  Data: 0.017 (0.049)
Train: 0 [ 550/1251 ( 44%)]  Loss: 6.936 (6.94)  Time: 0.167s, 6135.90/s  (0.247s, 4151.88/s)  LR: 1.000e-06  Data: 0.017 (0.047)
Train: 0 [ 600/1251 ( 48%)]  Loss: 6.934 (6.94)  Time: 0.154s, 6668.62/s  (0.245s, 4174.04/s)  LR: 1.000e-06  Data: 0.024 (0.045)
Train: 0 [ 650/1251 ( 52%)]  Loss: 6.924 (6.94)  Time: 0.154s, 6660.92/s  (0.245s, 4185.24/s)  LR: 1.000e-06  Data: 0.023 (0.043)
Train: 0 [ 700/1251 ( 56%)]  Loss: 6.928 (6.94)  Time: 0.643s, 1593.72/s  (0.245s, 4174.65/s)  LR: 1.000e-06  Data: 0.022 (0.041)
Train: 0 [ 750/1251 ( 60%)]  Loss: 6.917 (6.94)  Time: 0.151s, 6767.02/s  (0.244s, 4188.93/s)  LR: 1.000e-06  Data: 0.021 (0.040)
Train: 0 [ 800/1251 ( 64%)]  Loss: 6.933 (6.94)  Time: 1.225s,  835.79/s  (0.245s, 4175.41/s)  LR: 1.000e-06  Data: 0.017 (0.039)
Train: 0 [ 850/1251 ( 68%)]  Loss: 6.925 (6.94)  Time: 0.165s, 6199.02/s  (0.245s, 4185.32/s)  LR: 1.000e-06  Data: 0.020 (0.038)
Train: 0 [ 900/1251 ( 72%)]  Loss: 6.924 (6.94)  Time: 0.829s, 1235.31/s  (0.245s, 4179.72/s)  LR: 1.000e-06  Data: 0.017 (0.037)
Train: 0 [ 950/1251 ( 76%)]  Loss: 6.920 (6.94)  Time: 0.139s, 7348.10/s  (0.247s, 4150.56/s)  LR: 1.000e-06  Data: 0.019 (0.036)
Train: 0 [1000/1251 ( 80%)]  Loss: 6.919 (6.93)  Time: 0.702s, 1457.93/s  (0.248s, 4135.74/s)  LR: 1.000e-06  Data: 0.033 (0.036)
Train: 0 [1050/1251 ( 84%)]  Loss: 6.922 (6.93)  Time: 0.154s, 6659.94/s  (0.248s, 4132.39/s)  LR: 1.000e-06  Data: 0.027 (0.035)
Train: 0 [1100/1251 ( 88%)]  Loss: 6.918 (6.93)  Time: 0.158s, 6467.08/s  (0.247s, 4144.32/s)  LR: 1.000e-06  Data: 0.018 (0.035)
Train: 0 [1150/1251 ( 92%)]  Loss: 6.923 (6.93)  Time: 0.655s, 1563.80/s  (0.248s, 4129.05/s)  LR: 1.000e-06  Data: 0.538 (0.037)
Train: 0 [1200/1251 ( 96%)]  Loss: 6.923 (6.93)  Time: 0.270s, 3788.40/s  (0.247s, 4137.79/s)  LR: 1.000e-06  Data: 0.021 (0.039)
Train: 0 [1250/1251 (100%)]  Loss: 6.917 (6.93)  Time: 0.114s, 8983.78/s  (0.247s, 4150.41/s)  LR: 1.000e-06  Data: 0.000 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.703 (2.703)  Loss:  6.8865 (6.8865)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0977 ( 0.0977)
Test: [  48/48]  Time: 0.843 (0.317)  Loss:  6.8797 (6.9178)  Acc@1:  0.1179 ( 0.0980)  Acc@5:  0.2358 ( 0.4900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 1 [   0/1251 (  0%)]  Loss: 6.920 (6.92)  Time: 1.884s,  543.56/s  (1.884s,  543.56/s)  LR: 2.008e-04  Data: 1.712 (1.712)
Train: 1 [  50/1251 (  4%)]  Loss: 6.929 (6.92)  Time: 0.164s, 6259.66/s  (0.225s, 4557.77/s)  LR: 2.008e-04  Data: 0.025 (0.083)
Train: 1 [ 100/1251 (  8%)]  Loss: 6.921 (6.92)  Time: 0.165s, 6208.98/s  (0.208s, 4931.40/s)  LR: 2.008e-04  Data: 0.024 (0.064)
Train: 1 [ 150/1251 ( 12%)]  Loss: 6.901 (6.92)  Time: 0.175s, 5838.85/s  (0.202s, 5074.94/s)  LR: 2.008e-04  Data: 0.022 (0.053)
Train: 1 [ 200/1251 ( 16%)]  Loss: 6.898 (6.91)  Time: 0.342s, 2992.49/s  (0.198s, 5166.01/s)  LR: 2.008e-04  Data: 0.026 (0.047)
Train: 1 [ 250/1251 ( 20%)]  Loss: 6.876 (6.91)  Time: 0.176s, 5817.89/s  (0.196s, 5235.90/s)  LR: 2.008e-04  Data: 0.028 (0.043)
Train: 1 [ 300/1251 ( 24%)]  Loss: 6.844 (6.90)  Time: 0.155s, 6626.38/s  (0.195s, 5240.20/s)  LR: 2.008e-04  Data: 0.029 (0.043)
Train: 1 [ 350/1251 ( 28%)]  Loss: 6.846 (6.89)  Time: 0.155s, 6607.32/s  (0.193s, 5304.95/s)  LR: 2.008e-04  Data: 0.027 (0.042)
Train: 1 [ 400/1251 ( 32%)]  Loss: 6.815 (6.88)  Time: 0.178s, 5745.58/s  (0.194s, 5285.81/s)  LR: 2.008e-04  Data: 0.028 (0.043)
Train: 1 [ 450/1251 ( 36%)]  Loss: 6.837 (6.88)  Time: 0.175s, 5861.60/s  (0.193s, 5301.01/s)  LR: 2.008e-04  Data: 0.029 (0.043)
Train: 1 [ 500/1251 ( 40%)]  Loss: 6.783 (6.87)  Time: 0.177s, 5774.31/s  (0.193s, 5293.85/s)  LR: 2.008e-04  Data: 0.019 (0.043)
Train: 1 [ 550/1251 ( 44%)]  Loss: 6.751 (6.86)  Time: 0.171s, 6001.89/s  (0.193s, 5302.65/s)  LR: 2.008e-04  Data: 0.023 (0.044)
Train: 1 [ 600/1251 ( 48%)]  Loss: 6.807 (6.86)  Time: 0.166s, 6173.36/s  (0.193s, 5310.96/s)  LR: 2.008e-04  Data: 0.021 (0.044)
Train: 1 [ 650/1251 ( 52%)]  Loss: 6.750 (6.85)  Time: 0.160s, 6412.92/s  (0.193s, 5318.27/s)  LR: 2.008e-04  Data: 0.028 (0.044)
Train: 1 [ 700/1251 ( 56%)]  Loss: 6.793 (6.84)  Time: 0.165s, 6220.11/s  (0.193s, 5317.21/s)  LR: 2.008e-04  Data: 0.022 (0.044)
Train: 1 [ 750/1251 ( 60%)]  Loss: 6.744 (6.84)  Time: 0.156s, 6553.34/s  (0.192s, 5323.31/s)  LR: 2.008e-04  Data: 0.024 (0.044)
Train: 1 [ 800/1251 ( 64%)]  Loss: 6.741 (6.83)  Time: 0.154s, 6640.20/s  (0.192s, 5322.87/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [ 850/1251 ( 68%)]  Loss: 6.672 (6.82)  Time: 0.181s, 5643.45/s  (0.192s, 5324.01/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [ 900/1251 ( 72%)]  Loss: 6.721 (6.82)  Time: 0.160s, 6410.85/s  (0.193s, 5314.22/s)  LR: 2.008e-04  Data: 0.034 (0.045)
Train: 1 [ 950/1251 ( 76%)]  Loss: 6.688 (6.81)  Time: 0.169s, 6066.06/s  (0.193s, 5310.83/s)  LR: 2.008e-04  Data: 0.026 (0.046)
Train: 1 [1000/1251 ( 80%)]  Loss: 6.684 (6.81)  Time: 0.196s, 5211.47/s  (0.193s, 5317.60/s)  LR: 2.008e-04  Data: 0.020 (0.045)
Train: 1 [1050/1251 ( 84%)]  Loss: 6.715 (6.80)  Time: 0.168s, 6105.35/s  (0.193s, 5313.12/s)  LR: 2.008e-04  Data: 0.028 (0.045)
Train: 1 [1100/1251 ( 88%)]  Loss: 6.644 (6.79)  Time: 0.298s, 3432.03/s  (0.193s, 5311.70/s)  LR: 2.008e-04  Data: 0.170 (0.045)
Train: 1 [1150/1251 ( 92%)]  Loss: 6.728 (6.79)  Time: 0.276s, 3713.60/s  (0.193s, 5307.11/s)  LR: 2.008e-04  Data: 0.019 (0.045)
Train: 1 [1200/1251 ( 96%)]  Loss: 6.652 (6.79)  Time: 0.181s, 5662.75/s  (0.193s, 5297.80/s)  LR: 2.008e-04  Data: 0.025 (0.045)
Train: 1 [1250/1251 (100%)]  Loss: 6.628 (6.78)  Time: 0.113s, 9089.13/s  (0.193s, 5314.61/s)  LR: 2.008e-04  Data: 0.000 (0.044)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.788 (1.788)  Loss:  5.9561 (5.9561)  Acc@1:  3.0273 ( 3.0273)  Acc@5: 12.5977 (12.5977)
Test: [  48/48]  Time: 0.019 (0.208)  Loss:  5.5965 (6.1005)  Acc@1: 10.2594 ( 2.5260)  Acc@5: 19.4575 ( 8.0560)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 2 [   0/1251 (  0%)]  Loss: 6.632 (6.63)  Time: 1.885s,  543.25/s  (1.885s,  543.25/s)  LR: 4.006e-04  Data: 1.746 (1.746)
Train: 2 [  50/1251 (  4%)]  Loss: 6.705 (6.67)  Time: 0.178s, 5768.31/s  (0.225s, 4559.32/s)  LR: 4.006e-04  Data: 0.024 (0.077)
Train: 2 [ 100/1251 (  8%)]  Loss: 6.646 (6.66)  Time: 0.160s, 6418.99/s  (0.207s, 4939.25/s)  LR: 4.006e-04  Data: 0.028 (0.052)
Train: 2 [ 150/1251 ( 12%)]  Loss: 6.697 (6.67)  Time: 0.172s, 5958.07/s  (0.202s, 5066.08/s)  LR: 4.006e-04  Data: 0.023 (0.043)
Train: 2 [ 200/1251 ( 16%)]  Loss: 6.651 (6.67)  Time: 0.168s, 6112.22/s  (0.198s, 5172.73/s)  LR: 4.006e-04  Data: 0.033 (0.039)
Train: 2 [ 250/1251 ( 20%)]  Loss: 6.623 (6.66)  Time: 0.160s, 6418.26/s  (0.196s, 5235.10/s)  LR: 4.006e-04  Data: 0.021 (0.037)
Train: 2 [ 300/1251 ( 24%)]  Loss: 6.569 (6.65)  Time: 0.188s, 5434.34/s  (0.196s, 5231.81/s)  LR: 4.006e-04  Data: 0.022 (0.035)
Train: 2 [ 350/1251 ( 28%)]  Loss: 6.607 (6.64)  Time: 0.161s, 6377.06/s  (0.196s, 5232.80/s)  LR: 4.006e-04  Data: 0.029 (0.034)
Train: 2 [ 400/1251 ( 32%)]  Loss: 6.563 (6.63)  Time: 0.166s, 6171.64/s  (0.194s, 5284.39/s)  LR: 4.006e-04  Data: 0.035 (0.033)
Train: 2 [ 450/1251 ( 36%)]  Loss: 6.581 (6.63)  Time: 0.166s, 6168.90/s  (0.194s, 5279.24/s)  LR: 4.006e-04  Data: 0.022 (0.032)
Train: 2 [ 500/1251 ( 40%)]  Loss: 6.435 (6.61)  Time: 0.172s, 5967.88/s  (0.194s, 5280.17/s)  LR: 4.006e-04  Data: 0.024 (0.032)
Train: 2 [ 550/1251 ( 44%)]  Loss: 6.497 (6.60)  Time: 0.180s, 5702.26/s  (0.193s, 5301.30/s)  LR: 4.006e-04  Data: 0.026 (0.031)
Train: 2 [ 600/1251 ( 48%)]  Loss: 6.601 (6.60)  Time: 0.173s, 5916.09/s  (0.193s, 5296.65/s)  LR: 4.006e-04  Data: 0.027 (0.031)
Train: 2 [ 650/1251 ( 52%)]  Loss: 6.540 (6.60)  Time: 0.158s, 6500.81/s  (0.193s, 5304.86/s)  LR: 4.006e-04  Data: 0.023 (0.030)
Train: 2 [ 700/1251 ( 56%)]  Loss: 6.489 (6.59)  Time: 0.423s, 2418.54/s  (0.193s, 5301.04/s)  LR: 4.006e-04  Data: 0.023 (0.030)
Train: 2 [ 750/1251 ( 60%)]  Loss: 6.503 (6.58)  Time: 0.167s, 6138.93/s  (0.192s, 5321.15/s)  LR: 4.006e-04  Data: 0.034 (0.030)
Train: 2 [ 800/1251 ( 64%)]  Loss: 6.476 (6.58)  Time: 0.457s, 2241.51/s  (0.192s, 5319.73/s)  LR: 4.006e-04  Data: 0.338 (0.031)
Train: 2 [ 850/1251 ( 68%)]  Loss: 6.424 (6.57)  Time: 0.178s, 5754.59/s  (0.193s, 5307.90/s)  LR: 4.006e-04  Data: 0.026 (0.032)
Train: 2 [ 900/1251 ( 72%)]  Loss: 6.428 (6.56)  Time: 0.171s, 5982.25/s  (0.193s, 5306.18/s)  LR: 4.006e-04  Data: 0.030 (0.033)
Train: 2 [ 950/1251 ( 76%)]  Loss: 6.438 (6.56)  Time: 0.168s, 6110.60/s  (0.193s, 5313.91/s)  LR: 4.006e-04  Data: 0.026 (0.033)
Train: 2 [1000/1251 ( 80%)]  Loss: 6.428 (6.55)  Time: 0.166s, 6182.17/s  (0.193s, 5312.35/s)  LR: 4.006e-04  Data: 0.035 (0.032)
Train: 2 [1050/1251 ( 84%)]  Loss: 6.500 (6.55)  Time: 0.362s, 2828.01/s  (0.193s, 5308.37/s)  LR: 4.006e-04  Data: 0.030 (0.032)
Train: 2 [1100/1251 ( 88%)]  Loss: 6.410 (6.54)  Time: 0.200s, 5114.01/s  (0.193s, 5300.43/s)  LR: 4.006e-04  Data: 0.026 (0.032)
Train: 2 [1150/1251 ( 92%)]  Loss: 6.324 (6.53)  Time: 0.157s, 6507.88/s  (0.193s, 5295.34/s)  LR: 4.006e-04  Data: 0.022 (0.031)
Train: 2 [1200/1251 ( 96%)]  Loss: 6.420 (6.53)  Time: 0.163s, 6284.36/s  (0.194s, 5289.00/s)  LR: 4.006e-04  Data: 0.022 (0.031)
Train: 2 [1250/1251 (100%)]  Loss: 6.468 (6.53)  Time: 0.113s, 9039.70/s  (0.193s, 5311.93/s)  LR: 4.006e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.778 (1.778)  Loss:  5.1876 (5.1876)  Acc@1:  7.4219 ( 7.4219)  Acc@5: 22.8516 (22.8516)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  4.5310 (5.3344)  Acc@1: 21.5802 ( 7.0440)  Acc@5: 37.9717 (19.7260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 3 [   0/1251 (  0%)]  Loss: 6.478 (6.48)  Time: 1.681s,  609.08/s  (1.681s,  609.08/s)  LR: 6.004e-04  Data: 1.556 (1.556)
Train: 3 [  50/1251 (  4%)]  Loss: 6.528 (6.50)  Time: 0.161s, 6351.71/s  (0.223s, 4593.47/s)  LR: 6.004e-04  Data: 0.024 (0.079)
Train: 3 [ 100/1251 (  8%)]  Loss: 6.382 (6.46)  Time: 0.159s, 6442.64/s  (0.207s, 4948.24/s)  LR: 6.004e-04  Data: 0.028 (0.063)
Train: 3 [ 150/1251 ( 12%)]  Loss: 6.431 (6.45)  Time: 0.194s, 5287.60/s  (0.201s, 5101.91/s)  LR: 6.004e-04  Data: 0.021 (0.054)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 3 [ 200/1251 ( 16%)]  Loss: 6.369 (6.44)  Time: 0.189s, 5419.32/s  (0.199s, 5145.06/s)  LR: 6.004e-04  Data: 0.020 (0.046)
Train: 3 [ 250/1251 ( 20%)]  Loss: 6.281 (6.41)  Time: 0.157s, 6515.78/s  (0.196s, 5213.16/s)  LR: 6.004e-04  Data: 0.027 (0.043)
Train: 3 [ 300/1251 ( 24%)]  Loss: 6.302 (6.40)  Time: 0.177s, 5772.80/s  (0.195s, 5259.82/s)  LR: 6.004e-04  Data: 0.021 (0.040)
Train: 3 [ 350/1251 ( 28%)]  Loss: 6.393 (6.40)  Time: 0.169s, 6060.77/s  (0.194s, 5285.30/s)  LR: 6.004e-04  Data: 0.019 (0.039)
Train: 3 [ 400/1251 ( 32%)]  Loss: 6.307 (6.39)  Time: 0.172s, 5947.23/s  (0.194s, 5264.83/s)  LR: 6.004e-04  Data: 0.029 (0.037)
Train: 3 [ 450/1251 ( 36%)]  Loss: 6.289 (6.38)  Time: 0.162s, 6325.39/s  (0.194s, 5283.72/s)  LR: 6.004e-04  Data: 0.027 (0.036)
Train: 3 [ 500/1251 ( 40%)]  Loss: 6.293 (6.37)  Time: 0.179s, 5718.72/s  (0.193s, 5299.76/s)  LR: 6.004e-04  Data: 0.030 (0.035)
Train: 3 [ 550/1251 ( 44%)]  Loss: 6.232 (6.36)  Time: 0.167s, 6123.26/s  (0.193s, 5309.05/s)  LR: 6.004e-04  Data: 0.033 (0.034)
Train: 3 [ 600/1251 ( 48%)]  Loss: 6.418 (6.36)  Time: 0.165s, 6210.14/s  (0.192s, 5322.70/s)  LR: 6.004e-04  Data: 0.031 (0.034)
Train: 3 [ 650/1251 ( 52%)]  Loss: 6.265 (6.35)  Time: 0.164s, 6232.66/s  (0.193s, 5308.94/s)  LR: 6.004e-04  Data: 0.021 (0.033)
Train: 3 [ 700/1251 ( 56%)]  Loss: 6.353 (6.35)  Time: 0.250s, 4093.05/s  (0.193s, 5317.07/s)  LR: 6.004e-04  Data: 0.030 (0.033)
Train: 3 [ 750/1251 ( 60%)]  Loss: 6.219 (6.35)  Time: 0.162s, 6335.02/s  (0.192s, 5323.27/s)  LR: 6.004e-04  Data: 0.035 (0.033)
Train: 3 [ 800/1251 ( 64%)]  Loss: 6.215 (6.34)  Time: 0.156s, 6568.54/s  (0.193s, 5310.78/s)  LR: 6.004e-04  Data: 0.024 (0.032)
Train: 3 [ 850/1251 ( 68%)]  Loss: 6.115 (6.33)  Time: 0.155s, 6588.13/s  (0.193s, 5310.45/s)  LR: 6.004e-04  Data: 0.023 (0.032)
Train: 3 [ 900/1251 ( 72%)]  Loss: 6.279 (6.32)  Time: 0.169s, 6047.86/s  (0.193s, 5305.48/s)  LR: 6.004e-04  Data: 0.030 (0.031)
Train: 3 [ 950/1251 ( 76%)]  Loss: 6.349 (6.32)  Time: 0.166s, 6160.04/s  (0.193s, 5305.56/s)  LR: 6.004e-04  Data: 0.020 (0.031)
Train: 3 [1000/1251 ( 80%)]  Loss: 6.307 (6.32)  Time: 0.170s, 6028.21/s  (0.193s, 5307.03/s)  LR: 6.004e-04  Data: 0.022 (0.031)
Train: 3 [1050/1251 ( 84%)]  Loss: 6.165 (6.32)  Time: 0.203s, 5032.51/s  (0.193s, 5303.50/s)  LR: 6.004e-04  Data: 0.031 (0.031)
Train: 3 [1100/1251 ( 88%)]  Loss: 6.124 (6.31)  Time: 0.740s, 1383.81/s  (0.193s, 5293.25/s)  LR: 6.004e-04  Data: 0.626 (0.031)
Train: 3 [1150/1251 ( 92%)]  Loss: 6.253 (6.31)  Time: 0.163s, 6301.38/s  (0.193s, 5294.71/s)  LR: 6.004e-04  Data: 0.019 (0.032)
Train: 3 [1200/1251 ( 96%)]  Loss: 6.259 (6.30)  Time: 0.181s, 5651.12/s  (0.194s, 5288.82/s)  LR: 6.004e-04  Data: 0.029 (0.033)
Train: 3 [1250/1251 (100%)]  Loss: 6.159 (6.30)  Time: 0.114s, 9016.87/s  (0.193s, 5306.40/s)  LR: 6.004e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.911 (1.911)  Loss:  4.2315 (4.2315)  Acc@1: 17.7734 (17.7734)  Acc@5: 42.7734 (42.7734)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  3.7917 (4.6785)  Acc@1: 33.4906 (13.6480)  Acc@5: 48.4670 (31.7440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 4 [   0/1251 (  0%)]  Loss: 6.110 (6.11)  Time: 1.893s,  541.07/s  (1.893s,  541.07/s)  LR: 8.002e-04  Data: 1.762 (1.762)
Train: 4 [  50/1251 (  4%)]  Loss: 6.034 (6.07)  Time: 0.191s, 5362.35/s  (0.228s, 4482.13/s)  LR: 8.002e-04  Data: 0.022 (0.085)
Train: 4 [ 100/1251 (  8%)]  Loss: 6.241 (6.13)  Time: 0.176s, 5814.96/s  (0.209s, 4900.18/s)  LR: 8.002e-04  Data: 0.025 (0.064)
Train: 4 [ 150/1251 ( 12%)]  Loss: 6.306 (6.17)  Time: 0.171s, 5996.63/s  (0.201s, 5106.96/s)  LR: 8.002e-04  Data: 0.022 (0.052)
Train: 4 [ 200/1251 ( 16%)]  Loss: 6.016 (6.14)  Time: 0.185s, 5520.23/s  (0.198s, 5171.24/s)  LR: 8.002e-04  Data: 0.022 (0.046)
Train: 4 [ 250/1251 ( 20%)]  Loss: 6.111 (6.14)  Time: 0.190s, 5389.13/s  (0.196s, 5230.65/s)  LR: 8.002e-04  Data: 0.024 (0.044)
Train: 4 [ 300/1251 ( 24%)]  Loss: 6.061 (6.13)  Time: 0.165s, 6193.69/s  (0.195s, 5263.42/s)  LR: 8.002e-04  Data: 0.023 (0.043)
Train: 4 [ 350/1251 ( 28%)]  Loss: 5.952 (6.10)  Time: 0.166s, 6159.81/s  (0.193s, 5307.38/s)  LR: 8.002e-04  Data: 0.019 (0.042)
Train: 4 [ 400/1251 ( 32%)]  Loss: 6.131 (6.11)  Time: 0.207s, 4937.19/s  (0.193s, 5306.14/s)  LR: 8.002e-04  Data: 0.025 (0.040)
Train: 4 [ 450/1251 ( 36%)]  Loss: 6.134 (6.11)  Time: 0.201s, 5093.19/s  (0.193s, 5319.43/s)  LR: 8.002e-04  Data: 0.029 (0.039)
Train: 4 [ 500/1251 ( 40%)]  Loss: 6.136 (6.11)  Time: 0.203s, 5051.02/s  (0.192s, 5319.97/s)  LR: 8.002e-04  Data: 0.022 (0.039)
Train: 4 [ 550/1251 ( 44%)]  Loss: 6.071 (6.11)  Time: 0.172s, 5943.81/s  (0.192s, 5337.16/s)  LR: 8.002e-04  Data: 0.024 (0.039)
Train: 4 [ 600/1251 ( 48%)]  Loss: 6.088 (6.11)  Time: 0.190s, 5379.46/s  (0.192s, 5333.66/s)  LR: 8.002e-04  Data: 0.027 (0.039)
Train: 4 [ 650/1251 ( 52%)]  Loss: 6.114 (6.11)  Time: 0.172s, 5940.91/s  (0.192s, 5344.58/s)  LR: 8.002e-04  Data: 0.031 (0.039)
Train: 4 [ 700/1251 ( 56%)]  Loss: 6.115 (6.11)  Time: 0.191s, 5352.56/s  (0.191s, 5352.79/s)  LR: 8.002e-04  Data: 0.024 (0.038)
Train: 4 [ 750/1251 ( 60%)]  Loss: 6.072 (6.11)  Time: 0.172s, 5945.97/s  (0.192s, 5333.58/s)  LR: 8.002e-04  Data: 0.032 (0.037)
Train: 4 [ 800/1251 ( 64%)]  Loss: 5.863 (6.09)  Time: 0.198s, 5182.72/s  (0.192s, 5322.56/s)  LR: 8.002e-04  Data: 0.024 (0.037)
Train: 4 [ 850/1251 ( 68%)]  Loss: 5.845 (6.08)  Time: 0.170s, 6036.96/s  (0.192s, 5334.39/s)  LR: 8.002e-04  Data: 0.028 (0.036)
Train: 4 [ 900/1251 ( 72%)]  Loss: 5.910 (6.07)  Time: 0.172s, 5942.15/s  (0.192s, 5323.23/s)  LR: 8.002e-04  Data: 0.022 (0.035)
Train: 4 [ 950/1251 ( 76%)]  Loss: 5.982 (6.06)  Time: 0.178s, 5740.15/s  (0.192s, 5330.27/s)  LR: 8.002e-04  Data: 0.037 (0.035)
Train: 4 [1000/1251 ( 80%)]  Loss: 6.020 (6.06)  Time: 0.174s, 5890.28/s  (0.192s, 5326.43/s)  LR: 8.002e-04  Data: 0.025 (0.035)
Train: 4 [1050/1251 ( 84%)]  Loss: 5.925 (6.06)  Time: 0.331s, 3091.36/s  (0.192s, 5323.34/s)  LR: 8.002e-04  Data: 0.025 (0.034)
Train: 4 [1100/1251 ( 88%)]  Loss: 5.946 (6.05)  Time: 0.183s, 5594.82/s  (0.192s, 5325.91/s)  LR: 8.002e-04  Data: 0.021 (0.034)
Train: 4 [1150/1251 ( 92%)]  Loss: 5.643 (6.03)  Time: 0.167s, 6146.75/s  (0.192s, 5322.64/s)  LR: 8.002e-04  Data: 0.031 (0.034)
Train: 4 [1200/1251 ( 96%)]  Loss: 6.082 (6.04)  Time: 0.182s, 5611.67/s  (0.193s, 5316.95/s)  LR: 8.002e-04  Data: 0.026 (0.033)
Train: 4 [1250/1251 (100%)]  Loss: 5.823 (6.03)  Time: 0.113s, 9039.03/s  (0.192s, 5322.59/s)  LR: 8.002e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.778 (1.778)  Loss:  3.4774 (3.4774)  Acc@1: 30.0781 (30.0781)  Acc@5: 58.5938 (58.5938)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  3.1454 (4.1957)  Acc@1: 42.2170 (19.4860)  Acc@5: 62.9717 (41.2960)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 5 [   0/1251 (  0%)]  Loss: 5.830 (5.83)  Time: 1.838s,  557.14/s  (1.838s,  557.14/s)  LR: 9.993e-04  Data: 1.721 (1.721)
Train: 5 [  50/1251 (  4%)]  Loss: 6.059 (5.94)  Time: 0.160s, 6418.27/s  (0.223s, 4589.01/s)  LR: 9.993e-04  Data: 0.028 (0.076)
Train: 5 [ 100/1251 (  8%)]  Loss: 5.874 (5.92)  Time: 0.165s, 6187.58/s  (0.203s, 5048.16/s)  LR: 9.993e-04  Data: 0.022 (0.055)
Train: 5 [ 150/1251 ( 12%)]  Loss: 6.002 (5.94)  Time: 0.160s, 6385.13/s  (0.200s, 5130.37/s)  LR: 9.993e-04  Data: 0.028 (0.051)
Train: 5 [ 200/1251 ( 16%)]  Loss: 6.004 (5.95)  Time: 0.457s, 2241.75/s  (0.200s, 5121.54/s)  LR: 9.993e-04  Data: 0.026 (0.048)
Train: 5 [ 250/1251 ( 20%)]  Loss: 5.780 (5.92)  Time: 0.167s, 6145.62/s  (0.196s, 5224.15/s)  LR: 9.993e-04  Data: 0.026 (0.044)
Train: 5 [ 300/1251 ( 24%)]  Loss: 6.019 (5.94)  Time: 0.186s, 5501.64/s  (0.194s, 5266.98/s)  LR: 9.993e-04  Data: 0.025 (0.041)
Train: 5 [ 350/1251 ( 28%)]  Loss: 5.993 (5.95)  Time: 0.161s, 6367.02/s  (0.195s, 5243.34/s)  LR: 9.993e-04  Data: 0.026 (0.039)
Train: 5 [ 400/1251 ( 32%)]  Loss: 5.939 (5.94)  Time: 0.155s, 6618.56/s  (0.194s, 5276.43/s)  LR: 9.993e-04  Data: 0.024 (0.037)
Train: 5 [ 450/1251 ( 36%)]  Loss: 5.669 (5.92)  Time: 0.193s, 5299.80/s  (0.193s, 5305.11/s)  LR: 9.993e-04  Data: 0.026 (0.036)
Train: 5 [ 500/1251 ( 40%)]  Loss: 6.091 (5.93)  Time: 0.166s, 6184.77/s  (0.193s, 5303.43/s)  LR: 9.993e-04  Data: 0.035 (0.035)
Train: 5 [ 550/1251 ( 44%)]  Loss: 6.045 (5.94)  Time: 0.169s, 6056.19/s  (0.193s, 5313.37/s)  LR: 9.993e-04  Data: 0.024 (0.035)
Train: 5 [ 600/1251 ( 48%)]  Loss: 5.581 (5.91)  Time: 0.185s, 5526.71/s  (0.193s, 5313.46/s)  LR: 9.993e-04  Data: 0.021 (0.034)
Train: 5 [ 650/1251 ( 52%)]  Loss: 5.702 (5.90)  Time: 0.173s, 5911.57/s  (0.192s, 5322.25/s)  LR: 9.993e-04  Data: 0.025 (0.033)
Train: 5 [ 700/1251 ( 56%)]  Loss: 5.934 (5.90)  Time: 0.161s, 6353.77/s  (0.193s, 5317.41/s)  LR: 9.993e-04  Data: 0.025 (0.033)
Train: 5 [ 750/1251 ( 60%)]  Loss: 5.636 (5.88)  Time: 0.152s, 6726.55/s  (0.194s, 5291.76/s)  LR: 9.993e-04  Data: 0.024 (0.032)
Train: 5 [ 800/1251 ( 64%)]  Loss: 5.769 (5.88)  Time: 0.160s, 6398.90/s  (0.194s, 5290.42/s)  LR: 9.993e-04  Data: 0.025 (0.032)
Train: 5 [ 850/1251 ( 68%)]  Loss: 5.892 (5.88)  Time: 0.380s, 2691.48/s  (0.193s, 5294.36/s)  LR: 9.993e-04  Data: 0.021 (0.032)
Train: 5 [ 900/1251 ( 72%)]  Loss: 5.802 (5.87)  Time: 0.164s, 6241.79/s  (0.193s, 5302.40/s)  LR: 9.993e-04  Data: 0.025 (0.032)
Train: 5 [ 950/1251 ( 76%)]  Loss: 5.947 (5.88)  Time: 0.193s, 5306.02/s  (0.194s, 5289.12/s)  LR: 9.993e-04  Data: 0.028 (0.031)
Train: 5 [1000/1251 ( 80%)]  Loss: 5.883 (5.88)  Time: 0.169s, 6044.97/s  (0.194s, 5288.37/s)  LR: 9.993e-04  Data: 0.039 (0.031)
Train: 5 [1050/1251 ( 84%)]  Loss: 5.767 (5.87)  Time: 0.180s, 5702.71/s  (0.194s, 5286.27/s)  LR: 9.993e-04  Data: 0.031 (0.031)
Train: 5 [1100/1251 ( 88%)]  Loss: 5.670 (5.86)  Time: 0.156s, 6559.06/s  (0.194s, 5272.24/s)  LR: 9.993e-04  Data: 0.028 (0.031)
Train: 5 [1150/1251 ( 92%)]  Loss: 5.942 (5.87)  Time: 0.369s, 2777.87/s  (0.194s, 5279.91/s)  LR: 9.993e-04  Data: 0.030 (0.031)
Train: 5 [1200/1251 ( 96%)]  Loss: 5.348 (5.85)  Time: 0.188s, 5461.33/s  (0.194s, 5282.53/s)  LR: 9.993e-04  Data: 0.024 (0.031)
Train: 5 [1250/1251 (100%)]  Loss: 5.513 (5.83)  Time: 0.113s, 9073.69/s  (0.193s, 5296.21/s)  LR: 9.993e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.794 (1.794)  Loss:  3.0450 (3.0450)  Acc@1: 38.0859 (38.0859)  Acc@5: 66.7969 (66.7969)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  2.6985 (3.8092)  Acc@1: 49.0566 (24.7440)  Acc@5: 68.8679 (48.5900)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 6 [   0/1251 (  0%)]  Loss: 5.912 (5.91)  Time: 1.748s,  585.89/s  (1.748s,  585.89/s)  LR: 9.990e-04  Data: 1.622 (1.622)
Train: 6 [  50/1251 (  4%)]  Loss: 5.469 (5.69)  Time: 0.157s, 6537.91/s  (0.225s, 4560.15/s)  LR: 9.990e-04  Data: 0.026 (0.064)
Train: 6 [ 100/1251 (  8%)]  Loss: 5.632 (5.67)  Time: 0.167s, 6147.79/s  (0.205s, 4985.76/s)  LR: 9.990e-04  Data: 0.029 (0.045)
Train: 6 [ 150/1251 ( 12%)]  Loss: 5.636 (5.66)  Time: 0.153s, 6686.75/s  (0.200s, 5113.45/s)  LR: 9.990e-04  Data: 0.023 (0.040)
Train: 6 [ 200/1251 ( 16%)]  Loss: 5.800 (5.69)  Time: 0.351s, 2920.94/s  (0.198s, 5162.74/s)  LR: 9.990e-04  Data: 0.026 (0.036)
Train: 6 [ 250/1251 ( 20%)]  Loss: 5.464 (5.65)  Time: 0.174s, 5880.14/s  (0.198s, 5182.02/s)  LR: 9.990e-04  Data: 0.022 (0.035)
Train: 6 [ 300/1251 ( 24%)]  Loss: 5.642 (5.65)  Time: 0.189s, 5423.06/s  (0.196s, 5223.12/s)  LR: 9.990e-04  Data: 0.038 (0.033)
Train: 6 [ 350/1251 ( 28%)]  Loss: 5.839 (5.67)  Time: 0.171s, 6002.58/s  (0.195s, 5250.76/s)  LR: 9.990e-04  Data: 0.018 (0.032)
Train: 6 [ 400/1251 ( 32%)]  Loss: 5.700 (5.68)  Time: 0.180s, 5690.68/s  (0.194s, 5278.55/s)  LR: 9.990e-04  Data: 0.029 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 6 [ 450/1251 ( 36%)]  Loss: 5.722 (5.68)  Time: 0.176s, 5832.00/s  (0.194s, 5290.92/s)  LR: 9.990e-04  Data: 0.025 (0.031)
Train: 6 [ 500/1251 ( 40%)]  Loss: 5.824 (5.69)  Time: 0.187s, 5473.81/s  (0.193s, 5307.85/s)  LR: 9.990e-04  Data: 0.026 (0.031)
Train: 6 [ 550/1251 ( 44%)]  Loss: 5.829 (5.71)  Time: 0.158s, 6493.02/s  (0.193s, 5306.72/s)  LR: 9.990e-04  Data: 0.024 (0.030)
Train: 6 [ 600/1251 ( 48%)]  Loss: 5.768 (5.71)  Time: 0.212s, 4830.63/s  (0.193s, 5304.80/s)  LR: 9.990e-04  Data: 0.028 (0.030)
Train: 6 [ 650/1251 ( 52%)]  Loss: 5.414 (5.69)  Time: 0.369s, 2773.99/s  (0.194s, 5285.01/s)  LR: 9.990e-04  Data: 0.031 (0.030)
Train: 6 [ 700/1251 ( 56%)]  Loss: 5.754 (5.69)  Time: 0.169s, 6072.44/s  (0.194s, 5274.39/s)  LR: 9.990e-04  Data: 0.027 (0.029)
Train: 6 [ 750/1251 ( 60%)]  Loss: 5.635 (5.69)  Time: 0.169s, 6060.91/s  (0.194s, 5282.81/s)  LR: 9.990e-04  Data: 0.024 (0.029)
Train: 6 [ 800/1251 ( 64%)]  Loss: 5.940 (5.70)  Time: 0.177s, 5798.85/s  (0.194s, 5276.11/s)  LR: 9.990e-04  Data: 0.030 (0.029)
Train: 6 [ 850/1251 ( 68%)]  Loss: 5.810 (5.71)  Time: 0.160s, 6408.74/s  (0.194s, 5271.70/s)  LR: 9.990e-04  Data: 0.025 (0.029)
Train: 6 [ 900/1251 ( 72%)]  Loss: 5.505 (5.70)  Time: 0.177s, 5795.40/s  (0.194s, 5268.14/s)  LR: 9.990e-04  Data: 0.028 (0.029)
Train: 6 [ 950/1251 ( 76%)]  Loss: 5.740 (5.70)  Time: 0.169s, 6059.13/s  (0.194s, 5267.29/s)  LR: 9.990e-04  Data: 0.029 (0.029)
Train: 6 [1000/1251 ( 80%)]  Loss: 5.468 (5.69)  Time: 0.178s, 5752.13/s  (0.195s, 5260.35/s)  LR: 9.990e-04  Data: 0.028 (0.029)
Train: 6 [1050/1251 ( 84%)]  Loss: 5.530 (5.68)  Time: 0.184s, 5566.44/s  (0.195s, 5259.10/s)  LR: 9.990e-04  Data: 0.029 (0.029)
Train: 6 [1100/1251 ( 88%)]  Loss: 6.006 (5.70)  Time: 0.209s, 4896.02/s  (0.195s, 5259.86/s)  LR: 9.990e-04  Data: 0.033 (0.029)
Train: 6 [1150/1251 ( 92%)]  Loss: 5.630 (5.69)  Time: 0.186s, 5507.34/s  (0.195s, 5255.65/s)  LR: 9.990e-04  Data: 0.030 (0.028)
Train: 6 [1200/1251 ( 96%)]  Loss: 5.834 (5.70)  Time: 0.157s, 6522.08/s  (0.195s, 5244.93/s)  LR: 9.990e-04  Data: 0.021 (0.028)
Train: 6 [1250/1251 (100%)]  Loss: 5.647 (5.70)  Time: 0.113s, 9056.64/s  (0.195s, 5256.96/s)  LR: 9.990e-04  Data: 0.000 (0.028)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.786 (1.786)  Loss:  2.6040 (2.6040)  Acc@1: 44.3359 (44.3359)  Acc@5: 73.3398 (73.3398)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  2.2701 (3.4928)  Acc@1: 54.9528 (29.5040)  Acc@5: 75.7076 (54.9720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 7 [   0/1251 (  0%)]  Loss: 5.356 (5.36)  Time: 1.729s,  592.31/s  (1.729s,  592.31/s)  LR: 9.987e-04  Data: 1.577 (1.577)
Train: 7 [  50/1251 (  4%)]  Loss: 5.615 (5.49)  Time: 0.170s, 6024.69/s  (0.227s, 4506.79/s)  LR: 9.987e-04  Data: 0.021 (0.059)
Train: 7 [ 100/1251 (  8%)]  Loss: 5.603 (5.52)  Time: 0.172s, 5941.43/s  (0.206s, 4965.18/s)  LR: 9.987e-04  Data: 0.021 (0.043)
Train: 7 [ 150/1251 ( 12%)]  Loss: 5.886 (5.62)  Time: 0.187s, 5465.71/s  (0.201s, 5094.65/s)  LR: 9.987e-04  Data: 0.020 (0.038)
Train: 7 [ 200/1251 ( 16%)]  Loss: 5.485 (5.59)  Time: 0.160s, 6414.75/s  (0.199s, 5137.34/s)  LR: 9.987e-04  Data: 0.025 (0.035)
Train: 7 [ 250/1251 ( 20%)]  Loss: 5.419 (5.56)  Time: 0.187s, 5464.25/s  (0.200s, 5132.23/s)  LR: 9.987e-04  Data: 0.025 (0.033)
Train: 7 [ 300/1251 ( 24%)]  Loss: 5.270 (5.52)  Time: 0.156s, 6565.20/s  (0.197s, 5208.34/s)  LR: 9.987e-04  Data: 0.021 (0.032)
Train: 7 [ 350/1251 ( 28%)]  Loss: 5.764 (5.55)  Time: 0.166s, 6170.98/s  (0.198s, 5180.70/s)  LR: 9.987e-04  Data: 0.026 (0.031)
Train: 7 [ 400/1251 ( 32%)]  Loss: 5.825 (5.58)  Time: 0.162s, 6338.48/s  (0.197s, 5208.26/s)  LR: 9.987e-04  Data: 0.027 (0.030)
Train: 7 [ 450/1251 ( 36%)]  Loss: 5.606 (5.58)  Time: 0.520s, 1970.62/s  (0.196s, 5219.02/s)  LR: 9.987e-04  Data: 0.023 (0.030)
Train: 7 [ 500/1251 ( 40%)]  Loss: 5.393 (5.57)  Time: 0.179s, 5728.17/s  (0.196s, 5234.12/s)  LR: 9.987e-04  Data: 0.025 (0.029)
Train: 7 [ 550/1251 ( 44%)]  Loss: 5.440 (5.56)  Time: 0.156s, 6562.56/s  (0.196s, 5236.48/s)  LR: 9.987e-04  Data: 0.030 (0.029)
Train: 7 [ 600/1251 ( 48%)]  Loss: 5.449 (5.55)  Time: 0.157s, 6529.27/s  (0.195s, 5241.62/s)  LR: 9.987e-04  Data: 0.029 (0.029)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 7 [ 650/1251 ( 52%)]  Loss: 5.547 (5.55)  Time: 0.268s, 3820.89/s  (0.195s, 5247.11/s)  LR: 9.987e-04  Data: 0.024 (0.029)
Train: 7 [ 700/1251 ( 56%)]  Loss: 5.868 (5.57)  Time: 0.158s, 6500.05/s  (0.195s, 5261.70/s)  LR: 9.987e-04  Data: 0.026 (0.029)
Train: 7 [ 750/1251 ( 60%)]  Loss: 5.552 (5.57)  Time: 0.164s, 6255.50/s  (0.194s, 5266.10/s)  LR: 9.987e-04  Data: 0.031 (0.029)
Train: 7 [ 800/1251 ( 64%)]  Loss: 5.440 (5.56)  Time: 0.445s, 2301.16/s  (0.195s, 5245.76/s)  LR: 9.987e-04  Data: 0.323 (0.031)
Train: 7 [ 850/1251 ( 68%)]  Loss: 5.701 (5.57)  Time: 0.182s, 5618.17/s  (0.195s, 5250.20/s)  LR: 9.987e-04  Data: 0.024 (0.032)
Train: 7 [ 900/1251 ( 72%)]  Loss: 5.524 (5.57)  Time: 0.161s, 6350.07/s  (0.195s, 5259.83/s)  LR: 9.987e-04  Data: 0.029 (0.032)
Train: 7 [ 950/1251 ( 76%)]  Loss: 5.233 (5.55)  Time: 0.163s, 6282.21/s  (0.195s, 5257.29/s)  LR: 9.987e-04  Data: 0.034 (0.033)
Train: 7 [1000/1251 ( 80%)]  Loss: 5.378 (5.54)  Time: 0.263s, 3898.02/s  (0.195s, 5248.40/s)  LR: 9.987e-04  Data: 0.136 (0.034)
Train: 7 [1050/1251 ( 84%)]  Loss: 5.560 (5.54)  Time: 0.160s, 6411.46/s  (0.196s, 5236.06/s)  LR: 9.987e-04  Data: 0.023 (0.033)
Train: 7 [1100/1251 ( 88%)]  Loss: 5.371 (5.53)  Time: 0.167s, 6114.62/s  (0.195s, 5239.70/s)  LR: 9.987e-04  Data: 0.031 (0.033)
Train: 7 [1150/1251 ( 92%)]  Loss: 5.249 (5.52)  Time: 0.157s, 6510.17/s  (0.197s, 5204.33/s)  LR: 9.987e-04  Data: 0.024 (0.033)
Train: 7 [1200/1251 ( 96%)]  Loss: 5.476 (5.52)  Time: 0.183s, 5589.69/s  (0.198s, 5172.56/s)  LR: 9.987e-04  Data: 0.028 (0.033)
Train: 7 [1250/1251 (100%)]  Loss: 5.640 (5.53)  Time: 0.114s, 8996.75/s  (0.197s, 5192.45/s)  LR: 9.987e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.881 (1.881)  Loss:  2.4212 (2.4212)  Acc@1: 49.8047 (49.8047)  Acc@5: 77.9297 (77.9297)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  2.2655 (3.2475)  Acc@1: 56.4859 (33.9980)  Acc@5: 76.8868 (59.8440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 8 [   0/1251 (  0%)]  Loss: 5.728 (5.73)  Time: 1.938s,  528.38/s  (1.938s,  528.38/s)  LR: 9.982e-04  Data: 1.468 (1.468)
Train: 8 [  50/1251 (  4%)]  Loss: 5.248 (5.49)  Time: 0.154s, 6650.72/s  (0.237s, 4326.94/s)  LR: 9.982e-04  Data: 0.026 (0.054)
Train: 8 [ 100/1251 (  8%)]  Loss: 5.755 (5.58)  Time: 0.161s, 6363.75/s  (0.214s, 4790.65/s)  LR: 9.982e-04  Data: 0.025 (0.040)
Train: 8 [ 150/1251 ( 12%)]  Loss: 5.058 (5.45)  Time: 0.167s, 6146.36/s  (0.204s, 5028.83/s)  LR: 9.982e-04  Data: 0.028 (0.036)
Train: 8 [ 200/1251 ( 16%)]  Loss: 5.273 (5.41)  Time: 0.191s, 5352.28/s  (0.199s, 5141.49/s)  LR: 9.982e-04  Data: 0.027 (0.034)
Train: 8 [ 250/1251 ( 20%)]  Loss: 5.303 (5.39)  Time: 0.186s, 5501.07/s  (0.198s, 5169.06/s)  LR: 9.982e-04  Data: 0.029 (0.033)
Train: 8 [ 300/1251 ( 24%)]  Loss: 5.249 (5.37)  Time: 0.264s, 3872.85/s  (0.197s, 5186.60/s)  LR: 9.982e-04  Data: 0.041 (0.032)
Train: 8 [ 350/1251 ( 28%)]  Loss: 5.284 (5.36)  Time: 0.172s, 5955.42/s  (0.196s, 5227.04/s)  LR: 9.982e-04  Data: 0.025 (0.031)
Train: 8 [ 400/1251 ( 32%)]  Loss: 5.514 (5.38)  Time: 0.163s, 6298.60/s  (0.195s, 5251.23/s)  LR: 9.982e-04  Data: 0.029 (0.032)
Train: 8 [ 450/1251 ( 36%)]  Loss: 5.168 (5.36)  Time: 0.164s, 6255.39/s  (0.195s, 5249.23/s)  LR: 9.982e-04  Data: 0.031 (0.032)
Train: 8 [ 500/1251 ( 40%)]  Loss: 5.579 (5.38)  Time: 0.159s, 6444.55/s  (0.197s, 5203.74/s)  LR: 9.982e-04  Data: 0.028 (0.031)
Train: 8 [ 550/1251 ( 44%)]  Loss: 5.316 (5.37)  Time: 0.175s, 5864.34/s  (0.196s, 5228.53/s)  LR: 9.982e-04  Data: 0.025 (0.031)
Train: 8 [ 600/1251 ( 48%)]  Loss: 5.608 (5.39)  Time: 0.145s, 7082.01/s  (0.196s, 5233.86/s)  LR: 9.982e-04  Data: 0.020 (0.031)
Train: 8 [ 650/1251 ( 52%)]  Loss: 4.938 (5.36)  Time: 0.184s, 5557.17/s  (0.195s, 5242.69/s)  LR: 9.982e-04  Data: 0.026 (0.030)
Train: 8 [ 700/1251 ( 56%)]  Loss: 5.641 (5.38)  Time: 0.158s, 6496.97/s  (0.196s, 5233.46/s)  LR: 9.982e-04  Data: 0.027 (0.030)
Train: 8 [ 750/1251 ( 60%)]  Loss: 5.352 (5.38)  Time: 0.177s, 5798.02/s  (0.196s, 5231.65/s)  LR: 9.982e-04  Data: 0.032 (0.030)
Train: 8 [ 800/1251 ( 64%)]  Loss: 5.180 (5.36)  Time: 0.162s, 6329.90/s  (0.196s, 5224.96/s)  LR: 9.982e-04  Data: 0.025 (0.030)
Train: 8 [ 850/1251 ( 68%)]  Loss: 5.461 (5.37)  Time: 0.175s, 5840.52/s  (0.196s, 5218.61/s)  LR: 9.982e-04  Data: 0.022 (0.029)
Train: 8 [ 900/1251 ( 72%)]  Loss: 5.301 (5.37)  Time: 0.159s, 6422.99/s  (0.196s, 5221.81/s)  LR: 9.982e-04  Data: 0.033 (0.029)
Train: 8 [ 950/1251 ( 76%)]  Loss: 5.174 (5.36)  Time: 0.163s, 6281.10/s  (0.196s, 5232.41/s)  LR: 9.982e-04  Data: 0.025 (0.029)
Train: 8 [1000/1251 ( 80%)]  Loss: 5.484 (5.36)  Time: 0.293s, 3495.68/s  (0.196s, 5229.49/s)  LR: 9.982e-04  Data: 0.030 (0.029)
Train: 8 [1050/1251 ( 84%)]  Loss: 5.327 (5.36)  Time: 0.173s, 5911.67/s  (0.196s, 5230.28/s)  LR: 9.982e-04  Data: 0.019 (0.029)
Train: 8 [1100/1251 ( 88%)]  Loss: 5.711 (5.38)  Time: 0.176s, 5832.50/s  (0.196s, 5224.77/s)  LR: 9.982e-04  Data: 0.023 (0.029)
Train: 8 [1150/1251 ( 92%)]  Loss: 5.521 (5.38)  Time: 0.182s, 5629.21/s  (0.196s, 5215.84/s)  LR: 9.982e-04  Data: 0.027 (0.029)
Train: 8 [1200/1251 ( 96%)]  Loss: 5.105 (5.37)  Time: 0.171s, 5998.73/s  (0.197s, 5210.02/s)  LR: 9.982e-04  Data: 0.027 (0.029)
Train: 8 [1250/1251 (100%)]  Loss: 5.468 (5.37)  Time: 0.114s, 8976.12/s  (0.196s, 5222.78/s)  LR: 9.982e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 4.029 (4.029)  Loss:  2.1832 (2.1832)  Acc@1: 53.7109 (53.7109)  Acc@5: 80.8594 (80.8594)
Test: [  48/48]  Time: 0.019 (0.262)  Loss:  2.0232 (3.0964)  Acc@1: 61.5566 (36.8840)  Acc@5: 79.4811 (62.9120)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 9 [   0/1251 (  0%)]  Loss: 5.303 (5.30)  Time: 1.919s,  533.50/s  (1.919s,  533.50/s)  LR: 9.978e-04  Data: 1.792 (1.792)
Train: 9 [  50/1251 (  4%)]  Loss: 5.595 (5.45)  Time: 0.152s, 6744.99/s  (0.224s, 4567.08/s)  LR: 9.978e-04  Data: 0.028 (0.072)
Train: 9 [ 100/1251 (  8%)]  Loss: 5.274 (5.39)  Time: 0.193s, 5309.30/s  (0.208s, 4931.32/s)  LR: 9.978e-04  Data: 0.023 (0.056)
Train: 9 [ 150/1251 ( 12%)]  Loss: 5.184 (5.34)  Time: 0.180s, 5673.46/s  (0.200s, 5107.92/s)  LR: 9.978e-04  Data: 0.024 (0.048)
Train: 9 [ 200/1251 ( 16%)]  Loss: 5.366 (5.34)  Time: 0.223s, 4586.86/s  (0.199s, 5155.84/s)  LR: 9.978e-04  Data: 0.100 (0.043)
Train: 9 [ 250/1251 ( 20%)]  Loss: 5.137 (5.31)  Time: 0.175s, 5853.28/s  (0.197s, 5188.80/s)  LR: 9.978e-04  Data: 0.023 (0.040)
Train: 9 [ 300/1251 ( 24%)]  Loss: 5.615 (5.35)  Time: 0.168s, 6086.46/s  (0.196s, 5224.75/s)  LR: 9.978e-04  Data: 0.024 (0.040)
Train: 9 [ 350/1251 ( 28%)]  Loss: 5.292 (5.35)  Time: 0.173s, 5906.15/s  (0.195s, 5251.95/s)  LR: 9.978e-04  Data: 0.024 (0.041)
Train: 9 [ 400/1251 ( 32%)]  Loss: 5.311 (5.34)  Time: 0.168s, 6089.07/s  (0.195s, 5256.22/s)  LR: 9.978e-04  Data: 0.024 (0.040)
Train: 9 [ 450/1251 ( 36%)]  Loss: 5.189 (5.33)  Time: 0.181s, 5672.77/s  (0.195s, 5258.48/s)  LR: 9.978e-04  Data: 0.021 (0.040)
Train: 9 [ 500/1251 ( 40%)]  Loss: 4.853 (5.28)  Time: 0.174s, 5901.92/s  (0.193s, 5292.62/s)  LR: 9.978e-04  Data: 0.030 (0.039)
Train: 9 [ 550/1251 ( 44%)]  Loss: 5.374 (5.29)  Time: 0.342s, 2991.86/s  (0.194s, 5287.22/s)  LR: 9.978e-04  Data: 0.213 (0.039)
Train: 9 [ 600/1251 ( 48%)]  Loss: 5.564 (5.31)  Time: 0.191s, 5352.72/s  (0.194s, 5284.88/s)  LR: 9.978e-04  Data: 0.024 (0.039)
Train: 9 [ 650/1251 ( 52%)]  Loss: 5.214 (5.30)  Time: 0.160s, 6399.08/s  (0.193s, 5293.71/s)  LR: 9.978e-04  Data: 0.019 (0.040)
Train: 9 [ 700/1251 ( 56%)]  Loss: 5.631 (5.33)  Time: 0.166s, 6160.64/s  (0.194s, 5282.22/s)  LR: 9.978e-04  Data: 0.026 (0.041)
Train: 9 [ 750/1251 ( 60%)]  Loss: 5.308 (5.33)  Time: 0.184s, 5567.91/s  (0.195s, 5264.44/s)  LR: 9.978e-04  Data: 0.024 (0.042)
Train: 9 [ 800/1251 ( 64%)]  Loss: 5.442 (5.33)  Time: 0.186s, 5493.63/s  (0.195s, 5264.43/s)  LR: 9.978e-04  Data: 0.028 (0.043)
Train: 9 [ 850/1251 ( 68%)]  Loss: 5.416 (5.34)  Time: 0.169s, 6052.09/s  (0.195s, 5261.11/s)  LR: 9.978e-04  Data: 0.025 (0.043)
Train: 9 [ 900/1251 ( 72%)]  Loss: 5.497 (5.35)  Time: 0.166s, 6163.82/s  (0.194s, 5267.10/s)  LR: 9.978e-04  Data: 0.021 (0.043)
Train: 9 [ 950/1251 ( 76%)]  Loss: 5.349 (5.35)  Time: 0.188s, 5446.34/s  (0.195s, 5252.76/s)  LR: 9.978e-04  Data: 0.023 (0.043)
Train: 9 [1000/1251 ( 80%)]  Loss: 5.467 (5.35)  Time: 0.161s, 6374.86/s  (0.195s, 5257.95/s)  LR: 9.978e-04  Data: 0.026 (0.043)
Train: 9 [1050/1251 ( 84%)]  Loss: 5.391 (5.35)  Time: 0.170s, 6013.88/s  (0.195s, 5253.10/s)  LR: 9.978e-04  Data: 0.025 (0.044)
Train: 9 [1100/1251 ( 88%)]  Loss: 5.130 (5.34)  Time: 0.182s, 5625.62/s  (0.195s, 5240.71/s)  LR: 9.978e-04  Data: 0.022 (0.045)
Train: 9 [1150/1251 ( 92%)]  Loss: 5.149 (5.34)  Time: 0.171s, 5980.56/s  (0.195s, 5240.06/s)  LR: 9.978e-04  Data: 0.026 (0.045)
Train: 9 [1200/1251 ( 96%)]  Loss: 5.052 (5.32)  Time: 0.172s, 5959.32/s  (0.196s, 5225.65/s)  LR: 9.978e-04  Data: 0.024 (0.044)
Train: 9 [1250/1251 (100%)]  Loss: 5.315 (5.32)  Time: 0.113s, 9042.38/s  (0.195s, 5239.37/s)  LR: 9.978e-04  Data: 0.000 (0.044)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.817 (1.817)  Loss:  2.0447 (2.0447)  Acc@1: 57.8125 (57.8125)  Acc@5: 82.6172 (82.6172)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.7527 (2.9423)  Acc@1: 66.3915 (39.7420)  Acc@5: 83.4906 (65.7860)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-0.pth.tar', 0.0979999999666214)

Train: 10 [   0/1251 (  0%)]  Loss: 5.185 (5.18)  Time: 1.842s,  555.84/s  (1.842s,  555.84/s)  LR: 9.973e-04  Data: 1.711 (1.711)
Train: 10 [  50/1251 (  4%)]  Loss: 5.266 (5.23)  Time: 0.163s, 6293.20/s  (0.225s, 4553.77/s)  LR: 9.973e-04  Data: 0.026 (0.075)
Train: 10 [ 100/1251 (  8%)]  Loss: 5.171 (5.21)  Time: 0.170s, 6033.97/s  (0.207s, 4953.55/s)  LR: 9.973e-04  Data: 0.022 (0.060)
Train: 10 [ 150/1251 ( 12%)]  Loss: 5.356 (5.24)  Time: 0.183s, 5592.71/s  (0.204s, 5012.21/s)  LR: 9.973e-04  Data: 0.018 (0.050)
Train: 10 [ 200/1251 ( 16%)]  Loss: 5.287 (5.25)  Time: 0.170s, 6012.35/s  (0.199s, 5154.32/s)  LR: 9.973e-04  Data: 0.027 (0.045)
Train: 10 [ 250/1251 ( 20%)]  Loss: 5.353 (5.27)  Time: 0.167s, 6114.49/s  (0.197s, 5189.76/s)  LR: 9.973e-04  Data: 0.026 (0.041)
Train: 10 [ 300/1251 ( 24%)]  Loss: 5.407 (5.29)  Time: 0.171s, 6004.60/s  (0.196s, 5221.74/s)  LR: 9.973e-04  Data: 0.030 (0.039)
Train: 10 [ 350/1251 ( 28%)]  Loss: 5.230 (5.28)  Time: 0.166s, 6163.54/s  (0.196s, 5236.20/s)  LR: 9.973e-04  Data: 0.024 (0.037)
Train: 10 [ 400/1251 ( 32%)]  Loss: 5.388 (5.29)  Time: 0.175s, 5850.82/s  (0.195s, 5252.06/s)  LR: 9.973e-04  Data: 0.031 (0.036)
Train: 10 [ 450/1251 ( 36%)]  Loss: 4.859 (5.25)  Time: 0.224s, 4562.53/s  (0.195s, 5255.59/s)  LR: 9.973e-04  Data: 0.031 (0.035)
Train: 10 [ 500/1251 ( 40%)]  Loss: 5.210 (5.25)  Time: 0.192s, 5341.32/s  (0.195s, 5253.42/s)  LR: 9.973e-04  Data: 0.021 (0.035)
Train: 10 [ 550/1251 ( 44%)]  Loss: 5.083 (5.23)  Time: 0.170s, 6032.02/s  (0.195s, 5252.44/s)  LR: 9.973e-04  Data: 0.021 (0.037)
Train: 10 [ 600/1251 ( 48%)]  Loss: 5.194 (5.23)  Time: 0.188s, 5442.07/s  (0.195s, 5263.56/s)  LR: 9.973e-04  Data: 0.033 (0.037)
Train: 10 [ 650/1251 ( 52%)]  Loss: 5.230 (5.23)  Time: 0.186s, 5502.53/s  (0.194s, 5266.79/s)  LR: 9.973e-04  Data: 0.030 (0.036)
Train: 10 [ 700/1251 ( 56%)]  Loss: 5.119 (5.22)  Time: 0.160s, 6407.49/s  (0.194s, 5270.13/s)  LR: 9.973e-04  Data: 0.028 (0.035)
Train: 10 [ 750/1251 ( 60%)]  Loss: 4.965 (5.21)  Time: 0.164s, 6225.27/s  (0.194s, 5274.71/s)  LR: 9.973e-04  Data: 0.030 (0.036)
Train: 10 [ 800/1251 ( 64%)]  Loss: 5.342 (5.21)  Time: 0.175s, 5847.65/s  (0.194s, 5269.80/s)  LR: 9.973e-04  Data: 0.022 (0.037)
Train: 10 [ 850/1251 ( 68%)]  Loss: 5.140 (5.21)  Time: 0.173s, 5920.85/s  (0.195s, 5260.93/s)  LR: 9.973e-04  Data: 0.024 (0.038)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Train: 10 [ 900/1251 ( 72%)]  Loss: 4.731 (5.19)  Time: 0.192s, 5321.34/s  (0.195s, 5249.78/s)  LR: 9.973e-04  Data: 0.021 (0.039)
Train: 10 [ 950/1251 ( 76%)]  Loss: 5.347 (5.19)  Time: 0.164s, 6236.85/s  (0.195s, 5243.12/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1000/1251 ( 80%)]  Loss: 5.041 (5.19)  Time: 0.176s, 5816.33/s  (0.195s, 5246.77/s)  LR: 9.973e-04  Data: 0.024 (0.040)
Train: 10 [1050/1251 ( 84%)]  Loss: 5.005 (5.18)  Time: 0.191s, 5370.43/s  (0.195s, 5252.49/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1100/1251 ( 88%)]  Loss: 5.218 (5.18)  Time: 0.148s, 6915.77/s  (0.195s, 5245.64/s)  LR: 9.973e-04  Data: 0.018 (0.041)
Train: 10 [1150/1251 ( 92%)]  Loss: 5.040 (5.17)  Time: 0.148s, 6935.11/s  (0.195s, 5241.42/s)  LR: 9.973e-04  Data: 0.025 (0.040)
Train: 10 [1200/1251 ( 96%)]  Loss: 4.922 (5.16)  Time: 0.223s, 4598.99/s  (0.195s, 5241.46/s)  LR: 9.973e-04  Data: 0.025 (0.039)
Train: 10 [1250/1251 (100%)]  Loss: 5.318 (5.17)  Time: 0.114s, 8996.54/s  (0.195s, 5257.48/s)  LR: 9.973e-04  Data: 0.000 (0.039)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.925 (1.925)  Loss:  1.9760 (1.9760)  Acc@1: 60.5469 (60.5469)  Acc@5: 82.8125 (82.8125)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.8242 (2.8344)  Acc@1: 64.5047 (41.6600)  Acc@5: 81.8396 (67.5520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-1.pth.tar', 2.526000012512207)

Train: 11 [   0/1251 (  0%)]  Loss: 5.336 (5.34)  Time: 1.853s,  552.55/s  (1.853s,  552.55/s)  LR: 9.967e-04  Data: 1.732 (1.732)
Train: 11 [  50/1251 (  4%)]  Loss: 5.435 (5.39)  Time: 0.181s, 5656.39/s  (0.222s, 4615.19/s)  LR: 9.967e-04  Data: 0.029 (0.065)
Train: 11 [ 100/1251 (  8%)]  Loss: 5.151 (5.31)  Time: 0.163s, 6296.71/s  (0.206s, 4982.79/s)  LR: 9.967e-04  Data: 0.027 (0.048)
Train: 11 [ 150/1251 ( 12%)]  Loss: 4.966 (5.22)  Time: 0.260s, 3932.55/s  (0.202s, 5057.10/s)  LR: 9.967e-04  Data: 0.027 (0.041)
Train: 11 [ 200/1251 ( 16%)]  Loss: 5.175 (5.21)  Time: 0.169s, 6060.82/s  (0.200s, 5125.52/s)  LR: 9.967e-04  Data: 0.028 (0.038)
Train: 11 [ 250/1251 ( 20%)]  Loss: 5.126 (5.20)  Time: 0.192s, 5337.79/s  (0.198s, 5178.99/s)  LR: 9.967e-04  Data: 0.031 (0.036)
Train: 11 [ 300/1251 ( 24%)]  Loss: 5.442 (5.23)  Time: 0.150s, 6846.35/s  (0.196s, 5229.42/s)  LR: 9.967e-04  Data: 0.023 (0.036)
Train: 11 [ 350/1251 ( 28%)]  Loss: 5.155 (5.22)  Time: 0.159s, 6443.28/s  (0.196s, 5233.22/s)  LR: 9.967e-04  Data: 0.026 (0.035)
Train: 11 [ 400/1251 ( 32%)]  Loss: 4.722 (5.17)  Time: 0.168s, 6097.85/s  (0.195s, 5255.91/s)  LR: 9.967e-04  Data: 0.026 (0.034)
Train: 11 [ 450/1251 ( 36%)]  Loss: 5.203 (5.17)  Time: 0.167s, 6116.37/s  (0.195s, 5256.60/s)  LR: 9.967e-04  Data: 0.031 (0.033)
Train: 11 [ 500/1251 ( 40%)]  Loss: 4.753 (5.13)  Time: 0.152s, 6740.92/s  (0.195s, 5252.96/s)  LR: 9.967e-04  Data: 0.023 (0.032)
Train: 11 [ 550/1251 ( 44%)]  Loss: 5.230 (5.14)  Time: 0.164s, 6230.34/s  (0.194s, 5275.35/s)  LR: 9.967e-04  Data: 0.030 (0.032)
Train: 11 [ 600/1251 ( 48%)]  Loss: 5.353 (5.16)  Time: 0.153s, 6711.52/s  (0.194s, 5280.31/s)  LR: 9.967e-04  Data: 0.026 (0.032)
Train: 11 [ 650/1251 ( 52%)]  Loss: 5.080 (5.15)  Time: 0.178s, 5768.25/s  (0.193s, 5299.05/s)  LR: 9.967e-04  Data: 0.026 (0.031)
Train: 11 [ 700/1251 ( 56%)]  Loss: 5.148 (5.15)  Time: 0.172s, 5962.28/s  (0.194s, 5290.31/s)  LR: 9.967e-04  Data: 0.025 (0.031)
Train: 11 [ 750/1251 ( 60%)]  Loss: 5.122 (5.15)  Time: 0.165s, 6217.76/s  (0.193s, 5293.52/s)  LR: 9.967e-04  Data: 0.027 (0.031)
Train: 11 [ 800/1251 ( 64%)]  Loss: 4.952 (5.14)  Time: 0.169s, 6074.21/s  (0.193s, 5302.21/s)  LR: 9.967e-04  Data: 0.024 (0.031)
Train: 11 [ 850/1251 ( 68%)]  Loss: 4.957 (5.13)  Time: 0.265s, 3870.04/s  (0.193s, 5298.85/s)  LR: 9.967e-04  Data: 0.020 (0.031)
Train: 11 [ 900/1251 ( 72%)]  Loss: 5.410 (5.14)  Time: 0.168s, 6082.06/s  (0.193s, 5296.18/s)  LR: 9.967e-04  Data: 0.029 (0.030)
Train: 11 [ 950/1251 ( 76%)]  Loss: 4.962 (5.13)  Time: 0.155s, 6587.22/s  (0.194s, 5284.29/s)  LR: 9.967e-04  Data: 0.023 (0.030)
Train: 11 [1000/1251 ( 80%)]  Loss: 5.038 (5.13)  Time: 0.374s, 2737.01/s  (0.194s, 5281.16/s)  LR: 9.967e-04  Data: 0.027 (0.030)
Train: 11 [1050/1251 ( 84%)]  Loss: 5.143 (5.13)  Time: 0.165s, 6200.10/s  (0.194s, 5282.29/s)  LR: 9.967e-04  Data: 0.034 (0.030)
Train: 11 [1100/1251 ( 88%)]  Loss: 4.955 (5.12)  Time: 0.178s, 5765.06/s  (0.195s, 5242.14/s)  LR: 9.967e-04  Data: 0.028 (0.030)
Train: 11 [1150/1251 ( 92%)]  Loss: 5.265 (5.13)  Time: 0.180s, 5681.02/s  (0.195s, 5241.52/s)  LR: 9.967e-04  Data: 0.025 (0.030)
Train: 11 [1200/1251 ( 96%)]  Loss: 4.903 (5.12)  Time: 0.371s, 2757.08/s  (0.195s, 5238.15/s)  LR: 9.967e-04  Data: 0.024 (0.029)
Train: 11 [1250/1251 (100%)]  Loss: 4.980 (5.11)  Time: 0.113s, 9022.60/s  (0.195s, 5251.52/s)  LR: 9.967e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.869 (1.869)  Loss:  1.8219 (1.8219)  Acc@1: 64.7461 (64.7461)  Acc@5: 85.4492 (85.4492)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.8821 (2.7561)  Acc@1: 66.1557 (43.6760)  Acc@5: 80.6604 (69.3620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-2.pth.tar', 7.044000001220703)

Train: 12 [   0/1251 (  0%)]  Loss: 5.164 (5.16)  Time: 1.940s,  527.97/s  (1.940s,  527.97/s)  LR: 9.961e-04  Data: 1.807 (1.807)
Train: 12 [  50/1251 (  4%)]  Loss: 5.031 (5.10)  Time: 0.180s, 5703.15/s  (0.228s, 4499.60/s)  LR: 9.961e-04  Data: 0.028 (0.086)
Train: 12 [ 100/1251 (  8%)]  Loss: 4.744 (4.98)  Time: 0.174s, 5893.09/s  (0.210s, 4868.72/s)  LR: 9.961e-04  Data: 0.025 (0.065)
Train: 12 [ 150/1251 ( 12%)]  Loss: 4.683 (4.91)  Time: 0.176s, 5821.09/s  (0.205s, 4995.49/s)  LR: 9.961e-04  Data: 0.022 (0.060)
Train: 12 [ 200/1251 ( 16%)]  Loss: 5.412 (5.01)  Time: 0.181s, 5662.66/s  (0.200s, 5117.40/s)  LR: 9.961e-04  Data: 0.035 (0.055)
Train: 12 [ 250/1251 ( 20%)]  Loss: 5.263 (5.05)  Time: 0.179s, 5707.68/s  (0.198s, 5177.64/s)  LR: 9.961e-04  Data: 0.025 (0.051)
Train: 12 [ 300/1251 ( 24%)]  Loss: 5.109 (5.06)  Time: 0.174s, 5899.01/s  (0.197s, 5209.20/s)  LR: 9.961e-04  Data: 0.022 (0.047)
Train: 12 [ 350/1251 ( 28%)]  Loss: 4.968 (5.05)  Time: 0.173s, 5919.37/s  (0.196s, 5227.40/s)  LR: 9.961e-04  Data: 0.024 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 12 [ 400/1251 ( 32%)]  Loss: 5.354 (5.08)  Time: 0.171s, 5980.27/s  (0.196s, 5237.18/s)  LR: 9.961e-04  Data: 0.025 (0.043)
Train: 12 [ 450/1251 ( 36%)]  Loss: 5.086 (5.08)  Time: 0.187s, 5490.52/s  (0.195s, 5255.79/s)  LR: 9.961e-04  Data: 0.035 (0.041)
Train: 12 [ 500/1251 ( 40%)]  Loss: 5.233 (5.10)  Time: 0.170s, 6022.33/s  (0.195s, 5260.06/s)  LR: 9.961e-04  Data: 0.037 (0.039)
Train: 12 [ 550/1251 ( 44%)]  Loss: 4.897 (5.08)  Time: 0.157s, 6501.87/s  (0.194s, 5277.15/s)  LR: 9.961e-04  Data: 0.029 (0.038)
Train: 12 [ 600/1251 ( 48%)]  Loss: 5.172 (5.09)  Time: 0.169s, 6073.78/s  (0.194s, 5270.57/s)  LR: 9.961e-04  Data: 0.023 (0.037)
Train: 12 [ 650/1251 ( 52%)]  Loss: 5.176 (5.09)  Time: 0.171s, 5971.29/s  (0.194s, 5275.41/s)  LR: 9.961e-04  Data: 0.027 (0.037)
Train: 12 [ 700/1251 ( 56%)]  Loss: 5.012 (5.09)  Time: 0.167s, 6120.02/s  (0.194s, 5274.03/s)  LR: 9.961e-04  Data: 0.028 (0.036)
Train: 12 [ 750/1251 ( 60%)]  Loss: 5.347 (5.10)  Time: 0.163s, 6274.02/s  (0.194s, 5269.04/s)  LR: 9.961e-04  Data: 0.024 (0.036)
Train: 12 [ 800/1251 ( 64%)]  Loss: 4.934 (5.09)  Time: 0.165s, 6205.64/s  (0.194s, 5267.60/s)  LR: 9.961e-04  Data: 0.029 (0.036)
Train: 12 [ 850/1251 ( 68%)]  Loss: 5.004 (5.09)  Time: 0.175s, 5835.71/s  (0.195s, 5261.68/s)  LR: 9.961e-04  Data: 0.027 (0.036)
Train: 12 [ 900/1251 ( 72%)]  Loss: 5.213 (5.09)  Time: 0.159s, 6453.10/s  (0.194s, 5265.77/s)  LR: 9.961e-04  Data: 0.024 (0.035)
Train: 12 [ 950/1251 ( 76%)]  Loss: 4.826 (5.08)  Time: 0.183s, 5593.06/s  (0.195s, 5261.99/s)  LR: 9.961e-04  Data: 0.031 (0.035)
Train: 12 [1000/1251 ( 80%)]  Loss: 5.081 (5.08)  Time: 0.407s, 2517.88/s  (0.195s, 5255.28/s)  LR: 9.961e-04  Data: 0.021 (0.034)
Train: 12 [1050/1251 ( 84%)]  Loss: 5.221 (5.09)  Time: 0.153s, 6702.40/s  (0.195s, 5260.98/s)  LR: 9.961e-04  Data: 0.024 (0.034)
Train: 12 [1100/1251 ( 88%)]  Loss: 5.274 (5.10)  Time: 0.225s, 4553.48/s  (0.195s, 5246.19/s)  LR: 9.961e-04  Data: 0.031 (0.034)
Train: 12 [1150/1251 ( 92%)]  Loss: 5.274 (5.10)  Time: 0.388s, 2640.59/s  (0.195s, 5242.75/s)  LR: 9.961e-04  Data: 0.028 (0.033)
Train: 12 [1200/1251 ( 96%)]  Loss: 4.968 (5.10)  Time: 0.158s, 6477.83/s  (0.196s, 5233.51/s)  LR: 9.961e-04  Data: 0.031 (0.033)
Train: 12 [1250/1251 (100%)]  Loss: 4.899 (5.09)  Time: 0.114s, 8984.54/s  (0.195s, 5242.83/s)  LR: 9.961e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.777 (1.777)  Loss:  1.8492 (1.8492)  Acc@1: 62.5977 (62.5977)  Acc@5: 84.4727 (84.4727)
Test: [  48/48]  Time: 0.019 (0.228)  Loss:  1.6095 (2.6183)  Acc@1: 69.4576 (45.7740)  Acc@5: 85.2594 (71.6360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-3.pth.tar', 13.648000068359375)

Train: 13 [   0/1251 (  0%)]  Loss: 5.309 (5.31)  Time: 1.975s,  518.58/s  (1.975s,  518.58/s)  LR: 9.954e-04  Data: 1.670 (1.670)
Train: 13 [  50/1251 (  4%)]  Loss: 4.837 (5.07)  Time: 0.174s, 5885.22/s  (0.225s, 4543.35/s)  LR: 9.954e-04  Data: 0.026 (0.059)
Train: 13 [ 100/1251 (  8%)]  Loss: 5.347 (5.16)  Time: 0.165s, 6216.79/s  (0.213s, 4818.72/s)  LR: 9.954e-04  Data: 0.024 (0.043)
Train: 13 [ 150/1251 ( 12%)]  Loss: 5.151 (5.16)  Time: 0.165s, 6198.63/s  (0.206s, 4973.62/s)  LR: 9.954e-04  Data: 0.026 (0.038)
Train: 13 [ 200/1251 ( 16%)]  Loss: 5.070 (5.14)  Time: 0.161s, 6353.27/s  (0.201s, 5094.66/s)  LR: 9.954e-04  Data: 0.032 (0.035)
Train: 13 [ 250/1251 ( 20%)]  Loss: 5.056 (5.13)  Time: 0.157s, 6542.19/s  (0.199s, 5138.36/s)  LR: 9.954e-04  Data: 0.030 (0.033)
Train: 13 [ 300/1251 ( 24%)]  Loss: 4.966 (5.11)  Time: 0.164s, 6237.55/s  (0.198s, 5184.31/s)  LR: 9.954e-04  Data: 0.020 (0.032)
Train: 13 [ 350/1251 ( 28%)]  Loss: 5.133 (5.11)  Time: 0.174s, 5889.43/s  (0.197s, 5194.06/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [ 400/1251 ( 32%)]  Loss: 5.214 (5.12)  Time: 0.185s, 5549.95/s  (0.196s, 5228.23/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [ 450/1251 ( 36%)]  Loss: 5.065 (5.11)  Time: 0.161s, 6361.81/s  (0.195s, 5242.76/s)  LR: 9.954e-04  Data: 0.025 (0.031)
Train: 13 [ 500/1251 ( 40%)]  Loss: 5.172 (5.12)  Time: 0.174s, 5882.84/s  (0.195s, 5248.31/s)  LR: 9.954e-04  Data: 0.022 (0.030)
Train: 13 [ 550/1251 ( 44%)]  Loss: 5.224 (5.13)  Time: 0.164s, 6230.49/s  (0.195s, 5240.98/s)  LR: 9.954e-04  Data: 0.023 (0.030)
Train: 13 [ 600/1251 ( 48%)]  Loss: 5.233 (5.14)  Time: 0.177s, 5784.80/s  (0.195s, 5240.55/s)  LR: 9.954e-04  Data: 0.028 (0.030)
Train: 13 [ 650/1251 ( 52%)]  Loss: 5.081 (5.13)  Time: 0.167s, 6138.92/s  (0.195s, 5254.91/s)  LR: 9.954e-04  Data: 0.029 (0.030)
Train: 13 [ 700/1251 ( 56%)]  Loss: 5.225 (5.14)  Time: 0.156s, 6567.80/s  (0.195s, 5240.16/s)  LR: 9.954e-04  Data: 0.026 (0.030)
Train: 13 [ 750/1251 ( 60%)]  Loss: 5.093 (5.14)  Time: 0.190s, 5396.13/s  (0.196s, 5237.02/s)  LR: 9.954e-04  Data: 0.021 (0.030)
Train: 13 [ 800/1251 ( 64%)]  Loss: 5.359 (5.15)  Time: 0.195s, 5250.47/s  (0.196s, 5228.29/s)  LR: 9.954e-04  Data: 0.026 (0.030)
Train: 13 [ 850/1251 ( 68%)]  Loss: 4.945 (5.14)  Time: 0.163s, 6289.01/s  (0.196s, 5235.40/s)  LR: 9.954e-04  Data: 0.027 (0.030)
Train: 13 [ 900/1251 ( 72%)]  Loss: 5.257 (5.14)  Time: 0.181s, 5650.39/s  (0.195s, 5240.54/s)  LR: 9.954e-04  Data: 0.027 (0.031)
Train: 13 [ 950/1251 ( 76%)]  Loss: 5.202 (5.15)  Time: 0.170s, 6021.15/s  (0.195s, 5238.17/s)  LR: 9.954e-04  Data: 0.029 (0.031)
Train: 13 [1000/1251 ( 80%)]  Loss: 4.949 (5.14)  Time: 0.163s, 6295.89/s  (0.196s, 5225.50/s)  LR: 9.954e-04  Data: 0.026 (0.031)
Train: 13 [1050/1251 ( 84%)]  Loss: 5.179 (5.14)  Time: 0.407s, 2516.99/s  (0.197s, 5210.26/s)  LR: 9.954e-04  Data: 0.028 (0.031)
Train: 13 [1100/1251 ( 88%)]  Loss: 5.187 (5.14)  Time: 0.181s, 5658.14/s  (0.196s, 5212.86/s)  LR: 9.954e-04  Data: 0.031 (0.031)
Train: 13 [1150/1251 ( 92%)]  Loss: 5.398 (5.15)  Time: 0.160s, 6394.56/s  (0.197s, 5211.05/s)  LR: 9.954e-04  Data: 0.030 (0.030)
Train: 13 [1200/1251 ( 96%)]  Loss: 4.883 (5.14)  Time: 0.169s, 6045.33/s  (0.197s, 5208.67/s)  LR: 9.954e-04  Data: 0.036 (0.030)
Train: 13 [1250/1251 (100%)]  Loss: 5.102 (5.14)  Time: 0.130s, 7881.57/s  (0.196s, 5221.99/s)  LR: 9.954e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.979 (1.979)  Loss:  1.6583 (1.6583)  Acc@1: 68.4570 (68.4570)  Acc@5: 87.2070 (87.2070)
Test: [  48/48]  Time: 0.019 (0.227)  Loss:  1.6653 (2.5695)  Acc@1: 68.3962 (47.2420)  Acc@5: 84.1981 (72.8000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-4.pth.tar', 19.486000045166016)

Train: 14 [   0/1251 (  0%)]  Loss: 5.234 (5.23)  Time: 1.989s,  514.85/s  (1.989s,  514.85/s)  LR: 9.946e-04  Data: 1.865 (1.865)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 14 [  50/1251 (  4%)]  Loss: 5.030 (5.13)  Time: 0.182s, 5635.06/s  (0.234s, 4369.45/s)  LR: 9.946e-04  Data: 0.030 (0.090)
Train: 14 [ 100/1251 (  8%)]  Loss: 4.912 (5.06)  Time: 0.158s, 6467.43/s  (0.212s, 4833.67/s)  LR: 9.946e-04  Data: 0.026 (0.063)
Train: 14 [ 150/1251 ( 12%)]  Loss: 4.848 (5.01)  Time: 0.167s, 6116.75/s  (0.203s, 5046.57/s)  LR: 9.946e-04  Data: 0.029 (0.051)
Train: 14 [ 200/1251 ( 16%)]  Loss: 4.930 (4.99)  Time: 0.168s, 6091.30/s  (0.199s, 5136.84/s)  LR: 9.946e-04  Data: 0.025 (0.046)
Train: 14 [ 250/1251 ( 20%)]  Loss: 5.082 (5.01)  Time: 0.164s, 6252.79/s  (0.197s, 5187.61/s)  LR: 9.946e-04  Data: 0.029 (0.042)
Train: 14 [ 300/1251 ( 24%)]  Loss: 4.897 (4.99)  Time: 0.168s, 6106.83/s  (0.197s, 5206.17/s)  LR: 9.946e-04  Data: 0.030 (0.041)
Train: 14 [ 350/1251 ( 28%)]  Loss: 4.904 (4.98)  Time: 0.187s, 5464.79/s  (0.195s, 5238.65/s)  LR: 9.946e-04  Data: 0.025 (0.040)
Train: 14 [ 400/1251 ( 32%)]  Loss: 4.973 (4.98)  Time: 0.183s, 5592.01/s  (0.196s, 5233.04/s)  LR: 9.946e-04  Data: 0.024 (0.039)
Train: 14 [ 450/1251 ( 36%)]  Loss: 4.765 (4.96)  Time: 0.191s, 5348.97/s  (0.195s, 5256.96/s)  LR: 9.946e-04  Data: 0.026 (0.038)
Train: 14 [ 500/1251 ( 40%)]  Loss: 5.045 (4.97)  Time: 0.167s, 6124.29/s  (0.195s, 5264.51/s)  LR: 9.946e-04  Data: 0.029 (0.037)
Train: 14 [ 550/1251 ( 44%)]  Loss: 4.664 (4.94)  Time: 0.245s, 4184.84/s  (0.195s, 5252.30/s)  LR: 9.946e-04  Data: 0.025 (0.036)
Train: 14 [ 600/1251 ( 48%)]  Loss: 5.268 (4.97)  Time: 0.169s, 6051.46/s  (0.195s, 5264.06/s)  LR: 9.946e-04  Data: 0.027 (0.035)
Train: 14 [ 650/1251 ( 52%)]  Loss: 4.999 (4.97)  Time: 0.183s, 5606.64/s  (0.194s, 5271.56/s)  LR: 9.946e-04  Data: 0.024 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 14 [ 700/1251 ( 56%)]  Loss: 4.765 (4.95)  Time: 0.360s, 2846.81/s  (0.195s, 5257.87/s)  LR: 9.946e-04  Data: 0.032 (0.034)
Train: 14 [ 750/1251 ( 60%)]  Loss: 4.726 (4.94)  Time: 0.173s, 5905.19/s  (0.195s, 5249.30/s)  LR: 9.946e-04  Data: 0.021 (0.034)
Train: 14 [ 800/1251 ( 64%)]  Loss: 5.243 (4.96)  Time: 0.174s, 5869.02/s  (0.195s, 5251.90/s)  LR: 9.946e-04  Data: 0.028 (0.033)
Train: 14 [ 850/1251 ( 68%)]  Loss: 4.961 (4.96)  Time: 0.201s, 5095.20/s  (0.195s, 5251.93/s)  LR: 9.946e-04  Data: 0.028 (0.033)
Train: 14 [ 900/1251 ( 72%)]  Loss: 4.751 (4.95)  Time: 0.204s, 5011.65/s  (0.195s, 5251.41/s)  LR: 9.946e-04  Data: 0.024 (0.033)
Train: 14 [ 950/1251 ( 76%)]  Loss: 4.821 (4.94)  Time: 0.159s, 6436.81/s  (0.195s, 5254.82/s)  LR: 9.946e-04  Data: 0.029 (0.033)
Train: 14 [1000/1251 ( 80%)]  Loss: 5.177 (4.95)  Time: 0.184s, 5550.49/s  (0.195s, 5252.75/s)  LR: 9.946e-04  Data: 0.024 (0.032)
Train: 14 [1050/1251 ( 84%)]  Loss: 5.316 (4.97)  Time: 0.169s, 6051.48/s  (0.195s, 5257.56/s)  LR: 9.946e-04  Data: 0.024 (0.032)
Train: 14 [1100/1251 ( 88%)]  Loss: 4.697 (4.96)  Time: 0.183s, 5603.15/s  (0.195s, 5250.25/s)  LR: 9.946e-04  Data: 0.020 (0.032)
Train: 14 [1150/1251 ( 92%)]  Loss: 5.019 (4.96)  Time: 0.198s, 5181.91/s  (0.195s, 5248.22/s)  LR: 9.946e-04  Data: 0.073 (0.032)
Train: 14 [1200/1251 ( 96%)]  Loss: 4.541 (4.94)  Time: 0.159s, 6422.15/s  (0.199s, 5135.82/s)  LR: 9.946e-04  Data: 0.027 (0.032)
Train: 14 [1250/1251 (100%)]  Loss: 5.060 (4.95)  Time: 0.114s, 8999.37/s  (0.199s, 5155.13/s)  LR: 9.946e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.899 (1.899)  Loss:  1.6816 (1.6816)  Acc@1: 66.7969 (66.7969)  Acc@5: 87.3047 (87.3047)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.6490 (2.5300)  Acc@1: 68.1604 (48.1040)  Acc@5: 85.6132 (73.7400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-5.pth.tar', 24.744000042724608)

Train: 15 [   0/1251 (  0%)]  Loss: 4.841 (4.84)  Time: 1.765s,  580.02/s  (1.765s,  580.02/s)  LR: 9.939e-04  Data: 1.550 (1.550)
Train: 15 [  50/1251 (  4%)]  Loss: 5.019 (4.93)  Time: 0.149s, 6857.75/s  (0.227s, 4509.09/s)  LR: 9.939e-04  Data: 0.023 (0.058)
Train: 15 [ 100/1251 (  8%)]  Loss: 5.043 (4.97)  Time: 0.163s, 6289.21/s  (0.207s, 4936.26/s)  LR: 9.939e-04  Data: 0.023 (0.043)
Train: 15 [ 150/1251 ( 12%)]  Loss: 5.191 (5.02)  Time: 0.190s, 5388.96/s  (0.203s, 5039.19/s)  LR: 9.939e-04  Data: 0.032 (0.038)
Train: 15 [ 200/1251 ( 16%)]  Loss: 4.853 (4.99)  Time: 0.179s, 5706.23/s  (0.201s, 5085.35/s)  LR: 9.939e-04  Data: 0.032 (0.035)
Train: 15 [ 250/1251 ( 20%)]  Loss: 4.694 (4.94)  Time: 0.176s, 5829.01/s  (0.199s, 5154.57/s)  LR: 9.939e-04  Data: 0.027 (0.033)
Train: 15 [ 300/1251 ( 24%)]  Loss: 5.032 (4.95)  Time: 0.174s, 5893.77/s  (0.197s, 5198.29/s)  LR: 9.939e-04  Data: 0.026 (0.032)
Train: 15 [ 350/1251 ( 28%)]  Loss: 4.831 (4.94)  Time: 0.245s, 4174.36/s  (0.196s, 5211.65/s)  LR: 9.939e-04  Data: 0.031 (0.032)
Train: 15 [ 400/1251 ( 32%)]  Loss: 4.665 (4.91)  Time: 0.165s, 6195.33/s  (0.195s, 5237.91/s)  LR: 9.939e-04  Data: 0.035 (0.031)
Train: 15 [ 450/1251 ( 36%)]  Loss: 4.648 (4.88)  Time: 0.175s, 5863.55/s  (0.195s, 5254.06/s)  LR: 9.939e-04  Data: 0.027 (0.031)
Train: 15 [ 500/1251 ( 40%)]  Loss: 4.754 (4.87)  Time: 0.173s, 5907.20/s  (0.196s, 5228.63/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 550/1251 ( 44%)]  Loss: 4.896 (4.87)  Time: 0.165s, 6188.31/s  (0.195s, 5244.24/s)  LR: 9.939e-04  Data: 0.028 (0.030)
Train: 15 [ 600/1251 ( 48%)]  Loss: 4.848 (4.87)  Time: 0.163s, 6281.75/s  (0.195s, 5247.85/s)  LR: 9.939e-04  Data: 0.031 (0.030)
Train: 15 [ 650/1251 ( 52%)]  Loss: 4.901 (4.87)  Time: 0.187s, 5487.85/s  (0.195s, 5261.87/s)  LR: 9.939e-04  Data: 0.056 (0.030)
Train: 15 [ 700/1251 ( 56%)]  Loss: 4.937 (4.88)  Time: 0.159s, 6433.91/s  (0.195s, 5260.01/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 750/1251 ( 60%)]  Loss: 5.197 (4.90)  Time: 0.170s, 6011.14/s  (0.195s, 5254.16/s)  LR: 9.939e-04  Data: 0.024 (0.031)
Train: 15 [ 800/1251 ( 64%)]  Loss: 4.846 (4.89)  Time: 0.397s, 2582.58/s  (0.195s, 5250.20/s)  LR: 9.939e-04  Data: 0.025 (0.030)
Train: 15 [ 850/1251 ( 68%)]  Loss: 4.958 (4.90)  Time: 0.176s, 5831.63/s  (0.195s, 5253.59/s)  LR: 9.939e-04  Data: 0.032 (0.030)
Train: 15 [ 900/1251 ( 72%)]  Loss: 4.851 (4.90)  Time: 0.215s, 4761.96/s  (0.195s, 5251.90/s)  LR: 9.939e-04  Data: 0.024 (0.030)
Train: 15 [ 950/1251 ( 76%)]  Loss: 4.896 (4.90)  Time: 0.187s, 5473.46/s  (0.195s, 5246.98/s)  LR: 9.939e-04  Data: 0.029 (0.030)
Train: 15 [1000/1251 ( 80%)]  Loss: 4.887 (4.89)  Time: 0.339s, 3024.51/s  (0.195s, 5252.13/s)  LR: 9.939e-04  Data: 0.024 (0.030)
Train: 15 [1050/1251 ( 84%)]  Loss: 4.689 (4.89)  Time: 0.168s, 6098.75/s  (0.195s, 5250.13/s)  LR: 9.939e-04  Data: 0.026 (0.030)
Train: 15 [1100/1251 ( 88%)]  Loss: 5.008 (4.89)  Time: 0.175s, 5858.64/s  (0.195s, 5258.15/s)  LR: 9.939e-04  Data: 0.033 (0.030)
Train: 15 [1150/1251 ( 92%)]  Loss: 4.931 (4.89)  Time: 0.179s, 5710.26/s  (0.195s, 5257.93/s)  LR: 9.939e-04  Data: 0.033 (0.030)
Train: 15 [1200/1251 ( 96%)]  Loss: 4.802 (4.89)  Time: 0.160s, 6417.04/s  (0.195s, 5260.07/s)  LR: 9.939e-04  Data: 0.024 (0.029)
Train: 15 [1250/1251 (100%)]  Loss: 5.082 (4.90)  Time: 0.113s, 9039.03/s  (0.194s, 5267.52/s)  LR: 9.939e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.784 (1.784)  Loss:  1.5635 (1.5635)  Acc@1: 68.0664 (68.0664)  Acc@5: 88.4766 (88.4766)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.5166 (2.4494)  Acc@1: 70.7547 (49.3220)  Acc@5: 85.3774 (74.5940)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-6.pth.tar', 29.50399995361328)

Train: 16 [   0/1251 (  0%)]  Loss: 4.888 (4.89)  Time: 1.612s,  635.07/s  (1.612s,  635.07/s)  LR: 9.930e-04  Data: 1.488 (1.488)
Train: 16 [  50/1251 (  4%)]  Loss: 5.054 (4.97)  Time: 0.178s, 5766.25/s  (0.219s, 4684.55/s)  LR: 9.930e-04  Data: 0.031 (0.066)
Train: 16 [ 100/1251 (  8%)]  Loss: 4.902 (4.95)  Time: 0.171s, 5984.02/s  (0.206s, 4976.49/s)  LR: 9.930e-04  Data: 0.028 (0.050)
Train: 16 [ 150/1251 ( 12%)]  Loss: 5.156 (5.00)  Time: 0.164s, 6238.09/s  (0.201s, 5105.88/s)  LR: 9.930e-04  Data: 0.033 (0.042)
Train: 16 [ 200/1251 ( 16%)]  Loss: 5.250 (5.05)  Time: 0.319s, 3214.73/s  (0.197s, 5186.76/s)  LR: 9.930e-04  Data: 0.033 (0.039)
Train: 16 [ 250/1251 ( 20%)]  Loss: 4.914 (5.03)  Time: 0.183s, 5595.14/s  (0.196s, 5231.85/s)  LR: 9.930e-04  Data: 0.032 (0.037)
Train: 16 [ 300/1251 ( 24%)]  Loss: 5.380 (5.08)  Time: 0.164s, 6254.42/s  (0.195s, 5256.28/s)  LR: 9.930e-04  Data: 0.026 (0.035)
Train: 16 [ 350/1251 ( 28%)]  Loss: 5.229 (5.10)  Time: 0.189s, 5408.85/s  (0.194s, 5279.01/s)  LR: 9.930e-04  Data: 0.028 (0.034)
Train: 16 [ 400/1251 ( 32%)]  Loss: 4.755 (5.06)  Time: 0.179s, 5721.78/s  (0.194s, 5290.23/s)  LR: 9.930e-04  Data: 0.029 (0.033)
Train: 16 [ 450/1251 ( 36%)]  Loss: 5.121 (5.06)  Time: 0.191s, 5368.19/s  (0.193s, 5296.82/s)  LR: 9.930e-04  Data: 0.028 (0.033)
Train: 16 [ 500/1251 ( 40%)]  Loss: 4.978 (5.06)  Time: 0.168s, 6084.75/s  (0.192s, 5322.14/s)  LR: 9.930e-04  Data: 0.026 (0.032)
Train: 16 [ 550/1251 ( 44%)]  Loss: 5.093 (5.06)  Time: 0.178s, 5750.11/s  (0.193s, 5317.55/s)  LR: 9.930e-04  Data: 0.027 (0.032)
Train: 16 [ 600/1251 ( 48%)]  Loss: 4.899 (5.05)  Time: 0.273s, 3756.86/s  (0.193s, 5313.26/s)  LR: 9.930e-04  Data: 0.034 (0.031)
Train: 16 [ 650/1251 ( 52%)]  Loss: 4.981 (5.04)  Time: 0.163s, 6275.77/s  (0.193s, 5311.89/s)  LR: 9.930e-04  Data: 0.032 (0.031)
Train: 16 [ 700/1251 ( 56%)]  Loss: 5.054 (5.04)  Time: 0.203s, 5037.68/s  (0.193s, 5302.79/s)  LR: 9.930e-04  Data: 0.023 (0.031)
Train: 16 [ 750/1251 ( 60%)]  Loss: 4.692 (5.02)  Time: 0.171s, 5982.72/s  (0.193s, 5298.51/s)  LR: 9.930e-04  Data: 0.019 (0.031)
Train: 16 [ 800/1251 ( 64%)]  Loss: 5.118 (5.03)  Time: 0.402s, 2547.84/s  (0.193s, 5293.51/s)  LR: 9.930e-04  Data: 0.025 (0.031)
Train: 16 [ 850/1251 ( 68%)]  Loss: 4.663 (5.01)  Time: 0.159s, 6422.04/s  (0.193s, 5303.85/s)  LR: 9.930e-04  Data: 0.026 (0.030)
Train: 16 [ 900/1251 ( 72%)]  Loss: 5.032 (5.01)  Time: 0.162s, 6309.03/s  (0.193s, 5309.01/s)  LR: 9.930e-04  Data: 0.031 (0.030)
Train: 16 [ 950/1251 ( 76%)]  Loss: 4.417 (4.98)  Time: 0.178s, 5750.91/s  (0.193s, 5308.25/s)  LR: 9.930e-04  Data: 0.021 (0.030)
Train: 16 [1000/1251 ( 80%)]  Loss: 4.954 (4.98)  Time: 0.182s, 5632.03/s  (0.195s, 5257.39/s)  LR: 9.930e-04  Data: 0.022 (0.030)
Train: 16 [1050/1251 ( 84%)]  Loss: 4.744 (4.97)  Time: 0.153s, 6704.85/s  (0.195s, 5261.16/s)  LR: 9.930e-04  Data: 0.024 (0.030)
Train: 16 [1100/1251 ( 88%)]  Loss: 5.052 (4.97)  Time: 0.164s, 6256.53/s  (0.195s, 5251.16/s)  LR: 9.930e-04  Data: 0.028 (0.030)
Train: 16 [1150/1251 ( 92%)]  Loss: 5.558 (5.00)  Time: 0.166s, 6155.52/s  (0.195s, 5252.11/s)  LR: 9.930e-04  Data: 0.024 (0.030)
Train: 16 [1200/1251 ( 96%)]  Loss: 4.441 (4.97)  Time: 0.175s, 5845.82/s  (0.195s, 5253.17/s)  LR: 9.930e-04  Data: 0.035 (0.029)
Train: 16 [1250/1251 (100%)]  Loss: 5.030 (4.98)  Time: 0.114s, 9018.30/s  (0.195s, 5264.57/s)  LR: 9.930e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.789 (1.789)  Loss:  1.6334 (1.6334)  Acc@1: 68.1641 (68.1641)  Acc@5: 89.0625 (89.0625)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.5166 (2.4221)  Acc@1: 71.6981 (50.3340)  Acc@5: 86.5566 (75.4720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-7.pth.tar', 33.9980000378418)

Train: 17 [   0/1251 (  0%)]  Loss: 4.900 (4.90)  Time: 1.724s,  593.94/s  (1.724s,  593.94/s)  LR: 9.921e-04  Data: 1.502 (1.502)
Train: 17 [  50/1251 (  4%)]  Loss: 5.004 (4.95)  Time: 0.168s, 6092.12/s  (0.222s, 4614.59/s)  LR: 9.921e-04  Data: 0.026 (0.058)
Train: 17 [ 100/1251 (  8%)]  Loss: 4.870 (4.92)  Time: 0.170s, 6020.91/s  (0.212s, 4824.17/s)  LR: 9.921e-04  Data: 0.024 (0.053)
Train: 17 [ 150/1251 ( 12%)]  Loss: 4.500 (4.82)  Time: 0.176s, 5811.12/s  (0.204s, 5015.53/s)  LR: 9.921e-04  Data: 0.025 (0.050)
Train: 17 [ 200/1251 ( 16%)]  Loss: 4.853 (4.83)  Time: 0.164s, 6250.03/s  (0.200s, 5109.68/s)  LR: 9.921e-04  Data: 0.027 (0.048)
Train: 17 [ 250/1251 ( 20%)]  Loss: 4.667 (4.80)  Time: 0.166s, 6184.64/s  (0.199s, 5146.12/s)  LR: 9.921e-04  Data: 0.021 (0.045)
Train: 17 [ 300/1251 ( 24%)]  Loss: 4.837 (4.80)  Time: 0.178s, 5743.65/s  (0.197s, 5198.69/s)  LR: 9.921e-04  Data: 0.029 (0.043)
Train: 17 [ 350/1251 ( 28%)]  Loss: 4.802 (4.80)  Time: 0.170s, 6017.43/s  (0.196s, 5234.42/s)  LR: 9.921e-04  Data: 0.020 (0.041)
Train: 17 [ 400/1251 ( 32%)]  Loss: 4.926 (4.82)  Time: 0.170s, 6020.93/s  (0.195s, 5250.17/s)  LR: 9.921e-04  Data: 0.030 (0.041)
Train: 17 [ 450/1251 ( 36%)]  Loss: 4.824 (4.82)  Time: 0.168s, 6105.27/s  (0.194s, 5266.62/s)  LR: 9.921e-04  Data: 0.030 (0.040)
Train: 17 [ 500/1251 ( 40%)]  Loss: 4.899 (4.83)  Time: 0.169s, 6071.04/s  (0.194s, 5274.67/s)  LR: 9.921e-04  Data: 0.019 (0.039)
Train: 17 [ 550/1251 ( 44%)]  Loss: 5.032 (4.84)  Time: 0.171s, 5973.65/s  (0.194s, 5283.03/s)  LR: 9.921e-04  Data: 0.025 (0.037)
Train: 17 [ 600/1251 ( 48%)]  Loss: 4.459 (4.81)  Time: 0.164s, 6233.58/s  (0.194s, 5285.39/s)  LR: 9.921e-04  Data: 0.024 (0.037)
Train: 17 [ 650/1251 ( 52%)]  Loss: 4.823 (4.81)  Time: 0.176s, 5833.96/s  (0.194s, 5280.31/s)  LR: 9.921e-04  Data: 0.025 (0.036)
Train: 17 [ 700/1251 ( 56%)]  Loss: 5.051 (4.83)  Time: 0.167s, 6121.25/s  (0.194s, 5287.36/s)  LR: 9.921e-04  Data: 0.031 (0.035)
Train: 17 [ 750/1251 ( 60%)]  Loss: 4.913 (4.84)  Time: 0.161s, 6357.63/s  (0.194s, 5290.83/s)  LR: 9.921e-04  Data: 0.027 (0.035)
Train: 17 [ 800/1251 ( 64%)]  Loss: 5.269 (4.86)  Time: 0.161s, 6361.86/s  (0.193s, 5299.20/s)  LR: 9.921e-04  Data: 0.029 (0.035)
Train: 17 [ 850/1251 ( 68%)]  Loss: 4.854 (4.86)  Time: 0.169s, 6062.97/s  (0.193s, 5299.75/s)  LR: 9.921e-04  Data: 0.029 (0.035)
Train: 17 [ 900/1251 ( 72%)]  Loss: 4.843 (4.86)  Time: 0.181s, 5641.96/s  (0.194s, 5291.53/s)  LR: 9.921e-04  Data: 0.040 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Train: 17 [ 950/1251 ( 76%)]  Loss: 4.859 (4.86)  Time: 0.166s, 6154.51/s  (0.193s, 5297.34/s)  LR: 9.921e-04  Data: 0.024 (0.034)
Train: 17 [1000/1251 ( 80%)]  Loss: 4.649 (4.85)  Time: 0.163s, 6280.72/s  (0.193s, 5292.59/s)  LR: 9.921e-04  Data: 0.030 (0.034)
Train: 17 [1050/1251 ( 84%)]  Loss: 4.902 (4.85)  Time: 0.266s, 3847.15/s  (0.194s, 5285.49/s)  LR: 9.921e-04  Data: 0.027 (0.034)
Train: 17 [1100/1251 ( 88%)]  Loss: 4.831 (4.85)  Time: 0.174s, 5871.39/s  (0.194s, 5282.19/s)  LR: 9.921e-04  Data: 0.022 (0.033)
Train: 17 [1150/1251 ( 92%)]  Loss: 4.833 (4.85)  Time: 0.177s, 5797.47/s  (0.194s, 5276.89/s)  LR: 9.921e-04  Data: 0.030 (0.033)
Train: 17 [1200/1251 ( 96%)]  Loss: 4.849 (4.85)  Time: 0.163s, 6279.21/s  (0.194s, 5271.44/s)  LR: 9.921e-04  Data: 0.024 (0.033)
Train: 17 [1250/1251 (100%)]  Loss: 5.141 (4.86)  Time: 0.114s, 9008.62/s  (0.194s, 5284.97/s)  LR: 9.921e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.914 (1.914)  Loss:  1.5740 (1.5740)  Acc@1: 70.9961 (70.9961)  Acc@5: 89.1602 (89.1602)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4577 (2.4026)  Acc@1: 72.1698 (51.3420)  Acc@5: 87.9717 (76.3420)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-8.pth.tar', 36.88399997802734)

Train: 18 [   0/1251 (  0%)]  Loss: 5.159 (5.16)  Time: 1.805s,  567.35/s  (1.805s,  567.35/s)  LR: 9.912e-04  Data: 1.675 (1.675)
Train: 18 [  50/1251 (  4%)]  Loss: 4.945 (5.05)  Time: 0.174s, 5872.49/s  (0.222s, 4615.61/s)  LR: 9.912e-04  Data: 0.032 (0.077)
Train: 18 [ 100/1251 (  8%)]  Loss: 4.680 (4.93)  Time: 0.167s, 6141.90/s  (0.210s, 4872.53/s)  LR: 9.912e-04  Data: 0.031 (0.053)
Train: 18 [ 150/1251 ( 12%)]  Loss: 5.083 (4.97)  Time: 0.164s, 6233.86/s  (0.205s, 4996.87/s)  LR: 9.912e-04  Data: 0.021 (0.044)
Train: 18 [ 200/1251 ( 16%)]  Loss: 4.511 (4.88)  Time: 0.154s, 6664.85/s  (0.201s, 5097.35/s)  LR: 9.912e-04  Data: 0.024 (0.040)
Train: 18 [ 250/1251 ( 20%)]  Loss: 5.289 (4.94)  Time: 0.172s, 5943.69/s  (0.199s, 5133.35/s)  LR: 9.912e-04  Data: 0.024 (0.042)
Train: 18 [ 300/1251 ( 24%)]  Loss: 4.775 (4.92)  Time: 0.193s, 5294.26/s  (0.197s, 5208.78/s)  LR: 9.912e-04  Data: 0.019 (0.041)
Train: 18 [ 350/1251 ( 28%)]  Loss: 4.950 (4.92)  Time: 0.174s, 5896.51/s  (0.196s, 5228.06/s)  LR: 9.912e-04  Data: 0.027 (0.039)
Train: 18 [ 400/1251 ( 32%)]  Loss: 5.042 (4.94)  Time: 0.163s, 6270.52/s  (0.197s, 5210.91/s)  LR: 9.912e-04  Data: 0.028 (0.038)
Train: 18 [ 450/1251 ( 36%)]  Loss: 4.719 (4.92)  Time: 0.194s, 5274.90/s  (0.196s, 5215.01/s)  LR: 9.912e-04  Data: 0.026 (0.037)
Train: 18 [ 500/1251 ( 40%)]  Loss: 4.625 (4.89)  Time: 0.192s, 5343.92/s  (0.196s, 5221.81/s)  LR: 9.912e-04  Data: 0.027 (0.036)
Train: 18 [ 550/1251 ( 44%)]  Loss: 4.775 (4.88)  Time: 0.159s, 6453.91/s  (0.195s, 5244.26/s)  LR: 9.912e-04  Data: 0.029 (0.035)
Train: 18 [ 600/1251 ( 48%)]  Loss: 4.925 (4.88)  Time: 0.406s, 2525.10/s  (0.195s, 5246.37/s)  LR: 9.912e-04  Data: 0.029 (0.035)
Train: 18 [ 650/1251 ( 52%)]  Loss: 5.244 (4.91)  Time: 0.178s, 5741.84/s  (0.195s, 5256.68/s)  LR: 9.912e-04  Data: 0.024 (0.034)
Train: 18 [ 700/1251 ( 56%)]  Loss: 4.893 (4.91)  Time: 0.171s, 5972.03/s  (0.194s, 5271.22/s)  LR: 9.912e-04  Data: 0.031 (0.034)
Train: 18 [ 750/1251 ( 60%)]  Loss: 4.945 (4.91)  Time: 0.162s, 6336.14/s  (0.194s, 5276.36/s)  LR: 9.912e-04  Data: 0.024 (0.035)
Train: 18 [ 800/1251 ( 64%)]  Loss: 4.694 (4.90)  Time: 0.171s, 5987.65/s  (0.194s, 5286.17/s)  LR: 9.912e-04  Data: 0.022 (0.036)
Train: 18 [ 850/1251 ( 68%)]  Loss: 4.802 (4.89)  Time: 0.182s, 5631.14/s  (0.194s, 5271.05/s)  LR: 9.912e-04  Data: 0.030 (0.036)
Train: 18 [ 900/1251 ( 72%)]  Loss: 5.010 (4.90)  Time: 0.163s, 6299.12/s  (0.194s, 5277.05/s)  LR: 9.912e-04  Data: 0.023 (0.035)
Train: 18 [ 950/1251 ( 76%)]  Loss: 4.604 (4.88)  Time: 0.164s, 6259.83/s  (0.194s, 5279.18/s)  LR: 9.912e-04  Data: 0.024 (0.035)
Train: 18 [1000/1251 ( 80%)]  Loss: 4.954 (4.89)  Time: 0.154s, 6670.60/s  (0.194s, 5282.83/s)  LR: 9.912e-04  Data: 0.028 (0.035)
Train: 18 [1050/1251 ( 84%)]  Loss: 4.159 (4.85)  Time: 0.195s, 5247.84/s  (0.194s, 5271.99/s)  LR: 9.912e-04  Data: 0.019 (0.034)
Train: 18 [1100/1251 ( 88%)]  Loss: 4.821 (4.85)  Time: 0.164s, 6245.63/s  (0.194s, 5269.18/s)  LR: 9.912e-04  Data: 0.025 (0.034)
Train: 18 [1150/1251 ( 92%)]  Loss: 4.596 (4.84)  Time: 0.161s, 6358.94/s  (0.194s, 5274.18/s)  LR: 9.912e-04  Data: 0.024 (0.034)
Train: 18 [1200/1251 ( 96%)]  Loss: 4.882 (4.84)  Time: 0.173s, 5926.76/s  (0.194s, 5268.53/s)  LR: 9.912e-04  Data: 0.030 (0.034)
Train: 18 [1250/1251 (100%)]  Loss: 5.035 (4.85)  Time: 0.114s, 9016.51/s  (0.194s, 5285.39/s)  LR: 9.912e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.789 (1.789)  Loss:  1.5596 (1.5596)  Acc@1: 69.6289 (69.6289)  Acc@5: 88.3789 (88.3789)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4804 (2.3719)  Acc@1: 71.1085 (51.4680)  Acc@5: 87.7358 (76.2440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-9.pth.tar', 39.742000009765626)

Train: 19 [   0/1251 (  0%)]  Loss: 4.998 (5.00)  Time: 2.011s,  509.18/s  (2.011s,  509.18/s)  LR: 9.901e-04  Data: 1.884 (1.884)
Train: 19 [  50/1251 (  4%)]  Loss: 4.622 (4.81)  Time: 0.179s, 5720.81/s  (0.223s, 4600.85/s)  LR: 9.901e-04  Data: 0.021 (0.076)
Train: 19 [ 100/1251 (  8%)]  Loss: 4.928 (4.85)  Time: 0.172s, 5956.27/s  (0.207s, 4947.36/s)  LR: 9.901e-04  Data: 0.027 (0.060)
Train: 19 [ 150/1251 ( 12%)]  Loss: 4.935 (4.87)  Time: 0.163s, 6280.75/s  (0.201s, 5085.64/s)  LR: 9.901e-04  Data: 0.026 (0.054)
Train: 19 [ 200/1251 ( 16%)]  Loss: 4.907 (4.88)  Time: 0.322s, 3181.37/s  (0.199s, 5140.03/s)  LR: 9.901e-04  Data: 0.195 (0.053)
Train: 19 [ 250/1251 ( 20%)]  Loss: 4.738 (4.85)  Time: 0.395s, 2590.19/s  (0.199s, 5156.86/s)  LR: 9.901e-04  Data: 0.028 (0.048)
Train: 19 [ 300/1251 ( 24%)]  Loss: 4.915 (4.86)  Time: 0.172s, 5946.19/s  (0.198s, 5175.55/s)  LR: 9.901e-04  Data: 0.037 (0.044)
Train: 19 [ 350/1251 ( 28%)]  Loss: 5.099 (4.89)  Time: 0.208s, 4920.44/s  (0.197s, 5206.92/s)  LR: 9.901e-04  Data: 0.020 (0.042)
Train: 19 [ 400/1251 ( 32%)]  Loss: 4.824 (4.89)  Time: 0.174s, 5873.27/s  (0.196s, 5222.06/s)  LR: 9.901e-04  Data: 0.025 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0


Train: 19 [ 450/1251 ( 36%)]  Loss: 4.573 (4.85)  Time: 0.314s, 3264.94/s  (0.196s, 5231.04/s)  LR: 9.901e-04  Data: 0.025 (0.039)
Train: 19 [ 500/1251 ( 40%)]  Loss: 4.989 (4.87)  Time: 0.177s, 5797.49/s  (0.195s, 5255.08/s)  LR: 9.901e-04  Data: 0.032 (0.037)
Train: 19 [ 550/1251 ( 44%)]  Loss: 4.715 (4.85)  Time: 0.165s, 6199.45/s  (0.195s, 5255.95/s)  LR: 9.901e-04  Data: 0.030 (0.037)
Train: 19 [ 600/1251 ( 48%)]  Loss: 5.200 (4.88)  Time: 0.177s, 5774.75/s  (0.195s, 5254.72/s)  LR: 9.901e-04  Data: 0.020 (0.036)
Train: 19 [ 650/1251 ( 52%)]  Loss: 4.780 (4.87)  Time: 0.185s, 5531.79/s  (0.195s, 5255.23/s)  LR: 9.901e-04  Data: 0.023 (0.035)
Train: 19 [ 700/1251 ( 56%)]  Loss: 4.634 (4.86)  Time: 0.182s, 5626.35/s  (0.195s, 5251.88/s)  LR: 9.901e-04  Data: 0.021 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 19 [ 750/1251 ( 60%)]  Loss: 5.003 (4.87)  Time: 0.154s, 6643.55/s  (0.195s, 5243.80/s)  LR: 9.901e-04  Data: 0.025 (0.034)
Train: 19 [ 800/1251 ( 64%)]  Loss: 4.851 (4.87)  Time: 0.169s, 6067.49/s  (0.195s, 5244.89/s)  LR: 9.901e-04  Data: 0.029 (0.034)
Train: 19 [ 850/1251 ( 68%)]  Loss: 4.648 (4.85)  Time: 0.165s, 6213.42/s  (0.195s, 5252.41/s)  LR: 9.901e-04  Data: 0.029 (0.033)
Train: 19 [ 900/1251 ( 72%)]  Loss: 4.981 (4.86)  Time: 0.184s, 5571.65/s  (0.195s, 5254.48/s)  LR: 9.901e-04  Data: 0.032 (0.033)
Train: 19 [ 950/1251 ( 76%)]  Loss: 5.224 (4.88)  Time: 0.176s, 5807.25/s  (0.195s, 5249.71/s)  LR: 9.901e-04  Data: 0.026 (0.033)
Train: 19 [1000/1251 ( 80%)]  Loss: 5.005 (4.88)  Time: 0.167s, 6114.22/s  (0.195s, 5247.99/s)  LR: 9.901e-04  Data: 0.024 (0.033)
Train: 19 [1050/1251 ( 84%)]  Loss: 4.579 (4.87)  Time: 0.148s, 6939.15/s  (0.195s, 5253.89/s)  LR: 9.901e-04  Data: 0.026 (0.032)
Train: 19 [1100/1251 ( 88%)]  Loss: 5.132 (4.88)  Time: 0.150s, 6832.31/s  (0.195s, 5253.14/s)  LR: 9.901e-04  Data: 0.023 (0.032)
Train: 19 [1150/1251 ( 92%)]  Loss: 5.065 (4.89)  Time: 0.160s, 6417.24/s  (0.195s, 5248.35/s)  LR: 9.901e-04  Data: 0.030 (0.032)
Train: 19 [1200/1251 ( 96%)]  Loss: 4.871 (4.89)  Time: 0.175s, 5855.67/s  (0.195s, 5245.20/s)  LR: 9.901e-04  Data: 0.031 (0.032)
Train: 19 [1250/1251 (100%)]  Loss: 4.647 (4.88)  Time: 0.114s, 8994.43/s  (0.195s, 5259.91/s)  LR: 9.901e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.883 (1.883)  Loss:  1.4531 (1.4531)  Acc@1: 71.0938 (71.0938)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.5398 (2.3122)  Acc@1: 71.6981 (52.1760)  Acc@5: 87.1462 (76.9760)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-10.pth.tar', 41.65999996582031)

Train: 20 [   0/1251 (  0%)]  Loss: 4.769 (4.77)  Time: 1.912s,  535.54/s  (1.912s,  535.54/s)  LR: 9.891e-04  Data: 1.625 (1.625)
Train: 20 [  50/1251 (  4%)]  Loss: 4.953 (4.86)  Time: 0.154s, 6651.71/s  (0.226s, 4530.93/s)  LR: 9.891e-04  Data: 0.027 (0.061)
Train: 20 [ 100/1251 (  8%)]  Loss: 4.486 (4.74)  Time: 0.194s, 5284.71/s  (0.211s, 4853.95/s)  LR: 9.891e-04  Data: 0.025 (0.046)
Train: 20 [ 150/1251 ( 12%)]  Loss: 5.216 (4.86)  Time: 0.153s, 6676.86/s  (0.202s, 5061.78/s)  LR: 9.891e-04  Data: 0.028 (0.040)
Train: 20 [ 200/1251 ( 16%)]  Loss: 4.390 (4.76)  Time: 0.247s, 4146.96/s  (0.200s, 5120.86/s)  LR: 9.891e-04  Data: 0.025 (0.037)
Train: 20 [ 250/1251 ( 20%)]  Loss: 4.526 (4.72)  Time: 0.187s, 5479.20/s  (0.198s, 5162.74/s)  LR: 9.891e-04  Data: 0.027 (0.036)
Train: 20 [ 300/1251 ( 24%)]  Loss: 4.835 (4.74)  Time: 0.190s, 5377.76/s  (0.197s, 5200.98/s)  LR: 9.891e-04  Data: 0.023 (0.035)
Train: 20 [ 350/1251 ( 28%)]  Loss: 4.675 (4.73)  Time: 0.164s, 6262.59/s  (0.196s, 5237.59/s)  LR: 9.891e-04  Data: 0.033 (0.034)
Train: 20 [ 400/1251 ( 32%)]  Loss: 4.936 (4.75)  Time: 0.282s, 3629.25/s  (0.194s, 5268.22/s)  LR: 9.891e-04  Data: 0.029 (0.033)
Train: 20 [ 450/1251 ( 36%)]  Loss: 4.928 (4.77)  Time: 0.172s, 5939.55/s  (0.195s, 5264.01/s)  LR: 9.891e-04  Data: 0.031 (0.032)
Train: 20 [ 500/1251 ( 40%)]  Loss: 4.842 (4.78)  Time: 0.175s, 5842.37/s  (0.194s, 5279.28/s)  LR: 9.891e-04  Data: 0.029 (0.032)
Train: 20 [ 550/1251 ( 44%)]  Loss: 4.744 (4.77)  Time: 0.184s, 5571.65/s  (0.196s, 5234.99/s)  LR: 9.891e-04  Data: 0.054 (0.031)
Train: 20 [ 600/1251 ( 48%)]  Loss: 4.965 (4.79)  Time: 0.166s, 6173.40/s  (0.195s, 5252.29/s)  LR: 9.891e-04  Data: 0.038 (0.031)
Train: 20 [ 650/1251 ( 52%)]  Loss: 4.767 (4.79)  Time: 0.164s, 6246.07/s  (0.195s, 5257.65/s)  LR: 9.891e-04  Data: 0.030 (0.031)
Train: 20 [ 700/1251 ( 56%)]  Loss: 4.824 (4.79)  Time: 0.154s, 6646.09/s  (0.195s, 5263.84/s)  LR: 9.891e-04  Data: 0.026 (0.031)
Train: 20 [ 750/1251 ( 60%)]  Loss: 4.897 (4.80)  Time: 0.431s, 2374.54/s  (0.195s, 5259.86/s)  LR: 9.891e-04  Data: 0.029 (0.030)
Train: 20 [ 800/1251 ( 64%)]  Loss: 4.746 (4.79)  Time: 0.160s, 6399.03/s  (0.194s, 5265.99/s)  LR: 9.891e-04  Data: 0.026 (0.030)
Train: 20 [ 850/1251 ( 68%)]  Loss: 4.810 (4.79)  Time: 0.185s, 5535.86/s  (0.195s, 5264.26/s)  LR: 9.891e-04  Data: 0.034 (0.030)
Train: 20 [ 900/1251 ( 72%)]  Loss: 4.881 (4.80)  Time: 0.176s, 5818.87/s  (0.195s, 5264.67/s)  LR: 9.891e-04  Data: 0.025 (0.030)
Train: 20 [ 950/1251 ( 76%)]  Loss: 4.830 (4.80)  Time: 0.172s, 5957.64/s  (0.195s, 5255.10/s)  LR: 9.891e-04  Data: 0.032 (0.030)
Train: 20 [1000/1251 ( 80%)]  Loss: 5.048 (4.81)  Time: 0.161s, 6375.70/s  (0.195s, 5253.00/s)  LR: 9.891e-04  Data: 0.023 (0.030)
Train: 20 [1050/1251 ( 84%)]  Loss: 4.421 (4.79)  Time: 0.452s, 2267.67/s  (0.195s, 5248.70/s)  LR: 9.891e-04  Data: 0.035 (0.030)
Train: 20 [1100/1251 ( 88%)]  Loss: 4.862 (4.80)  Time: 0.165s, 6210.12/s  (0.195s, 5250.17/s)  LR: 9.891e-04  Data: 0.030 (0.030)
Train: 20 [1150/1251 ( 92%)]  Loss: 4.466 (4.78)  Time: 0.167s, 6121.42/s  (0.196s, 5215.99/s)  LR: 9.891e-04  Data: 0.020 (0.030)
Train: 20 [1200/1251 ( 96%)]  Loss: 4.867 (4.79)  Time: 0.188s, 5434.20/s  (0.197s, 5188.76/s)  LR: 9.891e-04  Data: 0.029 (0.030)
Train: 20 [1250/1251 (100%)]  Loss: 4.584 (4.78)  Time: 0.114s, 9003.80/s  (0.197s, 5204.35/s)  LR: 9.891e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.896 (1.896)  Loss:  1.4645 (1.4645)  Acc@1: 73.0469 (73.0469)  Acc@5: 89.6484 (89.6484)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.4092 (2.2538)  Acc@1: 73.2311 (53.4980)  Acc@5: 88.3255 (77.9080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-11.pth.tar', 43.67599990722656)

Train: 21 [   0/1251 (  0%)]  Loss: 4.360 (4.36)  Time: 1.779s,  575.53/s  (1.779s,  575.53/s)  LR: 9.880e-04  Data: 1.650 (1.650)
Train: 21 [  50/1251 (  4%)]  Loss: 4.768 (4.56)  Time: 0.175s, 5849.37/s  (0.222s, 4603.02/s)  LR: 9.880e-04  Data: 0.021 (0.075)
Train: 21 [ 100/1251 (  8%)]  Loss: 4.594 (4.57)  Time: 0.161s, 6364.77/s  (0.207s, 4945.27/s)  LR: 9.880e-04  Data: 0.025 (0.055)
Train: 21 [ 150/1251 ( 12%)]  Loss: 4.628 (4.59)  Time: 0.155s, 6587.75/s  (0.201s, 5099.57/s)  LR: 9.880e-04  Data: 0.024 (0.050)
Train: 21 [ 200/1251 ( 16%)]  Loss: 4.465 (4.56)  Time: 0.166s, 6150.45/s  (0.199s, 5151.08/s)  LR: 9.880e-04  Data: 0.035 (0.045)
Train: 21 [ 250/1251 ( 20%)]  Loss: 5.284 (4.68)  Time: 0.162s, 6322.39/s  (0.197s, 5199.77/s)  LR: 9.880e-04  Data: 0.027 (0.041)
Train: 21 [ 300/1251 ( 24%)]  Loss: 4.701 (4.69)  Time: 0.167s, 6116.05/s  (0.197s, 5192.32/s)  LR: 9.880e-04  Data: 0.025 (0.039)
Train: 21 [ 350/1251 ( 28%)]  Loss: 4.907 (4.71)  Time: 0.171s, 5981.67/s  (0.196s, 5234.88/s)  LR: 9.880e-04  Data: 0.029 (0.038)
Train: 21 [ 400/1251 ( 32%)]  Loss: 4.784 (4.72)  Time: 0.364s, 2811.04/s  (0.195s, 5240.83/s)  LR: 9.880e-04  Data: 0.023 (0.036)
Train: 21 [ 450/1251 ( 36%)]  Loss: 5.332 (4.78)  Time: 0.172s, 5955.08/s  (0.195s, 5255.66/s)  LR: 9.880e-04  Data: 0.026 (0.036)
Train: 21 [ 500/1251 ( 40%)]  Loss: 4.718 (4.78)  Time: 0.171s, 5993.43/s  (0.194s, 5270.71/s)  LR: 9.880e-04  Data: 0.028 (0.035)
Train: 21 [ 550/1251 ( 44%)]  Loss: 4.941 (4.79)  Time: 0.169s, 6055.92/s  (0.194s, 5267.60/s)  LR: 9.880e-04  Data: 0.029 (0.035)
Train: 21 [ 600/1251 ( 48%)]  Loss: 4.619 (4.78)  Time: 0.162s, 6323.05/s  (0.194s, 5273.13/s)  LR: 9.880e-04  Data: 0.030 (0.034)
Train: 21 [ 650/1251 ( 52%)]  Loss: 5.013 (4.79)  Time: 0.188s, 5459.47/s  (0.194s, 5269.07/s)  LR: 9.880e-04  Data: 0.033 (0.034)
Train: 21 [ 700/1251 ( 56%)]  Loss: 4.816 (4.80)  Time: 0.188s, 5448.16/s  (0.194s, 5268.83/s)  LR: 9.880e-04  Data: 0.025 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Train: 21 [ 750/1251 ( 60%)]  Loss: 4.644 (4.79)  Time: 0.166s, 6152.65/s  (0.194s, 5286.08/s)  LR: 9.880e-04  Data: 0.033 (0.033)
Train: 21 [ 800/1251 ( 64%)]  Loss: 4.855 (4.79)  Time: 0.174s, 5900.57/s  (0.194s, 5279.79/s)  LR: 9.880e-04  Data: 0.030 (0.033)
Train: 21 [ 850/1251 ( 68%)]  Loss: 4.868 (4.79)  Time: 0.243s, 4209.72/s  (0.194s, 5276.99/s)  LR: 9.880e-04  Data: 0.028 (0.033)
Train: 21 [ 900/1251 ( 72%)]  Loss: 4.693 (4.79)  Time: 0.159s, 6434.00/s  (0.194s, 5274.66/s)  LR: 9.880e-04  Data: 0.023 (0.032)
Train: 21 [ 950/1251 ( 76%)]  Loss: 4.996 (4.80)  Time: 0.166s, 6150.60/s  (0.194s, 5274.43/s)  LR: 9.880e-04  Data: 0.024 (0.032)
Train: 21 [1000/1251 ( 80%)]  Loss: 4.729 (4.80)  Time: 0.202s, 5062.22/s  (0.194s, 5277.48/s)  LR: 9.880e-04  Data: 0.026 (0.032)
Train: 21 [1050/1251 ( 84%)]  Loss: 4.909 (4.80)  Time: 0.174s, 5882.16/s  (0.194s, 5273.60/s)  LR: 9.880e-04  Data: 0.037 (0.032)
Train: 21 [1100/1251 ( 88%)]  Loss: 4.897 (4.81)  Time: 0.188s, 5442.10/s  (0.195s, 5263.55/s)  LR: 9.880e-04  Data: 0.034 (0.032)
Train: 21 [1150/1251 ( 92%)]  Loss: 4.648 (4.80)  Time: 0.160s, 6411.23/s  (0.194s, 5273.32/s)  LR: 9.880e-04  Data: 0.027 (0.031)
Train: 21 [1200/1251 ( 96%)]  Loss: 4.542 (4.79)  Time: 0.190s, 5377.25/s  (0.195s, 5264.16/s)  LR: 9.880e-04  Data: 0.026 (0.031)
Train: 21 [1250/1251 (100%)]  Loss: 4.668 (4.78)  Time: 0.113s, 9025.07/s  (0.194s, 5276.98/s)  LR: 9.880e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.791 (1.791)  Loss:  1.4235 (1.4235)  Acc@1: 72.5586 (72.5586)  Acc@5: 89.7461 (89.7461)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3573 (2.2244)  Acc@1: 72.2877 (53.5500)  Acc@5: 89.2689 (78.0860)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-12.pth.tar', 45.77400004882813)

Train: 22 [   0/1251 (  0%)]  Loss: 4.848 (4.85)  Time: 1.795s,  570.36/s  (1.795s,  570.36/s)  LR: 9.868e-04  Data: 1.486 (1.486)
Train: 22 [  50/1251 (  4%)]  Loss: 4.630 (4.74)  Time: 0.182s, 5640.86/s  (0.227s, 4505.18/s)  LR: 9.868e-04  Data: 0.026 (0.055)
Train: 22 [ 100/1251 (  8%)]  Loss: 4.779 (4.75)  Time: 0.168s, 6109.37/s  (0.209s, 4907.19/s)  LR: 9.868e-04  Data: 0.023 (0.041)
Train: 22 [ 150/1251 ( 12%)]  Loss: 4.585 (4.71)  Time: 0.182s, 5626.94/s  (0.201s, 5103.99/s)  LR: 9.868e-04  Data: 0.033 (0.036)
Train: 22 [ 200/1251 ( 16%)]  Loss: 5.039 (4.78)  Time: 0.169s, 6050.33/s  (0.201s, 5103.64/s)  LR: 9.868e-04  Data: 0.025 (0.034)
Train: 22 [ 250/1251 ( 20%)]  Loss: 4.735 (4.77)  Time: 0.182s, 5615.63/s  (0.201s, 5096.75/s)  LR: 9.868e-04  Data: 0.032 (0.033)
Train: 22 [ 300/1251 ( 24%)]  Loss: 4.851 (4.78)  Time: 0.184s, 5573.80/s  (0.199s, 5156.46/s)  LR: 9.868e-04  Data: 0.020 (0.032)
Train: 22 [ 350/1251 ( 28%)]  Loss: 4.508 (4.75)  Time: 0.169s, 6056.77/s  (0.198s, 5162.13/s)  LR: 9.868e-04  Data: 0.021 (0.031)
Train: 22 [ 400/1251 ( 32%)]  Loss: 4.184 (4.68)  Time: 0.192s, 5321.59/s  (0.198s, 5179.63/s)  LR: 9.868e-04  Data: 0.027 (0.031)
Train: 22 [ 450/1251 ( 36%)]  Loss: 5.161 (4.73)  Time: 0.177s, 5788.13/s  (0.197s, 5207.02/s)  LR: 9.868e-04  Data: 0.026 (0.031)
Train: 22 [ 500/1251 ( 40%)]  Loss: 5.136 (4.77)  Time: 0.190s, 5385.78/s  (0.196s, 5215.44/s)  LR: 9.868e-04  Data: 0.022 (0.031)
Train: 22 [ 550/1251 ( 44%)]  Loss: 4.599 (4.75)  Time: 0.187s, 5466.78/s  (0.196s, 5236.10/s)  LR: 9.868e-04  Data: 0.022 (0.031)
Train: 22 [ 600/1251 ( 48%)]  Loss: 4.862 (4.76)  Time: 0.336s, 3049.53/s  (0.196s, 5235.45/s)  LR: 9.868e-04  Data: 0.023 (0.031)
Train: 22 [ 650/1251 ( 52%)]  Loss: 4.729 (4.76)  Time: 0.176s, 5804.59/s  (0.195s, 5250.13/s)  LR: 9.868e-04  Data: 0.024 (0.030)
Train: 22 [ 700/1251 ( 56%)]  Loss: 4.738 (4.76)  Time: 0.172s, 5958.99/s  (0.195s, 5254.36/s)  LR: 9.868e-04  Data: 0.029 (0.030)
Train: 22 [ 750/1251 ( 60%)]  Loss: 4.530 (4.74)  Time: 0.356s, 2880.43/s  (0.195s, 5256.14/s)  LR: 9.868e-04  Data: 0.028 (0.030)
Train: 22 [ 800/1251 ( 64%)]  Loss: 4.738 (4.74)  Time: 0.177s, 5782.22/s  (0.195s, 5260.48/s)  LR: 9.868e-04  Data: 0.025 (0.030)
Train: 22 [ 850/1251 ( 68%)]  Loss: 4.554 (4.73)  Time: 0.175s, 5834.89/s  (0.195s, 5262.11/s)  LR: 9.868e-04  Data: 0.030 (0.030)
Train: 22 [ 900/1251 ( 72%)]  Loss: 4.462 (4.72)  Time: 0.196s, 5227.49/s  (0.195s, 5261.68/s)  LR: 9.868e-04  Data: 0.036 (0.030)
Train: 22 [ 950/1251 ( 76%)]  Loss: 4.809 (4.72)  Time: 0.320s, 3200.20/s  (0.195s, 5250.10/s)  LR: 9.868e-04  Data: 0.024 (0.030)
Train: 22 [1000/1251 ( 80%)]  Loss: 4.478 (4.71)  Time: 0.484s, 2116.96/s  (0.195s, 5243.82/s)  LR: 9.868e-04  Data: 0.026 (0.030)
Train: 22 [1050/1251 ( 84%)]  Loss: 4.826 (4.72)  Time: 0.164s, 6257.15/s  (0.195s, 5250.64/s)  LR: 9.868e-04  Data: 0.025 (0.030)
Train: 22 [1100/1251 ( 88%)]  Loss: 4.716 (4.72)  Time: 0.192s, 5331.93/s  (0.195s, 5246.28/s)  LR: 9.868e-04  Data: 0.028 (0.030)
Train: 22 [1150/1251 ( 92%)]  Loss: 4.818 (4.72)  Time: 0.164s, 6250.47/s  (0.195s, 5242.91/s)  LR: 9.868e-04  Data: 0.029 (0.030)
Train: 22 [1200/1251 ( 96%)]  Loss: 4.919 (4.73)  Time: 0.401s, 2554.15/s  (0.195s, 5241.31/s)  LR: 9.868e-04  Data: 0.026 (0.030)
Train: 22 [1250/1251 (100%)]  Loss: 4.877 (4.74)  Time: 0.113s, 9086.42/s  (0.195s, 5254.35/s)  LR: 9.868e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.859 (1.859)  Loss:  1.4890 (1.4890)  Acc@1: 72.1680 (72.1680)  Acc@5: 88.8672 (88.8672)
Test: [  48/48]  Time: 0.019 (0.233)  Loss:  1.4443 (2.2145)  Acc@1: 71.9340 (54.6000)  Acc@5: 87.1462 (78.5060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-13.pth.tar', 47.24200010498047)

Train: 23 [   0/1251 (  0%)]  Loss: 4.672 (4.67)  Time: 1.819s,  562.90/s  (1.819s,  562.90/s)  LR: 9.856e-04  Data: 1.690 (1.690)
Train: 23 [  50/1251 (  4%)]  Loss: 4.203 (4.44)  Time: 0.163s, 6282.98/s  (0.218s, 4697.02/s)  LR: 9.856e-04  Data: 0.029 (0.070)
Train: 23 [ 100/1251 (  8%)]  Loss: 4.745 (4.54)  Time: 0.176s, 5813.30/s  (0.203s, 5042.18/s)  LR: 9.856e-04  Data: 0.022 (0.054)
Train: 23 [ 150/1251 ( 12%)]  Loss: 4.490 (4.53)  Time: 0.176s, 5805.80/s  (0.200s, 5116.88/s)  LR: 9.856e-04  Data: 0.026 (0.051)
Train: 23 [ 200/1251 ( 16%)]  Loss: 4.619 (4.55)  Time: 0.180s, 5697.51/s  (0.199s, 5147.73/s)  LR: 9.856e-04  Data: 0.032 (0.049)
Train: 23 [ 250/1251 ( 20%)]  Loss: 5.020 (4.62)  Time: 0.186s, 5513.80/s  (0.196s, 5225.70/s)  LR: 9.856e-04  Data: 0.027 (0.045)
Train: 23 [ 300/1251 ( 24%)]  Loss: 4.820 (4.65)  Time: 0.184s, 5558.91/s  (0.195s, 5259.37/s)  LR: 9.856e-04  Data: 0.024 (0.045)
Train: 23 [ 350/1251 ( 28%)]  Loss: 4.646 (4.65)  Time: 0.175s, 5850.17/s  (0.194s, 5273.49/s)  LR: 9.856e-04  Data: 0.025 (0.045)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 23 [ 400/1251 ( 32%)]  Loss: 4.746 (4.66)  Time: 0.177s, 5793.70/s  (0.194s, 5276.97/s)  LR: 9.856e-04  Data: 0.030 (0.045)
Train: 23 [ 450/1251 ( 36%)]  Loss: 4.829 (4.68)  Time: 0.175s, 5849.56/s  (0.194s, 5270.77/s)  LR: 9.856e-04  Data: 0.029 (0.045)
Train: 23 [ 500/1251 ( 40%)]  Loss: 4.554 (4.67)  Time: 0.165s, 6221.84/s  (0.193s, 5293.96/s)  LR: 9.856e-04  Data: 0.026 (0.044)
Train: 23 [ 550/1251 ( 44%)]  Loss: 4.700 (4.67)  Time: 0.163s, 6274.53/s  (0.193s, 5297.72/s)  LR: 9.856e-04  Data: 0.030 (0.042)
Train: 23 [ 600/1251 ( 48%)]  Loss: 4.601 (4.66)  Time: 0.196s, 5236.18/s  (0.194s, 5288.56/s)  LR: 9.856e-04  Data: 0.027 (0.041)
Train: 23 [ 650/1251 ( 52%)]  Loss: 4.865 (4.68)  Time: 0.175s, 5839.82/s  (0.193s, 5293.23/s)  LR: 9.856e-04  Data: 0.026 (0.040)
Train: 23 [ 700/1251 ( 56%)]  Loss: 4.901 (4.69)  Time: 0.167s, 6124.42/s  (0.193s, 5296.02/s)  LR: 9.856e-04  Data: 0.030 (0.039)
Train: 23 [ 750/1251 ( 60%)]  Loss: 4.781 (4.70)  Time: 0.185s, 5544.70/s  (0.193s, 5297.85/s)  LR: 9.856e-04  Data: 0.024 (0.039)
Train: 23 [ 800/1251 ( 64%)]  Loss: 4.789 (4.70)  Time: 0.186s, 5498.32/s  (0.193s, 5299.43/s)  LR: 9.856e-04  Data: 0.028 (0.038)
Train: 23 [ 850/1251 ( 68%)]  Loss: 4.731 (4.71)  Time: 0.175s, 5861.02/s  (0.193s, 5292.31/s)  LR: 9.856e-04  Data: 0.021 (0.037)
Train: 23 [ 900/1251 ( 72%)]  Loss: 4.733 (4.71)  Time: 0.177s, 5774.99/s  (0.194s, 5287.22/s)  LR: 9.856e-04  Data: 0.038 (0.037)
Train: 23 [ 950/1251 ( 76%)]  Loss: 5.034 (4.72)  Time: 0.159s, 6432.92/s  (0.194s, 5289.04/s)  LR: 9.856e-04  Data: 0.027 (0.036)
Train: 23 [1000/1251 ( 80%)]  Loss: 4.784 (4.73)  Time: 0.160s, 6389.63/s  (0.194s, 5268.57/s)  LR: 9.856e-04  Data: 0.029 (0.036)
Train: 23 [1050/1251 ( 84%)]  Loss: 4.730 (4.73)  Time: 0.163s, 6288.97/s  (0.195s, 5263.33/s)  LR: 9.856e-04  Data: 0.031 (0.036)
Train: 23 [1100/1251 ( 88%)]  Loss: 4.683 (4.72)  Time: 0.163s, 6286.61/s  (0.195s, 5254.79/s)  LR: 9.856e-04  Data: 0.026 (0.035)
Train: 23 [1150/1251 ( 92%)]  Loss: 4.550 (4.72)  Time: 0.156s, 6550.01/s  (0.195s, 5241.26/s)  LR: 9.856e-04  Data: 0.033 (0.035)
Train: 23 [1200/1251 ( 96%)]  Loss: 4.434 (4.71)  Time: 0.157s, 6538.85/s  (0.195s, 5247.46/s)  LR: 9.856e-04  Data: 0.030 (0.035)
Train: 23 [1250/1251 (100%)]  Loss: 4.954 (4.72)  Time: 0.114s, 9001.26/s  (0.195s, 5263.40/s)  LR: 9.856e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.811 (1.811)  Loss:  1.5588 (1.5588)  Acc@1: 70.9961 (70.9961)  Acc@5: 90.1367 (90.1367)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.4963 (2.2015)  Acc@1: 72.2877 (54.6160)  Acc@5: 88.2076 (78.9700)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-14.pth.tar', 48.1040000024414)

Train: 24 [   0/1251 (  0%)]  Loss: 4.834 (4.83)  Time: 1.748s,  585.94/s  (1.748s,  585.94/s)  LR: 9.843e-04  Data: 1.500 (1.500)
Train: 24 [  50/1251 (  4%)]  Loss: 4.547 (4.69)  Time: 0.167s, 6118.74/s  (0.223s, 4585.87/s)  LR: 9.843e-04  Data: 0.027 (0.060)
Train: 24 [ 100/1251 (  8%)]  Loss: 4.420 (4.60)  Time: 0.198s, 5171.85/s  (0.207s, 4954.32/s)  LR: 9.843e-04  Data: 0.030 (0.045)
Train: 24 [ 150/1251 ( 12%)]  Loss: 4.708 (4.63)  Time: 0.169s, 6041.74/s  (0.202s, 5069.31/s)  LR: 9.843e-04  Data: 0.029 (0.039)
Train: 24 [ 200/1251 ( 16%)]  Loss: 4.508 (4.60)  Time: 0.196s, 5234.95/s  (0.201s, 5092.76/s)  LR: 9.843e-04  Data: 0.029 (0.037)
Train: 24 [ 250/1251 ( 20%)]  Loss: 4.977 (4.67)  Time: 0.165s, 6191.78/s  (0.198s, 5165.65/s)  LR: 9.843e-04  Data: 0.037 (0.035)
Train: 24 [ 300/1251 ( 24%)]  Loss: 4.584 (4.65)  Time: 0.165s, 6215.01/s  (0.196s, 5231.73/s)  LR: 9.843e-04  Data: 0.027 (0.034)
Train: 24 [ 350/1251 ( 28%)]  Loss: 4.960 (4.69)  Time: 0.177s, 5774.70/s  (0.195s, 5247.33/s)  LR: 9.843e-04  Data: 0.032 (0.033)
Train: 24 [ 400/1251 ( 32%)]  Loss: 4.666 (4.69)  Time: 0.180s, 5685.46/s  (0.194s, 5265.71/s)  LR: 9.843e-04  Data: 0.028 (0.033)
Train: 24 [ 450/1251 ( 36%)]  Loss: 4.746 (4.70)  Time: 0.172s, 5938.94/s  (0.195s, 5253.07/s)  LR: 9.843e-04  Data: 0.025 (0.032)
Train: 24 [ 500/1251 ( 40%)]  Loss: 4.789 (4.70)  Time: 0.164s, 6241.33/s  (0.194s, 5289.16/s)  LR: 9.843e-04  Data: 0.026 (0.032)
Train: 24 [ 550/1251 ( 44%)]  Loss: 4.636 (4.70)  Time: 0.195s, 5260.29/s  (0.194s, 5282.20/s)  LR: 9.843e-04  Data: 0.023 (0.031)
Train: 24 [ 600/1251 ( 48%)]  Loss: 4.584 (4.69)  Time: 0.358s, 2859.25/s  (0.194s, 5276.45/s)  LR: 9.843e-04  Data: 0.025 (0.031)
Train: 24 [ 650/1251 ( 52%)]  Loss: 4.969 (4.71)  Time: 0.173s, 5902.66/s  (0.194s, 5291.73/s)  LR: 9.843e-04  Data: 0.028 (0.031)
Train: 24 [ 700/1251 ( 56%)]  Loss: 4.604 (4.70)  Time: 0.173s, 5933.24/s  (0.193s, 5292.24/s)  LR: 9.843e-04  Data: 0.037 (0.031)
Train: 24 [ 750/1251 ( 60%)]  Loss: 4.849 (4.71)  Time: 0.165s, 6222.80/s  (0.194s, 5288.55/s)  LR: 9.843e-04  Data: 0.022 (0.030)
Train: 24 [ 800/1251 ( 64%)]  Loss: 4.729 (4.71)  Time: 0.153s, 6700.68/s  (0.195s, 5247.80/s)  LR: 9.843e-04  Data: 0.030 (0.030)
Train: 24 [ 850/1251 ( 68%)]  Loss: 4.694 (4.71)  Time: 0.159s, 6445.61/s  (0.195s, 5252.53/s)  LR: 9.843e-04  Data: 0.024 (0.030)
Train: 24 [ 900/1251 ( 72%)]  Loss: 4.564 (4.70)  Time: 0.184s, 5568.70/s  (0.195s, 5244.01/s)  LR: 9.843e-04  Data: 0.056 (0.030)
Train: 24 [ 950/1251 ( 76%)]  Loss: 4.742 (4.71)  Time: 0.173s, 5923.25/s  (0.196s, 5237.17/s)  LR: 9.843e-04  Data: 0.031 (0.030)
Train: 24 [1000/1251 ( 80%)]  Loss: 4.731 (4.71)  Time: 0.179s, 5725.88/s  (0.195s, 5238.12/s)  LR: 9.843e-04  Data: 0.023 (0.030)
Train: 24 [1050/1251 ( 84%)]  Loss: 4.914 (4.72)  Time: 0.175s, 5867.39/s  (0.195s, 5245.64/s)  LR: 9.843e-04  Data: 0.025 (0.030)
Train: 24 [1100/1251 ( 88%)]  Loss: 4.449 (4.70)  Time: 0.167s, 6141.76/s  (0.195s, 5240.53/s)  LR: 9.843e-04  Data: 0.029 (0.030)
Train: 24 [1150/1251 ( 92%)]  Loss: 5.066 (4.72)  Time: 0.149s, 6859.63/s  (0.195s, 5240.87/s)  LR: 9.843e-04  Data: 0.029 (0.030)
Train: 24 [1200/1251 ( 96%)]  Loss: 4.288 (4.70)  Time: 0.397s, 2577.63/s  (0.195s, 5237.92/s)  LR: 9.843e-04  Data: 0.028 (0.029)
Train: 24 [1250/1251 (100%)]  Loss: 4.635 (4.70)  Time: 0.114s, 8994.36/s  (0.195s, 5253.81/s)  LR: 9.843e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.792 (1.792)  Loss:  1.4283 (1.4283)  Acc@1: 73.7305 (73.7305)  Acc@5: 90.7227 (90.7227)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3861 (2.1953)  Acc@1: 73.5849 (54.7220)  Acc@5: 89.0330 (78.9380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-15.pth.tar', 49.32199996582031)

Train: 25 [   0/1251 (  0%)]  Loss: 4.738 (4.74)  Time: 1.732s,  591.24/s  (1.732s,  591.24/s)  LR: 9.830e-04  Data: 1.602 (1.602)
Train: 25 [  50/1251 (  4%)]  Loss: 4.231 (4.48)  Time: 0.155s, 6619.60/s  (0.229s, 4476.10/s)  LR: 9.830e-04  Data: 0.023 (0.078)
Train: 25 [ 100/1251 (  8%)]  Loss: 4.834 (4.60)  Time: 0.166s, 6164.79/s  (0.208s, 4934.87/s)  LR: 9.830e-04  Data: 0.028 (0.059)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 25 [ 150/1251 ( 12%)]  Loss: 4.369 (4.54)  Time: 0.167s, 6133.21/s  (0.202s, 5066.04/s)  LR: 9.830e-04  Data: 0.030 (0.049)
Train: 25 [ 200/1251 ( 16%)]  Loss: 4.129 (4.46)  Time: 0.173s, 5909.01/s  (0.196s, 5215.76/s)  LR: 9.830e-04  Data: 0.025 (0.044)
Train: 25 [ 250/1251 ( 20%)]  Loss: 4.641 (4.49)  Time: 0.154s, 6668.24/s  (0.196s, 5219.29/s)  LR: 9.830e-04  Data: 0.029 (0.041)
Train: 25 [ 300/1251 ( 24%)]  Loss: 4.417 (4.48)  Time: 0.164s, 6227.55/s  (0.195s, 5262.16/s)  LR: 9.830e-04  Data: 0.022 (0.039)
Train: 25 [ 350/1251 ( 28%)]  Loss: 4.743 (4.51)  Time: 0.161s, 6367.98/s  (0.194s, 5272.81/s)  LR: 9.830e-04  Data: 0.027 (0.037)
Train: 25 [ 400/1251 ( 32%)]  Loss: 4.773 (4.54)  Time: 0.299s, 3429.60/s  (0.193s, 5294.87/s)  LR: 9.830e-04  Data: 0.029 (0.036)
Train: 25 [ 450/1251 ( 36%)]  Loss: 4.516 (4.54)  Time: 0.177s, 5769.10/s  (0.193s, 5312.63/s)  LR: 9.830e-04  Data: 0.027 (0.035)
Train: 25 [ 500/1251 ( 40%)]  Loss: 4.392 (4.53)  Time: 0.173s, 5911.14/s  (0.192s, 5322.04/s)  LR: 9.830e-04  Data: 0.026 (0.034)
Train: 25 [ 550/1251 ( 44%)]  Loss: 4.665 (4.54)  Time: 0.151s, 6763.27/s  (0.192s, 5319.74/s)  LR: 9.830e-04  Data: 0.025 (0.034)
Train: 25 [ 600/1251 ( 48%)]  Loss: 4.900 (4.57)  Time: 0.171s, 5972.95/s  (0.193s, 5295.48/s)  LR: 9.830e-04  Data: 0.025 (0.034)
Train: 25 [ 650/1251 ( 52%)]  Loss: 4.551 (4.56)  Time: 0.179s, 5714.32/s  (0.193s, 5304.08/s)  LR: 9.830e-04  Data: 0.026 (0.033)
Train: 25 [ 700/1251 ( 56%)]  Loss: 4.929 (4.59)  Time: 0.163s, 6284.28/s  (0.193s, 5311.80/s)  LR: 9.830e-04  Data: 0.027 (0.033)
Train: 25 [ 750/1251 ( 60%)]  Loss: 4.647 (4.59)  Time: 0.169s, 6072.59/s  (0.193s, 5310.55/s)  LR: 9.830e-04  Data: 0.028 (0.033)
Train: 25 [ 800/1251 ( 64%)]  Loss: 4.606 (4.59)  Time: 0.156s, 6557.23/s  (0.193s, 5312.35/s)  LR: 9.830e-04  Data: 0.024 (0.032)
Train: 25 [ 850/1251 ( 68%)]  Loss: 4.589 (4.59)  Time: 0.176s, 5828.89/s  (0.193s, 5302.96/s)  LR: 9.830e-04  Data: 0.024 (0.032)
Train: 25 [ 900/1251 ( 72%)]  Loss: 4.558 (4.59)  Time: 0.357s, 2868.47/s  (0.193s, 5295.30/s)  LR: 9.830e-04  Data: 0.026 (0.032)
Train: 25 [ 950/1251 ( 76%)]  Loss: 4.954 (4.61)  Time: 0.167s, 6131.68/s  (0.194s, 5291.75/s)  LR: 9.830e-04  Data: 0.025 (0.031)
Train: 25 [1000/1251 ( 80%)]  Loss: 4.798 (4.62)  Time: 0.159s, 6443.42/s  (0.193s, 5295.71/s)  LR: 9.830e-04  Data: 0.028 (0.031)
Train: 25 [1050/1251 ( 84%)]  Loss: 4.453 (4.61)  Time: 0.180s, 5673.44/s  (0.193s, 5299.92/s)  LR: 9.830e-04  Data: 0.025 (0.031)
Train: 25 [1100/1251 ( 88%)]  Loss: 4.745 (4.62)  Time: 0.170s, 6036.60/s  (0.194s, 5289.72/s)  LR: 9.830e-04  Data: 0.029 (0.031)
Train: 25 [1150/1251 ( 92%)]  Loss: 4.457 (4.61)  Time: 0.156s, 6581.51/s  (0.193s, 5294.74/s)  LR: 9.830e-04  Data: 0.026 (0.031)
Train: 25 [1200/1251 ( 96%)]  Loss: 4.669 (4.61)  Time: 0.173s, 5919.37/s  (0.194s, 5291.82/s)  LR: 9.830e-04  Data: 0.028 (0.031)
Train: 25 [1250/1251 (100%)]  Loss: 4.694 (4.62)  Time: 0.114s, 9001.22/s  (0.193s, 5301.24/s)  LR: 9.830e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.823 (1.823)  Loss:  1.6186 (1.6186)  Acc@1: 71.3867 (71.3867)  Acc@5: 89.9414 (89.9414)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.5512 (2.2422)  Acc@1: 73.8208 (55.3380)  Acc@5: 88.4434 (79.3780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-16.pth.tar', 50.3340001171875)

Train: 26 [   0/1251 (  0%)]  Loss: 4.806 (4.81)  Time: 1.664s,  615.45/s  (1.664s,  615.45/s)  LR: 9.816e-04  Data: 1.537 (1.537)
Train: 26 [  50/1251 (  4%)]  Loss: 4.278 (4.54)  Time: 0.179s, 5729.08/s  (0.223s, 4593.50/s)  LR: 9.816e-04  Data: 0.026 (0.079)
Train: 26 [ 100/1251 (  8%)]  Loss: 4.789 (4.62)  Time: 0.187s, 5487.08/s  (0.207s, 4947.52/s)  LR: 9.816e-04  Data: 0.031 (0.064)
Train: 26 [ 150/1251 ( 12%)]  Loss: 4.905 (4.69)  Time: 0.151s, 6802.14/s  (0.201s, 5083.80/s)  LR: 9.816e-04  Data: 0.022 (0.058)
Train: 26 [ 200/1251 ( 16%)]  Loss: 4.393 (4.63)  Time: 0.163s, 6269.05/s  (0.199s, 5134.31/s)  LR: 9.816e-04  Data: 0.030 (0.055)
Train: 26 [ 250/1251 ( 20%)]  Loss: 4.858 (4.67)  Time: 0.166s, 6165.55/s  (0.195s, 5239.14/s)  LR: 9.816e-04  Data: 0.034 (0.051)
Train: 26 [ 300/1251 ( 24%)]  Loss: 4.962 (4.71)  Time: 0.161s, 6351.45/s  (0.195s, 5261.17/s)  LR: 9.816e-04  Data: 0.026 (0.049)
Train: 26 [ 350/1251 ( 28%)]  Loss: 4.767 (4.72)  Time: 0.169s, 6073.43/s  (0.194s, 5274.46/s)  LR: 9.816e-04  Data: 0.026 (0.047)
Train: 26 [ 400/1251 ( 32%)]  Loss: 5.047 (4.76)  Time: 0.534s, 1917.77/s  (0.195s, 5251.85/s)  LR: 9.816e-04  Data: 0.410 (0.046)
Train: 26 [ 450/1251 ( 36%)]  Loss: 4.741 (4.75)  Time: 0.164s, 6259.02/s  (0.195s, 5256.94/s)  LR: 9.816e-04  Data: 0.029 (0.046)
Train: 26 [ 500/1251 ( 40%)]  Loss: 4.845 (4.76)  Time: 0.182s, 5627.86/s  (0.194s, 5282.60/s)  LR: 9.816e-04  Data: 0.022 (0.045)
Train: 26 [ 550/1251 ( 44%)]  Loss: 4.848 (4.77)  Time: 0.172s, 5953.34/s  (0.195s, 5254.28/s)  LR: 9.816e-04  Data: 0.020 (0.046)
Train: 26 [ 600/1251 ( 48%)]  Loss: 4.679 (4.76)  Time: 0.392s, 2609.69/s  (0.195s, 5250.95/s)  LR: 9.816e-04  Data: 0.044 (0.045)
Train: 26 [ 650/1251 ( 52%)]  Loss: 4.738 (4.76)  Time: 0.188s, 5460.75/s  (0.195s, 5250.57/s)  LR: 9.816e-04  Data: 0.027 (0.044)
Train: 26 [ 700/1251 ( 56%)]  Loss: 4.525 (4.75)  Time: 0.152s, 6736.72/s  (0.194s, 5273.35/s)  LR: 9.816e-04  Data: 0.025 (0.043)
Train: 26 [ 750/1251 ( 60%)]  Loss: 4.511 (4.73)  Time: 0.192s, 5324.41/s  (0.193s, 5293.98/s)  LR: 9.816e-04  Data: 0.025 (0.042)
Train: 26 [ 800/1251 ( 64%)]  Loss: 4.790 (4.73)  Time: 0.241s, 4256.56/s  (0.194s, 5287.16/s)  LR: 9.816e-04  Data: 0.029 (0.041)
Train: 26 [ 850/1251 ( 68%)]  Loss: 4.697 (4.73)  Time: 0.168s, 6111.35/s  (0.194s, 5286.50/s)  LR: 9.816e-04  Data: 0.026 (0.041)
Train: 26 [ 900/1251 ( 72%)]  Loss: 4.353 (4.71)  Time: 0.164s, 6227.88/s  (0.194s, 5274.72/s)  LR: 9.816e-04  Data: 0.032 (0.041)
Train: 26 [ 950/1251 ( 76%)]  Loss: 4.642 (4.71)  Time: 0.175s, 5840.28/s  (0.194s, 5280.77/s)  LR: 9.816e-04  Data: 0.023 (0.040)
Train: 26 [1000/1251 ( 80%)]  Loss: 4.618 (4.70)  Time: 0.172s, 5952.71/s  (0.194s, 5272.50/s)  LR: 9.816e-04  Data: 0.027 (0.041)
Train: 26 [1050/1251 ( 84%)]  Loss: 4.388 (4.69)  Time: 0.183s, 5581.44/s  (0.194s, 5275.92/s)  LR: 9.816e-04  Data: 0.021 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Train: 26 [1100/1251 ( 88%)]  Loss: 4.097 (4.66)  Time: 0.180s, 5683.62/s  (0.194s, 5279.07/s)  LR: 9.816e-04  Data: 0.021 (0.040)
Train: 26 [1150/1251 ( 92%)]  Loss: 4.728 (4.67)  Time: 0.177s, 5789.40/s  (0.194s, 5274.24/s)  LR: 9.816e-04  Data: 0.027 (0.039)
Train: 26 [1200/1251 ( 96%)]  Loss: 4.781 (4.67)  Time: 0.172s, 5961.10/s  (0.194s, 5269.87/s)  LR: 9.816e-04  Data: 0.033 (0.039)
Train: 26 [1250/1251 (100%)]  Loss: 4.377 (4.66)  Time: 0.112s, 9126.17/s  (0.194s, 5276.35/s)  LR: 9.816e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.932 (1.932)  Loss:  1.4827 (1.4827)  Acc@1: 73.6328 (73.6328)  Acc@5: 90.6250 (90.6250)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.4961 (2.1920)  Acc@1: 73.4670 (56.2160)  Acc@5: 88.6792 (79.8680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-17.pth.tar', 51.34200006347656)

Train: 27 [   0/1251 (  0%)]  Loss: 4.779 (4.78)  Time: 1.996s,  513.02/s  (1.996s,  513.02/s)  LR: 9.802e-04  Data: 1.867 (1.867)
Train: 27 [  50/1251 (  4%)]  Loss: 4.610 (4.69)  Time: 0.193s, 5297.71/s  (0.225s, 4555.93/s)  LR: 9.802e-04  Data: 0.020 (0.079)
Train: 27 [ 100/1251 (  8%)]  Loss: 4.565 (4.65)  Time: 0.188s, 5459.16/s  (0.208s, 4922.90/s)  LR: 9.802e-04  Data: 0.036 (0.063)
Train: 27 [ 150/1251 ( 12%)]  Loss: 4.939 (4.72)  Time: 0.169s, 6066.66/s  (0.201s, 5092.40/s)  LR: 9.802e-04  Data: 0.027 (0.053)
Train: 27 [ 200/1251 ( 16%)]  Loss: 4.823 (4.74)  Time: 0.164s, 6231.90/s  (0.198s, 5163.86/s)  LR: 9.802e-04  Data: 0.034 (0.047)
Train: 27 [ 250/1251 ( 20%)]  Loss: 4.673 (4.73)  Time: 0.193s, 5303.42/s  (0.196s, 5227.43/s)  LR: 9.802e-04  Data: 0.027 (0.043)
Train: 27 [ 300/1251 ( 24%)]  Loss: 4.610 (4.71)  Time: 0.163s, 6276.12/s  (0.195s, 5250.38/s)  LR: 9.802e-04  Data: 0.025 (0.041)
Train: 27 [ 350/1251 ( 28%)]  Loss: 4.953 (4.74)  Time: 0.188s, 5453.40/s  (0.194s, 5280.68/s)  LR: 9.802e-04  Data: 0.027 (0.039)
Train: 27 [ 400/1251 ( 32%)]  Loss: 4.743 (4.74)  Time: 0.273s, 3755.52/s  (0.194s, 5283.58/s)  LR: 9.802e-04  Data: 0.029 (0.038)
Train: 27 [ 450/1251 ( 36%)]  Loss: 4.533 (4.72)  Time: 0.174s, 5892.54/s  (0.194s, 5283.82/s)  LR: 9.802e-04  Data: 0.022 (0.038)
Train: 27 [ 500/1251 ( 40%)]  Loss: 4.496 (4.70)  Time: 0.173s, 5911.98/s  (0.194s, 5286.85/s)  LR: 9.802e-04  Data: 0.026 (0.038)
Train: 27 [ 550/1251 ( 44%)]  Loss: 4.563 (4.69)  Time: 0.163s, 6269.95/s  (0.194s, 5288.70/s)  LR: 9.802e-04  Data: 0.030 (0.039)
Train: 27 [ 600/1251 ( 48%)]  Loss: 4.408 (4.67)  Time: 0.315s, 3246.63/s  (0.193s, 5293.44/s)  LR: 9.802e-04  Data: 0.187 (0.039)
Train: 27 [ 650/1251 ( 52%)]  Loss: 4.493 (4.66)  Time: 0.164s, 6244.66/s  (0.193s, 5314.12/s)  LR: 9.802e-04  Data: 0.027 (0.039)
Train: 27 [ 700/1251 ( 56%)]  Loss: 4.514 (4.65)  Time: 0.180s, 5683.22/s  (0.193s, 5319.19/s)  LR: 9.802e-04  Data: 0.023 (0.038)
Train: 27 [ 750/1251 ( 60%)]  Loss: 4.518 (4.64)  Time: 0.173s, 5930.00/s  (0.193s, 5311.48/s)  LR: 9.802e-04  Data: 0.042 (0.037)
Train: 27 [ 800/1251 ( 64%)]  Loss: 4.716 (4.64)  Time: 0.172s, 5946.14/s  (0.193s, 5305.29/s)  LR: 9.802e-04  Data: 0.031 (0.037)
Train: 27 [ 850/1251 ( 68%)]  Loss: 5.017 (4.66)  Time: 0.150s, 6848.82/s  (0.193s, 5308.46/s)  LR: 9.802e-04  Data: 0.023 (0.036)
Train: 27 [ 900/1251 ( 72%)]  Loss: 4.774 (4.67)  Time: 0.162s, 6301.88/s  (0.193s, 5310.49/s)  LR: 9.802e-04  Data: 0.023 (0.036)
Train: 27 [ 950/1251 ( 76%)]  Loss: 4.786 (4.68)  Time: 0.162s, 6326.53/s  (0.193s, 5310.84/s)  LR: 9.802e-04  Data: 0.021 (0.035)
Train: 27 [1000/1251 ( 80%)]  Loss: 4.861 (4.68)  Time: 0.181s, 5649.71/s  (0.193s, 5313.23/s)  LR: 9.802e-04  Data: 0.021 (0.035)
Train: 27 [1050/1251 ( 84%)]  Loss: 4.538 (4.68)  Time: 0.156s, 6544.14/s  (0.193s, 5306.46/s)  LR: 9.802e-04  Data: 0.028 (0.035)
Train: 27 [1100/1251 ( 88%)]  Loss: 4.852 (4.69)  Time: 0.168s, 6084.22/s  (0.193s, 5294.85/s)  LR: 9.802e-04  Data: 0.027 (0.035)
Train: 27 [1150/1251 ( 92%)]  Loss: 4.452 (4.68)  Time: 0.198s, 5159.05/s  (0.194s, 5289.96/s)  LR: 9.802e-04  Data: 0.025 (0.035)
Train: 27 [1200/1251 ( 96%)]  Loss: 4.785 (4.68)  Time: 0.184s, 5552.40/s  (0.194s, 5289.98/s)  LR: 9.802e-04  Data: 0.030 (0.034)
Train: 27 [1250/1251 (100%)]  Loss: 4.681 (4.68)  Time: 0.114s, 8967.33/s  (0.193s, 5297.96/s)  LR: 9.802e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 2.085 (2.085)  Loss:  1.4167 (1.4167)  Acc@1: 72.2656 (72.2656)  Acc@5: 90.3320 (90.3320)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.4034 (2.1559)  Acc@1: 72.6415 (55.7820)  Acc@5: 88.3255 (79.7800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-18.pth.tar', 51.467999990234375)

Train: 28 [   0/1251 (  0%)]  Loss: 4.501 (4.50)  Time: 1.934s,  529.61/s  (1.934s,  529.61/s)  LR: 9.787e-04  Data: 1.803 (1.803)
Train: 28 [  50/1251 (  4%)]  Loss: 4.453 (4.48)  Time: 0.161s, 6376.11/s  (0.220s, 4648.54/s)  LR: 9.787e-04  Data: 0.028 (0.076)
Train: 28 [ 100/1251 (  8%)]  Loss: 4.290 (4.41)  Time: 0.162s, 6324.77/s  (0.207s, 4940.97/s)  LR: 9.787e-04  Data: 0.028 (0.062)
Train: 28 [ 150/1251 ( 12%)]  Loss: 4.742 (4.50)  Time: 0.177s, 5769.93/s  (0.200s, 5108.37/s)  LR: 9.787e-04  Data: 0.027 (0.055)
Train: 28 [ 200/1251 ( 16%)]  Loss: 4.684 (4.53)  Time: 0.334s, 3066.98/s  (0.199s, 5148.17/s)  LR: 9.787e-04  Data: 0.025 (0.049)
Train: 28 [ 250/1251 ( 20%)]  Loss: 4.742 (4.57)  Time: 0.174s, 5891.19/s  (0.196s, 5232.27/s)  LR: 9.787e-04  Data: 0.020 (0.045)
Train: 28 [ 300/1251 ( 24%)]  Loss: 4.550 (4.57)  Time: 0.168s, 6112.00/s  (0.194s, 5276.68/s)  LR: 9.787e-04  Data: 0.023 (0.042)
Train: 28 [ 350/1251 ( 28%)]  Loss: 4.593 (4.57)  Time: 0.175s, 5849.61/s  (0.194s, 5288.50/s)  LR: 9.787e-04  Data: 0.024 (0.040)
Train: 28 [ 400/1251 ( 32%)]  Loss: 4.932 (4.61)  Time: 0.174s, 5891.99/s  (0.194s, 5275.88/s)  LR: 9.787e-04  Data: 0.026 (0.039)
Train: 28 [ 450/1251 ( 36%)]  Loss: 4.713 (4.62)  Time: 0.160s, 6417.40/s  (0.193s, 5299.35/s)  LR: 9.787e-04  Data: 0.028 (0.037)
Train: 28 [ 500/1251 ( 40%)]  Loss: 4.829 (4.64)  Time: 0.168s, 6087.09/s  (0.193s, 5310.01/s)  LR: 9.787e-04  Data: 0.028 (0.037)
Train: 28 [ 550/1251 ( 44%)]  Loss: 4.670 (4.64)  Time: 0.167s, 6140.79/s  (0.193s, 5315.08/s)  LR: 9.787e-04  Data: 0.024 (0.036)
Train: 28 [ 600/1251 ( 48%)]  Loss: 4.555 (4.63)  Time: 0.505s, 2029.28/s  (0.193s, 5298.61/s)  LR: 9.787e-04  Data: 0.020 (0.035)
Train: 28 [ 650/1251 ( 52%)]  Loss: 4.852 (4.65)  Time: 0.164s, 6230.29/s  (0.193s, 5299.71/s)  LR: 9.787e-04  Data: 0.028 (0.034)
Train: 28 [ 700/1251 ( 56%)]  Loss: 4.912 (4.67)  Time: 0.164s, 6258.85/s  (0.193s, 5312.20/s)  LR: 9.787e-04  Data: 0.031 (0.034)
Train: 28 [ 750/1251 ( 60%)]  Loss: 4.236 (4.64)  Time: 0.191s, 5353.02/s  (0.193s, 5302.17/s)  LR: 9.787e-04  Data: 0.029 (0.034)
Train: 28 [ 800/1251 ( 64%)]  Loss: 4.823 (4.65)  Time: 0.328s, 3126.05/s  (0.193s, 5298.20/s)  LR: 9.787e-04  Data: 0.020 (0.033)
Train: 28 [ 850/1251 ( 68%)]  Loss: 4.837 (4.66)  Time: 0.182s, 5616.72/s  (0.193s, 5299.85/s)  LR: 9.787e-04  Data: 0.027 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 28 [ 900/1251 ( 72%)]  Loss: 4.750 (4.67)  Time: 0.161s, 6370.73/s  (0.193s, 5312.15/s)  LR: 9.787e-04  Data: 0.024 (0.033)
Train: 28 [ 950/1251 ( 76%)]  Loss: 4.816 (4.67)  Time: 0.166s, 6161.99/s  (0.193s, 5310.78/s)  LR: 9.787e-04  Data: 0.024 (0.032)
Train: 28 [1000/1251 ( 80%)]  Loss: 4.343 (4.66)  Time: 0.187s, 5464.73/s  (0.193s, 5305.04/s)  LR: 9.787e-04  Data: 0.026 (0.033)
Train: 28 [1050/1251 ( 84%)]  Loss: 4.393 (4.65)  Time: 0.178s, 5741.52/s  (0.193s, 5299.04/s)  LR: 9.787e-04  Data: 0.025 (0.034)
Train: 28 [1100/1251 ( 88%)]  Loss: 4.558 (4.64)  Time: 0.467s, 2192.95/s  (0.193s, 5295.39/s)  LR: 9.787e-04  Data: 0.341 (0.034)
Train: 28 [1150/1251 ( 92%)]  Loss: 4.823 (4.65)  Time: 0.156s, 6559.62/s  (0.193s, 5293.77/s)  LR: 9.787e-04  Data: 0.026 (0.034)
Train: 28 [1200/1251 ( 96%)]  Loss: 4.258 (4.63)  Time: 0.201s, 5103.83/s  (0.193s, 5294.86/s)  LR: 9.787e-04  Data: 0.021 (0.035)
Train: 28 [1250/1251 (100%)]  Loss: 4.839 (4.64)  Time: 0.114s, 9016.87/s  (0.193s, 5303.67/s)  LR: 9.787e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.802 (1.802)  Loss:  1.2850 (1.2850)  Acc@1: 75.5859 (75.5859)  Acc@5: 91.6992 (91.6992)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.4462 (2.1537)  Acc@1: 73.2311 (56.1920)  Acc@5: 87.9717 (80.1040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-19.pth.tar', 52.17599998779297)

Train: 29 [   0/1251 (  0%)]  Loss: 4.754 (4.75)  Time: 1.672s,  612.55/s  (1.672s,  612.55/s)  LR: 9.771e-04  Data: 1.549 (1.549)
Train: 29 [  50/1251 (  4%)]  Loss: 4.422 (4.59)  Time: 0.168s, 6095.74/s  (0.228s, 4499.52/s)  LR: 9.771e-04  Data: 0.028 (0.066)
Train: 29 [ 100/1251 (  8%)]  Loss: 4.953 (4.71)  Time: 0.196s, 5223.90/s  (0.209s, 4907.49/s)  LR: 9.771e-04  Data: 0.027 (0.048)
Train: 29 [ 150/1251 ( 12%)]  Loss: 4.617 (4.69)  Time: 0.171s, 5980.27/s  (0.202s, 5074.49/s)  LR: 9.771e-04  Data: 0.026 (0.041)
Train: 29 [ 200/1251 ( 16%)]  Loss: 4.688 (4.69)  Time: 0.155s, 6591.31/s  (0.199s, 5157.87/s)  LR: 9.771e-04  Data: 0.023 (0.038)
Train: 29 [ 250/1251 ( 20%)]  Loss: 4.827 (4.71)  Time: 0.177s, 5799.36/s  (0.196s, 5225.04/s)  LR: 9.771e-04  Data: 0.025 (0.036)
Train: 29 [ 300/1251 ( 24%)]  Loss: 4.431 (4.67)  Time: 0.166s, 6173.01/s  (0.195s, 5262.90/s)  LR: 9.771e-04  Data: 0.025 (0.034)
Train: 29 [ 350/1251 ( 28%)]  Loss: 4.486 (4.65)  Time: 0.170s, 6033.44/s  (0.195s, 5254.47/s)  LR: 9.771e-04  Data: 0.026 (0.034)
Train: 29 [ 400/1251 ( 32%)]  Loss: 4.356 (4.61)  Time: 0.169s, 6061.95/s  (0.195s, 5263.35/s)  LR: 9.771e-04  Data: 0.025 (0.033)
Train: 29 [ 450/1251 ( 36%)]  Loss: 4.778 (4.63)  Time: 0.173s, 5921.04/s  (0.194s, 5291.89/s)  LR: 9.771e-04  Data: 0.025 (0.032)
Train: 29 [ 500/1251 ( 40%)]  Loss: 4.800 (4.65)  Time: 0.163s, 6272.74/s  (0.193s, 5301.17/s)  LR: 9.771e-04  Data: 0.025 (0.032)
Train: 29 [ 550/1251 ( 44%)]  Loss: 4.999 (4.68)  Time: 0.155s, 6595.43/s  (0.193s, 5297.03/s)  LR: 9.771e-04  Data: 0.020 (0.032)
Train: 29 [ 600/1251 ( 48%)]  Loss: 4.407 (4.66)  Time: 0.216s, 4741.42/s  (0.193s, 5309.70/s)  LR: 9.771e-04  Data: 0.080 (0.032)
Train: 29 [ 650/1251 ( 52%)]  Loss: 4.870 (4.67)  Time: 0.155s, 6589.93/s  (0.193s, 5310.54/s)  LR: 9.771e-04  Data: 0.024 (0.033)
Train: 29 [ 700/1251 ( 56%)]  Loss: 4.621 (4.67)  Time: 0.188s, 5444.19/s  (0.193s, 5308.88/s)  LR: 9.771e-04  Data: 0.030 (0.034)
Train: 29 [ 750/1251 ( 60%)]  Loss: 4.792 (4.68)  Time: 0.167s, 6129.04/s  (0.193s, 5317.22/s)  LR: 9.771e-04  Data: 0.028 (0.035)
Train: 29 [ 800/1251 ( 64%)]  Loss: 4.697 (4.68)  Time: 0.258s, 3975.94/s  (0.193s, 5316.65/s)  LR: 9.771e-04  Data: 0.138 (0.035)
Train: 29 [ 850/1251 ( 68%)]  Loss: 4.715 (4.68)  Time: 0.171s, 5986.30/s  (0.193s, 5303.90/s)  LR: 9.771e-04  Data: 0.021 (0.036)
Train: 29 [ 900/1251 ( 72%)]  Loss: 4.544 (4.67)  Time: 0.177s, 5801.59/s  (0.193s, 5309.58/s)  LR: 9.771e-04  Data: 0.029 (0.036)
Train: 29 [ 950/1251 ( 76%)]  Loss: 4.705 (4.67)  Time: 0.173s, 5923.77/s  (0.194s, 5284.30/s)  LR: 9.771e-04  Data: 0.024 (0.037)
Train: 29 [1000/1251 ( 80%)]  Loss: 4.462 (4.66)  Time: 0.168s, 6090.67/s  (0.193s, 5294.97/s)  LR: 9.771e-04  Data: 0.029 (0.037)
Train: 29 [1050/1251 ( 84%)]  Loss: 4.593 (4.66)  Time: 0.176s, 5803.55/s  (0.194s, 5283.58/s)  LR: 9.771e-04  Data: 0.018 (0.037)
Train: 29 [1100/1251 ( 88%)]  Loss: 4.579 (4.66)  Time: 0.567s, 1806.05/s  (0.194s, 5275.86/s)  LR: 9.771e-04  Data: 0.020 (0.036)
Train: 29 [1150/1251 ( 92%)]  Loss: 4.635 (4.66)  Time: 0.204s, 5019.69/s  (0.194s, 5282.24/s)  LR: 9.771e-04  Data: 0.027 (0.036)
Train: 29 [1200/1251 ( 96%)]  Loss: 4.900 (4.67)  Time: 0.173s, 5935.29/s  (0.194s, 5277.33/s)  LR: 9.771e-04  Data: 0.031 (0.036)
Train: 29 [1250/1251 (100%)]  Loss: 4.525 (4.66)  Time: 0.113s, 9044.70/s  (0.194s, 5291.20/s)  LR: 9.771e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.829 (1.829)  Loss:  1.5946 (1.5946)  Acc@1: 71.5820 (71.5820)  Acc@5: 89.2578 (89.2578)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.3838 (2.1340)  Acc@1: 74.7642 (56.0420)  Acc@5: 90.4481 (80.0780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-20.pth.tar', 53.49800013671875)

Train: 30 [   0/1251 (  0%)]  Loss: 4.699 (4.70)  Time: 1.817s,  563.43/s  (1.817s,  563.43/s)  LR: 9.756e-04  Data: 1.695 (1.695)
Train: 30 [  50/1251 (  4%)]  Loss: 4.556 (4.63)  Time: 0.166s, 6182.67/s  (0.221s, 4631.78/s)  LR: 9.756e-04  Data: 0.027 (0.077)
Train: 30 [ 100/1251 (  8%)]  Loss: 4.711 (4.66)  Time: 0.153s, 6684.77/s  (0.208s, 4914.74/s)  LR: 9.756e-04  Data: 0.022 (0.053)
Train: 30 [ 150/1251 ( 12%)]  Loss: 4.695 (4.67)  Time: 0.173s, 5931.99/s  (0.203s, 5042.58/s)  LR: 9.756e-04  Data: 0.033 (0.045)
Train: 30 [ 200/1251 ( 16%)]  Loss: 4.710 (4.67)  Time: 0.181s, 5654.81/s  (0.200s, 5130.10/s)  LR: 9.756e-04  Data: 0.031 (0.041)
Train: 30 [ 250/1251 ( 20%)]  Loss: 4.074 (4.57)  Time: 0.184s, 5570.67/s  (0.197s, 5191.47/s)  LR: 9.756e-04  Data: 0.031 (0.038)
Train: 30 [ 300/1251 ( 24%)]  Loss: 4.880 (4.62)  Time: 0.164s, 6252.06/s  (0.197s, 5195.76/s)  LR: 9.756e-04  Data: 0.021 (0.036)
Train: 30 [ 350/1251 ( 28%)]  Loss: 4.506 (4.60)  Time: 0.167s, 6116.91/s  (0.196s, 5223.46/s)  LR: 9.756e-04  Data: 0.024 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 30 [ 400/1251 ( 32%)]  Loss: 4.480 (4.59)  Time: 0.187s, 5486.06/s  (0.195s, 5247.35/s)  LR: 9.756e-04  Data: 0.029 (0.034)
Train: 30 [ 450/1251 ( 36%)]  Loss: 4.789 (4.61)  Time: 0.522s, 1960.74/s  (0.195s, 5250.68/s)  LR: 9.756e-04  Data: 0.025 (0.033)
Train: 30 [ 500/1251 ( 40%)]  Loss: 4.533 (4.60)  Time: 0.172s, 5962.48/s  (0.195s, 5255.42/s)  LR: 9.756e-04  Data: 0.020 (0.033)
Train: 30 [ 550/1251 ( 44%)]  Loss: 4.630 (4.61)  Time: 0.176s, 5823.31/s  (0.194s, 5267.70/s)  LR: 9.756e-04  Data: 0.027 (0.033)
Train: 30 [ 600/1251 ( 48%)]  Loss: 4.366 (4.59)  Time: 0.184s, 5574.46/s  (0.194s, 5280.41/s)  LR: 9.756e-04  Data: 0.028 (0.032)
Train: 30 [ 650/1251 ( 52%)]  Loss: 5.110 (4.62)  Time: 0.207s, 4938.46/s  (0.194s, 5281.54/s)  LR: 9.756e-04  Data: 0.032 (0.032)
Train: 30 [ 700/1251 ( 56%)]  Loss: 4.623 (4.62)  Time: 0.172s, 5957.46/s  (0.194s, 5285.15/s)  LR: 9.756e-04  Data: 0.028 (0.032)
Train: 30 [ 750/1251 ( 60%)]  Loss: 4.495 (4.62)  Time: 0.187s, 5472.44/s  (0.194s, 5277.53/s)  LR: 9.756e-04  Data: 0.026 (0.031)
Train: 30 [ 800/1251 ( 64%)]  Loss: 4.205 (4.59)  Time: 0.165s, 6203.97/s  (0.194s, 5269.73/s)  LR: 9.756e-04  Data: 0.022 (0.031)
Train: 30 [ 850/1251 ( 68%)]  Loss: 4.366 (4.58)  Time: 0.185s, 5531.57/s  (0.194s, 5275.41/s)  LR: 9.756e-04  Data: 0.024 (0.031)
Train: 30 [ 900/1251 ( 72%)]  Loss: 5.088 (4.61)  Time: 0.178s, 5741.26/s  (0.194s, 5274.32/s)  LR: 9.756e-04  Data: 0.026 (0.031)
Train: 30 [ 950/1251 ( 76%)]  Loss: 4.757 (4.61)  Time: 0.184s, 5576.97/s  (0.194s, 5269.90/s)  LR: 9.756e-04  Data: 0.028 (0.031)
Train: 30 [1000/1251 ( 80%)]  Loss: 4.690 (4.62)  Time: 0.169s, 6064.60/s  (0.194s, 5271.32/s)  LR: 9.756e-04  Data: 0.029 (0.031)
Train: 30 [1050/1251 ( 84%)]  Loss: 4.474 (4.61)  Time: 0.183s, 5605.70/s  (0.194s, 5276.81/s)  LR: 9.756e-04  Data: 0.028 (0.030)
Train: 30 [1100/1251 ( 88%)]  Loss: 4.119 (4.59)  Time: 0.185s, 5528.33/s  (0.194s, 5270.46/s)  LR: 9.756e-04  Data: 0.034 (0.030)
Train: 30 [1150/1251 ( 92%)]  Loss: 4.223 (4.57)  Time: 0.187s, 5480.95/s  (0.195s, 5249.05/s)  LR: 9.756e-04  Data: 0.019 (0.031)
Train: 30 [1200/1251 ( 96%)]  Loss: 4.549 (4.57)  Time: 0.166s, 6163.25/s  (0.195s, 5255.27/s)  LR: 9.756e-04  Data: 0.020 (0.031)
Train: 30 [1250/1251 (100%)]  Loss: 4.578 (4.57)  Time: 0.113s, 9038.25/s  (0.195s, 5260.36/s)  LR: 9.756e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.751 (1.751)  Loss:  1.5023 (1.5023)  Acc@1: 73.6328 (73.6328)  Acc@5: 90.4297 (90.4297)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.4832 (2.1485)  Acc@1: 73.5849 (56.7560)  Acc@5: 89.9764 (80.7280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-21.pth.tar', 53.54999998535156)

Train: 31 [   0/1251 (  0%)]  Loss: 4.695 (4.69)  Time: 1.889s,  541.96/s  (1.889s,  541.96/s)  LR: 9.739e-04  Data: 1.769 (1.769)
Train: 31 [  50/1251 (  4%)]  Loss: 4.899 (4.80)  Time: 0.175s, 5866.28/s  (0.227s, 4503.56/s)  LR: 9.739e-04  Data: 0.031 (0.080)
Train: 31 [ 100/1251 (  8%)]  Loss: 4.969 (4.85)  Time: 0.174s, 5885.61/s  (0.210s, 4881.69/s)  LR: 9.739e-04  Data: 0.032 (0.055)
Train: 31 [ 150/1251 ( 12%)]  Loss: 4.365 (4.73)  Time: 0.179s, 5710.66/s  (0.206s, 4963.29/s)  LR: 9.739e-04  Data: 0.031 (0.046)
Train: 31 [ 200/1251 ( 16%)]  Loss: 4.635 (4.71)  Time: 0.178s, 5743.97/s  (0.201s, 5092.78/s)  LR: 9.739e-04  Data: 0.028 (0.041)
Train: 31 [ 250/1251 ( 20%)]  Loss: 4.734 (4.72)  Time: 0.176s, 5826.29/s  (0.198s, 5163.44/s)  LR: 9.739e-04  Data: 0.022 (0.038)
Train: 31 [ 300/1251 ( 24%)]  Loss: 4.833 (4.73)  Time: 0.377s, 2714.62/s  (0.197s, 5210.38/s)  LR: 9.739e-04  Data: 0.241 (0.038)
Train: 31 [ 350/1251 ( 28%)]  Loss: 4.474 (4.70)  Time: 0.170s, 6009.03/s  (0.196s, 5230.03/s)  LR: 9.739e-04  Data: 0.027 (0.038)
Train: 31 [ 400/1251 ( 32%)]  Loss: 4.752 (4.71)  Time: 0.171s, 5989.56/s  (0.196s, 5237.03/s)  LR: 9.739e-04  Data: 0.024 (0.040)
Train: 31 [ 450/1251 ( 36%)]  Loss: 4.449 (4.68)  Time: 0.188s, 5447.61/s  (0.195s, 5246.55/s)  LR: 9.739e-04  Data: 0.025 (0.039)
Train: 31 [ 500/1251 ( 40%)]  Loss: 4.775 (4.69)  Time: 0.174s, 5879.27/s  (0.194s, 5264.83/s)  LR: 9.739e-04  Data: 0.040 (0.038)
Train: 31 [ 550/1251 ( 44%)]  Loss: 4.883 (4.71)  Time: 0.175s, 5851.29/s  (0.194s, 5281.81/s)  LR: 9.739e-04  Data: 0.025 (0.038)
Train: 31 [ 600/1251 ( 48%)]  Loss: 4.312 (4.68)  Time: 0.173s, 5914.63/s  (0.194s, 5275.89/s)  LR: 9.739e-04  Data: 0.035 (0.037)
Train: 31 [ 650/1251 ( 52%)]  Loss: 4.706 (4.68)  Time: 0.172s, 5949.38/s  (0.194s, 5287.88/s)  LR: 9.739e-04  Data: 0.029 (0.036)
Train: 31 [ 700/1251 ( 56%)]  Loss: 4.611 (4.67)  Time: 0.170s, 6033.08/s  (0.193s, 5294.20/s)  LR: 9.739e-04  Data: 0.035 (0.036)
Train: 31 [ 750/1251 ( 60%)]  Loss: 4.635 (4.67)  Time: 0.180s, 5690.39/s  (0.194s, 5291.31/s)  LR: 9.739e-04  Data: 0.025 (0.035)
Train: 31 [ 800/1251 ( 64%)]  Loss: 4.930 (4.69)  Time: 0.187s, 5489.52/s  (0.194s, 5280.93/s)  LR: 9.739e-04  Data: 0.028 (0.035)
Train: 31 [ 850/1251 ( 68%)]  Loss: 4.699 (4.69)  Time: 0.162s, 6338.85/s  (0.193s, 5292.37/s)  LR: 9.739e-04  Data: 0.024 (0.034)
Train: 31 [ 900/1251 ( 72%)]  Loss: 4.821 (4.69)  Time: 0.157s, 6519.50/s  (0.194s, 5286.26/s)  LR: 9.739e-04  Data: 0.036 (0.034)
Train: 31 [ 950/1251 ( 76%)]  Loss: 4.393 (4.68)  Time: 0.167s, 6126.96/s  (0.194s, 5291.73/s)  LR: 9.739e-04  Data: 0.027 (0.033)
Train: 31 [1000/1251 ( 80%)]  Loss: 4.278 (4.66)  Time: 0.167s, 6141.05/s  (0.193s, 5294.43/s)  LR: 9.739e-04  Data: 0.022 (0.033)
Train: 31 [1050/1251 ( 84%)]  Loss: 4.666 (4.66)  Time: 0.166s, 6167.85/s  (0.193s, 5293.44/s)  LR: 9.739e-04  Data: 0.040 (0.033)
Train: 31 [1100/1251 ( 88%)]  Loss: 4.856 (4.67)  Time: 0.183s, 5587.32/s  (0.194s, 5289.12/s)  LR: 9.739e-04  Data: 0.024 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 31 [1150/1251 ( 92%)]  Loss: 4.558 (4.66)  Time: 0.187s, 5483.80/s  (0.194s, 5285.47/s)  LR: 9.739e-04  Data: 0.030 (0.032)
Train: 31 [1200/1251 ( 96%)]  Loss: 4.978 (4.68)  Time: 0.168s, 6083.85/s  (0.194s, 5283.81/s)  LR: 9.739e-04  Data: 0.035 (0.032)
Train: 31 [1250/1251 (100%)]  Loss: 4.340 (4.66)  Time: 0.114s, 8998.14/s  (0.193s, 5295.99/s)  LR: 9.739e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.941 (1.941)  Loss:  1.4403 (1.4403)  Acc@1: 72.0703 (72.0703)  Acc@5: 90.7227 (90.7227)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.4116 (2.0946)  Acc@1: 73.3491 (56.9260)  Acc@5: 87.6179 (80.7080)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-22.pth.tar', 54.60000009033203)

Train: 32 [   0/1251 (  0%)]  Loss: 5.208 (5.21)  Time: 1.861s,  550.37/s  (1.861s,  550.37/s)  LR: 9.722e-04  Data: 1.731 (1.731)
Train: 32 [  50/1251 (  4%)]  Loss: 4.865 (5.04)  Time: 0.163s, 6294.35/s  (0.225s, 4542.04/s)  LR: 9.722e-04  Data: 0.025 (0.075)
Train: 32 [ 100/1251 (  8%)]  Loss: 4.724 (4.93)  Time: 0.171s, 6003.07/s  (0.205s, 4985.10/s)  LR: 9.722e-04  Data: 0.020 (0.056)
Train: 32 [ 150/1251 ( 12%)]  Loss: 4.527 (4.83)  Time: 0.172s, 5959.27/s  (0.199s, 5142.22/s)  LR: 9.722e-04  Data: 0.024 (0.051)
Train: 32 [ 200/1251 ( 16%)]  Loss: 4.301 (4.73)  Time: 0.177s, 5773.77/s  (0.197s, 5187.67/s)  LR: 9.722e-04  Data: 0.034 (0.049)
Train: 32 [ 250/1251 ( 20%)]  Loss: 4.709 (4.72)  Time: 0.161s, 6341.18/s  (0.195s, 5240.90/s)  LR: 9.722e-04  Data: 0.029 (0.048)
Train: 32 [ 300/1251 ( 24%)]  Loss: 5.125 (4.78)  Time: 0.151s, 6772.48/s  (0.195s, 5261.02/s)  LR: 9.722e-04  Data: 0.027 (0.047)
Train: 32 [ 350/1251 ( 28%)]  Loss: 4.601 (4.76)  Time: 0.177s, 5770.22/s  (0.193s, 5292.31/s)  LR: 9.722e-04  Data: 0.043 (0.047)
Train: 32 [ 400/1251 ( 32%)]  Loss: 4.622 (4.74)  Time: 0.212s, 4825.14/s  (0.194s, 5291.65/s)  LR: 9.722e-04  Data: 0.021 (0.045)
Train: 32 [ 450/1251 ( 36%)]  Loss: 4.724 (4.74)  Time: 0.155s, 6621.33/s  (0.193s, 5307.75/s)  LR: 9.722e-04  Data: 0.024 (0.043)
Train: 32 [ 500/1251 ( 40%)]  Loss: 4.769 (4.74)  Time: 0.177s, 5770.63/s  (0.192s, 5321.80/s)  LR: 9.722e-04  Data: 0.033 (0.042)
Train: 32 [ 550/1251 ( 44%)]  Loss: 4.751 (4.74)  Time: 0.149s, 6880.85/s  (0.192s, 5342.92/s)  LR: 9.722e-04  Data: 0.023 (0.041)
Train: 32 [ 600/1251 ( 48%)]  Loss: 4.590 (4.73)  Time: 0.160s, 6391.00/s  (0.192s, 5321.38/s)  LR: 9.722e-04  Data: 0.026 (0.041)
Train: 32 [ 650/1251 ( 52%)]  Loss: 4.436 (4.71)  Time: 0.169s, 6062.10/s  (0.192s, 5322.03/s)  LR: 9.722e-04  Data: 0.030 (0.041)
Train: 32 [ 700/1251 ( 56%)]  Loss: 4.518 (4.70)  Time: 0.180s, 5701.52/s  (0.192s, 5332.01/s)  LR: 9.722e-04  Data: 0.024 (0.040)
Train: 32 [ 750/1251 ( 60%)]  Loss: 4.551 (4.69)  Time: 0.181s, 5651.42/s  (0.192s, 5336.29/s)  LR: 9.722e-04  Data: 0.028 (0.039)
Train: 32 [ 800/1251 ( 64%)]  Loss: 4.651 (4.69)  Time: 0.177s, 5770.66/s  (0.192s, 5333.27/s)  LR: 9.722e-04  Data: 0.028 (0.038)
Train: 32 [ 850/1251 ( 68%)]  Loss: 4.738 (4.69)  Time: 0.188s, 5454.43/s  (0.192s, 5324.24/s)  LR: 9.722e-04  Data: 0.025 (0.038)
Train: 32 [ 900/1251 ( 72%)]  Loss: 4.569 (4.68)  Time: 0.191s, 5348.29/s  (0.193s, 5314.08/s)  LR: 9.722e-04  Data: 0.020 (0.037)
Train: 32 [ 950/1251 ( 76%)]  Loss: 4.473 (4.67)  Time: 0.173s, 5907.61/s  (0.193s, 5304.82/s)  LR: 9.722e-04  Data: 0.030 (0.037)
Train: 32 [1000/1251 ( 80%)]  Loss: 4.980 (4.69)  Time: 0.169s, 6055.07/s  (0.193s, 5299.13/s)  LR: 9.722e-04  Data: 0.022 (0.036)
Train: 32 [1050/1251 ( 84%)]  Loss: 4.870 (4.70)  Time: 0.187s, 5487.00/s  (0.193s, 5297.06/s)  LR: 9.722e-04  Data: 0.030 (0.036)
Train: 32 [1100/1251 ( 88%)]  Loss: 4.510 (4.69)  Time: 0.177s, 5793.85/s  (0.193s, 5302.70/s)  LR: 9.722e-04  Data: 0.031 (0.036)
Train: 32 [1150/1251 ( 92%)]  Loss: 4.887 (4.70)  Time: 0.167s, 6146.76/s  (0.193s, 5299.80/s)  LR: 9.722e-04  Data: 0.042 (0.035)
Train: 32 [1200/1251 ( 96%)]  Loss: 4.495 (4.69)  Time: 0.189s, 5423.03/s  (0.194s, 5286.25/s)  LR: 9.722e-04  Data: 0.021 (0.036)
Train: 32 [1250/1251 (100%)]  Loss: 4.698 (4.69)  Time: 0.114s, 8983.50/s  (0.193s, 5301.79/s)  LR: 9.722e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.884 (1.884)  Loss:  1.3608 (1.3608)  Acc@1: 76.2695 (76.2695)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3497 (2.0897)  Acc@1: 75.9434 (57.3940)  Acc@5: 90.5660 (80.9640)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-23.pth.tar', 54.615999985351564)

Train: 33 [   0/1251 (  0%)]  Loss: 4.738 (4.74)  Time: 1.651s,  620.11/s  (1.651s,  620.11/s)  LR: 9.705e-04  Data: 1.522 (1.522)
Train: 33 [  50/1251 (  4%)]  Loss: 4.797 (4.77)  Time: 0.177s, 5778.64/s  (0.226s, 4534.17/s)  LR: 9.705e-04  Data: 0.026 (0.059)
Train: 33 [ 100/1251 (  8%)]  Loss: 4.610 (4.72)  Time: 0.186s, 5516.14/s  (0.208s, 4927.27/s)  LR: 9.705e-04  Data: 0.036 (0.044)
Train: 33 [ 150/1251 ( 12%)]  Loss: 4.613 (4.69)  Time: 0.188s, 5450.68/s  (0.206s, 4982.54/s)  LR: 9.705e-04  Data: 0.020 (0.039)
Train: 33 [ 200/1251 ( 16%)]  Loss: 4.899 (4.73)  Time: 0.329s, 3110.03/s  (0.200s, 5109.09/s)  LR: 9.705e-04  Data: 0.024 (0.037)
Train: 33 [ 250/1251 ( 20%)]  Loss: 4.588 (4.71)  Time: 0.172s, 5959.74/s  (0.198s, 5172.55/s)  LR: 9.705e-04  Data: 0.033 (0.035)
Train: 33 [ 300/1251 ( 24%)]  Loss: 4.600 (4.69)  Time: 0.180s, 5698.11/s  (0.196s, 5227.77/s)  LR: 9.705e-04  Data: 0.028 (0.034)
Train: 33 [ 350/1251 ( 28%)]  Loss: 4.642 (4.69)  Time: 0.183s, 5581.38/s  (0.195s, 5243.53/s)  LR: 9.705e-04  Data: 0.032 (0.033)
Train: 33 [ 400/1251 ( 32%)]  Loss: 4.172 (4.63)  Time: 0.185s, 5546.31/s  (0.194s, 5265.33/s)  LR: 9.705e-04  Data: 0.028 (0.032)
Train: 33 [ 450/1251 ( 36%)]  Loss: 4.838 (4.65)  Time: 0.157s, 6504.17/s  (0.194s, 5290.14/s)  LR: 9.705e-04  Data: 0.029 (0.032)
Train: 33 [ 500/1251 ( 40%)]  Loss: 4.714 (4.66)  Time: 0.190s, 5382.30/s  (0.194s, 5285.02/s)  LR: 9.705e-04  Data: 0.034 (0.033)
Train: 33 [ 550/1251 ( 44%)]  Loss: 4.658 (4.66)  Time: 0.177s, 5795.50/s  (0.194s, 5291.56/s)  LR: 9.705e-04  Data: 0.022 (0.034)
Train: 33 [ 600/1251 ( 48%)]  Loss: 4.832 (4.67)  Time: 0.203s, 5036.51/s  (0.193s, 5300.82/s)  LR: 9.705e-04  Data: 0.020 (0.034)
Train: 33 [ 650/1251 ( 52%)]  Loss: 4.489 (4.66)  Time: 0.161s, 6358.09/s  (0.193s, 5298.93/s)  LR: 9.705e-04  Data: 0.022 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 33 [ 700/1251 ( 56%)]  Loss: 4.516 (4.65)  Time: 0.313s, 3272.91/s  (0.193s, 5299.42/s)  LR: 9.705e-04  Data: 0.039 (0.033)
Train: 33 [ 750/1251 ( 60%)]  Loss: 4.871 (4.66)  Time: 0.182s, 5636.62/s  (0.193s, 5296.27/s)  LR: 9.705e-04  Data: 0.021 (0.033)
Train: 33 [ 800/1251 ( 64%)]  Loss: 4.729 (4.67)  Time: 0.181s, 5668.83/s  (0.193s, 5297.15/s)  LR: 9.705e-04  Data: 0.024 (0.033)
Train: 33 [ 850/1251 ( 68%)]  Loss: 4.377 (4.65)  Time: 0.174s, 5896.17/s  (0.193s, 5293.29/s)  LR: 9.705e-04  Data: 0.028 (0.032)
Train: 33 [ 900/1251 ( 72%)]  Loss: 4.633 (4.65)  Time: 0.175s, 5849.54/s  (0.193s, 5295.31/s)  LR: 9.705e-04  Data: 0.034 (0.032)
Train: 33 [ 950/1251 ( 76%)]  Loss: 4.398 (4.64)  Time: 0.173s, 5935.52/s  (0.193s, 5293.00/s)  LR: 9.705e-04  Data: 0.033 (0.032)
Train: 33 [1000/1251 ( 80%)]  Loss: 4.914 (4.65)  Time: 0.183s, 5607.14/s  (0.193s, 5294.63/s)  LR: 9.705e-04  Data: 0.025 (0.032)
Train: 33 [1050/1251 ( 84%)]  Loss: 4.623 (4.65)  Time: 0.323s, 3171.16/s  (0.194s, 5289.52/s)  LR: 9.705e-04  Data: 0.036 (0.032)
Train: 33 [1100/1251 ( 88%)]  Loss: 4.089 (4.62)  Time: 0.177s, 5793.64/s  (0.194s, 5284.19/s)  LR: 9.705e-04  Data: 0.030 (0.031)
Train: 33 [1150/1251 ( 92%)]  Loss: 4.130 (4.60)  Time: 0.183s, 5590.45/s  (0.194s, 5278.55/s)  LR: 9.705e-04  Data: 0.025 (0.031)
Train: 33 [1200/1251 ( 96%)]  Loss: 4.578 (4.60)  Time: 0.188s, 5461.26/s  (0.194s, 5268.32/s)  LR: 9.705e-04  Data: 0.030 (0.031)
Train: 33 [1250/1251 (100%)]  Loss: 4.391 (4.59)  Time: 0.114s, 9008.86/s  (0.194s, 5283.81/s)  LR: 9.705e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.721 (1.721)  Loss:  1.4203 (1.4203)  Acc@1: 74.5117 (74.5117)  Acc@5: 90.8203 (90.8203)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3158 (2.0935)  Acc@1: 74.6462 (57.0140)  Acc@5: 90.2123 (80.8220)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-24.pth.tar', 54.72199990234375)

Train: 34 [   0/1251 (  0%)]  Loss: 4.730 (4.73)  Time: 1.702s,  601.69/s  (1.702s,  601.69/s)  LR: 9.687e-04  Data: 1.562 (1.562)
Train: 34 [  50/1251 (  4%)]  Loss: 4.540 (4.63)  Time: 0.161s, 6351.32/s  (0.226s, 4532.62/s)  LR: 9.687e-04  Data: 0.030 (0.064)
Train: 34 [ 100/1251 (  8%)]  Loss: 4.207 (4.49)  Time: 0.164s, 6235.36/s  (0.208s, 4929.95/s)  LR: 9.687e-04  Data: 0.027 (0.046)
Train: 34 [ 150/1251 ( 12%)]  Loss: 4.758 (4.56)  Time: 0.197s, 5185.50/s  (0.202s, 5065.68/s)  LR: 9.687e-04  Data: 0.025 (0.040)
Train: 34 [ 200/1251 ( 16%)]  Loss: 4.685 (4.58)  Time: 0.178s, 5765.98/s  (0.198s, 5168.94/s)  LR: 9.687e-04  Data: 0.033 (0.037)
Train: 34 [ 250/1251 ( 20%)]  Loss: 4.617 (4.59)  Time: 0.151s, 6760.93/s  (0.197s, 5185.14/s)  LR: 9.687e-04  Data: 0.022 (0.037)
Train: 34 [ 300/1251 ( 24%)]  Loss: 4.701 (4.61)  Time: 0.177s, 5792.14/s  (0.196s, 5211.67/s)  LR: 9.687e-04  Data: 0.026 (0.036)
Train: 34 [ 350/1251 ( 28%)]  Loss: 4.672 (4.61)  Time: 0.163s, 6298.45/s  (0.196s, 5218.70/s)  LR: 9.687e-04  Data: 0.027 (0.034)
Train: 34 [ 400/1251 ( 32%)]  Loss: 4.633 (4.62)  Time: 0.174s, 5890.27/s  (0.195s, 5247.74/s)  LR: 9.687e-04  Data: 0.025 (0.034)
Train: 34 [ 450/1251 ( 36%)]  Loss: 4.683 (4.62)  Time: 0.166s, 6163.42/s  (0.195s, 5255.35/s)  LR: 9.687e-04  Data: 0.029 (0.033)
Train: 34 [ 500/1251 ( 40%)]  Loss: 4.652 (4.63)  Time: 0.214s, 4795.05/s  (0.194s, 5268.36/s)  LR: 9.687e-04  Data: 0.023 (0.032)
Train: 34 [ 550/1251 ( 44%)]  Loss: 4.761 (4.64)  Time: 0.188s, 5444.45/s  (0.194s, 5280.73/s)  LR: 9.687e-04  Data: 0.031 (0.032)
Train: 34 [ 600/1251 ( 48%)]  Loss: 4.954 (4.66)  Time: 0.166s, 6154.91/s  (0.194s, 5270.59/s)  LR: 9.687e-04  Data: 0.030 (0.032)
Train: 34 [ 650/1251 ( 52%)]  Loss: 4.467 (4.65)  Time: 0.173s, 5911.55/s  (0.195s, 5261.82/s)  LR: 9.687e-04  Data: 0.036 (0.032)
Train: 34 [ 700/1251 ( 56%)]  Loss: 4.530 (4.64)  Time: 0.178s, 5737.04/s  (0.194s, 5269.01/s)  LR: 9.687e-04  Data: 0.032 (0.031)
Train: 34 [ 750/1251 ( 60%)]  Loss: 4.783 (4.65)  Time: 0.175s, 5858.85/s  (0.194s, 5275.64/s)  LR: 9.687e-04  Data: 0.030 (0.031)
Train: 34 [ 800/1251 ( 64%)]  Loss: 4.811 (4.66)  Time: 0.189s, 5410.27/s  (0.195s, 5257.80/s)  LR: 9.687e-04  Data: 0.036 (0.031)
Train: 34 [ 850/1251 ( 68%)]  Loss: 4.780 (4.66)  Time: 0.172s, 5944.67/s  (0.195s, 5244.46/s)  LR: 9.687e-04  Data: 0.020 (0.031)
Train: 34 [ 900/1251 ( 72%)]  Loss: 4.596 (4.66)  Time: 0.156s, 6546.08/s  (0.195s, 5244.34/s)  LR: 9.687e-04  Data: 0.027 (0.031)
Train: 34 [ 950/1251 ( 76%)]  Loss: 4.760 (4.67)  Time: 0.182s, 5625.29/s  (0.195s, 5259.52/s)  LR: 9.687e-04  Data: 0.028 (0.031)
Train: 34 [1000/1251 ( 80%)]  Loss: 4.760 (4.67)  Time: 0.187s, 5469.15/s  (0.195s, 5249.53/s)  LR: 9.687e-04  Data: 0.022 (0.031)
Train: 34 [1050/1251 ( 84%)]  Loss: 4.706 (4.67)  Time: 0.224s, 4562.67/s  (0.195s, 5248.60/s)  LR: 9.687e-04  Data: 0.028 (0.030)
Train: 34 [1100/1251 ( 88%)]  Loss: 4.442 (4.66)  Time: 0.175s, 5838.00/s  (0.195s, 5244.62/s)  LR: 9.687e-04  Data: 0.020 (0.030)
Train: 34 [1150/1251 ( 92%)]  Loss: 4.536 (4.66)  Time: 0.159s, 6438.97/s  (0.195s, 5239.03/s)  LR: 9.687e-04  Data: 0.026 (0.030)
Train: 34 [1200/1251 ( 96%)]  Loss: 4.383 (4.65)  Time: 0.168s, 6096.00/s  (0.195s, 5241.40/s)  LR: 9.687e-04  Data: 0.023 (0.030)
Train: 34 [1250/1251 (100%)]  Loss: 4.505 (4.64)  Time: 0.113s, 9043.98/s  (0.195s, 5260.11/s)  LR: 9.687e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.821 (1.821)  Loss:  1.4551 (1.4551)  Acc@1: 76.2695 (76.2695)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.5025 (2.1310)  Acc@1: 75.7075 (58.2760)  Acc@5: 91.1557 (81.2820)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-25.pth.tar', 55.338000004882815)

Train: 35 [   0/1251 (  0%)]  Loss: 4.752 (4.75)  Time: 1.853s,  552.47/s  (1.853s,  552.47/s)  LR: 9.668e-04  Data: 1.716 (1.716)
Train: 35 [  50/1251 (  4%)]  Loss: 4.604 (4.68)  Time: 0.162s, 6327.56/s  (0.219s, 4683.44/s)  LR: 9.668e-04  Data: 0.036 (0.070)
Train: 35 [ 100/1251 (  8%)]  Loss: 4.724 (4.69)  Time: 0.175s, 5856.71/s  (0.205s, 4992.52/s)  LR: 9.668e-04  Data: 0.038 (0.049)
Train: 35 [ 150/1251 ( 12%)]  Loss: 4.726 (4.70)  Time: 0.172s, 5941.67/s  (0.203s, 5046.16/s)  LR: 9.668e-04  Data: 0.026 (0.042)
Train: 35 [ 200/1251 ( 16%)]  Loss: 4.816 (4.72)  Time: 0.176s, 5831.39/s  (0.200s, 5111.32/s)  LR: 9.668e-04  Data: 0.025 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 35 [ 250/1251 ( 20%)]  Loss: 4.392 (4.67)  Time: 0.184s, 5575.24/s  (0.199s, 5150.25/s)  LR: 9.668e-04  Data: 0.033 (0.036)
Train: 35 [ 300/1251 ( 24%)]  Loss: 4.998 (4.72)  Time: 0.185s, 5520.50/s  (0.196s, 5217.06/s)  LR: 9.668e-04  Data: 0.027 (0.035)
Train: 35 [ 350/1251 ( 28%)]  Loss: 4.523 (4.69)  Time: 0.175s, 5857.85/s  (0.197s, 5197.36/s)  LR: 9.668e-04  Data: 0.028 (0.034)
Train: 35 [ 400/1251 ( 32%)]  Loss: 4.287 (4.65)  Time: 0.170s, 6023.13/s  (0.196s, 5224.18/s)  LR: 9.668e-04  Data: 0.027 (0.033)
Train: 35 [ 450/1251 ( 36%)]  Loss: 4.672 (4.65)  Time: 0.165s, 6219.56/s  (0.196s, 5233.29/s)  LR: 9.668e-04  Data: 0.030 (0.033)
Train: 35 [ 500/1251 ( 40%)]  Loss: 4.884 (4.67)  Time: 0.167s, 6138.08/s  (0.197s, 5203.43/s)  LR: 9.668e-04  Data: 0.030 (0.032)
Train: 35 [ 550/1251 ( 44%)]  Loss: 4.528 (4.66)  Time: 0.184s, 5578.09/s  (0.197s, 5196.29/s)  LR: 9.668e-04  Data: 0.021 (0.033)
Train: 35 [ 600/1251 ( 48%)]  Loss: 4.612 (4.66)  Time: 0.166s, 6169.38/s  (0.196s, 5224.47/s)  LR: 9.668e-04  Data: 0.020 (0.034)
Train: 35 [ 650/1251 ( 52%)]  Loss: 4.720 (4.66)  Time: 0.164s, 6251.47/s  (0.196s, 5231.30/s)  LR: 9.668e-04  Data: 0.029 (0.035)
Train: 35 [ 700/1251 ( 56%)]  Loss: 4.713 (4.66)  Time: 0.390s, 2627.02/s  (0.196s, 5220.66/s)  LR: 9.668e-04  Data: 0.270 (0.037)
Train: 35 [ 750/1251 ( 60%)]  Loss: 4.523 (4.65)  Time: 0.173s, 5910.63/s  (0.196s, 5230.73/s)  LR: 9.668e-04  Data: 0.028 (0.037)
Train: 35 [ 800/1251 ( 64%)]  Loss: 4.605 (4.65)  Time: 0.167s, 6117.52/s  (0.196s, 5232.59/s)  LR: 9.668e-04  Data: 0.030 (0.036)
Train: 35 [ 850/1251 ( 68%)]  Loss: 4.772 (4.66)  Time: 0.170s, 6017.66/s  (0.195s, 5238.67/s)  LR: 9.668e-04  Data: 0.031 (0.036)
Train: 35 [ 900/1251 ( 72%)]  Loss: 4.558 (4.65)  Time: 0.371s, 2760.88/s  (0.196s, 5230.08/s)  LR: 9.668e-04  Data: 0.248 (0.036)
Train: 35 [ 950/1251 ( 76%)]  Loss: 3.900 (4.62)  Time: 0.182s, 5629.44/s  (0.196s, 5228.32/s)  LR: 9.668e-04  Data: 0.021 (0.037)
Train: 35 [1000/1251 ( 80%)]  Loss: 4.844 (4.63)  Time: 0.171s, 5996.10/s  (0.196s, 5226.72/s)  LR: 9.668e-04  Data: 0.022 (0.037)
Train: 35 [1050/1251 ( 84%)]  Loss: 4.405 (4.62)  Time: 0.159s, 6454.36/s  (0.196s, 5228.42/s)  LR: 9.668e-04  Data: 0.030 (0.038)
Train: 35 [1100/1251 ( 88%)]  Loss: 4.562 (4.61)  Time: 0.166s, 6179.20/s  (0.196s, 5232.56/s)  LR: 9.668e-04  Data: 0.031 (0.038)
Train: 35 [1150/1251 ( 92%)]  Loss: 4.921 (4.63)  Time: 0.176s, 5810.98/s  (0.196s, 5217.27/s)  LR: 9.668e-04  Data: 0.024 (0.039)
Train: 35 [1200/1251 ( 96%)]  Loss: 4.902 (4.64)  Time: 0.174s, 5899.92/s  (0.196s, 5212.43/s)  LR: 9.668e-04  Data: 0.021 (0.040)
Train: 35 [1250/1251 (100%)]  Loss: 4.509 (4.63)  Time: 0.114s, 9021.26/s  (0.196s, 5226.49/s)  LR: 9.668e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.848 (1.848)  Loss:  1.3998 (1.3998)  Acc@1: 75.3906 (75.3906)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3662 (2.0761)  Acc@1: 75.0000 (57.3940)  Acc@5: 88.6793 (81.0740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-27.pth.tar', 55.782000009765625)

Train: 36 [   0/1251 (  0%)]  Loss: 4.819 (4.82)  Time: 1.695s,  603.96/s  (1.695s,  603.96/s)  LR: 9.649e-04  Data: 1.523 (1.523)
Train: 36 [  50/1251 (  4%)]  Loss: 4.589 (4.70)  Time: 0.169s, 6063.84/s  (0.224s, 4576.81/s)  LR: 9.649e-04  Data: 0.024 (0.061)
Train: 36 [ 100/1251 (  8%)]  Loss: 4.438 (4.62)  Time: 0.156s, 6550.82/s  (0.211s, 4859.55/s)  LR: 9.649e-04  Data: 0.024 (0.044)
Train: 36 [ 150/1251 ( 12%)]  Loss: 4.613 (4.61)  Time: 0.171s, 5976.18/s  (0.205s, 4993.83/s)  LR: 9.649e-04  Data: 0.034 (0.039)
Train: 36 [ 200/1251 ( 16%)]  Loss: 4.684 (4.63)  Time: 0.155s, 6589.86/s  (0.200s, 5131.77/s)  LR: 9.649e-04  Data: 0.028 (0.036)
Train: 36 [ 250/1251 ( 20%)]  Loss: 4.867 (4.67)  Time: 0.182s, 5637.77/s  (0.197s, 5190.24/s)  LR: 9.649e-04  Data: 0.023 (0.035)
Train: 36 [ 300/1251 ( 24%)]  Loss: 4.626 (4.66)  Time: 0.194s, 5274.02/s  (0.196s, 5229.22/s)  LR: 9.649e-04  Data: 0.021 (0.034)
Train: 36 [ 350/1251 ( 28%)]  Loss: 4.604 (4.65)  Time: 0.178s, 5767.74/s  (0.195s, 5240.53/s)  LR: 9.649e-04  Data: 0.026 (0.033)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 36 [ 400/1251 ( 32%)]  Loss: 4.833 (4.67)  Time: 0.364s, 2811.33/s  (0.195s, 5255.70/s)  LR: 9.649e-04  Data: 0.025 (0.032)
Train: 36 [ 450/1251 ( 36%)]  Loss: 4.877 (4.69)  Time: 0.165s, 6209.83/s  (0.194s, 5269.82/s)  LR: 9.649e-04  Data: 0.028 (0.033)
Train: 36 [ 500/1251 ( 40%)]  Loss: 4.507 (4.68)  Time: 0.180s, 5674.19/s  (0.194s, 5274.45/s)  LR: 9.649e-04  Data: 0.024 (0.034)
Train: 36 [ 550/1251 ( 44%)]  Loss: 4.600 (4.67)  Time: 0.168s, 6095.52/s  (0.194s, 5287.05/s)  LR: 9.649e-04  Data: 0.019 (0.035)
Train: 36 [ 600/1251 ( 48%)]  Loss: 4.715 (4.67)  Time: 0.180s, 5692.93/s  (0.195s, 5251.07/s)  LR: 9.649e-04  Data: 0.050 (0.034)
Train: 36 [ 650/1251 ( 52%)]  Loss: 4.671 (4.67)  Time: 0.172s, 5951.01/s  (0.194s, 5267.65/s)  LR: 9.649e-04  Data: 0.035 (0.034)
Train: 36 [ 700/1251 ( 56%)]  Loss: 4.564 (4.67)  Time: 0.194s, 5289.17/s  (0.195s, 5251.57/s)  LR: 9.649e-04  Data: 0.024 (0.033)
Train: 36 [ 750/1251 ( 60%)]  Loss: 4.788 (4.67)  Time: 0.174s, 5872.95/s  (0.195s, 5262.93/s)  LR: 9.649e-04  Data: 0.028 (0.033)
Train: 36 [ 800/1251 ( 64%)]  Loss: 4.540 (4.67)  Time: 0.466s, 2196.43/s  (0.195s, 5255.96/s)  LR: 9.649e-04  Data: 0.024 (0.033)
Train: 36 [ 850/1251 ( 68%)]  Loss: 4.757 (4.67)  Time: 0.185s, 5537.54/s  (0.194s, 5266.72/s)  LR: 9.649e-04  Data: 0.027 (0.032)
Train: 36 [ 900/1251 ( 72%)]  Loss: 4.389 (4.66)  Time: 0.161s, 6370.66/s  (0.194s, 5269.44/s)  LR: 9.649e-04  Data: 0.022 (0.032)
Train: 36 [ 950/1251 ( 76%)]  Loss: 4.806 (4.66)  Time: 0.172s, 5967.55/s  (0.194s, 5265.71/s)  LR: 9.649e-04  Data: 0.020 (0.032)
Train: 36 [1000/1251 ( 80%)]  Loss: 4.475 (4.66)  Time: 0.293s, 3490.82/s  (0.195s, 5260.09/s)  LR: 9.649e-04  Data: 0.026 (0.032)
Train: 36 [1050/1251 ( 84%)]  Loss: 4.507 (4.65)  Time: 0.409s, 2501.65/s  (0.195s, 5248.88/s)  LR: 9.649e-04  Data: 0.027 (0.031)
Train: 36 [1100/1251 ( 88%)]  Loss: 4.512 (4.64)  Time: 0.165s, 6208.24/s  (0.195s, 5253.03/s)  LR: 9.649e-04  Data: 0.038 (0.031)
Train: 36 [1150/1251 ( 92%)]  Loss: 4.507 (4.64)  Time: 0.181s, 5645.23/s  (0.195s, 5241.93/s)  LR: 9.649e-04  Data: 0.021 (0.032)
Train: 36 [1200/1251 ( 96%)]  Loss: 4.563 (4.63)  Time: 0.164s, 6252.42/s  (0.197s, 5188.53/s)  LR: 9.649e-04  Data: 0.025 (0.032)
Train: 36 [1250/1251 (100%)]  Loss: 4.526 (4.63)  Time: 0.114s, 8989.48/s  (0.197s, 5202.48/s)  LR: 9.649e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.817 (1.817)  Loss:  1.4197 (1.4197)  Acc@1: 75.5859 (75.5859)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.227)  Loss:  1.3823 (2.0555)  Acc@1: 76.1792 (58.9820)  Acc@5: 90.4481 (81.9440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-29.pth.tar', 56.04200002685547)

Train: 37 [   0/1251 (  0%)]  Loss: 4.416 (4.42)  Time: 1.737s,  589.41/s  (1.737s,  589.41/s)  LR: 9.630e-04  Data: 1.607 (1.607)
Train: 37 [  50/1251 (  4%)]  Loss: 4.662 (4.54)  Time: 0.179s, 5731.38/s  (0.230s, 4451.28/s)  LR: 9.630e-04  Data: 0.019 (0.066)
Train: 37 [ 100/1251 (  8%)]  Loss: 4.892 (4.66)  Time: 0.156s, 6578.43/s  (0.210s, 4884.15/s)  LR: 9.630e-04  Data: 0.025 (0.046)
Train: 37 [ 150/1251 ( 12%)]  Loss: 4.430 (4.60)  Time: 0.159s, 6451.91/s  (0.204s, 5030.65/s)  LR: 9.630e-04  Data: 0.032 (0.040)
Train: 37 [ 200/1251 ( 16%)]  Loss: 4.507 (4.58)  Time: 0.225s, 4550.55/s  (0.200s, 5116.16/s)  LR: 9.630e-04  Data: 0.031 (0.037)
Train: 37 [ 250/1251 ( 20%)]  Loss: 4.319 (4.54)  Time: 0.207s, 4949.27/s  (0.199s, 5149.11/s)  LR: 9.630e-04  Data: 0.025 (0.035)
Train: 37 [ 300/1251 ( 24%)]  Loss: 4.577 (4.54)  Time: 0.171s, 5978.79/s  (0.196s, 5218.62/s)  LR: 9.630e-04  Data: 0.025 (0.034)
Train: 37 [ 350/1251 ( 28%)]  Loss: 4.929 (4.59)  Time: 0.186s, 5497.11/s  (0.197s, 5209.78/s)  LR: 9.630e-04  Data: 0.030 (0.033)
Train: 37 [ 400/1251 ( 32%)]  Loss: 4.775 (4.61)  Time: 0.240s, 4274.16/s  (0.195s, 5250.51/s)  LR: 9.630e-04  Data: 0.031 (0.032)
Train: 37 [ 450/1251 ( 36%)]  Loss: 4.911 (4.64)  Time: 0.162s, 6337.32/s  (0.196s, 5234.71/s)  LR: 9.630e-04  Data: 0.029 (0.032)
Train: 37 [ 500/1251 ( 40%)]  Loss: 4.390 (4.62)  Time: 0.157s, 6530.69/s  (0.195s, 5252.21/s)  LR: 9.630e-04  Data: 0.028 (0.032)
Train: 37 [ 550/1251 ( 44%)]  Loss: 4.501 (4.61)  Time: 0.164s, 6245.07/s  (0.194s, 5275.30/s)  LR: 9.630e-04  Data: 0.033 (0.031)
Train: 37 [ 600/1251 ( 48%)]  Loss: 4.117 (4.57)  Time: 0.185s, 5528.81/s  (0.194s, 5272.63/s)  LR: 9.630e-04  Data: 0.037 (0.031)
Train: 37 [ 650/1251 ( 52%)]  Loss: 4.431 (4.56)  Time: 0.349s, 2931.22/s  (0.194s, 5272.58/s)  LR: 9.630e-04  Data: 0.030 (0.031)
Train: 37 [ 700/1251 ( 56%)]  Loss: 4.642 (4.57)  Time: 0.168s, 6082.08/s  (0.194s, 5272.96/s)  LR: 9.630e-04  Data: 0.027 (0.030)
Train: 37 [ 750/1251 ( 60%)]  Loss: 4.638 (4.57)  Time: 0.182s, 5627.29/s  (0.198s, 5168.46/s)  LR: 9.630e-04  Data: 0.027 (0.035)
Train: 37 [ 800/1251 ( 64%)]  Loss: 4.556 (4.57)  Time: 0.175s, 5835.12/s  (0.197s, 5186.02/s)  LR: 9.630e-04  Data: 0.027 (0.034)
Train: 37 [ 850/1251 ( 68%)]  Loss: 3.981 (4.54)  Time: 0.160s, 6391.32/s  (0.197s, 5186.02/s)  LR: 9.630e-04  Data: 0.019 (0.034)
Train: 37 [ 900/1251 ( 72%)]  Loss: 4.501 (4.54)  Time: 0.161s, 6358.10/s  (0.198s, 5182.64/s)  LR: 9.630e-04  Data: 0.021 (0.034)
Train: 37 [ 950/1251 ( 76%)]  Loss: 4.653 (4.54)  Time: 0.346s, 2956.56/s  (0.198s, 5179.92/s)  LR: 9.630e-04  Data: 0.023 (0.033)
Train: 37 [1000/1251 ( 80%)]  Loss: 4.587 (4.54)  Time: 0.170s, 6012.78/s  (0.198s, 5183.44/s)  LR: 9.630e-04  Data: 0.028 (0.033)
Train: 37 [1050/1251 ( 84%)]  Loss: 4.552 (4.54)  Time: 0.156s, 6567.20/s  (0.197s, 5186.62/s)  LR: 9.630e-04  Data: 0.023 (0.033)
Train: 37 [1100/1251 ( 88%)]  Loss: 4.335 (4.53)  Time: 0.171s, 5978.77/s  (0.197s, 5185.33/s)  LR: 9.630e-04  Data: 0.031 (0.032)
Train: 37 [1150/1251 ( 92%)]  Loss: 4.488 (4.53)  Time: 0.171s, 5996.06/s  (0.197s, 5191.87/s)  LR: 9.630e-04  Data: 0.032 (0.032)
Train: 37 [1200/1251 ( 96%)]  Loss: 4.287 (4.52)  Time: 0.284s, 3607.56/s  (0.197s, 5190.86/s)  LR: 9.630e-04  Data: 0.024 (0.032)
Train: 37 [1250/1251 (100%)]  Loss: 4.409 (4.52)  Time: 0.114s, 9019.91/s  (0.197s, 5204.59/s)  LR: 9.630e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.892 (1.892)  Loss:  1.4256 (1.4256)  Acc@1: 72.3633 (72.3633)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.5069 (2.2327)  Acc@1: 75.0000 (55.1820)  Acc@5: 89.2689 (79.4920)
Train: 38 [   0/1251 (  0%)]  Loss: 4.744 (4.74)  Time: 2.225s,  460.20/s  (2.225s,  460.20/s)  LR: 9.610e-04  Data: 2.095 (2.095)
Train: 38 [  50/1251 (  4%)]  Loss: 4.782 (4.76)  Time: 0.164s, 6262.97/s  (0.227s, 4514.09/s)  LR: 9.610e-04  Data: 0.026 (0.083)
Train: 38 [ 100/1251 (  8%)]  Loss: 4.441 (4.66)  Time: 0.161s, 6370.45/s  (0.207s, 4945.71/s)  LR: 9.610e-04  Data: 0.033 (0.056)
Train: 38 [ 150/1251 ( 12%)]  Loss: 4.677 (4.66)  Time: 0.174s, 5889.04/s  (0.203s, 5047.42/s)  LR: 9.610e-04  Data: 0.032 (0.047)
Train: 38 [ 200/1251 ( 16%)]  Loss: 4.444 (4.62)  Time: 0.317s, 3227.72/s  (0.201s, 5085.38/s)  LR: 9.610e-04  Data: 0.028 (0.042)
Train: 38 [ 250/1251 ( 20%)]  Loss: 4.780 (4.64)  Time: 0.169s, 6060.59/s  (0.197s, 5193.70/s)  LR: 9.610e-04  Data: 0.027 (0.039)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 38 [ 300/1251 ( 24%)]  Loss: 4.567 (4.63)  Time: 0.174s, 5884.22/s  (0.196s, 5236.63/s)  LR: 9.610e-04  Data: 0.044 (0.039)
Train: 38 [ 350/1251 ( 28%)]  Loss: 4.411 (4.61)  Time: 0.170s, 6025.31/s  (0.196s, 5237.29/s)  LR: 9.610e-04  Data: 0.023 (0.041)
Train: 38 [ 400/1251 ( 32%)]  Loss: 4.734 (4.62)  Time: 0.176s, 5804.56/s  (0.196s, 5224.46/s)  LR: 9.610e-04  Data: 0.021 (0.042)
Train: 38 [ 450/1251 ( 36%)]  Loss: 4.167 (4.57)  Time: 0.166s, 6156.04/s  (0.194s, 5265.78/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 500/1251 ( 40%)]  Loss: 4.426 (4.56)  Time: 0.257s, 3982.75/s  (0.194s, 5275.05/s)  LR: 9.610e-04  Data: 0.118 (0.042)
Train: 38 [ 550/1251 ( 44%)]  Loss: 4.694 (4.57)  Time: 0.171s, 5973.44/s  (0.194s, 5291.20/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 600/1251 ( 48%)]  Loss: 4.511 (4.57)  Time: 0.163s, 6286.09/s  (0.193s, 5294.29/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [ 650/1251 ( 52%)]  Loss: 4.072 (4.53)  Time: 0.181s, 5652.25/s  (0.194s, 5277.26/s)  LR: 9.610e-04  Data: 0.024 (0.042)
Train: 38 [ 700/1251 ( 56%)]  Loss: 4.740 (4.55)  Time: 0.287s, 3569.05/s  (0.194s, 5286.65/s)  LR: 9.610e-04  Data: 0.052 (0.042)
Train: 38 [ 750/1251 ( 60%)]  Loss: 4.572 (4.55)  Time: 0.310s, 3302.26/s  (0.194s, 5275.70/s)  LR: 9.610e-04  Data: 0.023 (0.041)
Train: 38 [ 800/1251 ( 64%)]  Loss: 4.268 (4.53)  Time: 0.175s, 5848.66/s  (0.194s, 5288.13/s)  LR: 9.610e-04  Data: 0.024 (0.040)
Train: 38 [ 850/1251 ( 68%)]  Loss: 4.779 (4.54)  Time: 0.182s, 5611.10/s  (0.194s, 5280.61/s)  LR: 9.610e-04  Data: 0.028 (0.040)
Train: 38 [ 900/1251 ( 72%)]  Loss: 4.187 (4.53)  Time: 0.221s, 4642.56/s  (0.194s, 5282.19/s)  LR: 9.610e-04  Data: 0.099 (0.040)
Train: 38 [ 950/1251 ( 76%)]  Loss: 4.381 (4.52)  Time: 0.166s, 6180.65/s  (0.194s, 5283.07/s)  LR: 9.610e-04  Data: 0.025 (0.041)
Train: 38 [1000/1251 ( 80%)]  Loss: 4.455 (4.52)  Time: 0.199s, 5157.69/s  (0.194s, 5284.56/s)  LR: 9.610e-04  Data: 0.026 (0.041)
Train: 38 [1050/1251 ( 84%)]  Loss: 4.366 (4.51)  Time: 0.178s, 5740.79/s  (0.194s, 5270.99/s)  LR: 9.610e-04  Data: 0.029 (0.042)
Train: 38 [1100/1251 ( 88%)]  Loss: 4.080 (4.49)  Time: 0.203s, 5044.74/s  (0.194s, 5270.71/s)  LR: 9.610e-04  Data: 0.043 (0.042)
Train: 38 [1150/1251 ( 92%)]  Loss: 4.450 (4.49)  Time: 0.192s, 5337.76/s  (0.194s, 5276.70/s)  LR: 9.610e-04  Data: 0.031 (0.042)
Train: 38 [1200/1251 ( 96%)]  Loss: 4.740 (4.50)  Time: 0.172s, 5937.61/s  (0.194s, 5274.01/s)  LR: 9.610e-04  Data: 0.026 (0.042)
Train: 38 [1250/1251 (100%)]  Loss: 4.604 (4.50)  Time: 0.125s, 8189.25/s  (0.194s, 5275.93/s)  LR: 9.610e-04  Data: 0.000 (0.041)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.740 (1.740)  Loss:  1.3190 (1.3190)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3890 (2.0356)  Acc@1: 75.4717 (58.6360)  Acc@5: 89.8585 (81.8840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-28.pth.tar', 56.19200013671875)

Train: 39 [   0/1251 (  0%)]  Loss: 4.663 (4.66)  Time: 1.713s,  597.79/s  (1.713s,  597.79/s)  LR: 9.589e-04  Data: 1.576 (1.576)
Train: 39 [  50/1251 (  4%)]  Loss: 4.240 (4.45)  Time: 0.191s, 5364.36/s  (0.223s, 4592.64/s)  LR: 9.589e-04  Data: 0.032 (0.074)
Train: 39 [ 100/1251 (  8%)]  Loss: 4.381 (4.43)  Time: 0.164s, 6232.39/s  (0.209s, 4904.47/s)  LR: 9.589e-04  Data: 0.030 (0.062)
Train: 39 [ 150/1251 ( 12%)]  Loss: 4.698 (4.50)  Time: 0.179s, 5727.65/s  (0.200s, 5109.75/s)  LR: 9.589e-04  Data: 0.027 (0.053)
Train: 39 [ 200/1251 ( 16%)]  Loss: 4.375 (4.47)  Time: 0.164s, 6238.66/s  (0.198s, 5181.66/s)  LR: 9.589e-04  Data: 0.031 (0.051)
Train: 39 [ 250/1251 ( 20%)]  Loss: 4.391 (4.46)  Time: 0.174s, 5871.48/s  (0.196s, 5213.33/s)  LR: 9.589e-04  Data: 0.025 (0.048)
Train: 39 [ 300/1251 ( 24%)]  Loss: 4.293 (4.43)  Time: 0.248s, 4122.76/s  (0.195s, 5239.42/s)  LR: 9.589e-04  Data: 0.026 (0.045)
Train: 39 [ 350/1251 ( 28%)]  Loss: 4.419 (4.43)  Time: 0.172s, 5957.87/s  (0.195s, 5262.26/s)  LR: 9.589e-04  Data: 0.030 (0.043)
Train: 39 [ 400/1251 ( 32%)]  Loss: 4.435 (4.43)  Time: 0.182s, 5614.50/s  (0.193s, 5300.47/s)  LR: 9.589e-04  Data: 0.034 (0.041)
Train: 39 [ 450/1251 ( 36%)]  Loss: 4.308 (4.42)  Time: 0.217s, 4718.49/s  (0.193s, 5310.32/s)  LR: 9.589e-04  Data: 0.024 (0.039)
Train: 39 [ 500/1251 ( 40%)]  Loss: 4.562 (4.43)  Time: 0.179s, 5719.84/s  (0.193s, 5309.10/s)  LR: 9.589e-04  Data: 0.026 (0.038)
Train: 39 [ 550/1251 ( 44%)]  Loss: 4.746 (4.46)  Time: 0.159s, 6458.12/s  (0.193s, 5308.50/s)  LR: 9.589e-04  Data: 0.024 (0.037)
Train: 39 [ 600/1251 ( 48%)]  Loss: 4.536 (4.47)  Time: 0.192s, 5345.92/s  (0.193s, 5303.58/s)  LR: 9.589e-04  Data: 0.023 (0.038)
Train: 39 [ 650/1251 ( 52%)]  Loss: 4.486 (4.47)  Time: 0.157s, 6505.62/s  (0.193s, 5297.87/s)  LR: 9.589e-04  Data: 0.022 (0.039)
Train: 39 [ 700/1251 ( 56%)]  Loss: 4.194 (4.45)  Time: 0.186s, 5496.92/s  (0.193s, 5297.71/s)  LR: 9.589e-04  Data: 0.024 (0.040)
Train: 39 [ 750/1251 ( 60%)]  Loss: 4.957 (4.48)  Time: 0.172s, 5946.09/s  (0.195s, 5259.61/s)  LR: 9.589e-04  Data: 0.028 (0.042)
Train: 39 [ 800/1251 ( 64%)]  Loss: 4.358 (4.47)  Time: 0.191s, 5362.00/s  (0.195s, 5256.03/s)  LR: 9.589e-04  Data: 0.024 (0.042)
Train: 39 [ 850/1251 ( 68%)]  Loss: 4.659 (4.48)  Time: 0.180s, 5693.12/s  (0.194s, 5270.72/s)  LR: 9.589e-04  Data: 0.024 (0.042)
Train: 39 [ 900/1251 ( 72%)]  Loss: 4.480 (4.48)  Time: 0.172s, 5965.06/s  (0.194s, 5269.58/s)  LR: 9.589e-04  Data: 0.027 (0.042)
Train: 39 [ 950/1251 ( 76%)]  Loss: 4.403 (4.48)  Time: 0.186s, 5492.52/s  (0.195s, 5263.33/s)  LR: 9.589e-04  Data: 0.027 (0.042)
Train: 39 [1000/1251 ( 80%)]  Loss: 4.365 (4.47)  Time: 0.177s, 5782.80/s  (0.195s, 5264.22/s)  LR: 9.589e-04  Data: 0.028 (0.041)
Train: 39 [1050/1251 ( 84%)]  Loss: 4.304 (4.47)  Time: 0.170s, 6019.54/s  (0.194s, 5265.68/s)  LR: 9.589e-04  Data: 0.024 (0.041)
Train: 39 [1100/1251 ( 88%)]  Loss: 4.671 (4.47)  Time: 0.183s, 5585.50/s  (0.195s, 5260.86/s)  LR: 9.589e-04  Data: 0.020 (0.040)
Train: 39 [1150/1251 ( 92%)]  Loss: 4.678 (4.48)  Time: 0.305s, 3354.42/s  (0.195s, 5260.40/s)  LR: 9.589e-04  Data: 0.020 (0.040)
Train: 39 [1200/1251 ( 96%)]  Loss: 4.750 (4.49)  Time: 0.172s, 5962.56/s  (0.195s, 5256.12/s)  LR: 9.589e-04  Data: 0.022 (0.039)
Train: 39 [1250/1251 (100%)]  Loss: 4.682 (4.50)  Time: 0.120s, 8521.56/s  (0.194s, 5270.20/s)  LR: 9.589e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.770 (1.770)  Loss:  1.2820 (1.2820)  Acc@1: 74.9023 (74.9023)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3291 (2.0133)  Acc@1: 76.2972 (58.5220)  Acc@5: 90.8019 (81.7600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-26.pth.tar', 56.21599998046875)

Train: 40 [   0/1251 (  0%)]  Loss: 4.916 (4.92)  Time: 1.887s,  542.69/s  (1.887s,  542.69/s)  LR: 9.568e-04  Data: 1.750 (1.750)
Train: 40 [  50/1251 (  4%)]  Loss: 4.299 (4.61)  Time: 0.164s, 6251.59/s  (0.227s, 4512.16/s)  LR: 9.568e-04  Data: 0.033 (0.079)
Train: 40 [ 100/1251 (  8%)]  Loss: 4.735 (4.65)  Time: 0.205s, 4987.40/s  (0.211s, 4860.18/s)  LR: 9.568e-04  Data: 0.027 (0.054)
Train: 40 [ 150/1251 ( 12%)]  Loss: 4.771 (4.68)  Time: 0.175s, 5848.38/s  (0.203s, 5054.93/s)  LR: 9.568e-04  Data: 0.026 (0.049)
Train: 40 [ 200/1251 ( 16%)]  Loss: 4.392 (4.62)  Time: 0.164s, 6224.92/s  (0.198s, 5167.03/s)  LR: 9.568e-04  Data: 0.024 (0.047)
Train: 40 [ 250/1251 ( 20%)]  Loss: 4.355 (4.58)  Time: 0.151s, 6777.28/s  (0.196s, 5230.16/s)  LR: 9.568e-04  Data: 0.023 (0.045)
Train: 40 [ 300/1251 ( 24%)]  Loss: 4.732 (4.60)  Time: 0.158s, 6480.95/s  (0.195s, 5251.73/s)  LR: 9.568e-04  Data: 0.025 (0.045)
Train: 40 [ 350/1251 ( 28%)]  Loss: 4.715 (4.61)  Time: 0.184s, 5564.63/s  (0.195s, 5262.24/s)  LR: 9.568e-04  Data: 0.023 (0.045)
Train: 40 [ 400/1251 ( 32%)]  Loss: 4.539 (4.61)  Time: 0.315s, 3250.54/s  (0.196s, 5237.81/s)  LR: 9.568e-04  Data: 0.187 (0.047)
Train: 40 [ 450/1251 ( 36%)]  Loss: 4.448 (4.59)  Time: 0.179s, 5722.74/s  (0.194s, 5273.33/s)  LR: 9.568e-04  Data: 0.026 (0.046)
Train: 40 [ 500/1251 ( 40%)]  Loss: 4.838 (4.61)  Time: 0.155s, 6600.39/s  (0.196s, 5219.88/s)  LR: 9.568e-04  Data: 0.028 (0.047)
Train: 40 [ 550/1251 ( 44%)]  Loss: 4.336 (4.59)  Time: 0.165s, 6218.70/s  (0.196s, 5236.18/s)  LR: 9.568e-04  Data: 0.027 (0.046)
Train: 40 [ 600/1251 ( 48%)]  Loss: 4.460 (4.58)  Time: 0.160s, 6406.88/s  (0.196s, 5221.39/s)  LR: 9.568e-04  Data: 0.036 (0.047)
Train: 40 [ 650/1251 ( 52%)]  Loss: 4.436 (4.57)  Time: 0.158s, 6497.48/s  (0.196s, 5237.56/s)  LR: 9.568e-04  Data: 0.028 (0.047)
Train: 40 [ 700/1251 ( 56%)]  Loss: 4.516 (4.57)  Time: 0.167s, 6116.24/s  (0.196s, 5237.84/s)  LR: 9.568e-04  Data: 0.023 (0.047)
Train: 40 [ 750/1251 ( 60%)]  Loss: 4.261 (4.55)  Time: 0.169s, 6043.52/s  (0.195s, 5238.28/s)  LR: 9.568e-04  Data: 0.026 (0.047)
Train: 40 [ 800/1251 ( 64%)]  Loss: 4.098 (4.52)  Time: 0.257s, 3981.79/s  (0.195s, 5238.89/s)  LR: 9.568e-04  Data: 0.132 (0.048)
Train: 40 [ 850/1251 ( 68%)]  Loss: 4.327 (4.51)  Time: 0.184s, 5576.54/s  (0.195s, 5241.71/s)  LR: 9.568e-04  Data: 0.026 (0.047)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 40 [ 900/1251 ( 72%)]  Loss: 4.467 (4.51)  Time: 0.172s, 5940.73/s  (0.195s, 5243.78/s)  LR: 9.568e-04  Data: 0.025 (0.046)
Train: 40 [ 950/1251 ( 76%)]  Loss: 4.208 (4.49)  Time: 0.168s, 6104.33/s  (0.195s, 5249.91/s)  LR: 9.568e-04  Data: 0.030 (0.045)
Train: 40 [1000/1251 ( 80%)]  Loss: 4.844 (4.51)  Time: 0.158s, 6494.99/s  (0.196s, 5234.64/s)  LR: 9.568e-04  Data: 0.032 (0.044)
Train: 40 [1050/1251 ( 84%)]  Loss: 4.445 (4.51)  Time: 0.160s, 6411.23/s  (0.196s, 5236.56/s)  LR: 9.568e-04  Data: 0.024 (0.043)
Train: 40 [1100/1251 ( 88%)]  Loss: 4.599 (4.51)  Time: 0.171s, 5981.44/s  (0.195s, 5240.31/s)  LR: 9.568e-04  Data: 0.019 (0.043)
Train: 40 [1150/1251 ( 92%)]  Loss: 4.537 (4.51)  Time: 0.331s, 3092.78/s  (0.195s, 5240.59/s)  LR: 9.568e-04  Data: 0.191 (0.043)
Train: 40 [1200/1251 ( 96%)]  Loss: 4.744 (4.52)  Time: 0.182s, 5624.21/s  (0.196s, 5217.57/s)  LR: 9.568e-04  Data: 0.035 (0.044)
Train: 40 [1250/1251 (100%)]  Loss: 4.440 (4.52)  Time: 0.114s, 8988.69/s  (0.195s, 5240.75/s)  LR: 9.568e-04  Data: 0.000 (0.043)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.924 (1.924)  Loss:  1.2202 (1.2202)  Acc@1: 77.8320 (77.8320)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.019 (0.225)  Loss:  1.3308 (1.9965)  Acc@1: 76.2972 (58.9900)  Acc@5: 89.9764 (82.0460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-30.pth.tar', 56.75599990234375)

Train: 41 [   0/1251 (  0%)]  Loss: 4.470 (4.47)  Time: 1.774s,  577.33/s  (1.774s,  577.33/s)  LR: 9.547e-04  Data: 1.658 (1.658)
Train: 41 [  50/1251 (  4%)]  Loss: 4.347 (4.41)  Time: 0.175s, 5836.22/s  (0.225s, 4557.34/s)  LR: 9.547e-04  Data: 0.033 (0.081)
Train: 41 [ 100/1251 (  8%)]  Loss: 4.761 (4.53)  Time: 0.164s, 6228.43/s  (0.211s, 4845.02/s)  LR: 9.547e-04  Data: 0.026 (0.060)
Train: 41 [ 150/1251 ( 12%)]  Loss: 4.177 (4.44)  Time: 0.182s, 5618.12/s  (0.206s, 4969.05/s)  LR: 9.547e-04  Data: 0.059 (0.050)
Train: 41 [ 200/1251 ( 16%)]  Loss: 4.460 (4.44)  Time: 0.177s, 5770.81/s  (0.202s, 5060.86/s)  LR: 9.547e-04  Data: 0.025 (0.045)
Train: 41 [ 250/1251 ( 20%)]  Loss: 4.770 (4.50)  Time: 0.181s, 5656.37/s  (0.203s, 5050.05/s)  LR: 9.547e-04  Data: 0.027 (0.041)
Train: 41 [ 300/1251 ( 24%)]  Loss: 4.386 (4.48)  Time: 0.180s, 5677.58/s  (0.200s, 5115.82/s)  LR: 9.547e-04  Data: 0.029 (0.039)
Train: 41 [ 350/1251 ( 28%)]  Loss: 4.428 (4.48)  Time: 0.169s, 6042.45/s  (0.199s, 5157.78/s)  LR: 9.547e-04  Data: 0.025 (0.037)
Train: 41 [ 400/1251 ( 32%)]  Loss: 4.110 (4.43)  Time: 0.156s, 6583.91/s  (0.197s, 5184.96/s)  LR: 9.547e-04  Data: 0.022 (0.036)
Train: 41 [ 450/1251 ( 36%)]  Loss: 4.284 (4.42)  Time: 0.197s, 5208.08/s  (0.197s, 5198.40/s)  LR: 9.547e-04  Data: 0.026 (0.035)
Train: 41 [ 500/1251 ( 40%)]  Loss: 4.636 (4.44)  Time: 0.191s, 5374.62/s  (0.196s, 5222.33/s)  LR: 9.547e-04  Data: 0.043 (0.035)
Train: 41 [ 550/1251 ( 44%)]  Loss: 4.687 (4.46)  Time: 0.527s, 1943.99/s  (0.198s, 5182.82/s)  LR: 9.547e-04  Data: 0.028 (0.034)
Train: 41 [ 600/1251 ( 48%)]  Loss: 4.419 (4.46)  Time: 0.169s, 6062.08/s  (0.197s, 5196.35/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 650/1251 ( 52%)]  Loss: 4.827 (4.48)  Time: 0.173s, 5925.71/s  (0.197s, 5200.57/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 700/1251 ( 56%)]  Loss: 4.020 (4.45)  Time: 0.171s, 5994.67/s  (0.197s, 5211.11/s)  LR: 9.547e-04  Data: 0.033 (0.033)
Train: 41 [ 750/1251 ( 60%)]  Loss: 4.393 (4.45)  Time: 0.346s, 2961.55/s  (0.196s, 5217.91/s)  LR: 9.547e-04  Data: 0.033 (0.032)
Train: 41 [ 800/1251 ( 64%)]  Loss: 4.288 (4.44)  Time: 0.185s, 5540.45/s  (0.196s, 5222.14/s)  LR: 9.547e-04  Data: 0.033 (0.032)
Train: 41 [ 850/1251 ( 68%)]  Loss: 4.585 (4.45)  Time: 0.187s, 5490.12/s  (0.196s, 5227.79/s)  LR: 9.547e-04  Data: 0.020 (0.032)
Train: 41 [ 900/1251 ( 72%)]  Loss: 4.801 (4.47)  Time: 0.167s, 6146.97/s  (0.196s, 5231.20/s)  LR: 9.547e-04  Data: 0.024 (0.033)
Train: 41 [ 950/1251 ( 76%)]  Loss: 4.770 (4.48)  Time: 0.188s, 5448.16/s  (0.197s, 5188.59/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1000/1251 ( 80%)]  Loss: 4.665 (4.49)  Time: 0.191s, 5360.67/s  (0.198s, 5184.76/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1050/1251 ( 84%)]  Loss: 4.877 (4.51)  Time: 0.156s, 6546.73/s  (0.197s, 5191.77/s)  LR: 9.547e-04  Data: 0.024 (0.032)
Train: 41 [1100/1251 ( 88%)]  Loss: 4.437 (4.50)  Time: 0.189s, 5423.99/s  (0.197s, 5185.15/s)  LR: 9.547e-04  Data: 0.025 (0.032)
Train: 41 [1150/1251 ( 92%)]  Loss: 4.349 (4.50)  Time: 0.194s, 5267.79/s  (0.197s, 5189.00/s)  LR: 9.547e-04  Data: 0.026 (0.032)
Train: 41 [1200/1251 ( 96%)]  Loss: 4.548 (4.50)  Time: 0.156s, 6561.89/s  (0.197s, 5185.15/s)  LR: 9.547e-04  Data: 0.028 (0.031)
Train: 41 [1250/1251 (100%)]  Loss: 4.827 (4.51)  Time: 0.113s, 9033.99/s  (0.197s, 5200.97/s)  LR: 9.547e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.797 (1.797)  Loss:  1.3068 (1.3068)  Acc@1: 75.0977 (75.0977)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2053 (2.0709)  Acc@1: 79.2453 (58.8100)  Acc@5: 92.6887 (82.0060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-31.pth.tar', 56.92600005859375)

Train: 42 [   0/1251 (  0%)]  Loss: 4.621 (4.62)  Time: 1.980s,  517.07/s  (1.980s,  517.07/s)  LR: 9.525e-04  Data: 1.836 (1.836)
Train: 42 [  50/1251 (  4%)]  Loss: 4.281 (4.45)  Time: 0.151s, 6799.74/s  (0.228s, 4489.39/s)  LR: 9.525e-04  Data: 0.023 (0.081)
Train: 42 [ 100/1251 (  8%)]  Loss: 4.729 (4.54)  Time: 0.180s, 5687.61/s  (0.207s, 4954.55/s)  LR: 9.525e-04  Data: 0.031 (0.059)
Train: 42 [ 150/1251 ( 12%)]  Loss: 4.306 (4.48)  Time: 0.168s, 6096.94/s  (0.204s, 5014.01/s)  LR: 9.525e-04  Data: 0.027 (0.057)
Train: 42 [ 200/1251 ( 16%)]  Loss: 4.377 (4.46)  Time: 0.160s, 6412.55/s  (0.201s, 5098.30/s)  LR: 9.525e-04  Data: 0.023 (0.054)
Train: 42 [ 250/1251 ( 20%)]  Loss: 4.721 (4.51)  Time: 0.218s, 4688.67/s  (0.198s, 5160.23/s)  LR: 9.525e-04  Data: 0.020 (0.051)
Train: 42 [ 300/1251 ( 24%)]  Loss: 4.816 (4.55)  Time: 0.175s, 5865.21/s  (0.197s, 5191.50/s)  LR: 9.525e-04  Data: 0.024 (0.050)
Train: 42 [ 350/1251 ( 28%)]  Loss: 4.213 (4.51)  Time: 0.179s, 5730.34/s  (0.197s, 5203.19/s)  LR: 9.525e-04  Data: 0.023 (0.049)
Train: 42 [ 400/1251 ( 32%)]  Loss: 4.675 (4.53)  Time: 0.174s, 5892.53/s  (0.196s, 5232.18/s)  LR: 9.525e-04  Data: 0.020 (0.046)
Train: 42 [ 450/1251 ( 36%)]  Loss: 4.580 (4.53)  Time: 1.238s,  827.45/s  (0.198s, 5181.86/s)  LR: 9.525e-04  Data: 0.019 (0.044)
Train: 42 [ 500/1251 ( 40%)]  Loss: 4.871 (4.56)  Time: 0.162s, 6331.24/s  (0.197s, 5198.98/s)  LR: 9.525e-04  Data: 0.033 (0.042)
Train: 42 [ 550/1251 ( 44%)]  Loss: 4.637 (4.57)  Time: 0.167s, 6118.40/s  (0.196s, 5217.76/s)  LR: 9.525e-04  Data: 0.026 (0.041)
Train: 42 [ 600/1251 ( 48%)]  Loss: 4.880 (4.59)  Time: 0.156s, 6549.71/s  (0.197s, 5208.09/s)  LR: 9.525e-04  Data: 0.025 (0.043)
Train: 42 [ 650/1251 ( 52%)]  Loss: 4.369 (4.58)  Time: 0.174s, 5890.48/s  (0.196s, 5230.36/s)  LR: 9.525e-04  Data: 0.029 (0.043)
Train: 42 [ 700/1251 ( 56%)]  Loss: 4.428 (4.57)  Time: 0.179s, 5736.32/s  (0.196s, 5225.34/s)  LR: 9.525e-04  Data: 0.028 (0.042)
Train: 42 [ 750/1251 ( 60%)]  Loss: 4.800 (4.58)  Time: 0.174s, 5888.44/s  (0.196s, 5223.20/s)  LR: 9.525e-04  Data: 0.024 (0.041)
Train: 42 [ 800/1251 ( 64%)]  Loss: 4.716 (4.59)  Time: 0.176s, 5820.84/s  (0.196s, 5219.92/s)  LR: 9.525e-04  Data: 0.031 (0.040)
Train: 42 [ 850/1251 ( 68%)]  Loss: 4.994 (4.61)  Time: 0.190s, 5396.72/s  (0.196s, 5221.91/s)  LR: 9.525e-04  Data: 0.059 (0.040)
Train: 42 [ 900/1251 ( 72%)]  Loss: 4.676 (4.62)  Time: 0.168s, 6087.12/s  (0.196s, 5214.77/s)  LR: 9.525e-04  Data: 0.028 (0.039)
Train: 42 [ 950/1251 ( 76%)]  Loss: 4.347 (4.60)  Time: 0.178s, 5754.20/s  (0.196s, 5219.44/s)  LR: 9.525e-04  Data: 0.029 (0.038)
Train: 42 [1000/1251 ( 80%)]  Loss: 4.346 (4.59)  Time: 0.156s, 6561.62/s  (0.196s, 5229.99/s)  LR: 9.525e-04  Data: 0.027 (0.038)
Train: 42 [1050/1251 ( 84%)]  Loss: 4.254 (4.57)  Time: 0.182s, 5615.03/s  (0.195s, 5241.82/s)  LR: 9.525e-04  Data: 0.040 (0.038)
Train: 42 [1100/1251 ( 88%)]  Loss: 4.442 (4.57)  Time: 0.163s, 6290.74/s  (0.196s, 5230.20/s)  LR: 9.525e-04  Data: 0.019 (0.037)
Train: 42 [1150/1251 ( 92%)]  Loss: 4.326 (4.56)  Time: 0.155s, 6591.96/s  (0.196s, 5234.41/s)  LR: 9.525e-04  Data: 0.030 (0.037)
Train: 42 [1200/1251 ( 96%)]  Loss: 4.285 (4.55)  Time: 0.225s, 4557.84/s  (0.196s, 5227.40/s)  LR: 9.525e-04  Data: 0.036 (0.036)
Train: 42 [1250/1251 (100%)]  Loss: 4.615 (4.55)  Time: 0.113s, 9041.62/s  (0.195s, 5242.08/s)  LR: 9.525e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.879 (1.879)  Loss:  1.4100 (1.4100)  Acc@1: 73.7305 (73.7305)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2896 (2.0460)  Acc@1: 76.8868 (58.5360)  Acc@5: 90.6840 (81.7980)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-33.pth.tar', 57.01399997558594)

Train: 43 [   0/1251 (  0%)]  Loss: 4.545 (4.55)  Time: 1.822s,  561.92/s  (1.822s,  561.92/s)  LR: 9.502e-04  Data: 1.616 (1.616)
Train: 43 [  50/1251 (  4%)]  Loss: 4.831 (4.69)  Time: 0.170s, 6035.17/s  (0.226s, 4535.49/s)  LR: 9.502e-04  Data: 0.020 (0.080)
Train: 43 [ 100/1251 (  8%)]  Loss: 4.315 (4.56)  Time: 0.181s, 5662.02/s  (0.207s, 4936.55/s)  LR: 9.502e-04  Data: 0.029 (0.062)
Train: 43 [ 150/1251 ( 12%)]  Loss: 4.481 (4.54)  Time: 0.189s, 5406.34/s  (0.200s, 5119.19/s)  LR: 9.502e-04  Data: 0.026 (0.052)
Train: 43 [ 200/1251 ( 16%)]  Loss: 4.163 (4.47)  Time: 0.169s, 6060.88/s  (0.200s, 5120.47/s)  LR: 9.502e-04  Data: 0.024 (0.046)
Train: 43 [ 250/1251 ( 20%)]  Loss: 4.421 (4.46)  Time: 0.167s, 6147.73/s  (0.198s, 5179.84/s)  LR: 9.502e-04  Data: 0.029 (0.042)
Train: 43 [ 300/1251 ( 24%)]  Loss: 4.542 (4.47)  Time: 0.172s, 5952.94/s  (0.196s, 5228.32/s)  LR: 9.502e-04  Data: 0.021 (0.040)
Train: 43 [ 350/1251 ( 28%)]  Loss: 4.634 (4.49)  Time: 0.159s, 6454.74/s  (0.195s, 5253.59/s)  LR: 9.502e-04  Data: 0.026 (0.038)
Train: 43 [ 400/1251 ( 32%)]  Loss: 4.746 (4.52)  Time: 0.165s, 6204.98/s  (0.194s, 5286.81/s)  LR: 9.502e-04  Data: 0.020 (0.037)
Train: 43 [ 450/1251 ( 36%)]  Loss: 4.397 (4.51)  Time: 0.163s, 6300.72/s  (0.193s, 5295.80/s)  LR: 9.502e-04  Data: 0.029 (0.036)
Train: 43 [ 500/1251 ( 40%)]  Loss: 4.633 (4.52)  Time: 0.169s, 6048.53/s  (0.194s, 5283.64/s)  LR: 9.502e-04  Data: 0.031 (0.035)
Train: 43 [ 550/1251 ( 44%)]  Loss: 4.482 (4.52)  Time: 0.279s, 3670.83/s  (0.193s, 5297.45/s)  LR: 9.502e-04  Data: 0.031 (0.034)
Train: 43 [ 600/1251 ( 48%)]  Loss: 4.181 (4.49)  Time: 0.168s, 6093.70/s  (0.194s, 5278.24/s)  LR: 9.502e-04  Data: 0.025 (0.036)
Train: 43 [ 650/1251 ( 52%)]  Loss: 4.338 (4.48)  Time: 0.168s, 6098.16/s  (0.193s, 5299.22/s)  LR: 9.502e-04  Data: 0.027 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 43 [ 700/1251 ( 56%)]  Loss: 4.507 (4.48)  Time: 0.183s, 5609.69/s  (0.193s, 5313.95/s)  LR: 9.502e-04  Data: 0.026 (0.037)
Train: 43 [ 750/1251 ( 60%)]  Loss: 4.962 (4.51)  Time: 0.292s, 3506.71/s  (0.193s, 5310.93/s)  LR: 9.502e-04  Data: 0.179 (0.038)
Train: 43 [ 800/1251 ( 64%)]  Loss: 4.548 (4.51)  Time: 0.165s, 6220.36/s  (0.194s, 5289.09/s)  LR: 9.502e-04  Data: 0.025 (0.038)
Train: 43 [ 850/1251 ( 68%)]  Loss: 4.544 (4.51)  Time: 0.179s, 5735.20/s  (0.193s, 5305.18/s)  LR: 9.502e-04  Data: 0.033 (0.037)
Train: 43 [ 900/1251 ( 72%)]  Loss: 4.896 (4.54)  Time: 0.179s, 5711.83/s  (0.193s, 5303.95/s)  LR: 9.502e-04  Data: 0.031 (0.037)
Train: 43 [ 950/1251 ( 76%)]  Loss: 4.432 (4.53)  Time: 0.203s, 5054.02/s  (0.193s, 5295.42/s)  LR: 9.502e-04  Data: 0.037 (0.037)
Train: 43 [1000/1251 ( 80%)]  Loss: 4.611 (4.53)  Time: 0.165s, 6189.18/s  (0.193s, 5293.78/s)  LR: 9.502e-04  Data: 0.027 (0.036)
Train: 43 [1050/1251 ( 84%)]  Loss: 4.747 (4.54)  Time: 0.155s, 6594.60/s  (0.194s, 5280.03/s)  LR: 9.502e-04  Data: 0.022 (0.036)
Train: 43 [1100/1251 ( 88%)]  Loss: 4.766 (4.55)  Time: 0.168s, 6086.70/s  (0.194s, 5290.51/s)  LR: 9.502e-04  Data: 0.025 (0.035)
Train: 43 [1150/1251 ( 92%)]  Loss: 4.846 (4.57)  Time: 0.257s, 3985.02/s  (0.194s, 5286.76/s)  LR: 9.502e-04  Data: 0.029 (0.035)
Train: 43 [1200/1251 ( 96%)]  Loss: 4.700 (4.57)  Time: 0.162s, 6329.52/s  (0.194s, 5290.92/s)  LR: 9.502e-04  Data: 0.027 (0.035)
Train: 43 [1250/1251 (100%)]  Loss: 4.463 (4.57)  Time: 0.114s, 8991.40/s  (0.193s, 5305.59/s)  LR: 9.502e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.901 (1.901)  Loss:  1.2677 (1.2677)  Acc@1: 75.2930 (75.2930)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3233 (2.0123)  Acc@1: 75.9434 (59.2380)  Acc@5: 91.0377 (82.5200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-35.pth.tar', 57.394)

Train: 44 [   0/1251 (  0%)]  Loss: 4.448 (4.45)  Time: 1.800s,  568.86/s  (1.800s,  568.86/s)  LR: 9.479e-04  Data: 1.631 (1.631)
Train: 44 [  50/1251 (  4%)]  Loss: 4.321 (4.38)  Time: 0.160s, 6403.19/s  (0.221s, 4642.10/s)  LR: 9.479e-04  Data: 0.027 (0.073)
Train: 44 [ 100/1251 (  8%)]  Loss: 4.941 (4.57)  Time: 0.168s, 6087.12/s  (0.206s, 4979.45/s)  LR: 9.479e-04  Data: 0.028 (0.060)
Train: 44 [ 150/1251 ( 12%)]  Loss: 4.406 (4.53)  Time: 0.156s, 6572.35/s  (0.200s, 5127.32/s)  LR: 9.479e-04  Data: 0.026 (0.053)
Train: 44 [ 200/1251 ( 16%)]  Loss: 4.539 (4.53)  Time: 0.159s, 6428.77/s  (0.197s, 5193.37/s)  LR: 9.479e-04  Data: 0.028 (0.047)
Train: 44 [ 250/1251 ( 20%)]  Loss: 4.243 (4.48)  Time: 0.170s, 6025.82/s  (0.196s, 5225.24/s)  LR: 9.479e-04  Data: 0.027 (0.043)
Train: 44 [ 300/1251 ( 24%)]  Loss: 4.657 (4.51)  Time: 0.197s, 5211.14/s  (0.195s, 5240.98/s)  LR: 9.479e-04  Data: 0.031 (0.041)
Train: 44 [ 350/1251 ( 28%)]  Loss: 4.347 (4.49)  Time: 0.180s, 5691.02/s  (0.195s, 5239.85/s)  LR: 9.479e-04  Data: 0.032 (0.039)
Train: 44 [ 400/1251 ( 32%)]  Loss: 4.434 (4.48)  Time: 0.170s, 6039.80/s  (0.195s, 5246.78/s)  LR: 9.479e-04  Data: 0.028 (0.038)
Train: 44 [ 450/1251 ( 36%)]  Loss: 4.530 (4.49)  Time: 0.196s, 5237.16/s  (0.194s, 5270.18/s)  LR: 9.479e-04  Data: 0.025 (0.037)
Train: 44 [ 500/1251 ( 40%)]  Loss: 4.514 (4.49)  Time: 0.164s, 6235.54/s  (0.194s, 5288.01/s)  LR: 9.479e-04  Data: 0.026 (0.036)
Train: 44 [ 550/1251 ( 44%)]  Loss: 4.347 (4.48)  Time: 0.169s, 6045.71/s  (0.194s, 5286.43/s)  LR: 9.479e-04  Data: 0.024 (0.036)
Train: 44 [ 600/1251 ( 48%)]  Loss: 4.815 (4.50)  Time: 0.155s, 6612.01/s  (0.194s, 5291.98/s)  LR: 9.479e-04  Data: 0.029 (0.035)
Train: 44 [ 650/1251 ( 52%)]  Loss: 4.201 (4.48)  Time: 0.177s, 5777.42/s  (0.193s, 5297.19/s)  LR: 9.479e-04  Data: 0.025 (0.035)
Train: 44 [ 700/1251 ( 56%)]  Loss: 4.686 (4.50)  Time: 0.190s, 5400.33/s  (0.194s, 5287.17/s)  LR: 9.479e-04  Data: 0.027 (0.034)
Train: 44 [ 750/1251 ( 60%)]  Loss: 4.701 (4.51)  Time: 0.170s, 6035.91/s  (0.193s, 5292.76/s)  LR: 9.479e-04  Data: 0.029 (0.034)
Train: 44 [ 800/1251 ( 64%)]  Loss: 4.627 (4.52)  Time: 0.152s, 6741.02/s  (0.193s, 5304.59/s)  LR: 9.479e-04  Data: 0.031 (0.034)
Train: 44 [ 850/1251 ( 68%)]  Loss: 4.513 (4.52)  Time: 0.169s, 6057.31/s  (0.193s, 5302.77/s)  LR: 9.479e-04  Data: 0.027 (0.033)
Train: 44 [ 900/1251 ( 72%)]  Loss: 4.681 (4.52)  Time: 0.195s, 5254.15/s  (0.193s, 5301.22/s)  LR: 9.479e-04  Data: 0.032 (0.033)
Train: 44 [ 950/1251 ( 76%)]  Loss: 4.647 (4.53)  Time: 0.180s, 5700.63/s  (0.193s, 5295.96/s)  LR: 9.479e-04  Data: 0.022 (0.033)
Train: 44 [1000/1251 ( 80%)]  Loss: 4.623 (4.53)  Time: 0.165s, 6205.34/s  (0.193s, 5304.04/s)  LR: 9.479e-04  Data: 0.027 (0.033)
Train: 44 [1050/1251 ( 84%)]  Loss: 4.576 (4.54)  Time: 0.205s, 5000.34/s  (0.193s, 5300.56/s)  LR: 9.479e-04  Data: 0.033 (0.032)
Train: 44 [1100/1251 ( 88%)]  Loss: 4.340 (4.53)  Time: 0.170s, 6016.15/s  (0.193s, 5292.24/s)  LR: 9.479e-04  Data: 0.024 (0.033)
Train: 44 [1150/1251 ( 92%)]  Loss: 4.372 (4.52)  Time: 0.170s, 6016.46/s  (0.194s, 5286.94/s)  LR: 9.479e-04  Data: 0.024 (0.034)
Train: 44 [1200/1251 ( 96%)]  Loss: 4.587 (4.52)  Time: 0.174s, 5868.55/s  (0.194s, 5289.81/s)  LR: 9.479e-04  Data: 0.023 (0.034)
Train: 44 [1250/1251 (100%)]  Loss: 4.165 (4.51)  Time: 0.114s, 9010.72/s  (0.193s, 5303.31/s)  LR: 9.479e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.815 (1.815)  Loss:  1.3240 (1.3240)  Acc@1: 75.0000 (75.0000)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.019 (0.211)  Loss:  1.3077 (1.9754)  Acc@1: 75.5896 (59.4540)  Acc@5: 89.6226 (82.6480)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-32.pth.tar', 57.39400002197266)

Train: 45 [   0/1251 (  0%)]  Loss: 4.566 (4.57)  Time: 1.856s,  551.73/s  (1.856s,  551.73/s)  LR: 9.456e-04  Data: 1.725 (1.725)
Train: 45 [  50/1251 (  4%)]  Loss: 4.586 (4.58)  Time: 0.166s, 6177.53/s  (0.225s, 4545.95/s)  LR: 9.456e-04  Data: 0.025 (0.069)
Train: 45 [ 100/1251 (  8%)]  Loss: 4.313 (4.49)  Time: 0.161s, 6344.90/s  (0.207s, 4958.06/s)  LR: 9.456e-04  Data: 0.031 (0.049)
Train: 45 [ 150/1251 ( 12%)]  Loss: 4.823 (4.57)  Time: 0.192s, 5335.17/s  (0.202s, 5063.61/s)  LR: 9.456e-04  Data: 0.022 (0.044)
Train: 45 [ 200/1251 ( 16%)]  Loss: 5.011 (4.66)  Time: 0.168s, 6101.84/s  (0.197s, 5185.72/s)  LR: 9.456e-04  Data: 0.026 (0.042)
Train: 45 [ 250/1251 ( 20%)]  Loss: 4.306 (4.60)  Time: 0.146s, 7033.65/s  (0.196s, 5226.78/s)  LR: 9.456e-04  Data: 0.025 (0.042)
Train: 45 [ 300/1251 ( 24%)]  Loss: 4.384 (4.57)  Time: 0.159s, 6423.55/s  (0.196s, 5233.27/s)  LR: 9.456e-04  Data: 0.028 (0.042)
Train: 45 [ 350/1251 ( 28%)]  Loss: 4.653 (4.58)  Time: 0.175s, 5846.17/s  (0.195s, 5250.76/s)  LR: 9.456e-04  Data: 0.030 (0.041)
Train: 45 [ 400/1251 ( 32%)]  Loss: 4.847 (4.61)  Time: 0.163s, 6263.45/s  (0.195s, 5258.75/s)  LR: 9.456e-04  Data: 0.026 (0.039)
Train: 45 [ 450/1251 ( 36%)]  Loss: 4.657 (4.61)  Time: 0.184s, 5579.29/s  (0.194s, 5265.06/s)  LR: 9.456e-04  Data: 0.025 (0.038)
Train: 45 [ 500/1251 ( 40%)]  Loss: 4.417 (4.60)  Time: 0.182s, 5614.97/s  (0.193s, 5292.02/s)  LR: 9.456e-04  Data: 0.029 (0.037)
Train: 45 [ 550/1251 ( 44%)]  Loss: 4.101 (4.56)  Time: 0.158s, 6487.98/s  (0.193s, 5300.05/s)  LR: 9.456e-04  Data: 0.026 (0.036)
Train: 45 [ 600/1251 ( 48%)]  Loss: 4.785 (4.57)  Time: 0.194s, 5274.56/s  (0.193s, 5294.31/s)  LR: 9.456e-04  Data: 0.028 (0.036)
Train: 45 [ 650/1251 ( 52%)]  Loss: 4.728 (4.58)  Time: 0.178s, 5754.13/s  (0.194s, 5286.67/s)  LR: 9.456e-04  Data: 0.022 (0.035)
Train: 45 [ 700/1251 ( 56%)]  Loss: 4.720 (4.59)  Time: 0.180s, 5682.72/s  (0.194s, 5273.87/s)  LR: 9.456e-04  Data: 0.029 (0.035)
Train: 45 [ 750/1251 ( 60%)]  Loss: 4.753 (4.60)  Time: 0.177s, 5796.39/s  (0.193s, 5301.70/s)  LR: 9.456e-04  Data: 0.032 (0.034)
Train: 45 [ 800/1251 ( 64%)]  Loss: 4.446 (4.59)  Time: 0.177s, 5777.05/s  (0.194s, 5289.25/s)  LR: 9.456e-04  Data: 0.040 (0.034)
Train: 45 [ 850/1251 ( 68%)]  Loss: 4.590 (4.59)  Time: 0.181s, 5645.78/s  (0.194s, 5286.91/s)  LR: 9.456e-04  Data: 0.031 (0.034)
Train: 45 [ 900/1251 ( 72%)]  Loss: 4.263 (4.58)  Time: 0.165s, 6217.06/s  (0.194s, 5283.54/s)  LR: 9.456e-04  Data: 0.027 (0.033)
Train: 45 [ 950/1251 ( 76%)]  Loss: 4.441 (4.57)  Time: 0.167s, 6117.71/s  (0.194s, 5279.82/s)  LR: 9.456e-04  Data: 0.031 (0.033)
Train: 45 [1000/1251 ( 80%)]  Loss: 4.700 (4.58)  Time: 0.186s, 5513.79/s  (0.194s, 5268.21/s)  LR: 9.456e-04  Data: 0.024 (0.033)
Train: 45 [1050/1251 ( 84%)]  Loss: 4.484 (4.57)  Time: 0.190s, 5379.70/s  (0.194s, 5266.52/s)  LR: 9.456e-04  Data: 0.064 (0.033)
Train: 45 [1100/1251 ( 88%)]  Loss: 4.567 (4.57)  Time: 0.194s, 5268.93/s  (0.195s, 5260.91/s)  LR: 9.456e-04  Data: 0.034 (0.032)
Train: 45 [1150/1251 ( 92%)]  Loss: 4.769 (4.58)  Time: 0.161s, 6375.80/s  (0.194s, 5268.64/s)  LR: 9.456e-04  Data: 0.031 (0.032)
Train: 45 [1200/1251 ( 96%)]  Loss: 4.946 (4.59)  Time: 0.163s, 6270.29/s  (0.195s, 5259.61/s)  LR: 9.456e-04  Data: 0.029 (0.032)
Train: 45 [1250/1251 (100%)]  Loss: 4.809 (4.60)  Time: 0.113s, 9024.44/s  (0.194s, 5272.63/s)  LR: 9.456e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.899 (1.899)  Loss:  1.3263 (1.3263)  Acc@1: 75.6836 (75.6836)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3491 (2.0973)  Acc@1: 75.1179 (58.4420)  Acc@5: 90.5660 (82.0240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-45.pth.tar', 58.441999921875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-34.pth.tar', 58.275999919433595)

Train: 46 [   0/1251 (  0%)]  Loss: 4.509 (4.51)  Time: 1.701s,  601.87/s  (1.701s,  601.87/s)  LR: 9.432e-04  Data: 1.569 (1.569)
Train: 46 [  50/1251 (  4%)]  Loss: 4.392 (4.45)  Time: 0.161s, 6367.05/s  (0.223s, 4597.92/s)  LR: 9.432e-04  Data: 0.029 (0.063)
Train: 46 [ 100/1251 (  8%)]  Loss: 4.523 (4.47)  Time: 0.164s, 6237.95/s  (0.207s, 4935.35/s)  LR: 9.432e-04  Data: 0.029 (0.046)
Train: 46 [ 150/1251 ( 12%)]  Loss: 4.698 (4.53)  Time: 0.163s, 6277.80/s  (0.202s, 5062.99/s)  LR: 9.432e-04  Data: 0.034 (0.042)
Train: 46 [ 200/1251 ( 16%)]  Loss: 4.334 (4.49)  Time: 0.140s, 7305.21/s  (0.199s, 5145.45/s)  LR: 9.432e-04  Data: 0.023 (0.039)
Train: 46 [ 250/1251 ( 20%)]  Loss: 4.120 (4.43)  Time: 0.171s, 5988.01/s  (0.197s, 5206.58/s)  LR: 9.432e-04  Data: 0.026 (0.037)
Train: 46 [ 300/1251 ( 24%)]  Loss: 4.826 (4.49)  Time: 0.186s, 5519.35/s  (0.197s, 5191.67/s)  LR: 9.432e-04  Data: 0.041 (0.035)
Train: 46 [ 350/1251 ( 28%)]  Loss: 4.701 (4.51)  Time: 0.176s, 5828.58/s  (0.197s, 5202.51/s)  LR: 9.432e-04  Data: 0.026 (0.034)
Train: 46 [ 400/1251 ( 32%)]  Loss: 4.482 (4.51)  Time: 0.164s, 6247.47/s  (0.195s, 5240.09/s)  LR: 9.432e-04  Data: 0.032 (0.033)
Train: 46 [ 450/1251 ( 36%)]  Loss: 4.498 (4.51)  Time: 0.345s, 2968.62/s  (0.195s, 5258.30/s)  LR: 9.432e-04  Data: 0.025 (0.033)
Train: 46 [ 500/1251 ( 40%)]  Loss: 4.153 (4.48)  Time: 0.172s, 5936.56/s  (0.194s, 5272.59/s)  LR: 9.432e-04  Data: 0.022 (0.032)
Train: 46 [ 550/1251 ( 44%)]  Loss: 4.530 (4.48)  Time: 0.169s, 6055.46/s  (0.194s, 5265.25/s)  LR: 9.432e-04  Data: 0.023 (0.032)
Train: 46 [ 600/1251 ( 48%)]  Loss: 4.760 (4.50)  Time: 0.223s, 4600.30/s  (0.194s, 5280.13/s)  LR: 9.432e-04  Data: 0.026 (0.031)
Train: 46 [ 650/1251 ( 52%)]  Loss: 4.065 (4.47)  Time: 0.158s, 6499.82/s  (0.194s, 5284.93/s)  LR: 9.432e-04  Data: 0.027 (0.031)
Train: 46 [ 700/1251 ( 56%)]  Loss: 4.813 (4.49)  Time: 0.164s, 6228.29/s  (0.194s, 5277.31/s)  LR: 9.432e-04  Data: 0.024 (0.031)
Train: 46 [ 750/1251 ( 60%)]  Loss: 4.323 (4.48)  Time: 0.199s, 5136.19/s  (0.194s, 5274.29/s)  LR: 9.432e-04  Data: 0.025 (0.031)
Train: 46 [ 800/1251 ( 64%)]  Loss: 4.625 (4.49)  Time: 0.174s, 5877.54/s  (0.194s, 5287.98/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [ 850/1251 ( 68%)]  Loss: 4.460 (4.49)  Time: 0.202s, 5060.71/s  (0.194s, 5281.30/s)  LR: 9.432e-04  Data: 0.024 (0.030)
Train: 46 [ 900/1251 ( 72%)]  Loss: 4.510 (4.49)  Time: 0.194s, 5271.52/s  (0.194s, 5281.92/s)  LR: 9.432e-04  Data: 0.029 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 46 [ 950/1251 ( 76%)]  Loss: 4.466 (4.49)  Time: 0.158s, 6466.97/s  (0.194s, 5270.81/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [1000/1251 ( 80%)]  Loss: 4.768 (4.50)  Time: 0.164s, 6251.14/s  (0.194s, 5272.45/s)  LR: 9.432e-04  Data: 0.029 (0.030)
Train: 46 [1050/1251 ( 84%)]  Loss: 4.386 (4.50)  Time: 0.173s, 5915.15/s  (0.194s, 5268.56/s)  LR: 9.432e-04  Data: 0.026 (0.030)
Train: 46 [1100/1251 ( 88%)]  Loss: 4.508 (4.50)  Time: 0.160s, 6380.95/s  (0.194s, 5266.66/s)  LR: 9.432e-04  Data: 0.024 (0.030)
Train: 46 [1150/1251 ( 92%)]  Loss: 4.109 (4.48)  Time: 0.165s, 6193.35/s  (0.195s, 5261.24/s)  LR: 9.432e-04  Data: 0.029 (0.029)
Train: 46 [1200/1251 ( 96%)]  Loss: 4.126 (4.47)  Time: 0.221s, 4629.52/s  (0.195s, 5255.01/s)  LR: 9.432e-04  Data: 0.028 (0.029)
Train: 46 [1250/1251 (100%)]  Loss: 4.804 (4.48)  Time: 0.113s, 9057.08/s  (0.194s, 5270.18/s)  LR: 9.432e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.771 (1.771)  Loss:  1.3549 (1.3549)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.3843 (2.0625)  Acc@1: 78.0660 (59.4940)  Acc@5: 90.5660 (82.3840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-45.pth.tar', 58.441999921875)

Train: 47 [   0/1251 (  0%)]  Loss: 4.554 (4.55)  Time: 1.706s,  600.12/s  (1.706s,  600.12/s)  LR: 9.407e-04  Data: 1.580 (1.580)
Train: 47 [  50/1251 (  4%)]  Loss: 4.372 (4.46)  Time: 0.182s, 5634.43/s  (0.223s, 4588.47/s)  LR: 9.407e-04  Data: 0.024 (0.072)
Train: 47 [ 100/1251 (  8%)]  Loss: 4.607 (4.51)  Time: 0.185s, 5528.17/s  (0.209s, 4899.47/s)  LR: 9.407e-04  Data: 0.024 (0.055)
Train: 47 [ 150/1251 ( 12%)]  Loss: 4.668 (4.55)  Time: 0.185s, 5528.73/s  (0.203s, 5050.72/s)  LR: 9.407e-04  Data: 0.024 (0.046)
Train: 47 [ 200/1251 ( 16%)]  Loss: 4.350 (4.51)  Time: 0.179s, 5716.93/s  (0.200s, 5118.01/s)  LR: 9.407e-04  Data: 0.028 (0.042)
Train: 47 [ 250/1251 ( 20%)]  Loss: 4.429 (4.50)  Time: 0.161s, 6372.30/s  (0.199s, 5153.02/s)  LR: 9.407e-04  Data: 0.031 (0.040)
Train: 47 [ 300/1251 ( 24%)]  Loss: 4.657 (4.52)  Time: 0.161s, 6367.41/s  (0.196s, 5215.18/s)  LR: 9.407e-04  Data: 0.026 (0.038)
Train: 47 [ 350/1251 ( 28%)]  Loss: 4.525 (4.52)  Time: 0.175s, 5843.07/s  (0.197s, 5204.90/s)  LR: 9.407e-04  Data: 0.027 (0.036)
Train: 47 [ 400/1251 ( 32%)]  Loss: 4.637 (4.53)  Time: 0.168s, 6100.48/s  (0.196s, 5229.14/s)  LR: 9.407e-04  Data: 0.038 (0.036)
Train: 47 [ 450/1251 ( 36%)]  Loss: 4.285 (4.51)  Time: 0.202s, 5060.52/s  (0.196s, 5237.73/s)  LR: 9.407e-04  Data: 0.026 (0.035)
Train: 47 [ 500/1251 ( 40%)]  Loss: 4.480 (4.51)  Time: 0.156s, 6572.18/s  (0.194s, 5282.74/s)  LR: 9.407e-04  Data: 0.031 (0.034)
Train: 47 [ 550/1251 ( 44%)]  Loss: 4.530 (4.51)  Time: 0.192s, 5331.41/s  (0.194s, 5267.31/s)  LR: 9.407e-04  Data: 0.052 (0.034)
Train: 47 [ 600/1251 ( 48%)]  Loss: 3.996 (4.47)  Time: 0.189s, 5407.22/s  (0.194s, 5272.89/s)  LR: 9.407e-04  Data: 0.037 (0.033)
Train: 47 [ 650/1251 ( 52%)]  Loss: 4.308 (4.46)  Time: 0.174s, 5893.33/s  (0.194s, 5278.49/s)  LR: 9.407e-04  Data: 0.025 (0.033)
Train: 47 [ 700/1251 ( 56%)]  Loss: 4.458 (4.46)  Time: 0.157s, 6529.69/s  (0.194s, 5286.66/s)  LR: 9.407e-04  Data: 0.028 (0.033)
Train: 47 [ 750/1251 ( 60%)]  Loss: 4.374 (4.45)  Time: 0.390s, 2626.17/s  (0.194s, 5278.39/s)  LR: 9.407e-04  Data: 0.247 (0.035)
Train: 47 [ 800/1251 ( 64%)]  Loss: 4.577 (4.46)  Time: 0.178s, 5753.09/s  (0.194s, 5282.56/s)  LR: 9.407e-04  Data: 0.025 (0.035)
Train: 47 [ 850/1251 ( 68%)]  Loss: 4.510 (4.46)  Time: 0.160s, 6387.94/s  (0.194s, 5278.62/s)  LR: 9.407e-04  Data: 0.031 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 47 [ 900/1251 ( 72%)]  Loss: 4.319 (4.45)  Time: 0.278s, 3681.37/s  (0.194s, 5273.87/s)  LR: 9.407e-04  Data: 0.028 (0.035)
Train: 47 [ 950/1251 ( 76%)]  Loss: 4.457 (4.45)  Time: 0.189s, 5414.44/s  (0.194s, 5271.06/s)  LR: 9.407e-04  Data: 0.022 (0.034)
Train: 47 [1000/1251 ( 80%)]  Loss: 4.703 (4.47)  Time: 0.177s, 5786.88/s  (0.194s, 5267.57/s)  LR: 9.407e-04  Data: 0.027 (0.034)
Train: 47 [1050/1251 ( 84%)]  Loss: 4.631 (4.47)  Time: 0.186s, 5500.67/s  (0.194s, 5266.90/s)  LR: 9.407e-04  Data: 0.023 (0.034)
Train: 47 [1100/1251 ( 88%)]  Loss: 4.479 (4.47)  Time: 0.833s, 1229.76/s  (0.195s, 5250.69/s)  LR: 9.407e-04  Data: 0.024 (0.033)
Train: 47 [1150/1251 ( 92%)]  Loss: 4.116 (4.46)  Time: 0.163s, 6265.22/s  (0.196s, 5224.86/s)  LR: 9.407e-04  Data: 0.022 (0.033)
Train: 47 [1200/1251 ( 96%)]  Loss: 4.680 (4.47)  Time: 0.159s, 6460.17/s  (0.196s, 5216.21/s)  LR: 9.407e-04  Data: 0.018 (0.033)
Train: 47 [1250/1251 (100%)]  Loss: 4.346 (4.46)  Time: 0.114s, 9015.16/s  (0.196s, 5229.01/s)  LR: 9.407e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.833 (1.833)  Loss:  1.3168 (1.3168)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.2297 (1.9813)  Acc@1: 78.1840 (59.7660)  Acc@5: 90.8019 (82.8580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-39.pth.tar', 58.52200004638672)

Train: 48 [   0/1251 (  0%)]  Loss: 4.264 (4.26)  Time: 1.801s,  568.47/s  (1.801s,  568.47/s)  LR: 9.382e-04  Data: 1.681 (1.681)
Train: 48 [  50/1251 (  4%)]  Loss: 4.861 (4.56)  Time: 0.166s, 6156.04/s  (0.229s, 4478.79/s)  LR: 9.382e-04  Data: 0.036 (0.063)
Train: 48 [ 100/1251 (  8%)]  Loss: 4.562 (4.56)  Time: 0.184s, 5569.50/s  (0.208s, 4914.14/s)  LR: 9.382e-04  Data: 0.022 (0.049)
Train: 48 [ 150/1251 ( 12%)]  Loss: 4.825 (4.63)  Time: 0.182s, 5621.58/s  (0.204s, 5030.19/s)  LR: 9.382e-04  Data: 0.019 (0.041)
Train: 48 [ 200/1251 ( 16%)]  Loss: 4.619 (4.63)  Time: 0.180s, 5703.80/s  (0.200s, 5126.73/s)  LR: 9.382e-04  Data: 0.029 (0.038)
Train: 48 [ 250/1251 ( 20%)]  Loss: 4.226 (4.56)  Time: 0.174s, 5893.36/s  (0.196s, 5237.39/s)  LR: 9.382e-04  Data: 0.027 (0.036)
Train: 48 [ 300/1251 ( 24%)]  Loss: 4.320 (4.53)  Time: 0.167s, 6140.54/s  (0.195s, 5260.15/s)  LR: 9.382e-04  Data: 0.033 (0.035)
Train: 48 [ 350/1251 ( 28%)]  Loss: 4.568 (4.53)  Time: 0.163s, 6284.85/s  (0.194s, 5274.55/s)  LR: 9.382e-04  Data: 0.026 (0.033)
Train: 48 [ 400/1251 ( 32%)]  Loss: 4.687 (4.55)  Time: 0.187s, 5482.81/s  (0.195s, 5255.64/s)  LR: 9.382e-04  Data: 0.035 (0.033)
Train: 48 [ 450/1251 ( 36%)]  Loss: 4.266 (4.52)  Time: 0.180s, 5699.80/s  (0.194s, 5268.91/s)  LR: 9.382e-04  Data: 0.051 (0.032)
Train: 48 [ 500/1251 ( 40%)]  Loss: 4.686 (4.54)  Time: 0.181s, 5666.52/s  (0.195s, 5261.76/s)  LR: 9.382e-04  Data: 0.025 (0.032)
Train: 48 [ 550/1251 ( 44%)]  Loss: 4.537 (4.54)  Time: 0.179s, 5728.95/s  (0.194s, 5271.34/s)  LR: 9.382e-04  Data: 0.031 (0.031)
Train: 48 [ 600/1251 ( 48%)]  Loss: 4.443 (4.53)  Time: 0.172s, 5952.53/s  (0.195s, 5262.43/s)  LR: 9.382e-04  Data: 0.036 (0.031)
Train: 48 [ 650/1251 ( 52%)]  Loss: 4.370 (4.52)  Time: 0.176s, 5807.72/s  (0.194s, 5275.31/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [ 700/1251 ( 56%)]  Loss: 4.895 (4.54)  Time: 0.164s, 6246.40/s  (0.194s, 5284.56/s)  LR: 9.382e-04  Data: 0.029 (0.031)
Train: 48 [ 750/1251 ( 60%)]  Loss: 4.518 (4.54)  Time: 0.245s, 4181.67/s  (0.194s, 5283.66/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [ 800/1251 ( 64%)]  Loss: 4.755 (4.55)  Time: 0.248s, 4131.22/s  (0.193s, 5295.49/s)  LR: 9.382e-04  Data: 0.118 (0.031)
Train: 48 [ 850/1251 ( 68%)]  Loss: 4.404 (4.54)  Time: 0.199s, 5150.78/s  (0.194s, 5277.98/s)  LR: 9.382e-04  Data: 0.021 (0.031)
Train: 48 [ 900/1251 ( 72%)]  Loss: 4.087 (4.52)  Time: 0.181s, 5654.93/s  (0.194s, 5272.89/s)  LR: 9.382e-04  Data: 0.019 (0.030)
Train: 48 [ 950/1251 ( 76%)]  Loss: 4.896 (4.54)  Time: 0.182s, 5629.58/s  (0.194s, 5282.17/s)  LR: 9.382e-04  Data: 0.029 (0.030)
Train: 48 [1000/1251 ( 80%)]  Loss: 4.445 (4.54)  Time: 0.184s, 5578.40/s  (0.194s, 5282.54/s)  LR: 9.382e-04  Data: 0.032 (0.030)
Train: 48 [1050/1251 ( 84%)]  Loss: 4.702 (4.54)  Time: 0.183s, 5607.07/s  (0.194s, 5285.23/s)  LR: 9.382e-04  Data: 0.022 (0.031)
Train: 48 [1100/1251 ( 88%)]  Loss: 4.912 (4.56)  Time: 0.163s, 6297.92/s  (0.194s, 5267.17/s)  LR: 9.382e-04  Data: 0.024 (0.032)
Train: 48 [1150/1251 ( 92%)]  Loss: 4.311 (4.55)  Time: 0.192s, 5339.45/s  (0.195s, 5261.22/s)  LR: 9.382e-04  Data: 0.031 (0.033)
Train: 48 [1200/1251 ( 96%)]  Loss: 4.813 (4.56)  Time: 0.174s, 5899.45/s  (0.194s, 5272.02/s)  LR: 9.382e-04  Data: 0.027 (0.033)
Train: 48 [1250/1251 (100%)]  Loss: 4.336 (4.55)  Time: 0.137s, 7491.40/s  (0.194s, 5272.36/s)  LR: 9.382e-04  Data: 0.000 (0.033)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.847 (1.847)  Loss:  1.4455 (1.4455)  Acc@1: 76.6602 (76.6602)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3859 (2.0725)  Acc@1: 77.5943 (59.2380)  Acc@5: 91.0377 (82.5580)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-42.pth.tar', 58.536000043945315)

Train: 49 [   0/1251 (  0%)]  Loss: 4.590 (4.59)  Time: 1.842s,  556.00/s  (1.842s,  556.00/s)  LR: 9.357e-04  Data: 1.719 (1.719)
Train: 49 [  50/1251 (  4%)]  Loss: 4.801 (4.70)  Time: 0.183s, 5585.53/s  (0.229s, 4475.60/s)  LR: 9.357e-04  Data: 0.019 (0.079)
Train: 49 [ 100/1251 (  8%)]  Loss: 4.561 (4.65)  Time: 0.186s, 5495.12/s  (0.208s, 4925.52/s)  LR: 9.357e-04  Data: 0.024 (0.054)
Train: 49 [ 150/1251 ( 12%)]  Loss: 4.578 (4.63)  Time: 0.157s, 6513.38/s  (0.203s, 5041.32/s)  LR: 9.357e-04  Data: 0.026 (0.046)
Train: 49 [ 200/1251 ( 16%)]  Loss: 4.452 (4.60)  Time: 0.300s, 3410.54/s  (0.202s, 5057.54/s)  LR: 9.357e-04  Data: 0.023 (0.042)
Train: 49 [ 250/1251 ( 20%)]  Loss: 4.666 (4.61)  Time: 0.204s, 5016.35/s  (0.198s, 5166.09/s)  LR: 9.357e-04  Data: 0.025 (0.039)
Train: 49 [ 300/1251 ( 24%)]  Loss: 4.781 (4.63)  Time: 0.162s, 6317.06/s  (0.196s, 5220.99/s)  LR: 9.357e-04  Data: 0.029 (0.037)
Train: 49 [ 350/1251 ( 28%)]  Loss: 4.581 (4.63)  Time: 0.169s, 6068.72/s  (0.196s, 5220.18/s)  LR: 9.357e-04  Data: 0.038 (0.036)
Train: 49 [ 400/1251 ( 32%)]  Loss: 4.245 (4.58)  Time: 0.231s, 4428.82/s  (0.196s, 5227.60/s)  LR: 9.357e-04  Data: 0.025 (0.035)
Train: 49 [ 450/1251 ( 36%)]  Loss: 4.589 (4.58)  Time: 0.162s, 6316.77/s  (0.194s, 5267.96/s)  LR: 9.357e-04  Data: 0.024 (0.034)
Train: 49 [ 500/1251 ( 40%)]  Loss: 4.683 (4.59)  Time: 0.174s, 5876.51/s  (0.194s, 5266.94/s)  LR: 9.357e-04  Data: 0.028 (0.034)
Train: 49 [ 550/1251 ( 44%)]  Loss: 4.609 (4.59)  Time: 0.160s, 6415.38/s  (0.194s, 5264.90/s)  LR: 9.357e-04  Data: 0.030 (0.034)
Train: 49 [ 600/1251 ( 48%)]  Loss: 4.198 (4.56)  Time: 0.178s, 5758.78/s  (0.194s, 5266.70/s)  LR: 9.357e-04  Data: 0.025 (0.034)
Train: 49 [ 650/1251 ( 52%)]  Loss: 4.159 (4.54)  Time: 0.174s, 5878.59/s  (0.194s, 5270.61/s)  LR: 9.357e-04  Data: 0.027 (0.034)
Train: 49 [ 700/1251 ( 56%)]  Loss: 4.474 (4.53)  Time: 0.172s, 5946.65/s  (0.194s, 5272.41/s)  LR: 9.357e-04  Data: 0.022 (0.033)
Train: 49 [ 750/1251 ( 60%)]  Loss: 4.654 (4.54)  Time: 0.164s, 6232.43/s  (0.195s, 5259.16/s)  LR: 9.357e-04  Data: 0.040 (0.033)
Train: 49 [ 800/1251 ( 64%)]  Loss: 4.641 (4.54)  Time: 0.569s, 1801.07/s  (0.195s, 5249.90/s)  LR: 9.357e-04  Data: 0.025 (0.033)
Train: 49 [ 850/1251 ( 68%)]  Loss: 4.539 (4.54)  Time: 0.156s, 6557.02/s  (0.195s, 5256.39/s)  LR: 9.357e-04  Data: 0.020 (0.032)
Train: 49 [ 900/1251 ( 72%)]  Loss: 4.448 (4.54)  Time: 0.193s, 5301.82/s  (0.195s, 5249.94/s)  LR: 9.357e-04  Data: 0.029 (0.032)
Train: 49 [ 950/1251 ( 76%)]  Loss: 4.367 (4.53)  Time: 0.173s, 5912.14/s  (0.196s, 5231.20/s)  LR: 9.357e-04  Data: 0.028 (0.032)
Train: 49 [1000/1251 ( 80%)]  Loss: 4.372 (4.52)  Time: 0.183s, 5605.70/s  (0.196s, 5225.57/s)  LR: 9.357e-04  Data: 0.022 (0.032)
Train: 49 [1050/1251 ( 84%)]  Loss: 4.313 (4.51)  Time: 0.153s, 6706.00/s  (0.196s, 5224.72/s)  LR: 9.357e-04  Data: 0.023 (0.031)
Train: 49 [1100/1251 ( 88%)]  Loss: 4.330 (4.51)  Time: 0.169s, 6054.39/s  (0.196s, 5220.25/s)  LR: 9.357e-04  Data: 0.033 (0.031)
Train: 49 [1150/1251 ( 92%)]  Loss: 4.554 (4.51)  Time: 0.177s, 5774.61/s  (0.196s, 5211.82/s)  LR: 9.357e-04  Data: 0.025 (0.031)
Train: 49 [1200/1251 ( 96%)]  Loss: 4.321 (4.50)  Time: 0.166s, 6177.56/s  (0.196s, 5213.48/s)  LR: 9.357e-04  Data: 0.022 (0.031)
Train: 49 [1250/1251 (100%)]  Loss: 4.620 (4.50)  Time: 0.114s, 9020.50/s  (0.196s, 5229.46/s)  LR: 9.357e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  1.4064 (1.4064)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3966 (2.0698)  Acc@1: 77.4764 (59.6220)  Acc@5: 91.6274 (82.5100)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-38.pth.tar', 58.63599994628906)

Train: 50 [   0/1251 (  0%)]  Loss: 4.279 (4.28)  Time: 1.764s,  580.38/s  (1.764s,  580.38/s)  LR: 9.331e-04  Data: 1.636 (1.636)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 50 [  50/1251 (  4%)]  Loss: 4.575 (4.43)  Time: 0.172s, 5949.52/s  (0.226s, 4528.27/s)  LR: 9.331e-04  Data: 0.022 (0.064)
Train: 50 [ 100/1251 (  8%)]  Loss: 4.314 (4.39)  Time: 0.166s, 6164.46/s  (0.207s, 4955.49/s)  LR: 9.331e-04  Data: 0.031 (0.048)
Train: 50 [ 150/1251 ( 12%)]  Loss: 4.059 (4.31)  Time: 0.181s, 5648.43/s  (0.201s, 5087.69/s)  LR: 9.331e-04  Data: 0.022 (0.041)
Train: 50 [ 200/1251 ( 16%)]  Loss: 4.239 (4.29)  Time: 0.180s, 5690.46/s  (0.199s, 5139.07/s)  LR: 9.331e-04  Data: 0.032 (0.038)
Train: 50 [ 250/1251 ( 20%)]  Loss: 4.416 (4.31)  Time: 0.169s, 6047.73/s  (0.197s, 5201.40/s)  LR: 9.331e-04  Data: 0.021 (0.037)
Train: 50 [ 300/1251 ( 24%)]  Loss: 4.492 (4.34)  Time: 0.165s, 6223.62/s  (0.196s, 5222.22/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 350/1251 ( 28%)]  Loss: 4.348 (4.34)  Time: 0.187s, 5466.16/s  (0.195s, 5244.43/s)  LR: 9.331e-04  Data: 0.025 (0.034)
Train: 50 [ 400/1251 ( 32%)]  Loss: 4.750 (4.39)  Time: 0.174s, 5873.08/s  (0.195s, 5260.51/s)  LR: 9.331e-04  Data: 0.034 (0.035)
Train: 50 [ 450/1251 ( 36%)]  Loss: 4.482 (4.40)  Time: 0.180s, 5678.13/s  (0.194s, 5269.40/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 500/1251 ( 40%)]  Loss: 4.722 (4.43)  Time: 0.192s, 5338.76/s  (0.194s, 5271.04/s)  LR: 9.331e-04  Data: 0.020 (0.037)
Train: 50 [ 550/1251 ( 44%)]  Loss: 4.773 (4.45)  Time: 0.170s, 6014.96/s  (0.193s, 5298.33/s)  LR: 9.331e-04  Data: 0.032 (0.037)
Train: 50 [ 600/1251 ( 48%)]  Loss: 4.367 (4.45)  Time: 0.186s, 5503.12/s  (0.193s, 5300.37/s)  LR: 9.331e-04  Data: 0.036 (0.036)
Train: 50 [ 650/1251 ( 52%)]  Loss: 4.178 (4.43)  Time: 0.274s, 3731.82/s  (0.193s, 5296.13/s)  LR: 9.331e-04  Data: 0.030 (0.036)
Train: 50 [ 700/1251 ( 56%)]  Loss: 4.633 (4.44)  Time: 0.161s, 6366.15/s  (0.193s, 5302.25/s)  LR: 9.331e-04  Data: 0.027 (0.035)
Train: 50 [ 750/1251 ( 60%)]  Loss: 4.363 (4.44)  Time: 0.184s, 5574.94/s  (0.193s, 5301.15/s)  LR: 9.331e-04  Data: 0.021 (0.035)
Train: 50 [ 800/1251 ( 64%)]  Loss: 4.720 (4.45)  Time: 0.158s, 6489.26/s  (0.194s, 5291.91/s)  LR: 9.331e-04  Data: 0.023 (0.034)
Train: 50 [ 850/1251 ( 68%)]  Loss: 4.946 (4.48)  Time: 0.157s, 6510.96/s  (0.193s, 5297.49/s)  LR: 9.331e-04  Data: 0.030 (0.034)
Train: 50 [ 900/1251 ( 72%)]  Loss: 4.634 (4.49)  Time: 0.158s, 6497.39/s  (0.193s, 5311.82/s)  LR: 9.331e-04  Data: 0.025 (0.034)
Train: 50 [ 950/1251 ( 76%)]  Loss: 4.911 (4.51)  Time: 0.264s, 3877.32/s  (0.193s, 5302.92/s)  LR: 9.331e-04  Data: 0.036 (0.033)
Train: 50 [1000/1251 ( 80%)]  Loss: 4.276 (4.50)  Time: 0.195s, 5244.21/s  (0.193s, 5295.43/s)  LR: 9.331e-04  Data: 0.035 (0.033)
Train: 50 [1050/1251 ( 84%)]  Loss: 4.511 (4.50)  Time: 0.171s, 5986.88/s  (0.196s, 5234.30/s)  LR: 9.331e-04  Data: 0.021 (0.033)
Train: 50 [1100/1251 ( 88%)]  Loss: 4.620 (4.50)  Time: 0.182s, 5634.17/s  (0.196s, 5221.35/s)  LR: 9.331e-04  Data: 0.028 (0.032)
Train: 50 [1150/1251 ( 92%)]  Loss: 4.542 (4.51)  Time: 0.175s, 5851.74/s  (0.196s, 5223.45/s)  LR: 9.331e-04  Data: 0.024 (0.032)
Train: 50 [1200/1251 ( 96%)]  Loss: 4.339 (4.50)  Time: 0.163s, 6286.03/s  (0.196s, 5223.94/s)  LR: 9.331e-04  Data: 0.020 (0.032)
Train: 50 [1250/1251 (100%)]  Loss: 4.301 (4.49)  Time: 0.117s, 8726.66/s  (0.196s, 5234.30/s)  LR: 9.331e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.832 (1.832)  Loss:  1.1946 (1.1946)  Acc@1: 76.0742 (76.0742)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.2061 (1.8838)  Acc@1: 77.4764 (60.4180)  Acc@5: 91.3915 (83.0720)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-41.pth.tar', 58.81000003417969)

Train: 51 [   0/1251 (  0%)]  Loss: 4.571 (4.57)  Time: 1.992s,  514.10/s  (1.992s,  514.10/s)  LR: 9.304e-04  Data: 1.846 (1.846)
Train: 51 [  50/1251 (  4%)]  Loss: 4.197 (4.38)  Time: 0.178s, 5766.61/s  (0.222s, 4618.07/s)  LR: 9.304e-04  Data: 0.031 (0.076)
Train: 51 [ 100/1251 (  8%)]  Loss: 4.795 (4.52)  Time: 0.167s, 6127.56/s  (0.211s, 4861.56/s)  LR: 9.304e-04  Data: 0.024 (0.053)
Train: 51 [ 150/1251 ( 12%)]  Loss: 4.301 (4.47)  Time: 0.169s, 6064.86/s  (0.202s, 5069.27/s)  LR: 9.304e-04  Data: 0.029 (0.044)
Train: 51 [ 200/1251 ( 16%)]  Loss: 4.472 (4.47)  Time: 0.166s, 6163.69/s  (0.202s, 5069.03/s)  LR: 9.304e-04  Data: 0.020 (0.040)
Train: 51 [ 250/1251 ( 20%)]  Loss: 4.309 (4.44)  Time: 0.164s, 6235.61/s  (0.198s, 5171.07/s)  LR: 9.304e-04  Data: 0.031 (0.037)
Train: 51 [ 300/1251 ( 24%)]  Loss: 4.591 (4.46)  Time: 0.167s, 6142.89/s  (0.197s, 5191.94/s)  LR: 9.304e-04  Data: 0.036 (0.036)
Train: 51 [ 350/1251 ( 28%)]  Loss: 4.616 (4.48)  Time: 0.199s, 5133.95/s  (0.196s, 5221.22/s)  LR: 9.304e-04  Data: 0.020 (0.035)
Train: 51 [ 400/1251 ( 32%)]  Loss: 4.628 (4.50)  Time: 0.162s, 6330.00/s  (0.197s, 5207.58/s)  LR: 9.304e-04  Data: 0.030 (0.034)
Train: 51 [ 450/1251 ( 36%)]  Loss: 4.768 (4.52)  Time: 0.180s, 5692.31/s  (0.196s, 5237.33/s)  LR: 9.304e-04  Data: 0.031 (0.033)
Train: 51 [ 500/1251 ( 40%)]  Loss: 4.861 (4.56)  Time: 0.168s, 6089.36/s  (0.195s, 5246.65/s)  LR: 9.304e-04  Data: 0.030 (0.033)
Train: 51 [ 550/1251 ( 44%)]  Loss: 4.270 (4.53)  Time: 0.157s, 6533.63/s  (0.195s, 5255.99/s)  LR: 9.304e-04  Data: 0.032 (0.033)
Train: 51 [ 600/1251 ( 48%)]  Loss: 4.269 (4.51)  Time: 0.196s, 5223.78/s  (0.195s, 5243.97/s)  LR: 9.304e-04  Data: 0.020 (0.032)
Train: 51 [ 650/1251 ( 52%)]  Loss: 4.430 (4.51)  Time: 0.176s, 5808.05/s  (0.195s, 5245.35/s)  LR: 9.304e-04  Data: 0.022 (0.032)
Train: 51 [ 700/1251 ( 56%)]  Loss: 4.565 (4.51)  Time: 0.213s, 4809.59/s  (0.195s, 5255.64/s)  LR: 9.304e-04  Data: 0.035 (0.032)
Train: 51 [ 750/1251 ( 60%)]  Loss: 4.530 (4.51)  Time: 0.163s, 6274.87/s  (0.194s, 5277.50/s)  LR: 9.304e-04  Data: 0.030 (0.032)
Train: 51 [ 800/1251 ( 64%)]  Loss: 4.370 (4.50)  Time: 0.175s, 5866.05/s  (0.195s, 5257.73/s)  LR: 9.304e-04  Data: 0.031 (0.032)
Train: 51 [ 850/1251 ( 68%)]  Loss: 4.282 (4.49)  Time: 0.164s, 6245.48/s  (0.194s, 5268.49/s)  LR: 9.304e-04  Data: 0.034 (0.031)
Train: 51 [ 900/1251 ( 72%)]  Loss: 4.639 (4.50)  Time: 0.167s, 6138.05/s  (0.194s, 5279.49/s)  LR: 9.304e-04  Data: 0.035 (0.031)
Train: 51 [ 950/1251 ( 76%)]  Loss: 4.527 (4.50)  Time: 0.515s, 1988.29/s  (0.195s, 5256.57/s)  LR: 9.304e-04  Data: 0.035 (0.031)
Train: 51 [1000/1251 ( 80%)]  Loss: 4.480 (4.50)  Time: 0.228s, 4489.98/s  (0.195s, 5253.70/s)  LR: 9.304e-04  Data: 0.029 (0.031)
Train: 51 [1050/1251 ( 84%)]  Loss: 4.509 (4.50)  Time: 0.163s, 6288.72/s  (0.195s, 5255.52/s)  LR: 9.304e-04  Data: 0.027 (0.031)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 51 [1100/1251 ( 88%)]  Loss: 4.364 (4.49)  Time: 0.190s, 5384.20/s  (0.195s, 5254.19/s)  LR: 9.304e-04  Data: 0.033 (0.031)
Train: 51 [1150/1251 ( 92%)]  Loss: 4.302 (4.49)  Time: 0.353s, 2899.26/s  (0.195s, 5248.97/s)  LR: 9.304e-04  Data: 0.028 (0.030)
Train: 51 [1200/1251 ( 96%)]  Loss: 4.920 (4.50)  Time: 0.169s, 6076.24/s  (0.195s, 5241.68/s)  LR: 9.304e-04  Data: 0.034 (0.030)
Train: 51 [1250/1251 (100%)]  Loss: 4.536 (4.50)  Time: 0.114s, 9012.23/s  (0.195s, 5257.39/s)  LR: 9.304e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.893 (1.893)  Loss:  1.2947 (1.2947)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3579 (2.0442)  Acc@1: 76.8868 (59.0820)  Acc@5: 91.3915 (82.3620)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-36.pth.tar', 58.981999995117185)

Train: 52 [   0/1251 (  0%)]  Loss: 4.512 (4.51)  Time: 1.693s,  604.97/s  (1.693s,  604.97/s)  LR: 9.278e-04  Data: 1.565 (1.565)
Train: 52 [  50/1251 (  4%)]  Loss: 4.229 (4.37)  Time: 0.177s, 5781.45/s  (0.220s, 4655.34/s)  LR: 9.278e-04  Data: 0.032 (0.067)
Train: 52 [ 100/1251 (  8%)]  Loss: 4.178 (4.31)  Time: 0.166s, 6185.07/s  (0.210s, 4870.14/s)  LR: 9.278e-04  Data: 0.024 (0.049)
Train: 52 [ 150/1251 ( 12%)]  Loss: 4.601 (4.38)  Time: 0.165s, 6206.60/s  (0.203s, 5055.42/s)  LR: 9.278e-04  Data: 0.024 (0.043)
Train: 52 [ 200/1251 ( 16%)]  Loss: 4.209 (4.35)  Time: 0.165s, 6192.10/s  (0.199s, 5150.79/s)  LR: 9.278e-04  Data: 0.028 (0.039)
Train: 52 [ 250/1251 ( 20%)]  Loss: 4.533 (4.38)  Time: 0.166s, 6151.86/s  (0.198s, 5181.44/s)  LR: 9.278e-04  Data: 0.025 (0.037)
Train: 52 [ 300/1251 ( 24%)]  Loss: 4.305 (4.37)  Time: 0.171s, 5984.65/s  (0.196s, 5228.32/s)  LR: 9.278e-04  Data: 0.027 (0.036)
Train: 52 [ 350/1251 ( 28%)]  Loss: 4.679 (4.41)  Time: 0.190s, 5393.42/s  (0.195s, 5244.54/s)  LR: 9.278e-04  Data: 0.020 (0.034)
Train: 52 [ 400/1251 ( 32%)]  Loss: 4.499 (4.42)  Time: 0.183s, 5587.42/s  (0.195s, 5261.51/s)  LR: 9.278e-04  Data: 0.035 (0.035)
Train: 52 [ 450/1251 ( 36%)]  Loss: 4.117 (4.39)  Time: 0.199s, 5135.67/s  (0.194s, 5287.07/s)  LR: 9.278e-04  Data: 0.028 (0.035)
Train: 52 [ 500/1251 ( 40%)]  Loss: 4.441 (4.39)  Time: 0.186s, 5510.37/s  (0.194s, 5274.82/s)  LR: 9.278e-04  Data: 0.038 (0.035)
Train: 52 [ 550/1251 ( 44%)]  Loss: 5.022 (4.44)  Time: 0.157s, 6525.71/s  (0.193s, 5299.30/s)  LR: 9.278e-04  Data: 0.027 (0.034)
Train: 52 [ 600/1251 ( 48%)]  Loss: 4.734 (4.47)  Time: 0.210s, 4876.49/s  (0.193s, 5294.55/s)  LR: 9.278e-04  Data: 0.024 (0.034)
Train: 52 [ 650/1251 ( 52%)]  Loss: 4.718 (4.48)  Time: 0.187s, 5467.28/s  (0.193s, 5292.96/s)  LR: 9.278e-04  Data: 0.025 (0.033)
Train: 52 [ 700/1251 ( 56%)]  Loss: 4.624 (4.49)  Time: 0.162s, 6310.12/s  (0.193s, 5310.31/s)  LR: 9.278e-04  Data: 0.030 (0.033)
Train: 52 [ 750/1251 ( 60%)]  Loss: 4.082 (4.47)  Time: 0.174s, 5882.64/s  (0.193s, 5294.51/s)  LR: 9.278e-04  Data: 0.024 (0.032)
Train: 52 [ 800/1251 ( 64%)]  Loss: 4.523 (4.47)  Time: 0.179s, 5733.86/s  (0.194s, 5278.67/s)  LR: 9.278e-04  Data: 0.027 (0.032)
Train: 52 [ 850/1251 ( 68%)]  Loss: 4.157 (4.45)  Time: 0.192s, 5332.32/s  (0.194s, 5281.63/s)  LR: 9.278e-04  Data: 0.037 (0.032)
Train: 52 [ 900/1251 ( 72%)]  Loss: 4.631 (4.46)  Time: 0.171s, 5988.28/s  (0.194s, 5288.98/s)  LR: 9.278e-04  Data: 0.023 (0.032)
Train: 52 [ 950/1251 ( 76%)]  Loss: 4.363 (4.46)  Time: 0.169s, 6048.96/s  (0.194s, 5282.76/s)  LR: 9.278e-04  Data: 0.023 (0.031)
Train: 52 [1000/1251 ( 80%)]  Loss: 4.621 (4.47)  Time: 0.180s, 5699.99/s  (0.194s, 5281.89/s)  LR: 9.278e-04  Data: 0.031 (0.031)
Train: 52 [1050/1251 ( 84%)]  Loss: 4.306 (4.46)  Time: 0.176s, 5813.97/s  (0.194s, 5278.57/s)  LR: 9.278e-04  Data: 0.027 (0.031)
Train: 52 [1100/1251 ( 88%)]  Loss: 4.486 (4.46)  Time: 0.176s, 5825.09/s  (0.194s, 5266.17/s)  LR: 9.278e-04  Data: 0.025 (0.031)
Train: 52 [1150/1251 ( 92%)]  Loss: 4.067 (4.44)  Time: 0.151s, 6798.69/s  (0.194s, 5271.89/s)  LR: 9.278e-04  Data: 0.027 (0.031)
Train: 52 [1200/1251 ( 96%)]  Loss: 4.292 (4.44)  Time: 0.169s, 6070.06/s  (0.194s, 5267.19/s)  LR: 9.278e-04  Data: 0.028 (0.031)
Train: 52 [1250/1251 (100%)]  Loss: 4.512 (4.44)  Time: 0.116s, 8840.15/s  (0.194s, 5279.13/s)  LR: 9.278e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.756 (1.756)  Loss:  1.3323 (1.3323)  Acc@1: 75.9766 (75.9766)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.019 (0.212)  Loss:  1.4428 (1.9896)  Acc@1: 75.3538 (59.0560)  Acc@5: 88.2076 (82.2220)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-52.pth.tar', 59.05600002441406)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-40.pth.tar', 58.98999991699219)

Train: 53 [   0/1251 (  0%)]  Loss: 4.625 (4.62)  Time: 1.963s,  521.74/s  (1.963s,  521.74/s)  LR: 9.250e-04  Data: 1.832 (1.832)
Train: 53 [  50/1251 (  4%)]  Loss: 4.467 (4.55)  Time: 0.163s, 6270.62/s  (0.224s, 4581.02/s)  LR: 9.250e-04  Data: 0.026 (0.077)
Train: 53 [ 100/1251 (  8%)]  Loss: 4.079 (4.39)  Time: 0.164s, 6238.63/s  (0.207s, 4949.93/s)  LR: 9.250e-04  Data: 0.022 (0.057)
Train: 53 [ 150/1251 ( 12%)]  Loss: 4.303 (4.37)  Time: 0.155s, 6615.63/s  (0.203s, 5050.61/s)  LR: 9.250e-04  Data: 0.030 (0.048)
Train: 53 [ 200/1251 ( 16%)]  Loss: 4.681 (4.43)  Time: 0.185s, 5528.60/s  (0.199s, 5155.57/s)  LR: 9.250e-04  Data: 0.031 (0.043)
Train: 53 [ 250/1251 ( 20%)]  Loss: 4.464 (4.44)  Time: 0.183s, 5595.78/s  (0.197s, 5188.33/s)  LR: 9.250e-04  Data: 0.020 (0.044)
Train: 53 [ 300/1251 ( 24%)]  Loss: 4.578 (4.46)  Time: 0.182s, 5613.99/s  (0.196s, 5226.64/s)  LR: 9.250e-04  Data: 0.020 (0.044)
Train: 53 [ 350/1251 ( 28%)]  Loss: 4.551 (4.47)  Time: 0.203s, 5032.36/s  (0.195s, 5259.01/s)  LR: 9.250e-04  Data: 0.031 (0.042)
Train: 53 [ 400/1251 ( 32%)]  Loss: 4.660 (4.49)  Time: 0.168s, 6100.02/s  (0.194s, 5285.79/s)  LR: 9.250e-04  Data: 0.028 (0.041)
Train: 53 [ 450/1251 ( 36%)]  Loss: 4.606 (4.50)  Time: 0.312s, 3280.61/s  (0.194s, 5283.82/s)  LR: 9.250e-04  Data: 0.029 (0.040)
Train: 53 [ 500/1251 ( 40%)]  Loss: 4.290 (4.48)  Time: 0.164s, 6237.31/s  (0.193s, 5297.22/s)  LR: 9.250e-04  Data: 0.026 (0.039)
Train: 53 [ 550/1251 ( 44%)]  Loss: 4.531 (4.49)  Time: 0.183s, 5600.29/s  (0.193s, 5295.05/s)  LR: 9.250e-04  Data: 0.023 (0.040)
Train: 53 [ 600/1251 ( 48%)]  Loss: 4.600 (4.50)  Time: 0.169s, 6044.00/s  (0.194s, 5290.82/s)  LR: 9.250e-04  Data: 0.020 (0.041)
Train: 53 [ 650/1251 ( 52%)]  Loss: 4.356 (4.49)  Time: 0.178s, 5744.96/s  (0.194s, 5291.19/s)  LR: 9.250e-04  Data: 0.026 (0.041)
Train: 53 [ 700/1251 ( 56%)]  Loss: 4.729 (4.50)  Time: 0.188s, 5442.08/s  (0.193s, 5294.08/s)  LR: 9.250e-04  Data: 0.029 (0.041)
Train: 53 [ 750/1251 ( 60%)]  Loss: 4.135 (4.48)  Time: 0.167s, 6122.58/s  (0.194s, 5290.11/s)  LR: 9.250e-04  Data: 0.028 (0.040)
Train: 53 [ 800/1251 ( 64%)]  Loss: 4.404 (4.47)  Time: 0.343s, 2986.00/s  (0.194s, 5290.79/s)  LR: 9.250e-04  Data: 0.029 (0.039)
Train: 53 [ 850/1251 ( 68%)]  Loss: 4.379 (4.47)  Time: 0.155s, 6590.41/s  (0.194s, 5289.94/s)  LR: 9.250e-04  Data: 0.028 (0.039)
Train: 53 [ 900/1251 ( 72%)]  Loss: 4.475 (4.47)  Time: 0.173s, 5919.49/s  (0.193s, 5294.51/s)  LR: 9.250e-04  Data: 0.028 (0.038)
Train: 53 [ 950/1251 ( 76%)]  Loss: 4.759 (4.48)  Time: 0.171s, 6000.07/s  (0.194s, 5288.72/s)  LR: 9.250e-04  Data: 0.034 (0.037)
Train: 53 [1000/1251 ( 80%)]  Loss: 4.470 (4.48)  Time: 0.302s, 3386.64/s  (0.194s, 5286.58/s)  LR: 9.250e-04  Data: 0.025 (0.037)
Train: 53 [1050/1251 ( 84%)]  Loss: 4.081 (4.46)  Time: 0.177s, 5784.02/s  (0.194s, 5271.97/s)  LR: 9.250e-04  Data: 0.037 (0.037)
Train: 53 [1100/1251 ( 88%)]  Loss: 4.390 (4.46)  Time: 0.172s, 5951.64/s  (0.194s, 5274.86/s)  LR: 9.250e-04  Data: 0.031 (0.036)
Train: 53 [1150/1251 ( 92%)]  Loss: 4.307 (4.46)  Time: 0.182s, 5614.89/s  (0.194s, 5270.91/s)  LR: 9.250e-04  Data: 0.024 (0.036)
Train: 53 [1200/1251 ( 96%)]  Loss: 4.484 (4.46)  Time: 0.513s, 1995.52/s  (0.194s, 5273.89/s)  LR: 9.250e-04  Data: 0.025 (0.036)
Train: 53 [1250/1251 (100%)]  Loss: 4.520 (4.46)  Time: 0.113s, 9023.61/s  (0.194s, 5290.57/s)  LR: 9.250e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.765 (1.765)  Loss:  1.5498 (1.5498)  Acc@1: 73.4375 (73.4375)  Acc@5: 89.2578 (89.2578)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.4994 (2.1486)  Acc@1: 77.1226 (58.0660)  Acc@5: 90.2123 (81.6680)
Train: 54 [   0/1251 (  0%)]  Loss: 4.149 (4.15)  Time: 1.665s,  615.17/s  (1.665s,  615.17/s)  LR: 9.222e-04  Data: 1.541 (1.541)
Train: 54 [  50/1251 (  4%)]  Loss: 4.526 (4.34)  Time: 0.180s, 5680.76/s  (0.225s, 4547.48/s)  LR: 9.222e-04  Data: 0.026 (0.057)
Train: 54 [ 100/1251 (  8%)]  Loss: 4.714 (4.46)  Time: 0.187s, 5469.70/s  (0.208s, 4916.22/s)  LR: 9.222e-04  Data: 0.031 (0.043)
Train: 54 [ 150/1251 ( 12%)]  Loss: 4.186 (4.39)  Time: 0.171s, 5986.18/s  (0.203s, 5050.71/s)  LR: 9.222e-04  Data: 0.028 (0.038)
Train: 54 [ 200/1251 ( 16%)]  Loss: 4.823 (4.48)  Time: 0.256s, 4006.14/s  (0.198s, 5164.80/s)  LR: 9.222e-04  Data: 0.027 (0.036)
Train: 54 [ 250/1251 ( 20%)]  Loss: 4.623 (4.50)  Time: 0.172s, 5937.07/s  (0.197s, 5193.53/s)  LR: 9.222e-04  Data: 0.024 (0.034)
Train: 54 [ 300/1251 ( 24%)]  Loss: 4.613 (4.52)  Time: 0.167s, 6119.79/s  (0.195s, 5249.42/s)  LR: 9.222e-04  Data: 0.030 (0.034)
Train: 54 [ 350/1251 ( 28%)]  Loss: 4.370 (4.50)  Time: 0.168s, 6109.66/s  (0.195s, 5263.81/s)  LR: 9.222e-04  Data: 0.027 (0.033)
Train: 54 [ 400/1251 ( 32%)]  Loss: 4.602 (4.51)  Time: 0.314s, 3262.81/s  (0.194s, 5271.97/s)  LR: 9.222e-04  Data: 0.038 (0.032)
Train: 54 [ 450/1251 ( 36%)]  Loss: 4.016 (4.46)  Time: 0.220s, 4664.62/s  (0.193s, 5311.85/s)  LR: 9.222e-04  Data: 0.026 (0.032)
Train: 54 [ 500/1251 ( 40%)]  Loss: 4.542 (4.47)  Time: 0.162s, 6314.59/s  (0.193s, 5303.48/s)  LR: 9.222e-04  Data: 0.026 (0.031)
Train: 54 [ 550/1251 ( 44%)]  Loss: 4.590 (4.48)  Time: 0.166s, 6176.95/s  (0.193s, 5315.73/s)  LR: 9.222e-04  Data: 0.036 (0.031)
Train: 54 [ 600/1251 ( 48%)]  Loss: 4.722 (4.50)  Time: 0.165s, 6217.80/s  (0.193s, 5311.89/s)  LR: 9.222e-04  Data: 0.040 (0.031)
Train: 54 [ 650/1251 ( 52%)]  Loss: 4.797 (4.52)  Time: 0.364s, 2811.59/s  (0.193s, 5300.16/s)  LR: 9.222e-04  Data: 0.025 (0.031)
Train: 54 [ 700/1251 ( 56%)]  Loss: 4.138 (4.49)  Time: 0.165s, 6217.55/s  (0.193s, 5308.37/s)  LR: 9.222e-04  Data: 0.031 (0.030)
Train: 54 [ 750/1251 ( 60%)]  Loss: 4.185 (4.47)  Time: 0.193s, 5311.90/s  (0.193s, 5312.51/s)  LR: 9.222e-04  Data: 0.026 (0.030)
Train: 54 [ 800/1251 ( 64%)]  Loss: 4.246 (4.46)  Time: 0.179s, 5720.07/s  (0.193s, 5311.88/s)  LR: 9.222e-04  Data: 0.022 (0.030)
Train: 54 [ 850/1251 ( 68%)]  Loss: 4.619 (4.47)  Time: 0.169s, 6055.21/s  (0.193s, 5313.15/s)  LR: 9.222e-04  Data: 0.021 (0.030)
Train: 54 [ 900/1251 ( 72%)]  Loss: 4.228 (4.46)  Time: 0.179s, 5724.38/s  (0.193s, 5317.03/s)  LR: 9.222e-04  Data: 0.022 (0.030)
Train: 54 [ 950/1251 ( 76%)]  Loss: 4.425 (4.46)  Time: 0.172s, 5953.12/s  (0.193s, 5313.42/s)  LR: 9.222e-04  Data: 0.025 (0.030)
Train: 54 [1000/1251 ( 80%)]  Loss: 4.363 (4.45)  Time: 0.180s, 5698.65/s  (0.193s, 5310.23/s)  LR: 9.222e-04  Data: 0.027 (0.029)
Train: 54 [1050/1251 ( 84%)]  Loss: 4.657 (4.46)  Time: 0.159s, 6460.38/s  (0.193s, 5308.93/s)  LR: 9.222e-04  Data: 0.023 (0.029)
Train: 54 [1100/1251 ( 88%)]  Loss: 4.668 (4.47)  Time: 0.167s, 6148.43/s  (0.193s, 5297.77/s)  LR: 9.222e-04  Data: 0.032 (0.029)
Train: 54 [1150/1251 ( 92%)]  Loss: 4.420 (4.47)  Time: 0.159s, 6445.62/s  (0.193s, 5302.91/s)  LR: 9.222e-04  Data: 0.029 (0.029)
Train: 54 [1200/1251 ( 96%)]  Loss: 4.865 (4.48)  Time: 0.176s, 5826.49/s  (0.193s, 5301.18/s)  LR: 9.222e-04  Data: 0.021 (0.030)
Train: 54 [1250/1251 (100%)]  Loss: 4.094 (4.47)  Time: 0.113s, 9034.54/s  (0.193s, 5303.81/s)  LR: 9.222e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.713 (1.713)  Loss:  1.3381 (1.3381)  Acc@1: 75.3906 (75.3906)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3156 (1.9465)  Acc@1: 73.9387 (59.9220)  Acc@5: 90.5660 (82.8840)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-52.pth.tar', 59.05600002441406)

Train: 55 [   0/1251 (  0%)]  Loss: 4.412 (4.41)  Time: 1.623s,  630.90/s  (1.623s,  630.90/s)  LR: 9.194e-04  Data: 1.473 (1.473)
Train: 55 [  50/1251 (  4%)]  Loss: 4.321 (4.37)  Time: 0.225s, 4552.33/s  (0.218s, 4702.42/s)  LR: 9.194e-04  Data: 0.100 (0.069)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 55 [ 100/1251 (  8%)]  Loss: 4.432 (4.39)  Time: 0.180s, 5675.58/s  (0.203s, 5037.49/s)  LR: 9.194e-04  Data: 0.020 (0.053)
Train: 55 [ 150/1251 ( 12%)]  Loss: 4.302 (4.37)  Time: 0.153s, 6706.09/s  (0.199s, 5152.32/s)  LR: 9.194e-04  Data: 0.025 (0.046)
Train: 55 [ 200/1251 ( 16%)]  Loss: 4.357 (4.36)  Time: 0.160s, 6382.12/s  (0.198s, 5175.07/s)  LR: 9.194e-04  Data: 0.032 (0.042)
Train: 55 [ 250/1251 ( 20%)]  Loss: 4.629 (4.41)  Time: 0.171s, 5974.22/s  (0.196s, 5230.21/s)  LR: 9.194e-04  Data: 0.042 (0.042)
Train: 55 [ 300/1251 ( 24%)]  Loss: 4.309 (4.39)  Time: 0.176s, 5828.32/s  (0.194s, 5284.55/s)  LR: 9.194e-04  Data: 0.028 (0.042)
Train: 55 [ 350/1251 ( 28%)]  Loss: 4.588 (4.42)  Time: 0.186s, 5507.81/s  (0.193s, 5293.07/s)  LR: 9.194e-04  Data: 0.028 (0.042)
Train: 55 [ 400/1251 ( 32%)]  Loss: 4.509 (4.43)  Time: 0.172s, 5953.87/s  (0.193s, 5299.38/s)  LR: 9.194e-04  Data: 0.031 (0.041)
Train: 55 [ 450/1251 ( 36%)]  Loss: 4.753 (4.46)  Time: 0.160s, 6395.12/s  (0.192s, 5326.28/s)  LR: 9.194e-04  Data: 0.030 (0.039)
Train: 55 [ 500/1251 ( 40%)]  Loss: 4.904 (4.50)  Time: 0.176s, 5821.52/s  (0.192s, 5325.33/s)  LR: 9.194e-04  Data: 0.027 (0.038)
Train: 55 [ 550/1251 ( 44%)]  Loss: 4.235 (4.48)  Time: 0.164s, 6227.89/s  (0.193s, 5312.47/s)  LR: 9.194e-04  Data: 0.025 (0.037)
Train: 55 [ 600/1251 ( 48%)]  Loss: 4.506 (4.48)  Time: 0.193s, 5293.68/s  (0.193s, 5312.64/s)  LR: 9.194e-04  Data: 0.023 (0.036)
Train: 55 [ 650/1251 ( 52%)]  Loss: 4.143 (4.46)  Time: 0.167s, 6118.62/s  (0.192s, 5333.74/s)  LR: 9.194e-04  Data: 0.033 (0.036)
Train: 55 [ 700/1251 ( 56%)]  Loss: 4.369 (4.45)  Time: 0.163s, 6289.56/s  (0.192s, 5329.09/s)  LR: 9.194e-04  Data: 0.023 (0.035)
Train: 55 [ 750/1251 ( 60%)]  Loss: 4.401 (4.45)  Time: 0.190s, 5381.28/s  (0.192s, 5325.91/s)  LR: 9.194e-04  Data: 0.025 (0.035)
Train: 55 [ 800/1251 ( 64%)]  Loss: 4.728 (4.46)  Time: 0.179s, 5708.86/s  (0.193s, 5319.28/s)  LR: 9.194e-04  Data: 0.034 (0.034)
Train: 55 [ 850/1251 ( 68%)]  Loss: 3.882 (4.43)  Time: 0.175s, 5866.26/s  (0.193s, 5314.88/s)  LR: 9.194e-04  Data: 0.030 (0.034)
Train: 55 [ 900/1251 ( 72%)]  Loss: 4.202 (4.42)  Time: 0.178s, 5757.31/s  (0.193s, 5313.65/s)  LR: 9.194e-04  Data: 0.028 (0.034)
Train: 55 [ 950/1251 ( 76%)]  Loss: 4.631 (4.43)  Time: 0.163s, 6278.82/s  (0.193s, 5319.14/s)  LR: 9.194e-04  Data: 0.033 (0.033)
Train: 55 [1000/1251 ( 80%)]  Loss: 4.563 (4.44)  Time: 0.181s, 5654.75/s  (0.193s, 5306.93/s)  LR: 9.194e-04  Data: 0.029 (0.033)
Train: 55 [1050/1251 ( 84%)]  Loss: 4.272 (4.43)  Time: 0.161s, 6366.82/s  (0.193s, 5307.35/s)  LR: 9.194e-04  Data: 0.026 (0.033)
Train: 55 [1100/1251 ( 88%)]  Loss: 4.751 (4.44)  Time: 0.167s, 6146.75/s  (0.193s, 5300.41/s)  LR: 9.194e-04  Data: 0.024 (0.033)
Train: 55 [1150/1251 ( 92%)]  Loss: 4.580 (4.45)  Time: 0.237s, 4314.56/s  (0.193s, 5302.70/s)  LR: 9.194e-04  Data: 0.032 (0.032)
Train: 55 [1200/1251 ( 96%)]  Loss: 4.178 (4.44)  Time: 0.161s, 6378.37/s  (0.193s, 5295.59/s)  LR: 9.194e-04  Data: 0.021 (0.032)
Train: 55 [1250/1251 (100%)]  Loss: 4.679 (4.45)  Time: 0.113s, 9036.65/s  (0.193s, 5309.71/s)  LR: 9.194e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.809 (1.809)  Loss:  1.3933 (1.3933)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3494 (2.0297)  Acc@1: 76.5330 (59.9680)  Acc@5: 91.5094 (83.0360)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-51.pth.tar', 59.082000043945314)

Train: 56 [   0/1251 (  0%)]  Loss: 4.435 (4.44)  Time: 1.776s,  576.51/s  (1.776s,  576.51/s)  LR: 9.165e-04  Data: 1.640 (1.640)
Train: 56 [  50/1251 (  4%)]  Loss: 4.901 (4.67)  Time: 0.153s, 6682.69/s  (0.220s, 4645.95/s)  LR: 9.165e-04  Data: 0.022 (0.073)
Train: 56 [ 100/1251 (  8%)]  Loss: 4.515 (4.62)  Time: 0.175s, 5859.60/s  (0.204s, 5007.48/s)  LR: 9.165e-04  Data: 0.026 (0.057)
Train: 56 [ 150/1251 ( 12%)]  Loss: 4.404 (4.56)  Time: 0.170s, 6037.31/s  (0.200s, 5114.44/s)  LR: 9.165e-04  Data: 0.029 (0.048)
Train: 56 [ 200/1251 ( 16%)]  Loss: 4.621 (4.58)  Time: 0.466s, 2197.81/s  (0.200s, 5119.00/s)  LR: 9.165e-04  Data: 0.021 (0.043)
Train: 56 [ 250/1251 ( 20%)]  Loss: 4.331 (4.53)  Time: 0.156s, 6549.08/s  (0.198s, 5165.86/s)  LR: 9.165e-04  Data: 0.026 (0.040)
Train: 56 [ 300/1251 ( 24%)]  Loss: 4.520 (4.53)  Time: 0.172s, 5966.84/s  (0.196s, 5227.17/s)  LR: 9.165e-04  Data: 0.025 (0.038)
Train: 56 [ 350/1251 ( 28%)]  Loss: 4.300 (4.50)  Time: 0.159s, 6458.58/s  (0.194s, 5266.06/s)  LR: 9.165e-04  Data: 0.024 (0.037)
Train: 56 [ 400/1251 ( 32%)]  Loss: 4.595 (4.51)  Time: 0.501s, 2043.56/s  (0.195s, 5252.36/s)  LR: 9.165e-04  Data: 0.020 (0.036)
Train: 56 [ 450/1251 ( 36%)]  Loss: 4.296 (4.49)  Time: 0.166s, 6170.05/s  (0.194s, 5285.65/s)  LR: 9.165e-04  Data: 0.034 (0.035)
Train: 56 [ 500/1251 ( 40%)]  Loss: 4.267 (4.47)  Time: 0.242s, 4239.73/s  (0.194s, 5283.71/s)  LR: 9.165e-04  Data: 0.026 (0.034)
Train: 56 [ 550/1251 ( 44%)]  Loss: 4.627 (4.48)  Time: 0.174s, 5883.98/s  (0.194s, 5289.54/s)  LR: 9.165e-04  Data: 0.026 (0.033)
Train: 56 [ 600/1251 ( 48%)]  Loss: 4.550 (4.49)  Time: 0.158s, 6462.13/s  (0.193s, 5307.47/s)  LR: 9.165e-04  Data: 0.025 (0.033)
Train: 56 [ 650/1251 ( 52%)]  Loss: 4.466 (4.49)  Time: 0.182s, 5632.63/s  (0.193s, 5299.98/s)  LR: 9.165e-04  Data: 0.021 (0.033)
Train: 56 [ 700/1251 ( 56%)]  Loss: 4.531 (4.49)  Time: 0.170s, 6022.25/s  (0.193s, 5300.03/s)  LR: 9.165e-04  Data: 0.024 (0.032)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 56 [ 750/1251 ( 60%)]  Loss: 4.195 (4.47)  Time: 0.168s, 6082.64/s  (0.193s, 5295.86/s)  LR: 9.165e-04  Data: 0.026 (0.032)
Train: 56 [ 800/1251 ( 64%)]  Loss: 4.583 (4.48)  Time: 0.434s, 2357.64/s  (0.193s, 5295.76/s)  LR: 9.165e-04  Data: 0.025 (0.032)
Train: 56 [ 850/1251 ( 68%)]  Loss: 4.700 (4.49)  Time: 0.164s, 6225.93/s  (0.193s, 5302.42/s)  LR: 9.165e-04  Data: 0.033 (0.032)
Train: 56 [ 900/1251 ( 72%)]  Loss: 4.503 (4.49)  Time: 0.160s, 6402.04/s  (0.193s, 5307.53/s)  LR: 9.165e-04  Data: 0.029 (0.031)
Train: 56 [ 950/1251 ( 76%)]  Loss: 4.651 (4.50)  Time: 0.161s, 6357.31/s  (0.193s, 5304.27/s)  LR: 9.165e-04  Data: 0.031 (0.031)
Train: 56 [1000/1251 ( 80%)]  Loss: 4.200 (4.49)  Time: 0.182s, 5631.98/s  (0.193s, 5292.39/s)  LR: 9.165e-04  Data: 0.020 (0.031)
Train: 56 [1050/1251 ( 84%)]  Loss: 4.324 (4.48)  Time: 0.167s, 6117.48/s  (0.194s, 5291.30/s)  LR: 9.165e-04  Data: 0.025 (0.031)
Train: 56 [1100/1251 ( 88%)]  Loss: 4.284 (4.47)  Time: 0.190s, 5379.42/s  (0.194s, 5289.66/s)  LR: 9.165e-04  Data: 0.023 (0.031)
Train: 56 [1150/1251 ( 92%)]  Loss: 4.081 (4.45)  Time: 0.181s, 5645.93/s  (0.194s, 5277.83/s)  LR: 9.165e-04  Data: 0.032 (0.031)
Train: 56 [1200/1251 ( 96%)]  Loss: 4.430 (4.45)  Time: 0.162s, 6332.58/s  (0.194s, 5270.78/s)  LR: 9.165e-04  Data: 0.027 (0.032)
Train: 56 [1250/1251 (100%)]  Loss: 5.002 (4.47)  Time: 0.113s, 9033.04/s  (0.194s, 5287.94/s)  LR: 9.165e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.862 (1.862)  Loss:  1.2177 (1.2177)  Acc@1: 76.1719 (76.1719)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.3770 (1.9930)  Acc@1: 76.4151 (59.6840)  Acc@5: 89.7406 (82.9200)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-48.pth.tar', 59.237999833984375)

Train: 57 [   0/1251 (  0%)]  Loss: 4.500 (4.50)  Time: 1.705s,  600.53/s  (1.705s,  600.53/s)  LR: 9.136e-04  Data: 1.549 (1.549)
Train: 57 [  50/1251 (  4%)]  Loss: 4.361 (4.43)  Time: 0.167s, 6134.84/s  (0.220s, 4645.90/s)  LR: 9.136e-04  Data: 0.025 (0.059)
Train: 57 [ 100/1251 (  8%)]  Loss: 4.519 (4.46)  Time: 0.173s, 5929.74/s  (0.206s, 4978.93/s)  LR: 9.136e-04  Data: 0.033 (0.044)
Train: 57 [ 150/1251 ( 12%)]  Loss: 4.220 (4.40)  Time: 0.184s, 5553.79/s  (0.201s, 5082.03/s)  LR: 9.136e-04  Data: 0.027 (0.038)
Train: 57 [ 200/1251 ( 16%)]  Loss: 4.125 (4.35)  Time: 0.187s, 5484.38/s  (0.197s, 5203.85/s)  LR: 9.136e-04  Data: 0.031 (0.036)
Train: 57 [ 250/1251 ( 20%)]  Loss: 3.927 (4.28)  Time: 0.178s, 5761.29/s  (0.196s, 5213.24/s)  LR: 9.136e-04  Data: 0.030 (0.034)
Train: 57 [ 300/1251 ( 24%)]  Loss: 4.279 (4.28)  Time: 0.170s, 6037.99/s  (0.195s, 5254.02/s)  LR: 9.136e-04  Data: 0.023 (0.033)
Train: 57 [ 350/1251 ( 28%)]  Loss: 4.452 (4.30)  Time: 0.166s, 6164.46/s  (0.193s, 5310.18/s)  LR: 9.136e-04  Data: 0.032 (0.032)
Train: 57 [ 400/1251 ( 32%)]  Loss: 4.571 (4.33)  Time: 0.182s, 5641.85/s  (0.193s, 5295.69/s)  LR: 9.136e-04  Data: 0.034 (0.032)
Train: 57 [ 450/1251 ( 36%)]  Loss: 4.393 (4.33)  Time: 0.174s, 5880.98/s  (0.193s, 5311.46/s)  LR: 9.136e-04  Data: 0.027 (0.032)
Train: 57 [ 500/1251 ( 40%)]  Loss: 4.357 (4.34)  Time: 0.176s, 5819.87/s  (0.192s, 5323.21/s)  LR: 9.136e-04  Data: 0.030 (0.031)
Train: 57 [ 550/1251 ( 44%)]  Loss: 4.128 (4.32)  Time: 0.186s, 5508.41/s  (0.192s, 5324.76/s)  LR: 9.136e-04  Data: 0.036 (0.031)
Train: 57 [ 600/1251 ( 48%)]  Loss: 4.236 (4.31)  Time: 0.170s, 6006.77/s  (0.192s, 5322.52/s)  LR: 9.136e-04  Data: 0.031 (0.031)
Train: 57 [ 650/1251 ( 52%)]  Loss: 4.532 (4.33)  Time: 0.195s, 5253.98/s  (0.192s, 5319.54/s)  LR: 9.136e-04  Data: 0.031 (0.031)
Train: 57 [ 700/1251 ( 56%)]  Loss: 4.591 (4.35)  Time: 0.166s, 6170.98/s  (0.193s, 5315.39/s)  LR: 9.136e-04  Data: 0.026 (0.030)
Train: 57 [ 750/1251 ( 60%)]  Loss: 4.400 (4.35)  Time: 0.176s, 5819.21/s  (0.193s, 5318.15/s)  LR: 9.136e-04  Data: 0.029 (0.030)
Train: 57 [ 800/1251 ( 64%)]  Loss: 4.438 (4.35)  Time: 0.178s, 5739.78/s  (0.192s, 5325.89/s)  LR: 9.136e-04  Data: 0.033 (0.030)
Train: 57 [ 850/1251 ( 68%)]  Loss: 4.203 (4.35)  Time: 0.182s, 5637.32/s  (0.192s, 5326.23/s)  LR: 9.136e-04  Data: 0.026 (0.030)
Train: 57 [ 900/1251 ( 72%)]  Loss: 4.243 (4.34)  Time: 0.194s, 5289.33/s  (0.192s, 5323.30/s)  LR: 9.136e-04  Data: 0.029 (0.030)
Train: 57 [ 950/1251 ( 76%)]  Loss: 4.284 (4.34)  Time: 0.263s, 3896.90/s  (0.193s, 5311.98/s)  LR: 9.136e-04  Data: 0.023 (0.030)
Train: 57 [1000/1251 ( 80%)]  Loss: 4.362 (4.34)  Time: 0.181s, 5642.12/s  (0.193s, 5305.03/s)  LR: 9.136e-04  Data: 0.023 (0.030)
Train: 57 [1050/1251 ( 84%)]  Loss: 4.249 (4.34)  Time: 0.196s, 5228.95/s  (0.193s, 5307.27/s)  LR: 9.136e-04  Data: 0.026 (0.029)
Train: 57 [1100/1251 ( 88%)]  Loss: 4.466 (4.34)  Time: 0.177s, 5798.12/s  (0.193s, 5315.25/s)  LR: 9.136e-04  Data: 0.025 (0.029)
Train: 57 [1150/1251 ( 92%)]  Loss: 4.275 (4.34)  Time: 0.163s, 6300.65/s  (0.193s, 5314.73/s)  LR: 9.136e-04  Data: 0.027 (0.029)
Train: 57 [1200/1251 ( 96%)]  Loss: 4.438 (4.34)  Time: 0.165s, 6212.53/s  (0.193s, 5310.30/s)  LR: 9.136e-04  Data: 0.026 (0.029)
Train: 57 [1250/1251 (100%)]  Loss: 4.395 (4.34)  Time: 0.113s, 9035.74/s  (0.192s, 5323.44/s)  LR: 9.136e-04  Data: 0.000 (0.029)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.764 (1.764)  Loss:  1.3002 (1.3002)  Acc@1: 78.7109 (78.7109)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2831 (1.9662)  Acc@1: 77.7123 (60.4860)  Acc@5: 91.5094 (83.0260)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-43.pth.tar', 59.23800002197266)

Train: 58 [   0/1251 (  0%)]  Loss: 4.437 (4.44)  Time: 1.751s,  584.70/s  (1.751s,  584.70/s)  LR: 9.107e-04  Data: 1.622 (1.622)
Train: 58 [  50/1251 (  4%)]  Loss: 4.657 (4.55)  Time: 0.178s, 5744.22/s  (0.226s, 4521.62/s)  LR: 9.107e-04  Data: 0.024 (0.081)
Train: 58 [ 100/1251 (  8%)]  Loss: 4.242 (4.45)  Time: 0.188s, 5444.88/s  (0.213s, 4816.63/s)  LR: 9.107e-04  Data: 0.029 (0.068)
Train: 58 [ 150/1251 ( 12%)]  Loss: 4.644 (4.50)  Time: 0.191s, 5352.51/s  (0.203s, 5040.36/s)  LR: 9.107e-04  Data: 0.020 (0.058)
Train: 58 [ 200/1251 ( 16%)]  Loss: 4.487 (4.49)  Time: 0.162s, 6321.62/s  (0.199s, 5134.36/s)  LR: 9.107e-04  Data: 0.028 (0.054)
Train: 58 [ 250/1251 ( 20%)]  Loss: 4.811 (4.55)  Time: 0.182s, 5638.28/s  (0.197s, 5192.06/s)  LR: 9.107e-04  Data: 0.031 (0.052)
Train: 58 [ 300/1251 ( 24%)]  Loss: 4.622 (4.56)  Time: 0.197s, 5189.28/s  (0.196s, 5233.24/s)  LR: 9.107e-04  Data: 0.025 (0.051)
Train: 58 [ 350/1251 ( 28%)]  Loss: 4.646 (4.57)  Time: 0.160s, 6382.53/s  (0.194s, 5272.70/s)  LR: 9.107e-04  Data: 0.026 (0.049)
Train: 58 [ 400/1251 ( 32%)]  Loss: 4.259 (4.53)  Time: 0.180s, 5692.85/s  (0.195s, 5262.38/s)  LR: 9.107e-04  Data: 0.024 (0.048)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 58 [ 450/1251 ( 36%)]  Loss: 4.642 (4.54)  Time: 0.364s, 2814.87/s  (0.194s, 5274.94/s)  LR: 9.107e-04  Data: 0.025 (0.046)
Train: 58 [ 500/1251 ( 40%)]  Loss: 4.608 (4.55)  Time: 0.165s, 6202.84/s  (0.194s, 5274.82/s)  LR: 9.107e-04  Data: 0.028 (0.044)
Train: 58 [ 550/1251 ( 44%)]  Loss: 4.712 (4.56)  Time: 0.164s, 6235.80/s  (0.193s, 5297.24/s)  LR: 9.107e-04  Data: 0.032 (0.043)
Train: 58 [ 600/1251 ( 48%)]  Loss: 4.351 (4.55)  Time: 0.196s, 5223.14/s  (0.194s, 5277.51/s)  LR: 9.107e-04  Data: 0.036 (0.041)
Train: 58 [ 650/1251 ( 52%)]  Loss: 4.415 (4.54)  Time: 0.181s, 5672.36/s  (0.194s, 5285.87/s)  LR: 9.107e-04  Data: 0.038 (0.040)
Train: 58 [ 700/1251 ( 56%)]  Loss: 4.225 (4.52)  Time: 0.193s, 5314.24/s  (0.193s, 5292.62/s)  LR: 9.107e-04  Data: 0.025 (0.040)
Train: 58 [ 750/1251 ( 60%)]  Loss: 4.472 (4.51)  Time: 0.168s, 6103.93/s  (0.194s, 5283.44/s)  LR: 9.107e-04  Data: 0.025 (0.039)
Train: 58 [ 800/1251 ( 64%)]  Loss: 4.499 (4.51)  Time: 0.163s, 6277.19/s  (0.193s, 5298.60/s)  LR: 9.107e-04  Data: 0.032 (0.038)
Train: 58 [ 850/1251 ( 68%)]  Loss: 4.568 (4.52)  Time: 0.244s, 4188.89/s  (0.194s, 5286.18/s)  LR: 9.107e-04  Data: 0.029 (0.037)
Train: 58 [ 900/1251 ( 72%)]  Loss: 4.598 (4.52)  Time: 0.181s, 5649.61/s  (0.194s, 5288.11/s)  LR: 9.107e-04  Data: 0.038 (0.037)
Train: 58 [ 950/1251 ( 76%)]  Loss: 4.464 (4.52)  Time: 0.179s, 5733.61/s  (0.194s, 5290.99/s)  LR: 9.107e-04  Data: 0.034 (0.037)
Train: 58 [1000/1251 ( 80%)]  Loss: 4.882 (4.54)  Time: 0.180s, 5703.42/s  (0.193s, 5292.76/s)  LR: 9.107e-04  Data: 0.027 (0.036)
Train: 58 [1050/1251 ( 84%)]  Loss: 4.064 (4.51)  Time: 0.204s, 5031.59/s  (0.194s, 5281.70/s)  LR: 9.107e-04  Data: 0.026 (0.036)
Train: 58 [1100/1251 ( 88%)]  Loss: 4.689 (4.52)  Time: 0.190s, 5390.79/s  (0.194s, 5275.69/s)  LR: 9.107e-04  Data: 0.026 (0.035)
Train: 58 [1150/1251 ( 92%)]  Loss: 4.337 (4.51)  Time: 0.169s, 6075.04/s  (0.194s, 5269.65/s)  LR: 9.107e-04  Data: 0.030 (0.035)
Train: 58 [1200/1251 ( 96%)]  Loss: 4.294 (4.51)  Time: 0.175s, 5859.20/s  (0.194s, 5267.27/s)  LR: 9.107e-04  Data: 0.019 (0.035)
Train: 58 [1250/1251 (100%)]  Loss: 4.536 (4.51)  Time: 0.116s, 8806.61/s  (0.194s, 5288.24/s)  LR: 9.107e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.3208 (1.3208)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.3406 (1.9672)  Acc@1: 76.4151 (60.3960)  Acc@5: 90.5660 (83.0440)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-44.pth.tar', 59.454000126953126)

Train: 59 [   0/1251 (  0%)]  Loss: 4.164 (4.16)  Time: 1.750s,  585.06/s  (1.750s,  585.06/s)  LR: 9.077e-04  Data: 1.628 (1.628)
Train: 59 [  50/1251 (  4%)]  Loss: 4.561 (4.36)  Time: 0.162s, 6313.29/s  (0.222s, 4617.02/s)  LR: 9.077e-04  Data: 0.027 (0.073)
Train: 59 [ 100/1251 (  8%)]  Loss: 4.521 (4.42)  Time: 0.167s, 6125.10/s  (0.208s, 4912.72/s)  LR: 9.077e-04  Data: 0.025 (0.051)
Train: 59 [ 150/1251 ( 12%)]  Loss: 4.534 (4.45)  Time: 0.158s, 6486.70/s  (0.202s, 5062.70/s)  LR: 9.077e-04  Data: 0.027 (0.043)
Train: 59 [ 200/1251 ( 16%)]  Loss: 4.492 (4.45)  Time: 0.202s, 5060.31/s  (0.200s, 5120.65/s)  LR: 9.077e-04  Data: 0.031 (0.039)
Train: 59 [ 250/1251 ( 20%)]  Loss: 4.632 (4.48)  Time: 0.187s, 5467.83/s  (0.197s, 5199.36/s)  LR: 9.077e-04  Data: 0.029 (0.037)
Train: 59 [ 300/1251 ( 24%)]  Loss: 4.508 (4.49)  Time: 0.182s, 5614.91/s  (0.195s, 5239.50/s)  LR: 9.077e-04  Data: 0.028 (0.035)
Train: 59 [ 350/1251 ( 28%)]  Loss: 4.383 (4.47)  Time: 0.182s, 5639.66/s  (0.193s, 5293.41/s)  LR: 9.077e-04  Data: 0.024 (0.034)
Train: 59 [ 400/1251 ( 32%)]  Loss: 4.220 (4.45)  Time: 0.164s, 6234.57/s  (0.193s, 5307.54/s)  LR: 9.077e-04  Data: 0.025 (0.034)
Train: 59 [ 450/1251 ( 36%)]  Loss: 4.457 (4.45)  Time: 0.202s, 5067.29/s  (0.193s, 5309.59/s)  LR: 9.077e-04  Data: 0.023 (0.035)
Train: 59 [ 500/1251 ( 40%)]  Loss: 4.340 (4.44)  Time: 0.170s, 6013.02/s  (0.192s, 5327.72/s)  LR: 9.077e-04  Data: 0.029 (0.036)
Train: 59 [ 550/1251 ( 44%)]  Loss: 4.385 (4.43)  Time: 0.190s, 5384.27/s  (0.193s, 5295.39/s)  LR: 9.077e-04  Data: 0.031 (0.036)
Train: 59 [ 600/1251 ( 48%)]  Loss: 4.601 (4.45)  Time: 0.161s, 6350.69/s  (0.193s, 5295.20/s)  LR: 9.077e-04  Data: 0.022 (0.036)
Train: 59 [ 650/1251 ( 52%)]  Loss: 4.473 (4.45)  Time: 0.172s, 5945.32/s  (0.193s, 5292.38/s)  LR: 9.077e-04  Data: 0.025 (0.037)
Train: 59 [ 700/1251 ( 56%)]  Loss: 4.328 (4.44)  Time: 0.199s, 5144.26/s  (0.193s, 5305.26/s)  LR: 9.077e-04  Data: 0.028 (0.037)
Train: 59 [ 750/1251 ( 60%)]  Loss: 4.518 (4.44)  Time: 0.173s, 5932.41/s  (0.193s, 5292.60/s)  LR: 9.077e-04  Data: 0.027 (0.038)
Train: 59 [ 800/1251 ( 64%)]  Loss: 4.733 (4.46)  Time: 0.162s, 6334.09/s  (0.193s, 5302.41/s)  LR: 9.077e-04  Data: 0.020 (0.038)
Train: 59 [ 850/1251 ( 68%)]  Loss: 4.041 (4.44)  Time: 0.180s, 5697.40/s  (0.193s, 5305.61/s)  LR: 9.077e-04  Data: 0.030 (0.039)
Train: 59 [ 900/1251 ( 72%)]  Loss: 4.668 (4.45)  Time: 0.228s, 4497.65/s  (0.193s, 5296.58/s)  LR: 9.077e-04  Data: 0.027 (0.039)
Train: 59 [ 950/1251 ( 76%)]  Loss: 4.472 (4.45)  Time: 0.172s, 5960.84/s  (0.194s, 5286.52/s)  LR: 9.077e-04  Data: 0.028 (0.040)
Train: 59 [1000/1251 ( 80%)]  Loss: 4.530 (4.46)  Time: 0.177s, 5781.28/s  (0.193s, 5293.88/s)  LR: 9.077e-04  Data: 0.026 (0.040)
Train: 59 [1050/1251 ( 84%)]  Loss: 4.331 (4.45)  Time: 0.185s, 5547.96/s  (0.193s, 5301.97/s)  LR: 9.077e-04  Data: 0.020 (0.040)
Train: 59 [1100/1251 ( 88%)]  Loss: 4.613 (4.46)  Time: 0.173s, 5910.55/s  (0.193s, 5296.33/s)  LR: 9.077e-04  Data: 0.030 (0.041)
Train: 59 [1150/1251 ( 92%)]  Loss: 4.050 (4.44)  Time: 0.157s, 6505.67/s  (0.193s, 5297.44/s)  LR: 9.077e-04  Data: 0.035 (0.041)
Train: 59 [1200/1251 ( 96%)]  Loss: 4.485 (4.44)  Time: 0.165s, 6213.49/s  (0.193s, 5299.52/s)  LR: 9.077e-04  Data: 0.021 (0.040)
Train: 59 [1250/1251 (100%)]  Loss: 4.595 (4.45)  Time: 0.114s, 8998.97/s  (0.193s, 5308.28/s)  LR: 9.077e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.845 (1.845)  Loss:  1.3268 (1.3268)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.3009 (1.9934)  Acc@1: 78.6557 (60.8040)  Acc@5: 92.0991 (83.4660)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-46.pth.tar', 59.4940000390625)

Train: 60 [   0/1251 (  0%)]  Loss: 4.430 (4.43)  Time: 1.717s,  596.25/s  (1.717s,  596.25/s)  LR: 9.046e-04  Data: 1.598 (1.598)
Train: 60 [  50/1251 (  4%)]  Loss: 4.607 (4.52)  Time: 0.171s, 5978.28/s  (0.222s, 4605.14/s)  LR: 9.046e-04  Data: 0.025 (0.077)
Train: 60 [ 100/1251 (  8%)]  Loss: 4.305 (4.45)  Time: 0.155s, 6605.44/s  (0.207s, 4956.62/s)  LR: 9.046e-04  Data: 0.030 (0.054)
Train: 60 [ 150/1251 ( 12%)]  Loss: 4.530 (4.47)  Time: 0.187s, 5466.13/s  (0.201s, 5096.54/s)  LR: 9.046e-04  Data: 0.021 (0.048)
Train: 60 [ 200/1251 ( 16%)]  Loss: 4.580 (4.49)  Time: 0.151s, 6784.46/s  (0.198s, 5165.64/s)  LR: 9.046e-04  Data: 0.023 (0.043)
Train: 60 [ 250/1251 ( 20%)]  Loss: 4.344 (4.47)  Time: 0.185s, 5520.37/s  (0.199s, 5137.54/s)  LR: 9.046e-04  Data: 0.027 (0.043)
Train: 60 [ 300/1251 ( 24%)]  Loss: 4.255 (4.44)  Time: 0.155s, 6592.63/s  (0.197s, 5195.65/s)  LR: 9.046e-04  Data: 0.026 (0.040)
Train: 60 [ 350/1251 ( 28%)]  Loss: 4.719 (4.47)  Time: 0.238s, 4303.07/s  (0.195s, 5257.12/s)  LR: 9.046e-04  Data: 0.028 (0.038)
Train: 60 [ 400/1251 ( 32%)]  Loss: 4.377 (4.46)  Time: 0.169s, 6041.82/s  (0.194s, 5266.00/s)  LR: 9.046e-04  Data: 0.025 (0.037)
Train: 60 [ 450/1251 ( 36%)]  Loss: 4.320 (4.45)  Time: 0.161s, 6355.85/s  (0.195s, 5259.59/s)  LR: 9.046e-04  Data: 0.024 (0.036)
Train: 60 [ 500/1251 ( 40%)]  Loss: 4.256 (4.43)  Time: 0.188s, 5454.46/s  (0.194s, 5275.85/s)  LR: 9.046e-04  Data: 0.023 (0.035)
Train: 60 [ 550/1251 ( 44%)]  Loss: 4.824 (4.46)  Time: 0.393s, 2603.48/s  (0.195s, 5263.51/s)  LR: 9.046e-04  Data: 0.026 (0.035)
Train: 60 [ 600/1251 ( 48%)]  Loss: 4.677 (4.48)  Time: 0.194s, 5272.18/s  (0.194s, 5285.33/s)  LR: 9.046e-04  Data: 0.021 (0.034)
Train: 60 [ 650/1251 ( 52%)]  Loss: 4.668 (4.49)  Time: 0.176s, 5830.90/s  (0.194s, 5290.46/s)  LR: 9.046e-04  Data: 0.019 (0.034)
Train: 60 [ 700/1251 ( 56%)]  Loss: 4.244 (4.48)  Time: 0.173s, 5926.19/s  (0.193s, 5294.48/s)  LR: 9.046e-04  Data: 0.039 (0.033)
Train: 60 [ 750/1251 ( 60%)]  Loss: 4.530 (4.48)  Time: 0.300s, 3417.94/s  (0.193s, 5296.89/s)  LR: 9.046e-04  Data: 0.025 (0.033)
Train: 60 [ 800/1251 ( 64%)]  Loss: 4.638 (4.49)  Time: 0.161s, 6355.01/s  (0.193s, 5299.37/s)  LR: 9.046e-04  Data: 0.035 (0.033)
Train: 60 [ 850/1251 ( 68%)]  Loss: 4.726 (4.50)  Time: 0.155s, 6607.62/s  (0.193s, 5296.26/s)  LR: 9.046e-04  Data: 0.027 (0.032)
Train: 60 [ 900/1251 ( 72%)]  Loss: 4.483 (4.50)  Time: 0.197s, 5207.93/s  (0.193s, 5297.53/s)  LR: 9.046e-04  Data: 0.029 (0.032)
Train: 60 [ 950/1251 ( 76%)]  Loss: 4.769 (4.51)  Time: 0.172s, 5936.88/s  (0.193s, 5296.72/s)  LR: 9.046e-04  Data: 0.025 (0.032)
Train: 60 [1000/1251 ( 80%)]  Loss: 4.703 (4.52)  Time: 0.185s, 5527.69/s  (0.193s, 5295.99/s)  LR: 9.046e-04  Data: 0.036 (0.032)
Train: 60 [1050/1251 ( 84%)]  Loss: 4.585 (4.53)  Time: 0.161s, 6353.76/s  (0.194s, 5284.59/s)  LR: 9.046e-04  Data: 0.027 (0.032)
Train: 60 [1100/1251 ( 88%)]  Loss: 4.819 (4.54)  Time: 0.162s, 6338.65/s  (0.194s, 5286.45/s)  LR: 9.046e-04  Data: 0.026 (0.031)
Train: 60 [1150/1251 ( 92%)]  Loss: 4.665 (4.54)  Time: 0.186s, 5506.91/s  (0.194s, 5284.17/s)  LR: 9.046e-04  Data: 0.027 (0.031)
Train: 60 [1200/1251 ( 96%)]  Loss: 4.487 (4.54)  Time: 0.155s, 6594.32/s  (0.194s, 5267.30/s)  LR: 9.046e-04  Data: 0.029 (0.031)
Train: 60 [1250/1251 (100%)]  Loss: 4.404 (4.54)  Time: 0.114s, 9021.43/s  (0.194s, 5283.57/s)  LR: 9.046e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.2521 (1.2521)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1636 (1.9180)  Acc@1: 77.3585 (60.7140)  Acc@5: 91.9811 (83.4800)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-49.pth.tar', 59.621999912109374)

Train: 61 [   0/1251 (  0%)]  Loss: 4.232 (4.23)  Time: 1.930s,  530.57/s  (1.930s,  530.57/s)  LR: 9.015e-04  Data: 1.815 (1.815)
Train: 61 [  50/1251 (  4%)]  Loss: 4.429 (4.33)  Time: 0.160s, 6385.07/s  (0.228s, 4481.40/s)  LR: 9.015e-04  Data: 0.030 (0.066)
Train: 61 [ 100/1251 (  8%)]  Loss: 4.138 (4.27)  Time: 0.177s, 5769.84/s  (0.211s, 4854.12/s)  LR: 9.015e-04  Data: 0.024 (0.048)
Train: 61 [ 150/1251 ( 12%)]  Loss: 4.550 (4.34)  Time: 0.172s, 5952.70/s  (0.206s, 4967.03/s)  LR: 9.015e-04  Data: 0.019 (0.041)
Train: 61 [ 200/1251 ( 16%)]  Loss: 4.316 (4.33)  Time: 0.159s, 6435.88/s  (0.202s, 5071.90/s)  LR: 9.015e-04  Data: 0.027 (0.038)
Train: 61 [ 250/1251 ( 20%)]  Loss: 4.487 (4.36)  Time: 0.179s, 5734.15/s  (0.200s, 5130.73/s)  LR: 9.015e-04  Data: 0.033 (0.036)
Train: 61 [ 300/1251 ( 24%)]  Loss: 4.477 (4.38)  Time: 0.168s, 6101.00/s  (0.198s, 5165.05/s)  LR: 9.015e-04  Data: 0.032 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 61 [ 350/1251 ( 28%)]  Loss: 4.824 (4.43)  Time: 0.160s, 6402.58/s  (0.197s, 5206.11/s)  LR: 9.015e-04  Data: 0.036 (0.034)
Train: 61 [ 400/1251 ( 32%)]  Loss: 4.469 (4.44)  Time: 0.176s, 5802.51/s  (0.196s, 5236.36/s)  LR: 9.015e-04  Data: 0.025 (0.034)
Train: 61 [ 450/1251 ( 36%)]  Loss: 4.482 (4.44)  Time: 0.169s, 6044.57/s  (0.195s, 5240.18/s)  LR: 9.015e-04  Data: 0.032 (0.033)
Train: 61 [ 500/1251 ( 40%)]  Loss: 4.363 (4.43)  Time: 0.185s, 5547.99/s  (0.195s, 5261.84/s)  LR: 9.015e-04  Data: 0.030 (0.032)
Train: 61 [ 550/1251 ( 44%)]  Loss: 4.596 (4.45)  Time: 0.170s, 6040.59/s  (0.194s, 5264.95/s)  LR: 9.015e-04  Data: 0.024 (0.032)
Train: 61 [ 600/1251 ( 48%)]  Loss: 4.228 (4.43)  Time: 0.169s, 6054.72/s  (0.195s, 5261.04/s)  LR: 9.015e-04  Data: 0.027 (0.032)
Train: 61 [ 650/1251 ( 52%)]  Loss: 4.274 (4.42)  Time: 0.169s, 6059.00/s  (0.194s, 5279.10/s)  LR: 9.015e-04  Data: 0.037 (0.032)
Train: 61 [ 700/1251 ( 56%)]  Loss: 4.761 (4.44)  Time: 0.161s, 6348.28/s  (0.194s, 5283.80/s)  LR: 9.015e-04  Data: 0.022 (0.033)
Train: 61 [ 750/1251 ( 60%)]  Loss: 4.263 (4.43)  Time: 0.189s, 5407.32/s  (0.194s, 5270.92/s)  LR: 9.015e-04  Data: 0.026 (0.034)
Train: 61 [ 800/1251 ( 64%)]  Loss: 4.167 (4.42)  Time: 0.325s, 3148.98/s  (0.194s, 5278.01/s)  LR: 9.015e-04  Data: 0.187 (0.034)
Train: 61 [ 850/1251 ( 68%)]  Loss: 4.270 (4.41)  Time: 0.230s, 4449.45/s  (0.194s, 5278.57/s)  LR: 9.015e-04  Data: 0.092 (0.035)
Train: 61 [ 900/1251 ( 72%)]  Loss: 4.449 (4.41)  Time: 0.185s, 5531.69/s  (0.194s, 5272.92/s)  LR: 9.015e-04  Data: 0.027 (0.035)
Train: 61 [ 950/1251 ( 76%)]  Loss: 4.235 (4.40)  Time: 0.165s, 6214.79/s  (0.194s, 5279.12/s)  LR: 9.015e-04  Data: 0.023 (0.035)
Train: 61 [1000/1251 ( 80%)]  Loss: 4.738 (4.42)  Time: 0.160s, 6392.40/s  (0.194s, 5267.99/s)  LR: 9.015e-04  Data: 0.028 (0.036)
Train: 61 [1050/1251 ( 84%)]  Loss: 4.313 (4.41)  Time: 0.159s, 6455.22/s  (0.194s, 5270.24/s)  LR: 9.015e-04  Data: 0.032 (0.036)
Train: 61 [1100/1251 ( 88%)]  Loss: 4.691 (4.42)  Time: 0.168s, 6078.64/s  (0.194s, 5279.54/s)  LR: 9.015e-04  Data: 0.023 (0.036)
Train: 61 [1150/1251 ( 92%)]  Loss: 4.409 (4.42)  Time: 0.189s, 5428.22/s  (0.194s, 5265.13/s)  LR: 9.015e-04  Data: 0.028 (0.036)
Train: 61 [1200/1251 ( 96%)]  Loss: 4.333 (4.42)  Time: 0.189s, 5431.57/s  (0.195s, 5259.88/s)  LR: 9.015e-04  Data: 0.029 (0.036)
Train: 61 [1250/1251 (100%)]  Loss: 4.575 (4.43)  Time: 0.113s, 9028.96/s  (0.194s, 5282.01/s)  LR: 9.015e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.889 (1.889)  Loss:  1.2800 (1.2800)  Acc@1: 77.1484 (77.1484)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.3598 (2.0005)  Acc@1: 77.5943 (60.7060)  Acc@5: 91.7453 (83.3820)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-56.pth.tar', 59.683999968261716)

Train: 62 [   0/1251 (  0%)]  Loss: 4.030 (4.03)  Time: 1.754s,  583.66/s  (1.754s,  583.66/s)  LR: 8.984e-04  Data: 1.575 (1.575)
Train: 62 [  50/1251 (  4%)]  Loss: 4.364 (4.20)  Time: 0.179s, 5712.98/s  (0.220s, 4649.14/s)  LR: 8.984e-04  Data: 0.049 (0.067)
Train: 62 [ 100/1251 (  8%)]  Loss: 4.479 (4.29)  Time: 0.179s, 5735.48/s  (0.208s, 4929.21/s)  LR: 8.984e-04  Data: 0.025 (0.049)
Train: 62 [ 150/1251 ( 12%)]  Loss: 4.450 (4.33)  Time: 0.181s, 5665.73/s  (0.201s, 5085.97/s)  LR: 8.984e-04  Data: 0.057 (0.042)
Train: 62 [ 200/1251 ( 16%)]  Loss: 4.313 (4.33)  Time: 0.186s, 5510.57/s  (0.198s, 5183.61/s)  LR: 8.984e-04  Data: 0.028 (0.038)
Train: 62 [ 250/1251 ( 20%)]  Loss: 4.272 (4.32)  Time: 0.164s, 6258.91/s  (0.196s, 5220.67/s)  LR: 8.984e-04  Data: 0.027 (0.040)
Train: 62 [ 300/1251 ( 24%)]  Loss: 4.217 (4.30)  Time: 0.171s, 5983.06/s  (0.195s, 5261.35/s)  LR: 8.984e-04  Data: 0.033 (0.040)
Train: 62 [ 350/1251 ( 28%)]  Loss: 4.592 (4.34)  Time: 0.163s, 6288.03/s  (0.194s, 5269.82/s)  LR: 8.984e-04  Data: 0.026 (0.041)
Train: 62 [ 400/1251 ( 32%)]  Loss: 4.596 (4.37)  Time: 0.183s, 5602.35/s  (0.195s, 5261.68/s)  LR: 8.984e-04  Data: 0.026 (0.042)
Train: 62 [ 450/1251 ( 36%)]  Loss: 4.241 (4.36)  Time: 0.174s, 5889.10/s  (0.194s, 5281.09/s)  LR: 8.984e-04  Data: 0.028 (0.041)
Train: 62 [ 500/1251 ( 40%)]  Loss: 4.259 (4.35)  Time: 0.161s, 6349.43/s  (0.193s, 5296.10/s)  LR: 8.984e-04  Data: 0.026 (0.041)
Train: 62 [ 550/1251 ( 44%)]  Loss: 4.912 (4.39)  Time: 0.163s, 6296.70/s  (0.193s, 5312.18/s)  LR: 8.984e-04  Data: 0.025 (0.041)
Train: 62 [ 600/1251 ( 48%)]  Loss: 4.149 (4.38)  Time: 0.203s, 5045.69/s  (0.193s, 5298.21/s)  LR: 8.984e-04  Data: 0.031 (0.042)
Train: 62 [ 650/1251 ( 52%)]  Loss: 4.791 (4.40)  Time: 0.172s, 5968.06/s  (0.193s, 5307.78/s)  LR: 8.984e-04  Data: 0.029 (0.042)
Train: 62 [ 700/1251 ( 56%)]  Loss: 4.571 (4.42)  Time: 0.169s, 6048.23/s  (0.193s, 5292.68/s)  LR: 8.984e-04  Data: 0.025 (0.042)
Train: 62 [ 750/1251 ( 60%)]  Loss: 4.210 (4.40)  Time: 0.177s, 5772.22/s  (0.193s, 5315.07/s)  LR: 8.984e-04  Data: 0.026 (0.042)
Train: 62 [ 800/1251 ( 64%)]  Loss: 4.664 (4.42)  Time: 0.165s, 6220.05/s  (0.193s, 5306.84/s)  LR: 8.984e-04  Data: 0.027 (0.042)
Train: 62 [ 850/1251 ( 68%)]  Loss: 4.714 (4.43)  Time: 0.156s, 6549.41/s  (0.193s, 5316.78/s)  LR: 8.984e-04  Data: 0.021 (0.041)
Train: 62 [ 900/1251 ( 72%)]  Loss: 4.907 (4.46)  Time: 0.165s, 6187.37/s  (0.193s, 5311.17/s)  LR: 8.984e-04  Data: 0.031 (0.041)
Train: 62 [ 950/1251 ( 76%)]  Loss: 4.199 (4.45)  Time: 0.175s, 5862.34/s  (0.193s, 5311.03/s)  LR: 8.984e-04  Data: 0.029 (0.040)
Train: 62 [1000/1251 ( 80%)]  Loss: 4.419 (4.45)  Time: 0.149s, 6861.59/s  (0.193s, 5302.30/s)  LR: 8.984e-04  Data: 0.023 (0.039)
Train: 62 [1050/1251 ( 84%)]  Loss: 4.732 (4.46)  Time: 0.329s, 3109.31/s  (0.193s, 5299.72/s)  LR: 8.984e-04  Data: 0.029 (0.039)
Train: 62 [1100/1251 ( 88%)]  Loss: 4.637 (4.47)  Time: 0.152s, 6726.97/s  (0.193s, 5298.83/s)  LR: 8.984e-04  Data: 0.026 (0.038)
Train: 62 [1150/1251 ( 92%)]  Loss: 4.413 (4.46)  Time: 0.176s, 5802.77/s  (0.193s, 5292.36/s)  LR: 8.984e-04  Data: 0.025 (0.038)
Train: 62 [1200/1251 ( 96%)]  Loss: 4.187 (4.45)  Time: 0.149s, 6886.71/s  (0.194s, 5291.44/s)  LR: 8.984e-04  Data: 0.026 (0.037)
Train: 62 [1250/1251 (100%)]  Loss: 4.169 (4.44)  Time: 0.114s, 8987.71/s  (0.193s, 5301.45/s)  LR: 8.984e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.775 (1.775)  Loss:  1.2710 (1.2710)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.1971 (1.8785)  Acc@1: 78.0660 (61.5760)  Acc@5: 91.6274 (83.7420)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-47.pth.tar', 59.7659999609375)

Train: 63 [   0/1251 (  0%)]  Loss: 3.935 (3.93)  Time: 1.627s,  629.46/s  (1.627s,  629.46/s)  LR: 8.952e-04  Data: 1.502 (1.502)
Train: 63 [  50/1251 (  4%)]  Loss: 4.151 (4.04)  Time: 0.172s, 5954.72/s  (0.226s, 4531.28/s)  LR: 8.952e-04  Data: 0.031 (0.073)
Train: 63 [ 100/1251 (  8%)]  Loss: 4.363 (4.15)  Time: 0.163s, 6300.03/s  (0.207s, 4956.96/s)  LR: 8.952e-04  Data: 0.024 (0.059)
Train: 63 [ 150/1251 ( 12%)]  Loss: 4.252 (4.18)  Time: 0.166s, 6170.59/s  (0.198s, 5171.56/s)  LR: 8.952e-04  Data: 0.027 (0.049)
Train: 63 [ 200/1251 ( 16%)]  Loss: 4.164 (4.17)  Time: 0.167s, 6132.65/s  (0.196s, 5213.55/s)  LR: 8.952e-04  Data: 0.026 (0.046)
Train: 63 [ 250/1251 ( 20%)]  Loss: 4.333 (4.20)  Time: 0.166s, 6173.37/s  (0.195s, 5257.49/s)  LR: 8.952e-04  Data: 0.029 (0.042)
Train: 63 [ 300/1251 ( 24%)]  Loss: 4.257 (4.21)  Time: 0.169s, 6046.58/s  (0.194s, 5279.35/s)  LR: 8.952e-04  Data: 0.031 (0.040)
Train: 63 [ 350/1251 ( 28%)]  Loss: 4.374 (4.23)  Time: 0.178s, 5749.32/s  (0.193s, 5305.13/s)  LR: 8.952e-04  Data: 0.026 (0.038)
Train: 63 [ 400/1251 ( 32%)]  Loss: 4.439 (4.25)  Time: 0.169s, 6046.47/s  (0.193s, 5314.32/s)  LR: 8.952e-04  Data: 0.030 (0.037)
Train: 63 [ 450/1251 ( 36%)]  Loss: 4.451 (4.27)  Time: 0.174s, 5895.06/s  (0.193s, 5312.87/s)  LR: 8.952e-04  Data: 0.032 (0.036)
Train: 63 [ 500/1251 ( 40%)]  Loss: 4.660 (4.31)  Time: 0.157s, 6527.51/s  (0.192s, 5327.61/s)  LR: 8.952e-04  Data: 0.024 (0.035)
Train: 63 [ 550/1251 ( 44%)]  Loss: 4.542 (4.33)  Time: 0.160s, 6393.03/s  (0.193s, 5318.66/s)  LR: 8.952e-04  Data: 0.025 (0.035)
Train: 63 [ 600/1251 ( 48%)]  Loss: 4.353 (4.33)  Time: 0.186s, 5509.28/s  (0.192s, 5326.75/s)  LR: 8.952e-04  Data: 0.030 (0.034)
Train: 63 [ 650/1251 ( 52%)]  Loss: 4.398 (4.33)  Time: 0.179s, 5711.73/s  (0.192s, 5327.94/s)  LR: 8.952e-04  Data: 0.020 (0.034)
Train: 63 [ 700/1251 ( 56%)]  Loss: 3.868 (4.30)  Time: 0.158s, 6462.60/s  (0.192s, 5336.94/s)  LR: 8.952e-04  Data: 0.023 (0.033)
Train: 63 [ 750/1251 ( 60%)]  Loss: 4.182 (4.30)  Time: 0.159s, 6443.38/s  (0.192s, 5332.33/s)  LR: 8.952e-04  Data: 0.032 (0.034)
Train: 63 [ 800/1251 ( 64%)]  Loss: 4.229 (4.29)  Time: 0.170s, 6015.55/s  (0.192s, 5334.54/s)  LR: 8.952e-04  Data: 0.028 (0.033)
Train: 63 [ 850/1251 ( 68%)]  Loss: 4.657 (4.31)  Time: 0.194s, 5290.66/s  (0.192s, 5336.80/s)  LR: 8.952e-04  Data: 0.023 (0.034)
Train: 63 [ 900/1251 ( 72%)]  Loss: 4.552 (4.32)  Time: 0.161s, 6367.55/s  (0.192s, 5335.76/s)  LR: 8.952e-04  Data: 0.026 (0.033)
Train: 63 [ 950/1251 ( 76%)]  Loss: 4.295 (4.32)  Time: 0.174s, 5877.37/s  (0.192s, 5336.57/s)  LR: 8.952e-04  Data: 0.024 (0.034)
Train: 63 [1000/1251 ( 80%)]  Loss: 4.464 (4.33)  Time: 0.329s, 3114.84/s  (0.192s, 5330.27/s)  LR: 8.952e-04  Data: 0.044 (0.034)
Train: 63 [1050/1251 ( 84%)]  Loss: 4.258 (4.33)  Time: 0.182s, 5624.81/s  (0.192s, 5324.84/s)  LR: 8.952e-04  Data: 0.024 (0.034)
Train: 63 [1100/1251 ( 88%)]  Loss: 4.498 (4.33)  Time: 0.171s, 5998.62/s  (0.192s, 5320.81/s)  LR: 8.952e-04  Data: 0.027 (0.035)
Train: 63 [1150/1251 ( 92%)]  Loss: 4.322 (4.33)  Time: 0.157s, 6532.11/s  (0.193s, 5319.28/s)  LR: 8.952e-04  Data: 0.021 (0.036)
Train: 63 [1200/1251 ( 96%)]  Loss: 4.308 (4.33)  Time: 0.177s, 5786.17/s  (0.193s, 5316.63/s)  LR: 8.952e-04  Data: 0.022 (0.036)
Train: 63 [1250/1251 (100%)]  Loss: 4.270 (4.33)  Time: 0.114s, 9011.49/s  (0.192s, 5330.60/s)  LR: 8.952e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.853 (1.853)  Loss:  1.2530 (1.2530)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.214)  Loss:  1.2788 (1.9649)  Acc@1: 75.9434 (60.5320)  Acc@5: 90.6840 (83.0520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-54.pth.tar', 59.92199992675781)

Train: 64 [   0/1251 (  0%)]  Loss: 4.596 (4.60)  Time: 1.894s,  540.68/s  (1.894s,  540.68/s)  LR: 8.920e-04  Data: 1.769 (1.769)
Train: 64 [  50/1251 (  4%)]  Loss: 4.577 (4.59)  Time: 0.190s, 5380.18/s  (0.232s, 4420.99/s)  LR: 8.920e-04  Data: 0.023 (0.078)
Train: 64 [ 100/1251 (  8%)]  Loss: 4.439 (4.54)  Time: 0.178s, 5752.65/s  (0.209s, 4890.63/s)  LR: 8.920e-04  Data: 0.024 (0.058)
Train: 64 [ 150/1251 ( 12%)]  Loss: 4.261 (4.47)  Time: 0.167s, 6144.51/s  (0.201s, 5102.06/s)  LR: 8.920e-04  Data: 0.034 (0.052)
Train: 64 [ 200/1251 ( 16%)]  Loss: 4.363 (4.45)  Time: 0.185s, 5542.43/s  (0.198s, 5167.03/s)  LR: 8.920e-04  Data: 0.049 (0.047)
Train: 64 [ 250/1251 ( 20%)]  Loss: 4.865 (4.52)  Time: 0.178s, 5758.17/s  (0.197s, 5195.62/s)  LR: 8.920e-04  Data: 0.021 (0.044)
Train: 64 [ 300/1251 ( 24%)]  Loss: 4.490 (4.51)  Time: 0.173s, 5912.62/s  (0.195s, 5251.20/s)  LR: 8.920e-04  Data: 0.025 (0.042)
Train: 64 [ 350/1251 ( 28%)]  Loss: 4.363 (4.49)  Time: 0.177s, 5786.50/s  (0.199s, 5144.25/s)  LR: 8.920e-04  Data: 0.030 (0.042)
Train: 64 [ 400/1251 ( 32%)]  Loss: 4.749 (4.52)  Time: 0.179s, 5728.49/s  (0.197s, 5185.51/s)  LR: 8.920e-04  Data: 0.028 (0.040)
Train: 64 [ 450/1251 ( 36%)]  Loss: 4.728 (4.54)  Time: 0.187s, 5480.30/s  (0.196s, 5216.53/s)  LR: 8.920e-04  Data: 0.039 (0.039)
Train: 64 [ 500/1251 ( 40%)]  Loss: 4.612 (4.55)  Time: 0.175s, 5845.62/s  (0.196s, 5237.10/s)  LR: 8.920e-04  Data: 0.032 (0.038)
Train: 64 [ 550/1251 ( 44%)]  Loss: 4.403 (4.54)  Time: 0.183s, 5593.25/s  (0.195s, 5260.83/s)  LR: 8.920e-04  Data: 0.032 (0.037)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 64 [ 600/1251 ( 48%)]  Loss: 4.492 (4.53)  Time: 0.351s, 2918.06/s  (0.195s, 5253.84/s)  LR: 8.920e-04  Data: 0.019 (0.036)
Train: 64 [ 650/1251 ( 52%)]  Loss: 4.092 (4.50)  Time: 0.176s, 5808.54/s  (0.195s, 5261.02/s)  LR: 8.920e-04  Data: 0.026 (0.036)
Train: 64 [ 700/1251 ( 56%)]  Loss: 4.837 (4.52)  Time: 0.192s, 5344.76/s  (0.194s, 5266.39/s)  LR: 8.920e-04  Data: 0.025 (0.035)
Train: 64 [ 750/1251 ( 60%)]  Loss: 4.540 (4.53)  Time: 0.177s, 5783.70/s  (0.194s, 5276.67/s)  LR: 8.920e-04  Data: 0.032 (0.035)
Train: 64 [ 800/1251 ( 64%)]  Loss: 4.542 (4.53)  Time: 0.157s, 6517.73/s  (0.194s, 5275.09/s)  LR: 8.920e-04  Data: 0.029 (0.035)
Train: 64 [ 850/1251 ( 68%)]  Loss: 3.829 (4.49)  Time: 0.205s, 5002.44/s  (0.194s, 5275.10/s)  LR: 8.920e-04  Data: 0.025 (0.034)
Train: 64 [ 900/1251 ( 72%)]  Loss: 4.458 (4.49)  Time: 0.181s, 5659.55/s  (0.194s, 5279.02/s)  LR: 8.920e-04  Data: 0.019 (0.034)
Train: 64 [ 950/1251 ( 76%)]  Loss: 4.617 (4.49)  Time: 0.156s, 6578.53/s  (0.194s, 5285.73/s)  LR: 8.920e-04  Data: 0.035 (0.034)
Train: 64 [1000/1251 ( 80%)]  Loss: 4.089 (4.47)  Time: 0.166s, 6180.74/s  (0.194s, 5284.52/s)  LR: 8.920e-04  Data: 0.022 (0.035)
Train: 64 [1050/1251 ( 84%)]  Loss: 4.623 (4.48)  Time: 0.156s, 6568.85/s  (0.194s, 5284.60/s)  LR: 8.920e-04  Data: 0.031 (0.035)
Train: 64 [1100/1251 ( 88%)]  Loss: 4.608 (4.49)  Time: 0.182s, 5620.90/s  (0.194s, 5278.88/s)  LR: 8.920e-04  Data: 0.024 (0.036)
Train: 64 [1150/1251 ( 92%)]  Loss: 4.642 (4.49)  Time: 0.219s, 4682.05/s  (0.194s, 5276.45/s)  LR: 8.920e-04  Data: 0.095 (0.036)
Train: 64 [1200/1251 ( 96%)]  Loss: 4.415 (4.49)  Time: 0.156s, 6565.37/s  (0.194s, 5279.52/s)  LR: 8.920e-04  Data: 0.031 (0.036)
Train: 64 [1250/1251 (100%)]  Loss: 3.924 (4.47)  Time: 0.113s, 9091.03/s  (0.193s, 5292.12/s)  LR: 8.920e-04  Data: 0.000 (0.036)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.842 (1.842)  Loss:  1.2874 (1.2874)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.3505 (1.9579)  Acc@1: 77.3585 (60.2460)  Acc@5: 91.7453 (82.9140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-64.pth.tar', 60.245999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-55.pth.tar', 59.96800001953125)

Train: 65 [   0/1251 (  0%)]  Loss: 4.412 (4.41)  Time: 1.815s,  564.16/s  (1.815s,  564.16/s)  LR: 8.887e-04  Data: 1.684 (1.684)
Train: 65 [  50/1251 (  4%)]  Loss: 4.520 (4.47)  Time: 0.297s, 3453.41/s  (0.222s, 4608.90/s)  LR: 8.887e-04  Data: 0.038 (0.064)
Train: 65 [ 100/1251 (  8%)]  Loss: 4.410 (4.45)  Time: 0.195s, 5239.73/s  (0.209s, 4905.53/s)  LR: 8.887e-04  Data: 0.022 (0.047)
Train: 65 [ 150/1251 ( 12%)]  Loss: 4.634 (4.49)  Time: 0.173s, 5926.83/s  (0.199s, 5157.33/s)  LR: 8.887e-04  Data: 0.025 (0.040)
Train: 65 [ 200/1251 ( 16%)]  Loss: 4.613 (4.52)  Time: 0.256s, 3999.86/s  (0.197s, 5185.36/s)  LR: 8.887e-04  Data: 0.033 (0.041)
Train: 65 [ 250/1251 ( 20%)]  Loss: 4.788 (4.56)  Time: 0.164s, 6250.94/s  (0.196s, 5221.36/s)  LR: 8.887e-04  Data: 0.029 (0.041)
Train: 65 [ 300/1251 ( 24%)]  Loss: 4.427 (4.54)  Time: 0.200s, 5130.69/s  (0.195s, 5252.82/s)  LR: 8.887e-04  Data: 0.024 (0.042)
Train: 65 [ 350/1251 ( 28%)]  Loss: 4.439 (4.53)  Time: 0.171s, 5974.01/s  (0.195s, 5260.04/s)  LR: 8.887e-04  Data: 0.019 (0.043)
Train: 65 [ 400/1251 ( 32%)]  Loss: 4.266 (4.50)  Time: 0.158s, 6499.42/s  (0.194s, 5282.49/s)  LR: 8.887e-04  Data: 0.024 (0.043)
Train: 65 [ 450/1251 ( 36%)]  Loss: 4.275 (4.48)  Time: 0.160s, 6392.05/s  (0.193s, 5307.56/s)  LR: 8.887e-04  Data: 0.028 (0.042)
Train: 65 [ 500/1251 ( 40%)]  Loss: 4.392 (4.47)  Time: 0.186s, 5501.15/s  (0.193s, 5301.98/s)  LR: 8.887e-04  Data: 0.026 (0.042)
Train: 65 [ 550/1251 ( 44%)]  Loss: 4.460 (4.47)  Time: 0.176s, 5802.49/s  (0.193s, 5305.62/s)  LR: 8.887e-04  Data: 0.028 (0.043)
Train: 65 [ 600/1251 ( 48%)]  Loss: 4.285 (4.46)  Time: 0.171s, 5979.85/s  (0.193s, 5315.06/s)  LR: 8.887e-04  Data: 0.023 (0.043)
Train: 65 [ 650/1251 ( 52%)]  Loss: 4.032 (4.43)  Time: 0.191s, 5372.50/s  (0.193s, 5316.88/s)  LR: 8.887e-04  Data: 0.022 (0.042)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0




Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 65 [ 700/1251 ( 56%)]  Loss: 4.294 (4.42)  Time: 0.171s, 5996.87/s  (0.192s, 5324.53/s)  LR: 8.887e-04  Data: 0.022 (0.041)
Train: 65 [ 750/1251 ( 60%)]  Loss: 4.172 (4.40)  Time: 0.164s, 6248.80/s  (0.193s, 5312.53/s)  LR: 8.887e-04  Data: 0.019 (0.040)
Train: 65 [ 800/1251 ( 64%)]  Loss: 4.213 (4.39)  Time: 0.279s, 3667.19/s  (0.193s, 5310.38/s)  LR: 8.887e-04  Data: 0.026 (0.039)
Train: 65 [ 850/1251 ( 68%)]  Loss: 4.569 (4.40)  Time: 0.185s, 5541.72/s  (0.193s, 5310.04/s)  LR: 8.887e-04  Data: 0.039 (0.039)
Train: 65 [ 900/1251 ( 72%)]  Loss: 4.386 (4.40)  Time: 0.160s, 6416.49/s  (0.193s, 5313.34/s)  LR: 8.887e-04  Data: 0.027 (0.038)
Train: 65 [ 950/1251 ( 76%)]  Loss: 4.333 (4.40)  Time: 0.176s, 5811.62/s  (0.193s, 5307.59/s)  LR: 8.887e-04  Data: 0.028 (0.037)
Train: 65 [1000/1251 ( 80%)]  Loss: 4.305 (4.39)  Time: 0.244s, 4198.47/s  (0.193s, 5311.37/s)  LR: 8.887e-04  Data: 0.025 (0.037)
Train: 65 [1050/1251 ( 84%)]  Loss: 4.370 (4.39)  Time: 0.189s, 5431.66/s  (0.193s, 5314.28/s)  LR: 8.887e-04  Data: 0.027 (0.036)
Train: 65 [1100/1251 ( 88%)]  Loss: 4.644 (4.40)  Time: 0.176s, 5816.78/s  (0.193s, 5301.14/s)  LR: 8.887e-04  Data: 0.030 (0.036)
Train: 65 [1150/1251 ( 92%)]  Loss: 4.417 (4.40)  Time: 0.178s, 5748.37/s  (0.193s, 5303.66/s)  LR: 8.887e-04  Data: 0.031 (0.036)
Train: 65 [1200/1251 ( 96%)]  Loss: 4.629 (4.41)  Time: 0.197s, 5193.15/s  (0.193s, 5299.97/s)  LR: 8.887e-04  Data: 0.030 (0.036)
Train: 65 [1250/1251 (100%)]  Loss: 4.701 (4.42)  Time: 0.114s, 9019.82/s  (0.193s, 5313.49/s)  LR: 8.887e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.831 (1.831)  Loss:  1.2910 (1.2910)  Acc@1: 75.7812 (75.7812)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2445 (1.9490)  Acc@1: 76.0613 (61.1340)  Acc@5: 92.0991 (83.8600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-64.pth.tar', 60.245999990234374)

Train: 66 [   0/1251 (  0%)]  Loss: 4.427 (4.43)  Time: 1.683s,  608.56/s  (1.683s,  608.56/s)  LR: 8.854e-04  Data: 1.569 (1.569)
Train: 66 [  50/1251 (  4%)]  Loss: 4.411 (4.42)  Time: 0.175s, 5836.50/s  (0.222s, 4607.42/s)  LR: 8.854e-04  Data: 0.025 (0.069)
Train: 66 [ 100/1251 (  8%)]  Loss: 4.400 (4.41)  Time: 0.170s, 6030.40/s  (0.208s, 4920.63/s)  LR: 8.854e-04  Data: 0.025 (0.050)
Train: 66 [ 150/1251 ( 12%)]  Loss: 4.658 (4.47)  Time: 0.181s, 5653.93/s  (0.201s, 5097.15/s)  LR: 8.854e-04  Data: 0.019 (0.043)
Train: 66 [ 200/1251 ( 16%)]  Loss: 4.664 (4.51)  Time: 0.168s, 6113.34/s  (0.198s, 5180.81/s)  LR: 8.854e-04  Data: 0.026 (0.042)
Train: 66 [ 250/1251 ( 20%)]  Loss: 4.341 (4.48)  Time: 0.186s, 5494.69/s  (0.197s, 5184.94/s)  LR: 8.854e-04  Data: 0.026 (0.040)
Train: 66 [ 300/1251 ( 24%)]  Loss: 4.155 (4.44)  Time: 0.185s, 5549.32/s  (0.196s, 5226.68/s)  LR: 8.854e-04  Data: 0.025 (0.040)
Train: 66 [ 350/1251 ( 28%)]  Loss: 4.419 (4.43)  Time: 0.164s, 6245.53/s  (0.194s, 5268.54/s)  LR: 8.854e-04  Data: 0.030 (0.040)
Train: 66 [ 400/1251 ( 32%)]  Loss: 4.463 (4.44)  Time: 0.346s, 2962.21/s  (0.194s, 5279.63/s)  LR: 8.854e-04  Data: 0.223 (0.041)
Train: 66 [ 450/1251 ( 36%)]  Loss: 4.724 (4.47)  Time: 0.197s, 5186.49/s  (0.194s, 5278.51/s)  LR: 8.854e-04  Data: 0.028 (0.041)
Train: 66 [ 500/1251 ( 40%)]  Loss: 4.713 (4.49)  Time: 0.160s, 6416.68/s  (0.193s, 5295.36/s)  LR: 8.854e-04  Data: 0.021 (0.041)
Train: 66 [ 550/1251 ( 44%)]  Loss: 4.312 (4.47)  Time: 0.197s, 5204.34/s  (0.193s, 5299.18/s)  LR: 8.854e-04  Data: 0.020 (0.041)
Train: 66 [ 600/1251 ( 48%)]  Loss: 4.238 (4.46)  Time: 0.330s, 3106.88/s  (0.193s, 5302.25/s)  LR: 8.854e-04  Data: 0.197 (0.040)
Train: 66 [ 650/1251 ( 52%)]  Loss: 4.375 (4.45)  Time: 0.165s, 6214.92/s  (0.193s, 5292.07/s)  LR: 8.854e-04  Data: 0.030 (0.041)
Train: 66 [ 700/1251 ( 56%)]  Loss: 4.455 (4.45)  Time: 0.166s, 6153.35/s  (0.193s, 5301.42/s)  LR: 8.854e-04  Data: 0.025 (0.041)
Train: 66 [ 750/1251 ( 60%)]  Loss: 4.435 (4.45)  Time: 0.184s, 5577.53/s  (0.193s, 5315.66/s)  LR: 8.854e-04  Data: 0.029 (0.041)
Train: 66 [ 800/1251 ( 64%)]  Loss: 4.710 (4.46)  Time: 0.241s, 4244.51/s  (0.193s, 5312.95/s)  LR: 8.854e-04  Data: 0.108 (0.041)
Train: 66 [ 850/1251 ( 68%)]  Loss: 4.632 (4.47)  Time: 0.162s, 6317.27/s  (0.193s, 5309.31/s)  LR: 8.854e-04  Data: 0.026 (0.041)
Train: 66 [ 900/1251 ( 72%)]  Loss: 4.280 (4.46)  Time: 0.193s, 5312.41/s  (0.193s, 5299.12/s)  LR: 8.854e-04  Data: 0.035 (0.041)
Train: 66 [ 950/1251 ( 76%)]  Loss: 4.419 (4.46)  Time: 0.429s, 2385.08/s  (0.193s, 5299.68/s)  LR: 8.854e-04  Data: 0.024 (0.041)
Train: 66 [1000/1251 ( 80%)]  Loss: 3.975 (4.44)  Time: 0.172s, 5960.77/s  (0.193s, 5295.95/s)  LR: 8.854e-04  Data: 0.042 (0.040)
Train: 66 [1050/1251 ( 84%)]  Loss: 4.283 (4.43)  Time: 0.177s, 5796.83/s  (0.193s, 5296.97/s)  LR: 8.854e-04  Data: 0.028 (0.040)
Train: 66 [1100/1251 ( 88%)]  Loss: 4.564 (4.44)  Time: 0.174s, 5870.30/s  (0.193s, 5296.25/s)  LR: 8.854e-04  Data: 0.026 (0.040)
Train: 66 [1150/1251 ( 92%)]  Loss: 4.275 (4.43)  Time: 0.270s, 3791.59/s  (0.193s, 5292.40/s)  LR: 8.854e-04  Data: 0.034 (0.039)
Train: 66 [1200/1251 ( 96%)]  Loss: 4.180 (4.42)  Time: 0.239s, 4285.40/s  (0.193s, 5295.58/s)  LR: 8.854e-04  Data: 0.032 (0.039)
Train: 66 [1250/1251 (100%)]  Loss: 4.858 (4.44)  Time: 0.114s, 8985.17/s  (0.193s, 5308.21/s)  LR: 8.854e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.883 (1.883)  Loss:  1.2578 (1.2578)  Acc@1: 75.7812 (75.7812)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.019 (0.224)  Loss:  1.3612 (1.9722)  Acc@1: 76.7689 (60.6980)  Acc@5: 89.5047 (83.0500)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-58.pth.tar', 60.39600009765625)

Train: 67 [   0/1251 (  0%)]  Loss: 4.375 (4.38)  Time: 1.722s,  594.55/s  (1.722s,  594.55/s)  LR: 8.820e-04  Data: 1.592 (1.592)
Train: 67 [  50/1251 (  4%)]  Loss: 4.508 (4.44)  Time: 0.173s, 5904.31/s  (0.221s, 4625.79/s)  LR: 8.820e-04  Data: 0.026 (0.069)
Train: 67 [ 100/1251 (  8%)]  Loss: 4.520 (4.47)  Time: 0.188s, 5454.89/s  (0.206s, 4969.36/s)  LR: 8.820e-04  Data: 0.026 (0.049)
Train: 67 [ 150/1251 ( 12%)]  Loss: 4.096 (4.37)  Time: 0.205s, 5000.42/s  (0.198s, 5174.52/s)  LR: 8.820e-04  Data: 0.022 (0.042)
Train: 67 [ 200/1251 ( 16%)]  Loss: 4.089 (4.32)  Time: 0.162s, 6329.93/s  (0.198s, 5179.04/s)  LR: 8.820e-04  Data: 0.025 (0.044)
Train: 67 [ 250/1251 ( 20%)]  Loss: 4.707 (4.38)  Time: 0.169s, 6065.42/s  (0.195s, 5242.11/s)  LR: 8.820e-04  Data: 0.026 (0.043)
Train: 67 [ 300/1251 ( 24%)]  Loss: 4.216 (4.36)  Time: 0.168s, 6077.53/s  (0.195s, 5249.73/s)  LR: 8.820e-04  Data: 0.027 (0.042)
Train: 67 [ 350/1251 ( 28%)]  Loss: 4.526 (4.38)  Time: 0.184s, 5562.25/s  (0.195s, 5254.54/s)  LR: 8.820e-04  Data: 0.032 (0.040)
Train: 67 [ 400/1251 ( 32%)]  Loss: 4.240 (4.36)  Time: 0.160s, 6405.02/s  (0.194s, 5267.28/s)  LR: 8.820e-04  Data: 0.030 (0.038)
Train: 67 [ 450/1251 ( 36%)]  Loss: 4.057 (4.33)  Time: 0.157s, 6521.15/s  (0.194s, 5277.62/s)  LR: 8.820e-04  Data: 0.031 (0.037)
Train: 67 [ 500/1251 ( 40%)]  Loss: 4.768 (4.37)  Time: 0.186s, 5491.38/s  (0.193s, 5303.60/s)  LR: 8.820e-04  Data: 0.027 (0.036)
Train: 67 [ 550/1251 ( 44%)]  Loss: 4.380 (4.37)  Time: 0.147s, 6986.79/s  (0.193s, 5292.96/s)  LR: 8.820e-04  Data: 0.027 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 67 [ 600/1251 ( 48%)]  Loss: 4.558 (4.39)  Time: 0.155s, 6586.15/s  (0.193s, 5307.65/s)  LR: 8.820e-04  Data: 0.035 (0.035)
Train: 67 [ 650/1251 ( 52%)]  Loss: 3.960 (4.36)  Time: 0.186s, 5518.98/s  (0.193s, 5303.20/s)  LR: 8.820e-04  Data: 0.022 (0.034)
Train: 67 [ 700/1251 ( 56%)]  Loss: 4.527 (4.37)  Time: 0.160s, 6403.14/s  (0.194s, 5291.93/s)  LR: 8.820e-04  Data: 0.029 (0.034)
Train: 67 [ 750/1251 ( 60%)]  Loss: 4.418 (4.37)  Time: 0.183s, 5600.92/s  (0.194s, 5288.08/s)  LR: 8.820e-04  Data: 0.023 (0.033)
Train: 67 [ 800/1251 ( 64%)]  Loss: 4.828 (4.40)  Time: 0.158s, 6497.10/s  (0.193s, 5301.36/s)  LR: 8.820e-04  Data: 0.036 (0.033)
Train: 67 [ 850/1251 ( 68%)]  Loss: 4.504 (4.40)  Time: 0.197s, 5195.17/s  (0.193s, 5299.89/s)  LR: 8.820e-04  Data: 0.027 (0.033)
Train: 67 [ 900/1251 ( 72%)]  Loss: 4.380 (4.40)  Time: 0.178s, 5742.59/s  (0.193s, 5295.79/s)  LR: 8.820e-04  Data: 0.031 (0.033)
Train: 67 [ 950/1251 ( 76%)]  Loss: 4.462 (4.41)  Time: 0.178s, 5753.70/s  (0.193s, 5292.77/s)  LR: 8.820e-04  Data: 0.023 (0.032)
Train: 67 [1000/1251 ( 80%)]  Loss: 4.553 (4.41)  Time: 0.170s, 6006.92/s  (0.193s, 5301.91/s)  LR: 8.820e-04  Data: 0.032 (0.032)
Train: 67 [1050/1251 ( 84%)]  Loss: 4.552 (4.42)  Time: 0.163s, 6294.01/s  (0.193s, 5306.05/s)  LR: 8.820e-04  Data: 0.027 (0.032)
Train: 67 [1100/1251 ( 88%)]  Loss: 4.355 (4.42)  Time: 0.167s, 6128.91/s  (0.193s, 5299.65/s)  LR: 8.820e-04  Data: 0.029 (0.032)
Train: 67 [1150/1251 ( 92%)]  Loss: 4.477 (4.42)  Time: 0.179s, 5723.08/s  (0.193s, 5295.14/s)  LR: 8.820e-04  Data: 0.042 (0.032)
Train: 67 [1200/1251 ( 96%)]  Loss: 4.537 (4.42)  Time: 0.178s, 5762.13/s  (0.193s, 5292.14/s)  LR: 8.820e-04  Data: 0.029 (0.031)
Train: 67 [1250/1251 (100%)]  Loss: 4.766 (4.44)  Time: 0.113s, 9052.29/s  (0.193s, 5300.05/s)  LR: 8.820e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.847 (1.847)  Loss:  1.2376 (1.2376)  Acc@1: 78.3203 (78.3203)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3623 (1.8988)  Acc@1: 77.4764 (61.7560)  Acc@5: 91.7453 (84.0040)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-50.pth.tar', 60.41799991210937)

Train: 68 [   0/1251 (  0%)]  Loss: 4.336 (4.34)  Time: 1.736s,  589.86/s  (1.736s,  589.86/s)  LR: 8.786e-04  Data: 1.615 (1.615)
Train: 68 [  50/1251 (  4%)]  Loss: 4.634 (4.48)  Time: 0.163s, 6295.34/s  (0.225s, 4544.23/s)  LR: 8.786e-04  Data: 0.029 (0.074)
Train: 68 [ 100/1251 (  8%)]  Loss: 4.197 (4.39)  Time: 0.154s, 6654.69/s  (0.204s, 5014.73/s)  LR: 8.786e-04  Data: 0.035 (0.051)
Train: 68 [ 150/1251 ( 12%)]  Loss: 4.660 (4.46)  Time: 0.174s, 5896.71/s  (0.200s, 5113.32/s)  LR: 8.786e-04  Data: 0.024 (0.044)
Train: 68 [ 200/1251 ( 16%)]  Loss: 4.544 (4.47)  Time: 0.175s, 5853.01/s  (0.197s, 5185.70/s)  LR: 8.786e-04  Data: 0.025 (0.040)
Train: 68 [ 250/1251 ( 20%)]  Loss: 4.610 (4.50)  Time: 0.180s, 5690.09/s  (0.196s, 5234.02/s)  LR: 8.786e-04  Data: 0.034 (0.038)
Train: 68 [ 300/1251 ( 24%)]  Loss: 4.727 (4.53)  Time: 0.177s, 5770.74/s  (0.194s, 5265.41/s)  LR: 8.786e-04  Data: 0.024 (0.036)
Train: 68 [ 350/1251 ( 28%)]  Loss: 4.274 (4.50)  Time: 0.186s, 5511.55/s  (0.193s, 5299.69/s)  LR: 8.786e-04  Data: 0.023 (0.035)
Train: 68 [ 400/1251 ( 32%)]  Loss: 4.411 (4.49)  Time: 0.166s, 6150.20/s  (0.193s, 5304.66/s)  LR: 8.786e-04  Data: 0.028 (0.034)
Train: 68 [ 450/1251 ( 36%)]  Loss: 4.482 (4.49)  Time: 0.174s, 5887.71/s  (0.192s, 5321.25/s)  LR: 8.786e-04  Data: 0.024 (0.035)
Train: 68 [ 500/1251 ( 40%)]  Loss: 4.621 (4.50)  Time: 0.182s, 5631.19/s  (0.193s, 5315.09/s)  LR: 8.786e-04  Data: 0.033 (0.037)
Train: 68 [ 550/1251 ( 44%)]  Loss: 4.392 (4.49)  Time: 0.206s, 4961.49/s  (0.193s, 5316.28/s)  LR: 8.786e-04  Data: 0.031 (0.038)
Train: 68 [ 600/1251 ( 48%)]  Loss: 4.424 (4.49)  Time: 0.165s, 6187.88/s  (0.192s, 5327.79/s)  LR: 8.786e-04  Data: 0.025 (0.038)
Train: 68 [ 650/1251 ( 52%)]  Loss: 4.353 (4.48)  Time: 0.173s, 5926.52/s  (0.193s, 5316.66/s)  LR: 8.786e-04  Data: 0.024 (0.040)
Train: 68 [ 700/1251 ( 56%)]  Loss: 4.753 (4.49)  Time: 0.157s, 6516.67/s  (0.193s, 5316.68/s)  LR: 8.786e-04  Data: 0.027 (0.040)
Train: 68 [ 750/1251 ( 60%)]  Loss: 4.239 (4.48)  Time: 0.172s, 5961.24/s  (0.192s, 5321.69/s)  LR: 8.786e-04  Data: 0.031 (0.040)
Train: 68 [ 800/1251 ( 64%)]  Loss: 4.288 (4.47)  Time: 0.168s, 6094.69/s  (0.192s, 5325.39/s)  LR: 8.786e-04  Data: 0.025 (0.040)
Train: 68 [ 850/1251 ( 68%)]  Loss: 4.415 (4.46)  Time: 0.156s, 6543.54/s  (0.192s, 5335.46/s)  LR: 8.786e-04  Data: 0.030 (0.039)
Train: 68 [ 900/1251 ( 72%)]  Loss: 4.497 (4.47)  Time: 0.171s, 5972.26/s  (0.192s, 5331.99/s)  LR: 8.786e-04  Data: 0.026 (0.039)
Train: 68 [ 950/1251 ( 76%)]  Loss: 4.649 (4.48)  Time: 0.183s, 5602.80/s  (0.192s, 5331.38/s)  LR: 8.786e-04  Data: 0.021 (0.038)
Train: 68 [1000/1251 ( 80%)]  Loss: 4.217 (4.46)  Time: 0.167s, 6121.52/s  (0.192s, 5326.60/s)  LR: 8.786e-04  Data: 0.025 (0.038)
Train: 68 [1050/1251 ( 84%)]  Loss: 4.674 (4.47)  Time: 0.184s, 5557.12/s  (0.192s, 5319.68/s)  LR: 8.786e-04  Data: 0.026 (0.037)
Train: 68 [1100/1251 ( 88%)]  Loss: 4.099 (4.46)  Time: 0.166s, 6159.96/s  (0.193s, 5315.13/s)  LR: 8.786e-04  Data: 0.022 (0.037)
Train: 68 [1150/1251 ( 92%)]  Loss: 4.564 (4.46)  Time: 0.158s, 6477.28/s  (0.193s, 5311.05/s)  LR: 8.786e-04  Data: 0.023 (0.036)
Train: 68 [1200/1251 ( 96%)]  Loss: 4.366 (4.46)  Time: 0.343s, 2984.09/s  (0.193s, 5310.20/s)  LR: 8.786e-04  Data: 0.025 (0.036)
Train: 68 [1250/1251 (100%)]  Loss: 4.633 (4.46)  Time: 0.114s, 9009.77/s  (0.192s, 5322.60/s)  LR: 8.786e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.832 (1.832)  Loss:  1.2730 (1.2730)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3389 (1.9335)  Acc@1: 77.0047 (61.2480)  Acc@5: 91.0377 (83.7520)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-57.pth.tar', 60.48600014404297)

Train: 69 [   0/1251 (  0%)]  Loss: 4.539 (4.54)  Time: 1.779s,  575.58/s  (1.779s,  575.58/s)  LR: 8.752e-04  Data: 1.631 (1.631)
Train: 69 [  50/1251 (  4%)]  Loss: 4.402 (4.47)  Time: 0.200s, 5131.53/s  (0.223s, 4586.94/s)  LR: 8.752e-04  Data: 0.020 (0.079)
Train: 69 [ 100/1251 (  8%)]  Loss: 4.571 (4.50)  Time: 0.163s, 6292.49/s  (0.208s, 4923.26/s)  LR: 8.752e-04  Data: 0.021 (0.062)
Train: 69 [ 150/1251 ( 12%)]  Loss: 4.220 (4.43)  Time: 0.182s, 5618.71/s  (0.199s, 5136.15/s)  LR: 8.752e-04  Data: 0.023 (0.054)
Train: 69 [ 200/1251 ( 16%)]  Loss: 4.441 (4.43)  Time: 0.180s, 5678.00/s  (0.198s, 5174.28/s)  LR: 8.752e-04  Data: 0.029 (0.050)
Train: 69 [ 250/1251 ( 20%)]  Loss: 4.156 (4.39)  Time: 0.159s, 6448.81/s  (0.197s, 5202.30/s)  LR: 8.752e-04  Data: 0.028 (0.048)
Train: 69 [ 300/1251 ( 24%)]  Loss: 4.616 (4.42)  Time: 0.162s, 6330.00/s  (0.195s, 5240.87/s)  LR: 8.752e-04  Data: 0.022 (0.044)
Train: 69 [ 350/1251 ( 28%)]  Loss: 4.711 (4.46)  Time: 0.166s, 6178.93/s  (0.194s, 5278.74/s)  LR: 8.752e-04  Data: 0.026 (0.042)
Train: 69 [ 400/1251 ( 32%)]  Loss: 4.572 (4.47)  Time: 0.164s, 6240.54/s  (0.193s, 5292.64/s)  LR: 8.752e-04  Data: 0.030 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 69 [ 450/1251 ( 36%)]  Loss: 4.849 (4.51)  Time: 0.229s, 4479.22/s  (0.193s, 5297.88/s)  LR: 8.752e-04  Data: 0.027 (0.039)
Train: 69 [ 500/1251 ( 40%)]  Loss: 4.596 (4.52)  Time: 0.157s, 6532.17/s  (0.193s, 5317.74/s)  LR: 8.752e-04  Data: 0.031 (0.038)
Train: 69 [ 550/1251 ( 44%)]  Loss: 4.057 (4.48)  Time: 0.175s, 5858.65/s  (0.192s, 5344.49/s)  LR: 8.752e-04  Data: 0.024 (0.037)
Train: 69 [ 600/1251 ( 48%)]  Loss: 4.577 (4.49)  Time: 0.177s, 5791.72/s  (0.192s, 5337.40/s)  LR: 8.752e-04  Data: 0.025 (0.036)
Train: 69 [ 650/1251 ( 52%)]  Loss: 4.314 (4.47)  Time: 0.181s, 5646.66/s  (0.192s, 5325.65/s)  LR: 8.752e-04  Data: 0.023 (0.036)
Train: 69 [ 700/1251 ( 56%)]  Loss: 4.210 (4.46)  Time: 0.185s, 5529.04/s  (0.192s, 5332.88/s)  LR: 8.752e-04  Data: 0.026 (0.035)
Train: 69 [ 750/1251 ( 60%)]  Loss: 4.514 (4.46)  Time: 0.207s, 4958.34/s  (0.192s, 5330.07/s)  LR: 8.752e-04  Data: 0.029 (0.034)
Train: 69 [ 800/1251 ( 64%)]  Loss: 4.381 (4.45)  Time: 0.149s, 6875.91/s  (0.192s, 5327.06/s)  LR: 8.752e-04  Data: 0.024 (0.034)
Train: 69 [ 850/1251 ( 68%)]  Loss: 4.468 (4.46)  Time: 0.191s, 5360.16/s  (0.193s, 5313.92/s)  LR: 8.752e-04  Data: 0.032 (0.034)
Train: 69 [ 900/1251 ( 72%)]  Loss: 4.641 (4.47)  Time: 0.200s, 5109.62/s  (0.193s, 5309.32/s)  LR: 8.752e-04  Data: 0.024 (0.033)
Train: 69 [ 950/1251 ( 76%)]  Loss: 4.421 (4.46)  Time: 0.185s, 5538.19/s  (0.193s, 5307.61/s)  LR: 8.752e-04  Data: 0.025 (0.033)
Train: 69 [1000/1251 ( 80%)]  Loss: 4.178 (4.45)  Time: 0.184s, 5568.81/s  (0.193s, 5315.96/s)  LR: 8.752e-04  Data: 0.026 (0.033)
Train: 69 [1050/1251 ( 84%)]  Loss: 4.347 (4.44)  Time: 0.183s, 5594.54/s  (0.193s, 5313.67/s)  LR: 8.752e-04  Data: 0.024 (0.033)
Train: 69 [1100/1251 ( 88%)]  Loss: 4.345 (4.44)  Time: 0.178s, 5747.86/s  (0.193s, 5314.85/s)  LR: 8.752e-04  Data: 0.031 (0.032)
Train: 69 [1150/1251 ( 92%)]  Loss: 4.321 (4.44)  Time: 0.223s, 4602.16/s  (0.193s, 5313.55/s)  LR: 8.752e-04  Data: 0.030 (0.032)
Train: 69 [1200/1251 ( 96%)]  Loss: 4.670 (4.44)  Time: 0.192s, 5339.70/s  (0.193s, 5303.96/s)  LR: 8.752e-04  Data: 0.021 (0.032)
Train: 69 [1250/1251 (100%)]  Loss: 4.588 (4.45)  Time: 0.114s, 9007.49/s  (0.193s, 5318.36/s)  LR: 8.752e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.819 (1.819)  Loss:  1.2460 (1.2460)  Acc@1: 78.2227 (78.2227)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.3057 (2.0157)  Acc@1: 77.3585 (61.1180)  Acc@5: 91.1557 (83.7680)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-63.pth.tar', 60.53200002197266)

Train: 70 [   0/1251 (  0%)]  Loss: 4.433 (4.43)  Time: 1.773s,  577.47/s  (1.773s,  577.47/s)  LR: 8.717e-04  Data: 1.644 (1.644)
Train: 70 [  50/1251 (  4%)]  Loss: 4.282 (4.36)  Time: 0.164s, 6256.61/s  (0.222s, 4617.00/s)  LR: 8.717e-04  Data: 0.030 (0.068)
Train: 70 [ 100/1251 (  8%)]  Loss: 4.138 (4.28)  Time: 0.164s, 6229.58/s  (0.204s, 5015.71/s)  LR: 8.717e-04  Data: 0.024 (0.048)
Train: 70 [ 150/1251 ( 12%)]  Loss: 4.048 (4.23)  Time: 0.166s, 6154.99/s  (0.200s, 5121.07/s)  LR: 8.717e-04  Data: 0.025 (0.042)
Train: 70 [ 200/1251 ( 16%)]  Loss: 4.373 (4.25)  Time: 0.167s, 6124.79/s  (0.199s, 5134.35/s)  LR: 8.717e-04  Data: 0.028 (0.038)
Train: 70 [ 250/1251 ( 20%)]  Loss: 4.280 (4.26)  Time: 0.182s, 5632.94/s  (0.196s, 5220.25/s)  LR: 8.717e-04  Data: 0.026 (0.036)
Train: 70 [ 300/1251 ( 24%)]  Loss: 4.569 (4.30)  Time: 0.196s, 5217.09/s  (0.195s, 5244.33/s)  LR: 8.717e-04  Data: 0.036 (0.035)
Train: 70 [ 350/1251 ( 28%)]  Loss: 4.208 (4.29)  Time: 0.182s, 5637.22/s  (0.194s, 5268.66/s)  LR: 8.717e-04  Data: 0.026 (0.034)
Train: 70 [ 400/1251 ( 32%)]  Loss: 4.070 (4.27)  Time: 0.181s, 5668.49/s  (0.193s, 5304.63/s)  LR: 8.717e-04  Data: 0.022 (0.035)
Train: 70 [ 450/1251 ( 36%)]  Loss: 4.197 (4.26)  Time: 0.175s, 5841.70/s  (0.193s, 5302.71/s)  LR: 8.717e-04  Data: 0.031 (0.037)
Train: 70 [ 500/1251 ( 40%)]  Loss: 4.213 (4.26)  Time: 0.276s, 3710.23/s  (0.193s, 5298.90/s)  LR: 8.717e-04  Data: 0.137 (0.038)
Train: 70 [ 550/1251 ( 44%)]  Loss: 4.413 (4.27)  Time: 0.165s, 6208.12/s  (0.193s, 5298.03/s)  LR: 8.717e-04  Data: 0.028 (0.038)
Train: 70 [ 600/1251 ( 48%)]  Loss: 4.721 (4.30)  Time: 0.185s, 5548.01/s  (0.193s, 5310.30/s)  LR: 8.717e-04  Data: 0.026 (0.037)
Train: 70 [ 650/1251 ( 52%)]  Loss: 4.185 (4.30)  Time: 0.154s, 6670.81/s  (0.192s, 5320.04/s)  LR: 8.717e-04  Data: 0.025 (0.038)
Train: 70 [ 700/1251 ( 56%)]  Loss: 4.699 (4.32)  Time: 0.156s, 6575.33/s  (0.193s, 5311.47/s)  LR: 8.717e-04  Data: 0.033 (0.039)
Train: 70 [ 750/1251 ( 60%)]  Loss: 4.557 (4.34)  Time: 0.182s, 5626.94/s  (0.193s, 5314.49/s)  LR: 8.717e-04  Data: 0.022 (0.039)
Train: 70 [ 800/1251 ( 64%)]  Loss: 4.555 (4.35)  Time: 0.178s, 5747.57/s  (0.193s, 5316.91/s)  LR: 8.717e-04  Data: 0.033 (0.040)
Train: 70 [ 850/1251 ( 68%)]  Loss: 4.210 (4.34)  Time: 0.168s, 6101.68/s  (0.193s, 5313.55/s)  LR: 8.717e-04  Data: 0.026 (0.040)
Train: 70 [ 900/1251 ( 72%)]  Loss: 4.388 (4.34)  Time: 0.490s, 2091.12/s  (0.192s, 5319.52/s)  LR: 8.717e-04  Data: 0.365 (0.040)
Train: 70 [ 950/1251 ( 76%)]  Loss: 4.793 (4.37)  Time: 0.163s, 6298.16/s  (0.192s, 5322.16/s)  LR: 8.717e-04  Data: 0.027 (0.040)
Train: 70 [1000/1251 ( 80%)]  Loss: 4.355 (4.37)  Time: 0.181s, 5661.75/s  (0.193s, 5312.45/s)  LR: 8.717e-04  Data: 0.022 (0.040)
Train: 70 [1050/1251 ( 84%)]  Loss: 4.485 (4.37)  Time: 0.163s, 6263.29/s  (0.192s, 5319.74/s)  LR: 8.717e-04  Data: 0.028 (0.039)
Train: 70 [1100/1251 ( 88%)]  Loss: 4.404 (4.37)  Time: 0.168s, 6088.05/s  (0.193s, 5319.41/s)  LR: 8.717e-04  Data: 0.033 (0.039)
Train: 70 [1150/1251 ( 92%)]  Loss: 4.568 (4.38)  Time: 0.172s, 5953.68/s  (0.193s, 5308.21/s)  LR: 8.717e-04  Data: 0.028 (0.039)
Train: 70 [1200/1251 ( 96%)]  Loss: 4.354 (4.38)  Time: 0.172s, 5937.05/s  (0.193s, 5308.11/s)  LR: 8.717e-04  Data: 0.032 (0.038)
Train: 70 [1250/1251 (100%)]  Loss: 4.394 (4.38)  Time: 0.113s, 9087.38/s  (0.193s, 5317.99/s)  LR: 8.717e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.824 (1.824)  Loss:  1.3614 (1.3614)  Acc@1: 79.1016 (79.1016)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.4231 (2.0018)  Acc@1: 78.0660 (61.3380)  Acc@5: 91.3915 (83.9460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-66.pth.tar', 60.69799999267578)

Train: 71 [   0/1251 (  0%)]  Loss: 4.094 (4.09)  Time: 1.720s,  595.49/s  (1.720s,  595.49/s)  LR: 8.682e-04  Data: 1.590 (1.590)
Train: 71 [  50/1251 (  4%)]  Loss: 4.483 (4.29)  Time: 0.157s, 6534.71/s  (0.219s, 4678.61/s)  LR: 8.682e-04  Data: 0.035 (0.063)
Train: 71 [ 100/1251 (  8%)]  Loss: 4.670 (4.42)  Time: 0.158s, 6492.46/s  (0.205s, 4985.99/s)  LR: 8.682e-04  Data: 0.028 (0.053)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 71 [ 150/1251 ( 12%)]  Loss: 4.433 (4.42)  Time: 0.156s, 6579.29/s  (0.200s, 5119.24/s)  LR: 8.682e-04  Data: 0.029 (0.048)
Train: 71 [ 200/1251 ( 16%)]  Loss: 4.272 (4.39)  Time: 0.180s, 5686.21/s  (0.197s, 5186.39/s)  LR: 8.682e-04  Data: 0.020 (0.047)
Train: 71 [ 250/1251 ( 20%)]  Loss: 4.567 (4.42)  Time: 0.193s, 5302.68/s  (0.197s, 5209.23/s)  LR: 8.682e-04  Data: 0.049 (0.046)
Train: 71 [ 300/1251 ( 24%)]  Loss: 4.948 (4.50)  Time: 0.178s, 5739.81/s  (0.196s, 5226.53/s)  LR: 8.682e-04  Data: 0.030 (0.046)
Train: 71 [ 350/1251 ( 28%)]  Loss: 4.476 (4.49)  Time: 0.175s, 5865.14/s  (0.195s, 5252.54/s)  LR: 8.682e-04  Data: 0.029 (0.046)
Train: 71 [ 400/1251 ( 32%)]  Loss: 4.307 (4.47)  Time: 0.167s, 6120.96/s  (0.194s, 5282.53/s)  LR: 8.682e-04  Data: 0.030 (0.045)
Train: 71 [ 450/1251 ( 36%)]  Loss: 4.426 (4.47)  Time: 0.348s, 2945.99/s  (0.193s, 5293.71/s)  LR: 8.682e-04  Data: 0.219 (0.045)
Train: 71 [ 500/1251 ( 40%)]  Loss: 4.403 (4.46)  Time: 0.169s, 6047.72/s  (0.193s, 5308.93/s)  LR: 8.682e-04  Data: 0.025 (0.045)
Train: 71 [ 550/1251 ( 44%)]  Loss: 3.977 (4.42)  Time: 0.158s, 6471.61/s  (0.192s, 5329.48/s)  LR: 8.682e-04  Data: 0.030 (0.045)
Train: 71 [ 600/1251 ( 48%)]  Loss: 3.987 (4.39)  Time: 0.168s, 6098.23/s  (0.192s, 5334.78/s)  LR: 8.682e-04  Data: 0.024 (0.045)
Train: 71 [ 650/1251 ( 52%)]  Loss: 4.616 (4.40)  Time: 0.169s, 6075.23/s  (0.192s, 5336.35/s)  LR: 8.682e-04  Data: 0.037 (0.045)
Train: 71 [ 700/1251 ( 56%)]  Loss: 4.208 (4.39)  Time: 0.170s, 6034.71/s  (0.192s, 5339.25/s)  LR: 8.682e-04  Data: 0.039 (0.045)
Train: 71 [ 750/1251 ( 60%)]  Loss: 4.351 (4.39)  Time: 0.226s, 4539.56/s  (0.192s, 5335.11/s)  LR: 8.682e-04  Data: 0.073 (0.044)
Train: 71 [ 800/1251 ( 64%)]  Loss: 4.703 (4.41)  Time: 0.166s, 6184.86/s  (0.192s, 5337.63/s)  LR: 8.682e-04  Data: 0.027 (0.043)
Train: 71 [ 850/1251 ( 68%)]  Loss: 4.269 (4.40)  Time: 0.157s, 6525.97/s  (0.192s, 5325.24/s)  LR: 8.682e-04  Data: 0.040 (0.042)
Train: 71 [ 900/1251 ( 72%)]  Loss: 4.812 (4.42)  Time: 0.181s, 5668.10/s  (0.192s, 5331.22/s)  LR: 8.682e-04  Data: 0.029 (0.041)
Train: 71 [ 950/1251 ( 76%)]  Loss: 4.429 (4.42)  Time: 0.168s, 6079.95/s  (0.192s, 5327.23/s)  LR: 8.682e-04  Data: 0.024 (0.041)
Train: 71 [1000/1251 ( 80%)]  Loss: 3.891 (4.40)  Time: 0.173s, 5906.78/s  (0.192s, 5327.77/s)  LR: 8.682e-04  Data: 0.034 (0.040)
Train: 71 [1050/1251 ( 84%)]  Loss: 4.519 (4.40)  Time: 0.171s, 5971.34/s  (0.192s, 5322.64/s)  LR: 8.682e-04  Data: 0.040 (0.040)
Train: 71 [1100/1251 ( 88%)]  Loss: 4.604 (4.41)  Time: 0.195s, 5261.15/s  (0.192s, 5322.21/s)  LR: 8.682e-04  Data: 0.022 (0.039)
Train: 71 [1150/1251 ( 92%)]  Loss: 4.780 (4.43)  Time: 0.172s, 5964.14/s  (0.193s, 5317.04/s)  LR: 8.682e-04  Data: 0.029 (0.040)
Train: 71 [1200/1251 ( 96%)]  Loss: 3.891 (4.40)  Time: 0.309s, 3314.18/s  (0.193s, 5313.78/s)  LR: 8.682e-04  Data: 0.181 (0.040)
Train: 71 [1250/1251 (100%)]  Loss: 4.746 (4.42)  Time: 0.116s, 8844.97/s  (0.192s, 5323.12/s)  LR: 8.682e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.761 (1.761)  Loss:  1.4394 (1.4394)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.3808 (1.9792)  Acc@1: 77.4764 (61.0000)  Acc@5: 91.2736 (83.7780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-61.pth.tar', 60.70600009277344)

Train: 72 [   0/1251 (  0%)]  Loss: 4.657 (4.66)  Time: 1.773s,  577.49/s  (1.773s,  577.49/s)  LR: 8.646e-04  Data: 1.656 (1.656)
Train: 72 [  50/1251 (  4%)]  Loss: 4.276 (4.47)  Time: 0.148s, 6923.55/s  (0.225s, 4552.03/s)  LR: 8.646e-04  Data: 0.023 (0.081)
Train: 72 [ 100/1251 (  8%)]  Loss: 4.746 (4.56)  Time: 0.164s, 6241.33/s  (0.211s, 4863.61/s)  LR: 8.646e-04  Data: 0.027 (0.066)
Train: 72 [ 150/1251 ( 12%)]  Loss: 4.098 (4.44)  Time: 0.197s, 5206.39/s  (0.203s, 5053.73/s)  LR: 8.646e-04  Data: 0.029 (0.057)
Train: 72 [ 200/1251 ( 16%)]  Loss: 4.543 (4.46)  Time: 0.189s, 5431.79/s  (0.198s, 5180.91/s)  LR: 8.646e-04  Data: 0.031 (0.051)
Train: 72 [ 250/1251 ( 20%)]  Loss: 4.526 (4.47)  Time: 0.160s, 6393.77/s  (0.196s, 5228.17/s)  LR: 8.646e-04  Data: 0.027 (0.047)
Train: 72 [ 300/1251 ( 24%)]  Loss: 4.345 (4.46)  Time: 0.171s, 5981.43/s  (0.195s, 5258.85/s)  LR: 8.646e-04  Data: 0.030 (0.046)
Train: 72 [ 350/1251 ( 28%)]  Loss: 4.356 (4.44)  Time: 0.171s, 6002.91/s  (0.194s, 5275.72/s)  LR: 8.646e-04  Data: 0.027 (0.046)
Train: 72 [ 400/1251 ( 32%)]  Loss: 4.684 (4.47)  Time: 0.169s, 6063.19/s  (0.193s, 5313.18/s)  LR: 8.646e-04  Data: 0.027 (0.045)
Train: 72 [ 450/1251 ( 36%)]  Loss: 4.378 (4.46)  Time: 0.178s, 5759.58/s  (0.193s, 5307.22/s)  LR: 8.646e-04  Data: 0.025 (0.045)
Train: 72 [ 500/1251 ( 40%)]  Loss: 4.577 (4.47)  Time: 0.163s, 6274.17/s  (0.193s, 5319.47/s)  LR: 8.646e-04  Data: 0.031 (0.044)
Train: 72 [ 550/1251 ( 44%)]  Loss: 4.482 (4.47)  Time: 0.170s, 6029.11/s  (0.193s, 5315.42/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 600/1251 ( 48%)]  Loss: 4.017 (4.44)  Time: 0.164s, 6226.21/s  (0.192s, 5333.19/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 650/1251 ( 52%)]  Loss: 4.030 (4.41)  Time: 0.182s, 5613.54/s  (0.192s, 5330.50/s)  LR: 8.646e-04  Data: 0.022 (0.044)
Train: 72 [ 700/1251 ( 56%)]  Loss: 4.461 (4.41)  Time: 0.160s, 6413.84/s  (0.192s, 5338.15/s)  LR: 8.646e-04  Data: 0.023 (0.044)
Train: 72 [ 750/1251 ( 60%)]  Loss: 4.234 (4.40)  Time: 0.170s, 6026.68/s  (0.192s, 5344.29/s)  LR: 8.646e-04  Data: 0.029 (0.044)
Train: 72 [ 800/1251 ( 64%)]  Loss: 4.641 (4.41)  Time: 0.189s, 5431.40/s  (0.192s, 5338.96/s)  LR: 8.646e-04  Data: 0.026 (0.044)
Train: 72 [ 850/1251 ( 68%)]  Loss: 4.670 (4.43)  Time: 0.176s, 5822.75/s  (0.192s, 5342.37/s)  LR: 8.646e-04  Data: 0.027 (0.043)
Train: 72 [ 900/1251 ( 72%)]  Loss: 4.901 (4.45)  Time: 0.174s, 5876.92/s  (0.192s, 5339.61/s)  LR: 8.646e-04  Data: 0.026 (0.042)
Train: 72 [ 950/1251 ( 76%)]  Loss: 4.485 (4.46)  Time: 0.173s, 5924.53/s  (0.192s, 5331.27/s)  LR: 8.646e-04  Data: 0.046 (0.042)
Train: 72 [1000/1251 ( 80%)]  Loss: 4.510 (4.46)  Time: 0.189s, 5413.51/s  (0.192s, 5329.66/s)  LR: 8.646e-04  Data: 0.046 (0.041)
Train: 72 [1050/1251 ( 84%)]  Loss: 4.431 (4.46)  Time: 0.177s, 5783.91/s  (0.192s, 5321.38/s)  LR: 8.646e-04  Data: 0.019 (0.040)
Train: 72 [1100/1251 ( 88%)]  Loss: 4.456 (4.46)  Time: 0.181s, 5664.93/s  (0.192s, 5323.04/s)  LR: 8.646e-04  Data: 0.031 (0.040)
Train: 72 [1150/1251 ( 92%)]  Loss: 4.385 (4.45)  Time: 0.186s, 5503.56/s  (0.193s, 5313.52/s)  LR: 8.646e-04  Data: 0.023 (0.040)
Train: 72 [1200/1251 ( 96%)]  Loss: 4.396 (4.45)  Time: 0.172s, 5942.62/s  (0.193s, 5300.38/s)  LR: 8.646e-04  Data: 0.021 (0.040)
Train: 72 [1250/1251 (100%)]  Loss: 4.615 (4.46)  Time: 0.114s, 8990.67/s  (0.193s, 5314.07/s)  LR: 8.646e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.730 (1.730)  Loss:  1.1955 (1.1955)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.019 (0.221)  Loss:  1.3375 (1.9334)  Acc@1: 75.3538 (61.1400)  Acc@5: 91.1557 (83.6540)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-60.pth.tar', 60.71399999023438)

Train: 73 [   0/1251 (  0%)]  Loss: 4.266 (4.27)  Time: 1.929s,  530.75/s  (1.929s,  530.75/s)  LR: 8.610e-04  Data: 1.805 (1.805)
Train: 73 [  50/1251 (  4%)]  Loss: 3.903 (4.08)  Time: 0.173s, 5920.28/s  (0.223s, 4584.88/s)  LR: 8.610e-04  Data: 0.030 (0.074)
Train: 73 [ 100/1251 (  8%)]  Loss: 4.164 (4.11)  Time: 0.184s, 5560.22/s  (0.209s, 4907.91/s)  LR: 8.610e-04  Data: 0.032 (0.052)
Train: 73 [ 150/1251 ( 12%)]  Loss: 4.201 (4.13)  Time: 0.164s, 6233.52/s  (0.201s, 5090.40/s)  LR: 8.610e-04  Data: 0.029 (0.046)
Train: 73 [ 200/1251 ( 16%)]  Loss: 4.376 (4.18)  Time: 0.225s, 4559.30/s  (0.199s, 5135.20/s)  LR: 8.610e-04  Data: 0.026 (0.042)
Train: 73 [ 250/1251 ( 20%)]  Loss: 4.952 (4.31)  Time: 0.171s, 5988.85/s  (0.197s, 5203.77/s)  LR: 8.610e-04  Data: 0.035 (0.040)
Train: 73 [ 300/1251 ( 24%)]  Loss: 4.307 (4.31)  Time: 0.224s, 4562.34/s  (0.194s, 5273.74/s)  LR: 8.610e-04  Data: 0.028 (0.038)
Train: 73 [ 350/1251 ( 28%)]  Loss: 4.289 (4.31)  Time: 0.169s, 6057.58/s  (0.195s, 5258.90/s)  LR: 8.610e-04  Data: 0.023 (0.036)
Train: 73 [ 400/1251 ( 32%)]  Loss: 4.479 (4.33)  Time: 0.290s, 3525.97/s  (0.194s, 5280.75/s)  LR: 8.610e-04  Data: 0.030 (0.035)
Train: 73 [ 450/1251 ( 36%)]  Loss: 4.479 (4.34)  Time: 0.159s, 6451.85/s  (0.194s, 5285.00/s)  LR: 8.610e-04  Data: 0.024 (0.036)
Train: 73 [ 500/1251 ( 40%)]  Loss: 4.308 (4.34)  Time: 0.175s, 5841.50/s  (0.193s, 5310.71/s)  LR: 8.610e-04  Data: 0.026 (0.036)
Train: 73 [ 550/1251 ( 44%)]  Loss: 4.270 (4.33)  Time: 0.201s, 5100.15/s  (0.193s, 5318.48/s)  LR: 8.610e-04  Data: 0.028 (0.036)
Train: 73 [ 600/1251 ( 48%)]  Loss: 4.668 (4.36)  Time: 0.176s, 5819.90/s  (0.193s, 5319.42/s)  LR: 8.610e-04  Data: 0.033 (0.035)
Train: 73 [ 650/1251 ( 52%)]  Loss: 4.340 (4.36)  Time: 0.155s, 6623.45/s  (0.193s, 5295.03/s)  LR: 8.610e-04  Data: 0.028 (0.034)
Train: 73 [ 700/1251 ( 56%)]  Loss: 4.786 (4.39)  Time: 0.160s, 6397.02/s  (0.193s, 5299.15/s)  LR: 8.610e-04  Data: 0.028 (0.034)
Train: 73 [ 750/1251 ( 60%)]  Loss: 4.661 (4.40)  Time: 0.196s, 5230.15/s  (0.193s, 5302.39/s)  LR: 8.610e-04  Data: 0.026 (0.034)
Train: 73 [ 800/1251 ( 64%)]  Loss: 4.501 (4.41)  Time: 0.186s, 5499.11/s  (0.193s, 5306.54/s)  LR: 8.610e-04  Data: 0.030 (0.033)
Train: 73 [ 850/1251 ( 68%)]  Loss: 4.171 (4.40)  Time: 0.180s, 5678.10/s  (0.192s, 5327.11/s)  LR: 8.610e-04  Data: 0.028 (0.033)
Train: 73 [ 900/1251 ( 72%)]  Loss: 4.319 (4.39)  Time: 0.453s, 2261.16/s  (0.193s, 5312.23/s)  LR: 8.610e-04  Data: 0.326 (0.033)
Train: 73 [ 950/1251 ( 76%)]  Loss: 4.602 (4.40)  Time: 0.183s, 5585.06/s  (0.193s, 5306.42/s)  LR: 8.610e-04  Data: 0.020 (0.035)
Train: 73 [1000/1251 ( 80%)]  Loss: 4.366 (4.40)  Time: 0.165s, 6221.79/s  (0.193s, 5307.36/s)  LR: 8.610e-04  Data: 0.025 (0.035)
Train: 73 [1050/1251 ( 84%)]  Loss: 4.268 (4.39)  Time: 0.313s, 3273.58/s  (0.193s, 5307.86/s)  LR: 8.610e-04  Data: 0.028 (0.035)
Train: 73 [1100/1251 ( 88%)]  Loss: 4.824 (4.41)  Time: 0.161s, 6377.71/s  (0.193s, 5305.71/s)  LR: 8.610e-04  Data: 0.033 (0.035)
Train: 73 [1150/1251 ( 92%)]  Loss: 4.740 (4.43)  Time: 0.166s, 6172.14/s  (0.193s, 5302.94/s)  LR: 8.610e-04  Data: 0.027 (0.035)
Train: 73 [1200/1251 ( 96%)]  Loss: 4.497 (4.43)  Time: 0.174s, 5883.28/s  (0.193s, 5294.47/s)  LR: 8.610e-04  Data: 0.027 (0.034)
Train: 73 [1250/1251 (100%)]  Loss: 4.472 (4.43)  Time: 0.113s, 9028.96/s  (0.193s, 5311.93/s)  LR: 8.610e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1868 (1.1868)  Acc@1: 79.0039 (79.0039)  Acc@5: 93.9453 (93.9453)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2417 (1.9178)  Acc@1: 77.5943 (61.2540)  Acc@5: 91.9811 (83.9340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-59.pth.tar', 60.80399990722656)

Train: 74 [   0/1251 (  0%)]  Loss: 4.501 (4.50)  Time: 1.723s,  594.14/s  (1.723s,  594.14/s)  LR: 8.574e-04  Data: 1.600 (1.600)
Train: 74 [  50/1251 (  4%)]  Loss: 4.466 (4.48)  Time: 0.165s, 6203.51/s  (0.218s, 4691.43/s)  LR: 8.574e-04  Data: 0.025 (0.069)
Train: 74 [ 100/1251 (  8%)]  Loss: 4.644 (4.54)  Time: 0.179s, 5731.11/s  (0.208s, 4914.09/s)  LR: 8.574e-04  Data: 0.026 (0.049)
Train: 74 [ 150/1251 ( 12%)]  Loss: 4.586 (4.55)  Time: 0.192s, 5334.33/s  (0.201s, 5091.46/s)  LR: 8.574e-04  Data: 0.025 (0.042)
Train: 74 [ 200/1251 ( 16%)]  Loss: 4.194 (4.48)  Time: 0.184s, 5566.10/s  (0.199s, 5139.19/s)  LR: 8.574e-04  Data: 0.024 (0.038)
Train: 74 [ 250/1251 ( 20%)]  Loss: 4.516 (4.48)  Time: 0.179s, 5723.91/s  (0.196s, 5211.53/s)  LR: 8.574e-04  Data: 0.028 (0.036)
Train: 74 [ 300/1251 ( 24%)]  Loss: 4.303 (4.46)  Time: 0.166s, 6155.89/s  (0.194s, 5273.83/s)  LR: 8.574e-04  Data: 0.027 (0.035)
Train: 74 [ 350/1251 ( 28%)]  Loss: 4.430 (4.45)  Time: 0.164s, 6251.45/s  (0.194s, 5274.71/s)  LR: 8.574e-04  Data: 0.029 (0.035)
Train: 74 [ 400/1251 ( 32%)]  Loss: 4.275 (4.43)  Time: 0.175s, 5836.89/s  (0.194s, 5270.88/s)  LR: 8.574e-04  Data: 0.019 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Train: 74 [ 450/1251 ( 36%)]  Loss: 4.145 (4.41)  Time: 0.195s, 5244.72/s  (0.194s, 5266.62/s)  LR: 8.574e-04  Data: 0.030 (0.037)
Train: 74 [ 500/1251 ( 40%)]  Loss: 4.542 (4.42)  Time: 0.152s, 6749.34/s  (0.193s, 5295.07/s)  LR: 8.574e-04  Data: 0.021 (0.037)
Train: 74 [ 550/1251 ( 44%)]  Loss: 4.193 (4.40)  Time: 0.154s, 6658.22/s  (0.193s, 5301.86/s)  LR: 8.574e-04  Data: 0.023 (0.038)
Train: 74 [ 600/1251 ( 48%)]  Loss: 4.487 (4.41)  Time: 0.166s, 6153.72/s  (0.193s, 5297.17/s)  LR: 8.574e-04  Data: 0.034 (0.039)
Train: 74 [ 650/1251 ( 52%)]  Loss: 4.162 (4.39)  Time: 0.163s, 6266.18/s  (0.193s, 5315.85/s)  LR: 8.574e-04  Data: 0.031 (0.038)
Train: 74 [ 700/1251 ( 56%)]  Loss: 4.628 (4.40)  Time: 0.163s, 6287.42/s  (0.192s, 5321.27/s)  LR: 8.574e-04  Data: 0.029 (0.038)
Train: 74 [ 750/1251 ( 60%)]  Loss: 4.276 (4.40)  Time: 0.175s, 5844.59/s  (0.193s, 5315.14/s)  LR: 8.574e-04  Data: 0.023 (0.039)
Train: 74 [ 800/1251 ( 64%)]  Loss: 4.853 (4.42)  Time: 0.166s, 6167.62/s  (0.193s, 5305.45/s)  LR: 8.574e-04  Data: 0.034 (0.040)
Train: 74 [ 850/1251 ( 68%)]  Loss: 4.687 (4.44)  Time: 0.194s, 5290.30/s  (0.193s, 5306.16/s)  LR: 8.574e-04  Data: 0.025 (0.040)
Train: 74 [ 900/1251 ( 72%)]  Loss: 4.518 (4.44)  Time: 0.182s, 5640.58/s  (0.193s, 5315.88/s)  LR: 8.574e-04  Data: 0.024 (0.040)
Train: 74 [ 950/1251 ( 76%)]  Loss: 4.170 (4.43)  Time: 0.331s, 3091.33/s  (0.193s, 5307.79/s)  LR: 8.574e-04  Data: 0.022 (0.040)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 74 [1000/1251 ( 80%)]  Loss: 4.819 (4.45)  Time: 0.166s, 6163.55/s  (0.193s, 5304.85/s)  LR: 8.574e-04  Data: 0.030 (0.039)
Train: 74 [1050/1251 ( 84%)]  Loss: 4.144 (4.43)  Time: 0.166s, 6159.61/s  (0.193s, 5299.89/s)  LR: 8.574e-04  Data: 0.023 (0.039)
Train: 74 [1100/1251 ( 88%)]  Loss: 4.183 (4.42)  Time: 0.179s, 5730.48/s  (0.193s, 5310.74/s)  LR: 8.574e-04  Data: 0.025 (0.038)
Train: 74 [1150/1251 ( 92%)]  Loss: 4.121 (4.41)  Time: 0.250s, 4093.77/s  (0.193s, 5307.28/s)  LR: 8.574e-04  Data: 0.026 (0.038)
Train: 74 [1200/1251 ( 96%)]  Loss: 4.412 (4.41)  Time: 0.297s, 3442.83/s  (0.193s, 5294.19/s)  LR: 8.574e-04  Data: 0.026 (0.037)
Train: 74 [1250/1251 (100%)]  Loss: 4.433 (4.41)  Time: 0.113s, 9046.34/s  (0.193s, 5303.61/s)  LR: 8.574e-04  Data: 0.000 (0.037)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.807 (1.807)  Loss:  1.2259 (1.2259)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.215)  Loss:  1.2322 (1.9228)  Acc@1: 77.4764 (61.1300)  Acc@5: 90.9198 (83.7240)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-71.pth.tar', 61.00000004150391)

Train: 75 [   0/1251 (  0%)]  Loss: 4.019 (4.02)  Time: 1.709s,  599.12/s  (1.709s,  599.12/s)  LR: 8.537e-04  Data: 1.586 (1.586)
Train: 75 [  50/1251 (  4%)]  Loss: 4.502 (4.26)  Time: 0.155s, 6595.08/s  (0.227s, 4507.08/s)  LR: 8.537e-04  Data: 0.028 (0.086)
Train: 75 [ 100/1251 (  8%)]  Loss: 4.488 (4.34)  Time: 0.169s, 6043.02/s  (0.209s, 4899.77/s)  LR: 8.537e-04  Data: 0.031 (0.066)
Train: 75 [ 150/1251 ( 12%)]  Loss: 4.606 (4.40)  Time: 0.163s, 6273.30/s  (0.203s, 5056.76/s)  LR: 8.537e-04  Data: 0.026 (0.058)
Train: 75 [ 200/1251 ( 16%)]  Loss: 4.341 (4.39)  Time: 0.164s, 6249.75/s  (0.201s, 5106.96/s)  LR: 8.537e-04  Data: 0.026 (0.055)
Train: 75 [ 250/1251 ( 20%)]  Loss: 4.503 (4.41)  Time: 0.178s, 5750.56/s  (0.199s, 5155.69/s)  LR: 8.537e-04  Data: 0.027 (0.054)
Train: 75 [ 300/1251 ( 24%)]  Loss: 4.508 (4.42)  Time: 0.170s, 6025.24/s  (0.197s, 5194.48/s)  LR: 8.537e-04  Data: 0.026 (0.052)
Train: 75 [ 350/1251 ( 28%)]  Loss: 4.700 (4.46)  Time: 0.187s, 5485.79/s  (0.196s, 5219.03/s)  LR: 8.537e-04  Data: 0.025 (0.051)
Train: 75 [ 400/1251 ( 32%)]  Loss: 4.471 (4.46)  Time: 0.170s, 6023.61/s  (0.196s, 5236.08/s)  LR: 8.537e-04  Data: 0.031 (0.050)
Train: 75 [ 450/1251 ( 36%)]  Loss: 4.379 (4.45)  Time: 0.177s, 5784.19/s  (0.195s, 5254.99/s)  LR: 8.537e-04  Data: 0.025 (0.050)
Train: 75 [ 500/1251 ( 40%)]  Loss: 4.270 (4.44)  Time: 0.155s, 6610.21/s  (0.194s, 5271.02/s)  LR: 8.537e-04  Data: 0.030 (0.048)
Train: 75 [ 550/1251 ( 44%)]  Loss: 4.364 (4.43)  Time: 0.199s, 5136.01/s  (0.194s, 5279.58/s)  LR: 8.537e-04  Data: 0.022 (0.047)
Train: 75 [ 600/1251 ( 48%)]  Loss: 4.529 (4.44)  Time: 0.166s, 6171.88/s  (0.194s, 5279.87/s)  LR: 8.537e-04  Data: 0.036 (0.046)
Train: 75 [ 650/1251 ( 52%)]  Loss: 3.997 (4.41)  Time: 0.170s, 6010.05/s  (0.194s, 5286.18/s)  LR: 8.537e-04  Data: 0.024 (0.044)
Train: 75 [ 700/1251 ( 56%)]  Loss: 4.244 (4.39)  Time: 0.194s, 5284.72/s  (0.194s, 5269.04/s)  LR: 8.537e-04  Data: 0.040 (0.043)
Train: 75 [ 750/1251 ( 60%)]  Loss: 4.279 (4.39)  Time: 0.189s, 5410.91/s  (0.194s, 5281.73/s)  LR: 8.537e-04  Data: 0.027 (0.042)
Train: 75 [ 800/1251 ( 64%)]  Loss: 4.902 (4.42)  Time: 0.191s, 5363.51/s  (0.194s, 5275.60/s)  LR: 8.537e-04  Data: 0.034 (0.043)
Train: 75 [ 850/1251 ( 68%)]  Loss: 4.498 (4.42)  Time: 0.164s, 6230.48/s  (0.194s, 5276.58/s)  LR: 8.537e-04  Data: 0.024 (0.042)
Train: 75 [ 900/1251 ( 72%)]  Loss: 4.426 (4.42)  Time: 0.173s, 5922.44/s  (0.194s, 5277.18/s)  LR: 8.537e-04  Data: 0.029 (0.042)
Train: 75 [ 950/1251 ( 76%)]  Loss: 4.219 (4.41)  Time: 0.174s, 5871.51/s  (0.194s, 5274.18/s)  LR: 8.537e-04  Data: 0.030 (0.042)
Train: 75 [1000/1251 ( 80%)]  Loss: 4.326 (4.41)  Time: 0.168s, 6087.96/s  (0.194s, 5272.14/s)  LR: 8.537e-04  Data: 0.036 (0.042)
Train: 75 [1050/1251 ( 84%)]  Loss: 4.510 (4.41)  Time: 0.166s, 6159.79/s  (0.194s, 5272.16/s)  LR: 8.537e-04  Data: 0.024 (0.041)
Train: 75 [1100/1251 ( 88%)]  Loss: 3.844 (4.39)  Time: 0.188s, 5447.36/s  (0.194s, 5270.10/s)  LR: 8.537e-04  Data: 0.025 (0.041)
Train: 75 [1150/1251 ( 92%)]  Loss: 4.229 (4.38)  Time: 0.172s, 5960.84/s  (0.194s, 5279.55/s)  LR: 8.537e-04  Data: 0.029 (0.041)
Train: 75 [1200/1251 ( 96%)]  Loss: 4.526 (4.39)  Time: 0.159s, 6425.79/s  (0.194s, 5269.47/s)  LR: 8.537e-04  Data: 0.038 (0.041)
Train: 75 [1250/1251 (100%)]  Loss: 4.220 (4.38)  Time: 0.113s, 9023.15/s  (0.194s, 5280.60/s)  LR: 8.537e-04  Data: 0.000 (0.040)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.790 (1.790)  Loss:  1.2838 (1.2838)  Acc@1: 76.8555 (76.8555)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.222)  Loss:  1.2998 (1.9152)  Acc@1: 76.1792 (61.2080)  Acc@5: 91.0377 (83.7140)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-69.pth.tar', 61.117999990234374)

Train: 76 [   0/1251 (  0%)]  Loss: 4.361 (4.36)  Time: 1.692s,  605.36/s  (1.692s,  605.36/s)  LR: 8.500e-04  Data: 1.561 (1.561)
Train: 76 [  50/1251 (  4%)]  Loss: 4.315 (4.34)  Time: 0.174s, 5870.22/s  (0.221s, 4637.94/s)  LR: 8.500e-04  Data: 0.035 (0.068)
Train: 76 [ 100/1251 (  8%)]  Loss: 4.414 (4.36)  Time: 0.172s, 5947.36/s  (0.207s, 4935.89/s)  LR: 8.500e-04  Data: 0.033 (0.051)
Train: 76 [ 150/1251 ( 12%)]  Loss: 4.291 (4.35)  Time: 0.154s, 6647.23/s  (0.200s, 5132.44/s)  LR: 8.500e-04  Data: 0.023 (0.046)
Train: 76 [ 200/1251 ( 16%)]  Loss: 4.166 (4.31)  Time: 0.186s, 5502.17/s  (0.196s, 5230.54/s)  LR: 8.500e-04  Data: 0.026 (0.042)
Train: 76 [ 250/1251 ( 20%)]  Loss: 4.854 (4.40)  Time: 0.177s, 5778.37/s  (0.195s, 5259.58/s)  LR: 8.500e-04  Data: 0.030 (0.039)
Train: 76 [ 300/1251 ( 24%)]  Loss: 4.474 (4.41)  Time: 0.154s, 6644.11/s  (0.194s, 5284.56/s)  LR: 8.500e-04  Data: 0.020 (0.037)
Train: 76 [ 350/1251 ( 28%)]  Loss: 4.438 (4.41)  Time: 0.161s, 6348.18/s  (0.194s, 5291.77/s)  LR: 8.500e-04  Data: 0.025 (0.037)
Train: 76 [ 400/1251 ( 32%)]  Loss: 4.330 (4.40)  Time: 0.293s, 3490.11/s  (0.193s, 5314.53/s)  LR: 8.500e-04  Data: 0.023 (0.037)
Train: 76 [ 450/1251 ( 36%)]  Loss: 4.535 (4.42)  Time: 0.185s, 5531.46/s  (0.193s, 5305.80/s)  LR: 8.500e-04  Data: 0.038 (0.036)
Train: 76 [ 500/1251 ( 40%)]  Loss: 4.500 (4.43)  Time: 0.174s, 5881.27/s  (0.193s, 5309.64/s)  LR: 8.500e-04  Data: 0.019 (0.035)
Train: 76 [ 550/1251 ( 44%)]  Loss: 4.740 (4.45)  Time: 0.155s, 6597.03/s  (0.192s, 5323.70/s)  LR: 8.500e-04  Data: 0.021 (0.036)
Train: 76 [ 600/1251 ( 48%)]  Loss: 4.377 (4.45)  Time: 0.177s, 5783.89/s  (0.192s, 5328.30/s)  LR: 8.500e-04  Data: 0.039 (0.036)
Train: 76 [ 650/1251 ( 52%)]  Loss: 3.971 (4.41)  Time: 0.176s, 5834.73/s  (0.192s, 5330.87/s)  LR: 8.500e-04  Data: 0.036 (0.035)
Train: 76 [ 700/1251 ( 56%)]  Loss: 4.416 (4.41)  Time: 0.165s, 6220.89/s  (0.192s, 5331.79/s)  LR: 8.500e-04  Data: 0.020 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 76 [ 750/1251 ( 60%)]  Loss: 4.288 (4.40)  Time: 0.178s, 5739.14/s  (0.192s, 5331.70/s)  LR: 8.500e-04  Data: 0.032 (0.034)
Train: 76 [ 800/1251 ( 64%)]  Loss: 4.225 (4.39)  Time: 0.179s, 5709.02/s  (0.192s, 5334.16/s)  LR: 8.500e-04  Data: 0.025 (0.034)
Train: 76 [ 850/1251 ( 68%)]  Loss: 4.315 (4.39)  Time: 0.174s, 5890.00/s  (0.192s, 5332.11/s)  LR: 8.500e-04  Data: 0.027 (0.033)
Train: 76 [ 900/1251 ( 72%)]  Loss: 4.271 (4.38)  Time: 0.173s, 5907.62/s  (0.192s, 5322.73/s)  LR: 8.500e-04  Data: 0.029 (0.033)
Train: 76 [ 950/1251 ( 76%)]  Loss: 4.068 (4.37)  Time: 0.180s, 5703.05/s  (0.193s, 5319.13/s)  LR: 8.500e-04  Data: 0.032 (0.033)
Train: 76 [1000/1251 ( 80%)]  Loss: 4.682 (4.38)  Time: 0.186s, 5514.77/s  (0.192s, 5329.93/s)  LR: 8.500e-04  Data: 0.034 (0.032)
Train: 76 [1050/1251 ( 84%)]  Loss: 3.999 (4.37)  Time: 0.186s, 5504.70/s  (0.192s, 5330.78/s)  LR: 8.500e-04  Data: 0.032 (0.032)
Train: 76 [1100/1251 ( 88%)]  Loss: 4.036 (4.35)  Time: 0.342s, 2992.30/s  (0.192s, 5326.51/s)  LR: 8.500e-04  Data: 0.023 (0.032)
Train: 76 [1150/1251 ( 92%)]  Loss: 4.496 (4.36)  Time: 0.159s, 6438.46/s  (0.192s, 5325.10/s)  LR: 8.500e-04  Data: 0.022 (0.032)
Train: 76 [1200/1251 ( 96%)]  Loss: 4.053 (4.34)  Time: 0.163s, 6281.82/s  (0.192s, 5321.94/s)  LR: 8.500e-04  Data: 0.031 (0.032)
Train: 76 [1250/1251 (100%)]  Loss: 4.494 (4.35)  Time: 0.113s, 9084.63/s  (0.192s, 5329.98/s)  LR: 8.500e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.826 (1.826)  Loss:  1.1483 (1.1483)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.226)  Loss:  1.3435 (1.9415)  Acc@1: 79.4811 (61.9800)  Acc@5: 92.0991 (84.3780)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-74.pth.tar', 61.130000170898434)

Train: 77 [   0/1251 (  0%)]  Loss: 4.260 (4.26)  Time: 1.744s,  587.28/s  (1.744s,  587.28/s)  LR: 8.462e-04  Data: 1.618 (1.618)
Train: 77 [  50/1251 (  4%)]  Loss: 4.803 (4.53)  Time: 0.172s, 5950.37/s  (0.221s, 4636.32/s)  LR: 8.462e-04  Data: 0.029 (0.078)
Train: 77 [ 100/1251 (  8%)]  Loss: 4.300 (4.45)  Time: 0.189s, 5429.01/s  (0.203s, 5039.80/s)  LR: 8.462e-04  Data: 0.028 (0.059)
Train: 77 [ 150/1251 ( 12%)]  Loss: 4.575 (4.48)  Time: 0.175s, 5847.41/s  (0.198s, 5172.34/s)  LR: 8.462e-04  Data: 0.035 (0.052)
Train: 77 [ 200/1251 ( 16%)]  Loss: 4.252 (4.44)  Time: 0.159s, 6457.99/s  (0.196s, 5220.17/s)  LR: 8.462e-04  Data: 0.027 (0.047)
Train: 77 [ 250/1251 ( 20%)]  Loss: 4.246 (4.41)  Time: 0.177s, 5801.13/s  (0.196s, 5235.30/s)  LR: 8.462e-04  Data: 0.034 (0.046)
Train: 77 [ 300/1251 ( 24%)]  Loss: 4.405 (4.41)  Time: 0.159s, 6435.09/s  (0.194s, 5275.66/s)  LR: 8.462e-04  Data: 0.025 (0.046)
Train: 77 [ 350/1251 ( 28%)]  Loss: 4.387 (4.40)  Time: 0.167s, 6133.73/s  (0.194s, 5285.37/s)  LR: 8.462e-04  Data: 0.024 (0.046)
Train: 77 [ 400/1251 ( 32%)]  Loss: 3.989 (4.36)  Time: 0.177s, 5800.29/s  (0.193s, 5296.86/s)  LR: 8.462e-04  Data: 0.025 (0.046)
Train: 77 [ 450/1251 ( 36%)]  Loss: 4.303 (4.35)  Time: 0.193s, 5292.33/s  (0.193s, 5301.80/s)  LR: 8.462e-04  Data: 0.022 (0.044)
Train: 77 [ 500/1251 ( 40%)]  Loss: 4.203 (4.34)  Time: 0.187s, 5472.31/s  (0.192s, 5325.64/s)  LR: 8.462e-04  Data: 0.029 (0.043)
Train: 77 [ 550/1251 ( 44%)]  Loss: 4.129 (4.32)  Time: 0.174s, 5890.59/s  (0.192s, 5328.50/s)  LR: 8.462e-04  Data: 0.026 (0.041)
Train: 77 [ 600/1251 ( 48%)]  Loss: 4.368 (4.32)  Time: 0.299s, 3421.16/s  (0.192s, 5322.66/s)  LR: 8.462e-04  Data: 0.024 (0.040)
Train: 77 [ 650/1251 ( 52%)]  Loss: 4.336 (4.33)  Time: 0.183s, 5606.85/s  (0.192s, 5341.12/s)  LR: 8.462e-04  Data: 0.037 (0.039)
Train: 77 [ 700/1251 ( 56%)]  Loss: 4.413 (4.33)  Time: 0.175s, 5863.89/s  (0.191s, 5349.74/s)  LR: 8.462e-04  Data: 0.027 (0.039)
Train: 77 [ 750/1251 ( 60%)]  Loss: 4.348 (4.33)  Time: 0.172s, 5963.95/s  (0.192s, 5336.08/s)  LR: 8.462e-04  Data: 0.026 (0.040)
Train: 77 [ 800/1251 ( 64%)]  Loss: 4.439 (4.34)  Time: 0.191s, 5370.31/s  (0.192s, 5319.63/s)  LR: 8.462e-04  Data: 0.024 (0.040)
Train: 77 [ 850/1251 ( 68%)]  Loss: 4.450 (4.34)  Time: 0.360s, 2846.55/s  (0.192s, 5321.21/s)  LR: 8.462e-04  Data: 0.022 (0.040)
Train: 77 [ 900/1251 ( 72%)]  Loss: 4.585 (4.36)  Time: 0.165s, 6197.86/s  (0.192s, 5329.16/s)  LR: 8.462e-04  Data: 0.025 (0.040)
Train: 77 [ 950/1251 ( 76%)]  Loss: 4.428 (4.36)  Time: 0.202s, 5065.54/s  (0.192s, 5327.76/s)  LR: 8.462e-04  Data: 0.025 (0.040)
Train: 77 [1000/1251 ( 80%)]  Loss: 4.567 (4.37)  Time: 0.201s, 5101.07/s  (0.192s, 5328.92/s)  LR: 8.462e-04  Data: 0.022 (0.040)
Train: 77 [1050/1251 ( 84%)]  Loss: 4.474 (4.38)  Time: 0.187s, 5486.67/s  (0.192s, 5333.22/s)  LR: 8.462e-04  Data: 0.029 (0.040)
Train: 77 [1100/1251 ( 88%)]  Loss: 4.447 (4.38)  Time: 0.170s, 6025.83/s  (0.192s, 5325.66/s)  LR: 8.462e-04  Data: 0.020 (0.039)
Train: 77 [1150/1251 ( 92%)]  Loss: 4.010 (4.36)  Time: 0.179s, 5707.63/s  (0.192s, 5325.26/s)  LR: 8.462e-04  Data: 0.021 (0.039)
Train: 77 [1200/1251 ( 96%)]  Loss: 4.130 (4.35)  Time: 0.153s, 6674.18/s  (0.193s, 5318.22/s)  LR: 8.462e-04  Data: 0.022 (0.039)
Train: 77 [1250/1251 (100%)]  Loss: 4.375 (4.35)  Time: 0.112s, 9103.25/s  (0.192s, 5329.70/s)  LR: 8.462e-04  Data: 0.000 (0.038)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.923 (1.923)  Loss:  1.2415 (1.2415)  Acc@1: 77.9297 (77.9297)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1628 (1.8729)  Acc@1: 80.0708 (62.1400)  Acc@5: 93.6321 (84.5740)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-65.pth.tar', 61.133999943847655)

Train: 78 [   0/1251 (  0%)]  Loss: 4.462 (4.46)  Time: 1.772s,  577.84/s  (1.772s,  577.84/s)  LR: 8.424e-04  Data: 1.645 (1.645)
Train: 78 [  50/1251 (  4%)]  Loss: 4.421 (4.44)  Time: 0.168s, 6096.05/s  (0.219s, 4674.91/s)  LR: 8.424e-04  Data: 0.025 (0.062)
Train: 78 [ 100/1251 (  8%)]  Loss: 4.386 (4.42)  Time: 0.211s, 4845.69/s  (0.203s, 5045.43/s)  LR: 8.424e-04  Data: 0.033 (0.045)
Train: 78 [ 150/1251 ( 12%)]  Loss: 4.857 (4.53)  Time: 0.154s, 6639.93/s  (0.199s, 5144.61/s)  LR: 8.424e-04  Data: 0.022 (0.039)
Train: 78 [ 200/1251 ( 16%)]  Loss: 4.079 (4.44)  Time: 0.195s, 5250.89/s  (0.198s, 5171.22/s)  LR: 8.424e-04  Data: 0.038 (0.036)
Train: 78 [ 250/1251 ( 20%)]  Loss: 4.348 (4.43)  Time: 0.171s, 5991.42/s  (0.196s, 5234.33/s)  LR: 8.424e-04  Data: 0.021 (0.035)
Train: 78 [ 300/1251 ( 24%)]  Loss: 4.795 (4.48)  Time: 0.201s, 5099.08/s  (0.195s, 5243.56/s)  LR: 8.424e-04  Data: 0.031 (0.034)
Train: 78 [ 350/1251 ( 28%)]  Loss: 4.252 (4.45)  Time: 0.199s, 5152.08/s  (0.194s, 5275.78/s)  LR: 8.424e-04  Data: 0.024 (0.033)
Train: 78 [ 400/1251 ( 32%)]  Loss: 4.540 (4.46)  Time: 0.163s, 6298.16/s  (0.193s, 5308.10/s)  LR: 8.424e-04  Data: 0.024 (0.032)
Train: 78 [ 450/1251 ( 36%)]  Loss: 4.427 (4.46)  Time: 0.181s, 5651.14/s  (0.193s, 5313.16/s)  LR: 8.424e-04  Data: 0.027 (0.032)
Train: 78 [ 500/1251 ( 40%)]  Loss: 4.299 (4.44)  Time: 0.166s, 6181.06/s  (0.192s, 5332.33/s)  LR: 8.424e-04  Data: 0.023 (0.031)
Train: 78 [ 550/1251 ( 44%)]  Loss: 4.196 (4.42)  Time: 0.185s, 5534.30/s  (0.192s, 5344.37/s)  LR: 8.424e-04  Data: 0.027 (0.031)
Train: 78 [ 600/1251 ( 48%)]  Loss: 4.655 (4.44)  Time: 0.168s, 6089.67/s  (0.192s, 5333.62/s)  LR: 8.424e-04  Data: 0.029 (0.031)
Train: 78 [ 650/1251 ( 52%)]  Loss: 4.288 (4.43)  Time: 0.162s, 6317.88/s  (0.192s, 5328.08/s)  LR: 8.424e-04  Data: 0.023 (0.030)
Train: 78 [ 700/1251 ( 56%)]  Loss: 4.490 (4.43)  Time: 0.164s, 6230.69/s  (0.192s, 5331.37/s)  LR: 8.424e-04  Data: 0.032 (0.030)
Train: 78 [ 750/1251 ( 60%)]  Loss: 4.421 (4.43)  Time: 0.336s, 3045.30/s  (0.192s, 5331.97/s)  LR: 8.424e-04  Data: 0.028 (0.030)
Train: 78 [ 800/1251 ( 64%)]  Loss: 4.352 (4.43)  Time: 0.159s, 6453.58/s  (0.192s, 5332.32/s)  LR: 8.424e-04  Data: 0.031 (0.030)
Train: 78 [ 850/1251 ( 68%)]  Loss: 4.180 (4.41)  Time: 0.202s, 5081.59/s  (0.192s, 5326.97/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [ 900/1251 ( 72%)]  Loss: 3.983 (4.39)  Time: 0.156s, 6567.38/s  (0.192s, 5322.00/s)  LR: 8.424e-04  Data: 0.032 (0.030)
Train: 78 [ 950/1251 ( 76%)]  Loss: 4.959 (4.42)  Time: 0.401s, 2555.16/s  (0.193s, 5308.37/s)  LR: 8.424e-04  Data: 0.042 (0.030)
Train: 78 [1000/1251 ( 80%)]  Loss: 4.562 (4.43)  Time: 0.178s, 5763.45/s  (0.193s, 5311.67/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [1050/1251 ( 84%)]  Loss: 4.538 (4.43)  Time: 0.170s, 6035.67/s  (0.193s, 5305.73/s)  LR: 8.424e-04  Data: 0.033 (0.030)
Train: 78 [1100/1251 ( 88%)]  Loss: 4.346 (4.43)  Time: 0.163s, 6269.02/s  (0.193s, 5300.42/s)  LR: 8.424e-04  Data: 0.035 (0.030)
Train: 78 [1150/1251 ( 92%)]  Loss: 4.633 (4.44)  Time: 0.174s, 5879.05/s  (0.193s, 5301.17/s)  LR: 8.424e-04  Data: 0.030 (0.030)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 78 [1200/1251 ( 96%)]  Loss: 4.285 (4.43)  Time: 0.622s, 1646.70/s  (0.193s, 5294.40/s)  LR: 8.424e-04  Data: 0.024 (0.030)
Train: 78 [1250/1251 (100%)]  Loss: 4.521 (4.43)  Time: 0.113s, 9031.56/s  (0.193s, 5317.43/s)  LR: 8.424e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.841 (1.841)  Loss:  1.1446 (1.1446)  Acc@1: 79.1016 (79.1016)  Acc@5: 94.2383 (94.2383)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.2895 (1.9020)  Acc@1: 78.3019 (62.3100)  Acc@5: 91.0377 (84.3400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-72.pth.tar', 61.140000024414064)

Train: 79 [   0/1251 (  0%)]  Loss: 4.349 (4.35)  Time: 1.666s,  614.50/s  (1.666s,  614.50/s)  LR: 8.386e-04  Data: 1.528 (1.528)
Train: 79 [  50/1251 (  4%)]  Loss: 4.179 (4.26)  Time: 0.170s, 6006.37/s  (0.218s, 4696.47/s)  LR: 8.386e-04  Data: 0.028 (0.063)
Train: 79 [ 100/1251 (  8%)]  Loss: 4.122 (4.22)  Time: 0.179s, 5719.48/s  (0.207s, 4957.48/s)  LR: 8.386e-04  Data: 0.026 (0.046)
Train: 79 [ 150/1251 ( 12%)]  Loss: 4.039 (4.17)  Time: 0.176s, 5819.34/s  (0.199s, 5137.82/s)  LR: 8.386e-04  Data: 0.026 (0.040)
Train: 79 [ 200/1251 ( 16%)]  Loss: 4.127 (4.16)  Time: 0.186s, 5513.45/s  (0.197s, 5194.19/s)  LR: 8.386e-04  Data: 0.028 (0.037)
Train: 79 [ 250/1251 ( 20%)]  Loss: 4.004 (4.14)  Time: 0.170s, 6008.59/s  (0.195s, 5247.63/s)  LR: 8.386e-04  Data: 0.025 (0.036)
Train: 79 [ 300/1251 ( 24%)]  Loss: 4.501 (4.19)  Time: 0.168s, 6113.14/s  (0.195s, 5255.16/s)  LR: 8.386e-04  Data: 0.024 (0.035)
Train: 79 [ 350/1251 ( 28%)]  Loss: 4.555 (4.23)  Time: 0.161s, 6374.20/s  (0.194s, 5268.33/s)  LR: 8.386e-04  Data: 0.024 (0.034)
Train: 79 [ 400/1251 ( 32%)]  Loss: 4.397 (4.25)  Time: 0.153s, 6701.21/s  (0.193s, 5297.55/s)  LR: 8.386e-04  Data: 0.023 (0.033)
Train: 79 [ 450/1251 ( 36%)]  Loss: 4.459 (4.27)  Time: 0.165s, 6207.76/s  (0.193s, 5301.34/s)  LR: 8.386e-04  Data: 0.030 (0.033)
Train: 79 [ 500/1251 ( 40%)]  Loss: 4.312 (4.28)  Time: 0.160s, 6407.15/s  (0.192s, 5324.14/s)  LR: 8.386e-04  Data: 0.034 (0.032)
Train: 79 [ 550/1251 ( 44%)]  Loss: 4.280 (4.28)  Time: 0.187s, 5477.74/s  (0.192s, 5336.46/s)  LR: 8.386e-04  Data: 0.027 (0.032)
Train: 79 [ 600/1251 ( 48%)]  Loss: 4.537 (4.30)  Time: 0.314s, 3259.67/s  (0.192s, 5333.44/s)  LR: 8.386e-04  Data: 0.041 (0.032)
Train: 79 [ 650/1251 ( 52%)]  Loss: 4.294 (4.30)  Time: 0.163s, 6271.88/s  (0.192s, 5333.11/s)  LR: 8.386e-04  Data: 0.024 (0.031)
Train: 79 [ 700/1251 ( 56%)]  Loss: 4.397 (4.30)  Time: 0.169s, 6045.00/s  (0.192s, 5334.97/s)  LR: 8.386e-04  Data: 0.023 (0.032)
Train: 79 [ 750/1251 ( 60%)]  Loss: 3.903 (4.28)  Time: 0.185s, 5540.24/s  (0.192s, 5336.83/s)  LR: 8.386e-04  Data: 0.027 (0.033)
Train: 79 [ 800/1251 ( 64%)]  Loss: 4.421 (4.29)  Time: 0.180s, 5685.44/s  (0.192s, 5335.40/s)  LR: 8.386e-04  Data: 0.039 (0.034)
Train: 79 [ 850/1251 ( 68%)]  Loss: 4.003 (4.27)  Time: 0.166s, 6175.44/s  (0.192s, 5334.41/s)  LR: 8.386e-04  Data: 0.028 (0.033)
Train: 79 [ 900/1251 ( 72%)]  Loss: 4.474 (4.28)  Time: 0.190s, 5398.55/s  (0.192s, 5329.41/s)  LR: 8.386e-04  Data: 0.023 (0.033)
Train: 79 [ 950/1251 ( 76%)]  Loss: 4.449 (4.29)  Time: 0.185s, 5521.65/s  (0.192s, 5329.45/s)  LR: 8.386e-04  Data: 0.033 (0.033)
Train: 79 [1000/1251 ( 80%)]  Loss: 3.951 (4.27)  Time: 0.188s, 5449.58/s  (0.192s, 5334.79/s)  LR: 8.386e-04  Data: 0.029 (0.033)
Train: 79 [1050/1251 ( 84%)]  Loss: 4.516 (4.28)  Time: 0.204s, 5007.89/s  (0.192s, 5336.17/s)  LR: 8.386e-04  Data: 0.023 (0.032)
Train: 79 [1100/1251 ( 88%)]  Loss: 4.365 (4.29)  Time: 0.171s, 5996.10/s  (0.192s, 5324.64/s)  LR: 8.386e-04  Data: 0.036 (0.032)
Train: 79 [1150/1251 ( 92%)]  Loss: 4.376 (4.29)  Time: 0.160s, 6388.58/s  (0.192s, 5320.42/s)  LR: 8.386e-04  Data: 0.030 (0.032)
Train: 79 [1200/1251 ( 96%)]  Loss: 4.116 (4.29)  Time: 0.177s, 5787.26/s  (0.193s, 5317.02/s)  LR: 8.386e-04  Data: 0.022 (0.032)
Train: 79 [1250/1251 (100%)]  Loss: 4.485 (4.29)  Time: 0.114s, 8996.03/s  (0.192s, 5331.14/s)  LR: 8.386e-04  Data: 0.000 (0.031)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.775 (1.775)  Loss:  1.1775 (1.1775)  Acc@1: 78.8086 (78.8086)  Acc@5: 94.4336 (94.4336)
Test: [  48/48]  Time: 0.019 (0.216)  Loss:  1.2608 (1.9369)  Acc@1: 78.3019 (61.9840)  Acc@5: 91.6274 (84.3460)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-75.pth.tar', 61.207999995117184)

Train: 80 [   0/1251 (  0%)]  Loss: 4.409 (4.41)  Time: 1.746s,  586.63/s  (1.746s,  586.63/s)  LR: 8.347e-04  Data: 1.618 (1.618)
Train: 80 [  50/1251 (  4%)]  Loss: 4.304 (4.36)  Time: 0.183s, 5591.55/s  (0.220s, 4655.10/s)  LR: 8.347e-04  Data: 0.020 (0.066)
Train: 80 [ 100/1251 (  8%)]  Loss: 4.615 (4.44)  Time: 0.167s, 6116.87/s  (0.208s, 4933.12/s)  LR: 8.347e-04  Data: 0.029 (0.048)
Train: 80 [ 150/1251 ( 12%)]  Loss: 4.253 (4.40)  Time: 0.160s, 6398.10/s  (0.201s, 5098.85/s)  LR: 8.347e-04  Data: 0.026 (0.041)
Train: 80 [ 200/1251 ( 16%)]  Loss: 4.176 (4.35)  Time: 0.177s, 5798.83/s  (0.199s, 5152.98/s)  LR: 8.347e-04  Data: 0.026 (0.038)
Train: 80 [ 250/1251 ( 20%)]  Loss: 4.671 (4.40)  Time: 0.196s, 5218.13/s  (0.196s, 5213.65/s)  LR: 8.347e-04  Data: 0.023 (0.036)
Train: 80 [ 300/1251 ( 24%)]  Loss: 4.359 (4.40)  Time: 0.159s, 6458.06/s  (0.195s, 5247.08/s)  LR: 8.347e-04  Data: 0.028 (0.035)
Train: 80 [ 350/1251 ( 28%)]  Loss: 4.197 (4.37)  Time: 0.164s, 6242.79/s  (0.193s, 5298.21/s)  LR: 8.347e-04  Data: 0.022 (0.035)
Train: 80 [ 400/1251 ( 32%)]  Loss: 4.351 (4.37)  Time: 0.416s, 2460.18/s  (0.194s, 5280.87/s)  LR: 8.347e-04  Data: 0.022 (0.034)
Train: 80 [ 450/1251 ( 36%)]  Loss: 4.109 (4.34)  Time: 0.156s, 6569.88/s  (0.193s, 5304.23/s)  LR: 8.347e-04  Data: 0.027 (0.033)
Train: 80 [ 500/1251 ( 40%)]  Loss: 4.558 (4.36)  Time: 0.153s, 6686.70/s  (0.192s, 5326.81/s)  LR: 8.347e-04  Data: 0.031 (0.033)
Train: 80 [ 550/1251 ( 44%)]  Loss: 4.316 (4.36)  Time: 0.176s, 5801.91/s  (0.193s, 5317.45/s)  LR: 8.347e-04  Data: 0.028 (0.033)
Train: 80 [ 600/1251 ( 48%)]  Loss: 4.487 (4.37)  Time: 0.174s, 5890.79/s  (0.193s, 5315.75/s)  LR: 8.347e-04  Data: 0.033 (0.032)
Train: 80 [ 650/1251 ( 52%)]  Loss: 4.334 (4.37)  Time: 0.167s, 6124.38/s  (0.192s, 5331.54/s)  LR: 8.347e-04  Data: 0.024 (0.032)
Train: 80 [ 700/1251 ( 56%)]  Loss: 4.557 (4.38)  Time: 0.171s, 6000.26/s  (0.192s, 5339.32/s)  LR: 8.347e-04  Data: 0.024 (0.032)
Train: 80 [ 750/1251 ( 60%)]  Loss: 4.272 (4.37)  Time: 0.197s, 5188.22/s  (0.192s, 5336.03/s)  LR: 8.347e-04  Data: 0.067 (0.033)
Train: 80 [ 800/1251 ( 64%)]  Loss: 4.041 (4.35)  Time: 0.205s, 4998.37/s  (0.192s, 5327.30/s)  LR: 8.347e-04  Data: 0.020 (0.035)
Train: 80 [ 850/1251 ( 68%)]  Loss: 4.575 (4.37)  Time: 0.175s, 5864.58/s  (0.192s, 5336.25/s)  LR: 8.347e-04  Data: 0.027 (0.035)
Train: 80 [ 900/1251 ( 72%)]  Loss: 4.289 (4.36)  Time: 0.168s, 6098.42/s  (0.192s, 5340.31/s)  LR: 8.347e-04  Data: 0.026 (0.036)
Train: 80 [ 950/1251 ( 76%)]  Loss: 3.930 (4.34)  Time: 0.319s, 3212.97/s  (0.192s, 5336.84/s)  LR: 8.347e-04  Data: 0.124 (0.036)
Train: 80 [1000/1251 ( 80%)]  Loss: 4.306 (4.34)  Time: 0.166s, 6162.74/s  (0.192s, 5330.77/s)  LR: 8.347e-04  Data: 0.022 (0.036)
Train: 80 [1050/1251 ( 84%)]  Loss: 4.476 (4.34)  Time: 0.163s, 6294.76/s  (0.192s, 5331.15/s)  LR: 8.347e-04  Data: 0.027 (0.036)
Train: 80 [1100/1251 ( 88%)]  Loss: 4.328 (4.34)  Time: 0.158s, 6494.15/s  (0.192s, 5324.26/s)  LR: 8.347e-04  Data: 0.026 (0.037)
Train: 80 [1150/1251 ( 92%)]  Loss: 4.163 (4.34)  Time: 0.554s, 1849.84/s  (0.193s, 5313.61/s)  LR: 8.347e-04  Data: 0.037 (0.036)
Train: 80 [1200/1251 ( 96%)]  Loss: 4.550 (4.35)  Time: 0.172s, 5952.72/s  (0.193s, 5307.72/s)  LR: 8.347e-04  Data: 0.023 (0.036)
Train: 80 [1250/1251 (100%)]  Loss: 4.118 (4.34)  Time: 0.114s, 8963.22/s  (0.192s, 5330.02/s)  LR: 8.347e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.850 (1.850)  Loss:  1.1093 (1.1093)  Acc@1: 79.1016 (79.1016)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.1664 (1.8430)  Acc@1: 77.4764 (62.6020)  Acc@5: 92.2170 (84.7180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-68.pth.tar', 61.24799996582031)

Train: 81 [   0/1251 (  0%)]  Loss: 4.351 (4.35)  Time: 1.727s,  592.78/s  (1.727s,  592.78/s)  LR: 8.308e-04  Data: 1.577 (1.577)
Train: 81 [  50/1251 (  4%)]  Loss: 4.338 (4.34)  Time: 0.186s, 5492.81/s  (0.225s, 4560.60/s)  LR: 8.308e-04  Data: 0.020 (0.070)
Train: 81 [ 100/1251 (  8%)]  Loss: 4.459 (4.38)  Time: 0.145s, 7042.30/s  (0.205s, 5000.57/s)  LR: 8.308e-04  Data: 0.027 (0.050)
Train: 81 [ 150/1251 ( 12%)]  Loss: 4.044 (4.30)  Time: 0.174s, 5886.24/s  (0.202s, 5068.37/s)  LR: 8.308e-04  Data: 0.027 (0.043)
Train: 81 [ 200/1251 ( 16%)]  Loss: 4.273 (4.29)  Time: 0.179s, 5718.97/s  (0.199s, 5143.03/s)  LR: 8.308e-04  Data: 0.029 (0.039)
Train: 81 [ 250/1251 ( 20%)]  Loss: 4.423 (4.31)  Time: 0.177s, 5791.18/s  (0.196s, 5222.49/s)  LR: 8.308e-04  Data: 0.030 (0.037)
Train: 81 [ 300/1251 ( 24%)]  Loss: 4.257 (4.31)  Time: 0.172s, 5936.51/s  (0.195s, 5249.41/s)  LR: 8.308e-04  Data: 0.025 (0.036)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 81 [ 350/1251 ( 28%)]  Loss: 4.040 (4.27)  Time: 0.165s, 6204.00/s  (0.194s, 5266.81/s)  LR: 8.308e-04  Data: 0.023 (0.034)
Train: 81 [ 400/1251 ( 32%)]  Loss: 4.630 (4.31)  Time: 0.177s, 5800.75/s  (0.194s, 5276.17/s)  LR: 8.308e-04  Data: 0.028 (0.034)
Train: 81 [ 450/1251 ( 36%)]  Loss: 3.952 (4.28)  Time: 0.170s, 6024.90/s  (0.193s, 5301.00/s)  LR: 8.308e-04  Data: 0.036 (0.033)
Train: 81 [ 500/1251 ( 40%)]  Loss: 4.210 (4.27)  Time: 0.160s, 6414.75/s  (0.193s, 5315.38/s)  LR: 8.308e-04  Data: 0.025 (0.033)
Train: 81 [ 550/1251 ( 44%)]  Loss: 4.822 (4.32)  Time: 0.167s, 6144.36/s  (0.193s, 5314.50/s)  LR: 8.308e-04  Data: 0.031 (0.032)
Train: 81 [ 600/1251 ( 48%)]  Loss: 4.256 (4.31)  Time: 0.163s, 6295.13/s  (0.193s, 5312.95/s)  LR: 8.308e-04  Data: 0.026 (0.032)
Train: 81 [ 650/1251 ( 52%)]  Loss: 4.470 (4.32)  Time: 0.162s, 6339.20/s  (0.192s, 5327.12/s)  LR: 8.308e-04  Data: 0.023 (0.032)
Train: 81 [ 700/1251 ( 56%)]  Loss: 4.466 (4.33)  Time: 0.155s, 6624.24/s  (0.192s, 5332.30/s)  LR: 8.308e-04  Data: 0.028 (0.031)
Train: 81 [ 750/1251 ( 60%)]  Loss: 4.448 (4.34)  Time: 0.202s, 5059.61/s  (0.192s, 5323.37/s)  LR: 8.308e-04  Data: 0.035 (0.031)
Train: 81 [ 800/1251 ( 64%)]  Loss: 4.386 (4.34)  Time: 0.164s, 6243.84/s  (0.192s, 5328.90/s)  LR: 8.308e-04  Data: 0.026 (0.031)
Train: 81 [ 850/1251 ( 68%)]  Loss: 4.346 (4.34)  Time: 0.174s, 5882.36/s  (0.192s, 5338.04/s)  LR: 8.308e-04  Data: 0.026 (0.031)
Train: 81 [ 900/1251 ( 72%)]  Loss: 4.264 (4.34)  Time: 0.214s, 4787.83/s  (0.192s, 5339.33/s)  LR: 8.308e-04  Data: 0.021 (0.031)
Train: 81 [ 950/1251 ( 76%)]  Loss: 4.127 (4.33)  Time: 0.292s, 3502.53/s  (0.192s, 5336.54/s)  LR: 8.308e-04  Data: 0.026 (0.030)
Train: 81 [1000/1251 ( 80%)]  Loss: 4.651 (4.34)  Time: 0.369s, 2773.59/s  (0.192s, 5336.87/s)  LR: 8.308e-04  Data: 0.027 (0.030)
Train: 81 [1050/1251 ( 84%)]  Loss: 4.270 (4.34)  Time: 0.152s, 6716.25/s  (0.192s, 5344.33/s)  LR: 8.308e-04  Data: 0.025 (0.030)
Train: 81 [1100/1251 ( 88%)]  Loss: 4.172 (4.33)  Time: 0.179s, 5725.78/s  (0.192s, 5341.45/s)  LR: 8.308e-04  Data: 0.019 (0.030)
Train: 81 [1150/1251 ( 92%)]  Loss: 4.375 (4.33)  Time: 0.520s, 1970.33/s  (0.192s, 5332.61/s)  LR: 8.308e-04  Data: 0.018 (0.030)
Train: 81 [1200/1251 ( 96%)]  Loss: 4.268 (4.33)  Time: 0.167s, 6127.45/s  (0.192s, 5325.30/s)  LR: 8.308e-04  Data: 0.025 (0.030)
Train: 81 [1250/1251 (100%)]  Loss: 4.198 (4.33)  Time: 0.114s, 9020.03/s  (0.192s, 5342.47/s)  LR: 8.308e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.875 (1.875)  Loss:  1.3802 (1.3802)  Acc@1: 76.0742 (76.0742)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.1955 (1.9297)  Acc@1: 78.7736 (62.1680)  Acc@5: 91.9811 (84.5600)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-73.pth.tar', 61.25399996337891)

Train: 82 [   0/1251 (  0%)]  Loss: 4.277 (4.28)  Time: 1.728s,  592.73/s  (1.728s,  592.73/s)  LR: 8.269e-04  Data: 1.596 (1.596)
Train: 82 [  50/1251 (  4%)]  Loss: 4.348 (4.31)  Time: 0.167s, 6113.55/s  (0.223s, 4585.79/s)  LR: 8.269e-04  Data: 0.026 (0.080)
Train: 82 [ 100/1251 (  8%)]  Loss: 4.004 (4.21)  Time: 0.172s, 5943.64/s  (0.205s, 4994.07/s)  LR: 8.269e-04  Data: 0.031 (0.061)
Train: 82 [ 150/1251 ( 12%)]  Loss: 4.290 (4.23)  Time: 0.160s, 6400.54/s  (0.199s, 5146.42/s)  LR: 8.269e-04  Data: 0.028 (0.053)
Train: 82 [ 200/1251 ( 16%)]  Loss: 4.121 (4.21)  Time: 0.187s, 5461.74/s  (0.197s, 5209.86/s)  LR: 8.269e-04  Data: 0.023 (0.047)
Train: 82 [ 250/1251 ( 20%)]  Loss: 4.378 (4.24)  Time: 0.176s, 5819.90/s  (0.195s, 5247.26/s)  LR: 8.269e-04  Data: 0.029 (0.045)
Train: 82 [ 300/1251 ( 24%)]  Loss: 4.451 (4.27)  Time: 0.168s, 6092.78/s  (0.194s, 5272.41/s)  LR: 8.269e-04  Data: 0.032 (0.043)
Train: 82 [ 350/1251 ( 28%)]  Loss: 4.842 (4.34)  Time: 0.177s, 5800.82/s  (0.193s, 5311.63/s)  LR: 8.269e-04  Data: 0.026 (0.041)
Train: 82 [ 400/1251 ( 32%)]  Loss: 4.508 (4.36)  Time: 0.170s, 6015.65/s  (0.193s, 5312.52/s)  LR: 8.269e-04  Data: 0.020 (0.039)
Train: 82 [ 450/1251 ( 36%)]  Loss: 4.426 (4.36)  Time: 0.179s, 5708.55/s  (0.193s, 5295.47/s)  LR: 8.269e-04  Data: 0.056 (0.038)
Train: 82 [ 500/1251 ( 40%)]  Loss: 4.393 (4.37)  Time: 0.259s, 3949.82/s  (0.193s, 5313.71/s)  LR: 8.269e-04  Data: 0.028 (0.037)
Train: 82 [ 550/1251 ( 44%)]  Loss: 4.437 (4.37)  Time: 0.186s, 5500.54/s  (0.192s, 5326.33/s)  LR: 8.269e-04  Data: 0.024 (0.037)
Train: 82 [ 600/1251 ( 48%)]  Loss: 4.475 (4.38)  Time: 0.162s, 6337.72/s  (0.192s, 5331.95/s)  LR: 8.269e-04  Data: 0.030 (0.036)
Train: 82 [ 650/1251 ( 52%)]  Loss: 4.283 (4.37)  Time: 0.155s, 6618.31/s  (0.193s, 5315.48/s)  LR: 8.269e-04  Data: 0.029 (0.035)
Train: 82 [ 700/1251 ( 56%)]  Loss: 4.780 (4.40)  Time: 0.159s, 6421.69/s  (0.192s, 5321.94/s)  LR: 8.269e-04  Data: 0.024 (0.035)
Train: 82 [ 750/1251 ( 60%)]  Loss: 4.531 (4.41)  Time: 0.168s, 6098.15/s  (0.192s, 5332.35/s)  LR: 8.269e-04  Data: 0.021 (0.034)
Train: 82 [ 800/1251 ( 64%)]  Loss: 4.121 (4.39)  Time: 0.168s, 6091.25/s  (0.192s, 5337.84/s)  LR: 8.269e-04  Data: 0.020 (0.034)
Train: 82 [ 850/1251 ( 68%)]  Loss: 4.198 (4.38)  Time: 0.177s, 5773.48/s  (0.192s, 5329.04/s)  LR: 8.269e-04  Data: 0.027 (0.034)
Train: 82 [ 900/1251 ( 72%)]  Loss: 4.329 (4.38)  Time: 0.184s, 5576.49/s  (0.192s, 5327.15/s)  LR: 8.269e-04  Data: 0.024 (0.033)
Train: 82 [ 950/1251 ( 76%)]  Loss: 4.173 (4.37)  Time: 0.176s, 5805.33/s  (0.192s, 5325.98/s)  LR: 8.269e-04  Data: 0.027 (0.033)
Train: 82 [1000/1251 ( 80%)]  Loss: 4.480 (4.37)  Time: 0.174s, 5885.83/s  (0.192s, 5321.58/s)  LR: 8.269e-04  Data: 0.031 (0.033)
Train: 82 [1050/1251 ( 84%)]  Loss: 4.503 (4.38)  Time: 0.195s, 5249.50/s  (0.193s, 5316.77/s)  LR: 8.269e-04  Data: 0.024 (0.033)
Train: 82 [1100/1251 ( 88%)]  Loss: 4.589 (4.39)  Time: 0.175s, 5841.09/s  (0.193s, 5311.14/s)  LR: 8.269e-04  Data: 0.028 (0.032)
Train: 82 [1150/1251 ( 92%)]  Loss: 4.197 (4.38)  Time: 0.168s, 6078.19/s  (0.193s, 5312.60/s)  LR: 8.269e-04  Data: 0.021 (0.032)
Train: 82 [1200/1251 ( 96%)]  Loss: 4.623 (4.39)  Time: 0.183s, 5589.75/s  (0.193s, 5312.78/s)  LR: 8.269e-04  Data: 0.034 (0.032)
Train: 82 [1250/1251 (100%)]  Loss: 4.450 (4.39)  Time: 0.114s, 9021.75/s  (0.192s, 5322.37/s)  LR: 8.269e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.818 (1.818)  Loss:  1.0422 (1.0422)  Acc@1: 78.6133 (78.6133)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.219)  Loss:  1.2565 (1.8634)  Acc@1: 76.8868 (61.6560)  Acc@5: 90.9198 (84.1380)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-70.pth.tar', 61.3380000390625)

Train: 83 [   0/1251 (  0%)]  Loss: 4.106 (4.11)  Time: 1.846s,  554.81/s  (1.846s,  554.81/s)  LR: 8.229e-04  Data: 1.716 (1.716)
Train: 83 [  50/1251 (  4%)]  Loss: 4.325 (4.22)  Time: 0.162s, 6337.32/s  (0.223s, 4592.96/s)  LR: 8.229e-04  Data: 0.029 (0.074)
Train: 83 [ 100/1251 (  8%)]  Loss: 4.207 (4.21)  Time: 0.182s, 5620.77/s  (0.209s, 4908.25/s)  LR: 8.229e-04  Data: 0.021 (0.060)
Train: 83 [ 150/1251 ( 12%)]  Loss: 4.015 (4.16)  Time: 0.189s, 5414.49/s  (0.199s, 5141.80/s)  LR: 8.229e-04  Data: 0.029 (0.052)
Train: 83 [ 200/1251 ( 16%)]  Loss: 4.036 (4.14)  Time: 0.171s, 5996.55/s  (0.197s, 5203.00/s)  LR: 8.229e-04  Data: 0.030 (0.048)
Train: 83 [ 250/1251 ( 20%)]  Loss: 4.474 (4.19)  Time: 0.179s, 5728.96/s  (0.196s, 5213.54/s)  LR: 8.229e-04  Data: 0.033 (0.044)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0



Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 83 [ 300/1251 ( 24%)]  Loss: 4.354 (4.22)  Time: 0.182s, 5632.17/s  (0.195s, 5240.40/s)  LR: 8.229e-04  Data: 0.028 (0.042)
Train: 83 [ 350/1251 ( 28%)]  Loss: 4.571 (4.26)  Time: 0.169s, 6043.28/s  (0.194s, 5270.59/s)  LR: 8.229e-04  Data: 0.020 (0.040)
Train: 83 [ 400/1251 ( 32%)]  Loss: 3.884 (4.22)  Time: 0.161s, 6359.37/s  (0.194s, 5278.62/s)  LR: 8.229e-04  Data: 0.025 (0.038)
Train: 83 [ 450/1251 ( 36%)]  Loss: 4.416 (4.24)  Time: 0.213s, 4797.92/s  (0.194s, 5284.58/s)  LR: 8.229e-04  Data: 0.027 (0.037)
Train: 83 [ 500/1251 ( 40%)]  Loss: 4.628 (4.27)  Time: 0.164s, 6260.08/s  (0.193s, 5297.98/s)  LR: 8.229e-04  Data: 0.028 (0.036)
Train: 83 [ 550/1251 ( 44%)]  Loss: 3.984 (4.25)  Time: 0.179s, 5707.47/s  (0.194s, 5287.87/s)  LR: 8.229e-04  Data: 0.027 (0.036)
Train: 83 [ 600/1251 ( 48%)]  Loss: 4.659 (4.28)  Time: 0.179s, 5726.48/s  (0.194s, 5291.30/s)  LR: 8.229e-04  Data: 0.021 (0.035)
Train: 83 [ 650/1251 ( 52%)]  Loss: 4.272 (4.28)  Time: 0.171s, 5971.36/s  (0.193s, 5295.08/s)  LR: 8.229e-04  Data: 0.024 (0.035)
Train: 83 [ 700/1251 ( 56%)]  Loss: 4.715 (4.31)  Time: 0.191s, 5355.29/s  (0.193s, 5306.72/s)  LR: 8.229e-04  Data: 0.029 (0.034)
Train: 83 [ 750/1251 ( 60%)]  Loss: 4.445 (4.32)  Time: 0.188s, 5442.81/s  (0.193s, 5303.97/s)  LR: 8.229e-04  Data: 0.026 (0.034)
Train: 83 [ 800/1251 ( 64%)]  Loss: 4.478 (4.33)  Time: 0.159s, 6445.23/s  (0.193s, 5300.41/s)  LR: 8.229e-04  Data: 0.025 (0.034)
Train: 83 [ 850/1251 ( 68%)]  Loss: 4.411 (4.33)  Time: 0.181s, 5656.95/s  (0.193s, 5313.47/s)  LR: 8.229e-04  Data: 0.025 (0.033)
Train: 83 [ 900/1251 ( 72%)]  Loss: 4.035 (4.32)  Time: 0.174s, 5874.83/s  (0.193s, 5302.38/s)  LR: 8.229e-04  Data: 0.031 (0.033)
Train: 83 [ 950/1251 ( 76%)]  Loss: 4.147 (4.31)  Time: 0.261s, 3929.68/s  (0.193s, 5311.91/s)  LR: 8.229e-04  Data: 0.034 (0.033)
Train: 83 [1000/1251 ( 80%)]  Loss: 4.109 (4.30)  Time: 0.156s, 6559.35/s  (0.193s, 5315.01/s)  LR: 8.229e-04  Data: 0.023 (0.033)
Train: 83 [1050/1251 ( 84%)]  Loss: 4.544 (4.31)  Time: 0.160s, 6398.25/s  (0.193s, 5312.69/s)  LR: 8.229e-04  Data: 0.031 (0.032)
Train: 83 [1100/1251 ( 88%)]  Loss: 3.879 (4.29)  Time: 0.439s, 2330.60/s  (0.193s, 5305.39/s)  LR: 8.229e-04  Data: 0.020 (0.032)
Train: 83 [1150/1251 ( 92%)]  Loss: 4.176 (4.29)  Time: 0.165s, 6209.91/s  (0.193s, 5307.92/s)  LR: 8.229e-04  Data: 0.026 (0.032)
Train: 83 [1200/1251 ( 96%)]  Loss: 4.644 (4.30)  Time: 0.303s, 3381.49/s  (0.193s, 5305.52/s)  LR: 8.229e-04  Data: 0.023 (0.032)
Train: 83 [1250/1251 (100%)]  Loss: 4.272 (4.30)  Time: 0.113s, 9026.42/s  (0.192s, 5323.50/s)  LR: 8.229e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.761 (1.761)  Loss:  1.3063 (1.3063)  Acc@1: 78.4180 (78.4180)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2702 (1.9176)  Acc@1: 78.0660 (62.1360)  Acc@5: 91.1557 (84.4280)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-62.pth.tar', 61.5760000390625)

Train: 84 [   0/1251 (  0%)]  Loss: 4.202 (4.20)  Time: 1.813s,  564.86/s  (1.813s,  564.86/s)  LR: 8.189e-04  Data: 1.688 (1.688)
Train: 84 [  50/1251 (  4%)]  Loss: 4.281 (4.24)  Time: 0.174s, 5872.47/s  (0.217s, 4711.09/s)  LR: 8.189e-04  Data: 0.033 (0.071)
Train: 84 [ 100/1251 (  8%)]  Loss: 4.104 (4.20)  Time: 0.170s, 6029.18/s  (0.204s, 5023.32/s)  LR: 8.189e-04  Data: 0.024 (0.050)
Train: 84 [ 150/1251 ( 12%)]  Loss: 4.508 (4.27)  Time: 0.168s, 6095.05/s  (0.199s, 5146.40/s)  LR: 8.189e-04  Data: 0.028 (0.043)
Train: 84 [ 200/1251 ( 16%)]  Loss: 3.858 (4.19)  Time: 0.179s, 5708.98/s  (0.196s, 5234.50/s)  LR: 8.189e-04  Data: 0.031 (0.040)
Train: 84 [ 250/1251 ( 20%)]  Loss: 4.315 (4.21)  Time: 0.215s, 4771.34/s  (0.195s, 5245.55/s)  LR: 8.189e-04  Data: 0.034 (0.038)
Train: 84 [ 300/1251 ( 24%)]  Loss: 4.404 (4.24)  Time: 0.163s, 6278.17/s  (0.194s, 5290.82/s)  LR: 8.189e-04  Data: 0.029 (0.036)
Train: 84 [ 350/1251 ( 28%)]  Loss: 4.443 (4.26)  Time: 0.194s, 5282.29/s  (0.194s, 5268.87/s)  LR: 8.189e-04  Data: 0.021 (0.035)
Train: 84 [ 400/1251 ( 32%)]  Loss: 4.039 (4.24)  Time: 0.358s, 2857.17/s  (0.194s, 5285.45/s)  LR: 8.189e-04  Data: 0.026 (0.034)
Train: 84 [ 450/1251 ( 36%)]  Loss: 4.275 (4.24)  Time: 0.190s, 5403.05/s  (0.194s, 5287.10/s)  LR: 8.189e-04  Data: 0.044 (0.033)
Train: 84 [ 500/1251 ( 40%)]  Loss: 4.362 (4.25)  Time: 0.158s, 6465.09/s  (0.193s, 5297.85/s)  LR: 8.189e-04  Data: 0.031 (0.033)
Train: 84 [ 550/1251 ( 44%)]  Loss: 4.225 (4.25)  Time: 0.167s, 6130.38/s  (0.193s, 5315.85/s)  LR: 8.189e-04  Data: 0.026 (0.032)
Train: 84 [ 600/1251 ( 48%)]  Loss: 4.593 (4.28)  Time: 0.162s, 6302.24/s  (0.193s, 5317.99/s)  LR: 8.189e-04  Data: 0.028 (0.032)
Train: 84 [ 650/1251 ( 52%)]  Loss: 4.524 (4.30)  Time: 0.177s, 5788.01/s  (0.192s, 5320.19/s)  LR: 8.189e-04  Data: 0.037 (0.032)
Train: 84 [ 700/1251 ( 56%)]  Loss: 4.273 (4.29)  Time: 0.255s, 4019.78/s  (0.192s, 5322.62/s)  LR: 8.189e-04  Data: 0.032 (0.032)
Train: 84 [ 750/1251 ( 60%)]  Loss: 4.416 (4.30)  Time: 0.227s, 4507.86/s  (0.192s, 5328.55/s)  LR: 8.189e-04  Data: 0.022 (0.032)
Train: 84 [ 800/1251 ( 64%)]  Loss: 4.693 (4.32)  Time: 0.178s, 5744.23/s  (0.193s, 5306.18/s)  LR: 8.189e-04  Data: 0.034 (0.033)
Train: 84 [ 850/1251 ( 68%)]  Loss: 4.034 (4.31)  Time: 0.183s, 5595.85/s  (0.193s, 5303.19/s)  LR: 8.189e-04  Data: 0.053 (0.034)
Train: 84 [ 900/1251 ( 72%)]  Loss: 4.353 (4.31)  Time: 0.169s, 6044.26/s  (0.193s, 5305.07/s)  LR: 8.189e-04  Data: 0.036 (0.034)
Train: 84 [ 950/1251 ( 76%)]  Loss: 4.539 (4.32)  Time: 0.185s, 5543.52/s  (0.193s, 5316.68/s)  LR: 8.189e-04  Data: 0.028 (0.034)
Train: 84 [1000/1251 ( 80%)]  Loss: 4.490 (4.33)  Time: 0.217s, 4729.78/s  (0.193s, 5305.89/s)  LR: 8.189e-04  Data: 0.024 (0.035)
Train: 84 [1050/1251 ( 84%)]  Loss: 4.176 (4.32)  Time: 0.275s, 3720.43/s  (0.193s, 5305.31/s)  LR: 8.189e-04  Data: 0.153 (0.036)
Train: 84 [1100/1251 ( 88%)]  Loss: 4.468 (4.33)  Time: 0.155s, 6610.32/s  (0.193s, 5307.07/s)  LR: 8.189e-04  Data: 0.027 (0.036)
Train: 84 [1150/1251 ( 92%)]  Loss: 4.517 (4.34)  Time: 0.183s, 5594.32/s  (0.193s, 5311.38/s)  LR: 8.189e-04  Data: 0.027 (0.036)
Train: 84 [1200/1251 ( 96%)]  Loss: 4.469 (4.34)  Time: 0.158s, 6478.15/s  (0.193s, 5297.12/s)  LR: 8.189e-04  Data: 0.032 (0.036)
Train: 84 [1250/1251 (100%)]  Loss: 4.353 (4.34)  Time: 0.113s, 9045.24/s  (0.193s, 5311.16/s)  LR: 8.189e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.852 (1.852)  Loss:  1.2175 (1.2175)  Acc@1: 77.3438 (77.3438)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.019 (0.213)  Loss:  1.2846 (1.8833)  Acc@1: 76.6509 (61.4180)  Acc@5: 91.7453 (83.8520)
Train: 85 [   0/1251 (  0%)]  Loss: 4.479 (4.48)  Time: 1.901s,  538.54/s  (1.901s,  538.54/s)  LR: 8.148e-04  Data: 1.760 (1.760)
Train: 85 [  50/1251 (  4%)]  Loss: 4.489 (4.48)  Time: 0.184s, 5556.30/s  (0.221s, 4634.18/s)  LR: 8.148e-04  Data: 0.019 (0.070)
Train: 85 [ 100/1251 (  8%)]  Loss: 4.444 (4.47)  Time: 0.178s, 5745.18/s  (0.208s, 4927.03/s)  LR: 8.148e-04  Data: 0.028 (0.049)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Train: 85 [ 150/1251 ( 12%)]  Loss: 4.346 (4.44)  Time: 0.166s, 6155.96/s  (0.200s, 5127.96/s)  LR: 8.148e-04  Data: 0.025 (0.042)
Train: 85 [ 200/1251 ( 16%)]  Loss: 4.038 (4.36)  Time: 0.357s, 2866.61/s  (0.198s, 5174.26/s)  LR: 8.148e-04  Data: 0.031 (0.039)
Train: 85 [ 250/1251 ( 20%)]  Loss: 4.902 (4.45)  Time: 0.165s, 6213.45/s  (0.196s, 5211.49/s)  LR: 8.148e-04  Data: 0.030 (0.037)
Train: 85 [ 300/1251 ( 24%)]  Loss: 4.454 (4.45)  Time: 0.153s, 6686.32/s  (0.194s, 5278.31/s)  LR: 8.148e-04  Data: 0.021 (0.035)
Train: 85 [ 350/1251 ( 28%)]  Loss: 3.996 (4.39)  Time: 0.169s, 6074.23/s  (0.194s, 5284.26/s)  LR: 8.148e-04  Data: 0.023 (0.034)
Train: 85 [ 400/1251 ( 32%)]  Loss: 4.143 (4.37)  Time: 0.180s, 5700.74/s  (0.194s, 5265.46/s)  LR: 8.148e-04  Data: 0.030 (0.033)
Train: 85 [ 450/1251 ( 36%)]  Loss: 4.182 (4.35)  Time: 0.176s, 5806.76/s  (0.193s, 5294.93/s)  LR: 8.148e-04  Data: 0.029 (0.033)
Train: 85 [ 500/1251 ( 40%)]  Loss: 4.726 (4.38)  Time: 0.163s, 6292.87/s  (0.193s, 5312.11/s)  LR: 8.148e-04  Data: 0.026 (0.032)
Train: 85 [ 550/1251 ( 44%)]  Loss: 4.039 (4.35)  Time: 0.197s, 5197.92/s  (0.192s, 5321.50/s)  LR: 8.148e-04  Data: 0.042 (0.032)
Train: 85 [ 600/1251 ( 48%)]  Loss: 4.262 (4.35)  Time: 0.163s, 6277.09/s  (0.193s, 5316.18/s)  LR: 8.148e-04  Data: 0.022 (0.032)
Train: 85 [ 650/1251 ( 52%)]  Loss: 4.287 (4.34)  Time: 0.158s, 6466.43/s  (0.192s, 5319.90/s)  LR: 8.148e-04  Data: 0.032 (0.031)
Train: 85 [ 700/1251 ( 56%)]  Loss: 4.446 (4.35)  Time: 0.189s, 5421.83/s  (0.193s, 5315.76/s)  LR: 8.148e-04  Data: 0.025 (0.031)
Train: 85 [ 750/1251 ( 60%)]  Loss: 4.284 (4.34)  Time: 0.170s, 6012.63/s  (0.192s, 5325.32/s)  LR: 8.148e-04  Data: 0.029 (0.031)
Train: 85 [ 800/1251 ( 64%)]  Loss: 4.477 (4.35)  Time: 0.157s, 6515.62/s  (0.193s, 5312.96/s)  LR: 8.148e-04  Data: 0.027 (0.031)
Train: 85 [ 850/1251 ( 68%)]  Loss: 4.502 (4.36)  Time: 0.155s, 6585.94/s  (0.193s, 5310.77/s)  LR: 8.148e-04  Data: 0.026 (0.030)
Train: 85 [ 900/1251 ( 72%)]  Loss: 4.353 (4.36)  Time: 0.164s, 6237.68/s  (0.193s, 5314.03/s)  LR: 8.148e-04  Data: 0.035 (0.030)
Train: 85 [ 950/1251 ( 76%)]  Loss: 4.277 (4.36)  Time: 0.168s, 6111.58/s  (0.193s, 5313.86/s)  LR: 8.148e-04  Data: 0.020 (0.030)
Train: 85 [1000/1251 ( 80%)]  Loss: 4.229 (4.35)  Time: 0.171s, 6001.43/s  (0.193s, 5312.66/s)  LR: 8.148e-04  Data: 0.027 (0.030)
Train: 85 [1050/1251 ( 84%)]  Loss: 4.597 (4.36)  Time: 0.178s, 5760.80/s  (0.193s, 5312.36/s)  LR: 8.148e-04  Data: 0.023 (0.030)
Train: 85 [1100/1251 ( 88%)]  Loss: 4.497 (4.37)  Time: 0.187s, 5477.01/s  (0.193s, 5312.16/s)  LR: 8.148e-04  Data: 0.031 (0.030)
Train: 85 [1150/1251 ( 92%)]  Loss: 4.390 (4.37)  Time: 0.167s, 6117.63/s  (0.193s, 5305.97/s)  LR: 8.148e-04  Data: 0.032 (0.030)
Train: 85 [1200/1251 ( 96%)]  Loss: 4.494 (4.37)  Time: 0.162s, 6305.93/s  (0.193s, 5303.99/s)  LR: 8.148e-04  Data: 0.023 (0.030)
Train: 85 [1250/1251 (100%)]  Loss: 4.298 (4.37)  Time: 0.114s, 9004.99/s  (0.193s, 5311.53/s)  LR: 8.148e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.767 (1.767)  Loss:  1.2436 (1.2436)  Acc@1: 78.5156 (78.5156)  Acc@5: 94.0430 (94.0430)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.2642 (1.9088)  Acc@1: 77.9481 (62.7100)  Acc@5: 92.2170 (84.7340)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-82.pth.tar', 61.65600004394531)

Train: 86 [   0/1251 (  0%)]  Loss: 4.256 (4.26)  Time: 1.759s,  582.27/s  (1.759s,  582.27/s)  LR: 8.108e-04  Data: 1.636 (1.636)
Train: 86 [  50/1251 (  4%)]  Loss: 4.178 (4.22)  Time: 0.181s, 5646.11/s  (0.225s, 4558.40/s)  LR: 8.108e-04  Data: 0.026 (0.074)
Train: 86 [ 100/1251 (  8%)]  Loss: 3.932 (4.12)  Time: 0.191s, 5361.44/s  (0.206s, 4982.57/s)  LR: 8.108e-04  Data: 0.024 (0.058)
Train: 86 [ 150/1251 ( 12%)]  Loss: 4.705 (4.27)  Time: 0.169s, 6046.59/s  (0.197s, 5195.25/s)  LR: 8.108e-04  Data: 0.024 (0.050)
Train: 86 [ 200/1251 ( 16%)]  Loss: 4.244 (4.26)  Time: 0.251s, 4077.86/s  (0.196s, 5225.19/s)  LR: 8.108e-04  Data: 0.029 (0.047)
Train: 86 [ 250/1251 ( 20%)]  Loss: 4.517 (4.31)  Time: 0.165s, 6205.76/s  (0.195s, 5250.22/s)  LR: 8.108e-04  Data: 0.020 (0.043)
Train: 86 [ 300/1251 ( 24%)]  Loss: 4.178 (4.29)  Time: 0.177s, 5770.42/s  (0.195s, 5254.06/s)  LR: 8.108e-04  Data: 0.031 (0.041)
Train: 86 [ 350/1251 ( 28%)]  Loss: 4.599 (4.33)  Time: 0.148s, 6931.26/s  (0.194s, 5286.38/s)  LR: 8.108e-04  Data: 0.029 (0.039)
Train: 86 [ 400/1251 ( 32%)]  Loss: 4.178 (4.31)  Time: 0.160s, 6392.87/s  (0.193s, 5310.04/s)  LR: 8.108e-04  Data: 0.029 (0.038)
Train: 86 [ 450/1251 ( 36%)]  Loss: 4.569 (4.34)  Time: 0.173s, 5920.00/s  (0.192s, 5343.66/s)  LR: 8.108e-04  Data: 0.038 (0.037)
Train: 86 [ 500/1251 ( 40%)]  Loss: 4.559 (4.36)  Time: 0.243s, 4214.43/s  (0.192s, 5338.04/s)  LR: 8.108e-04  Data: 0.021 (0.037)
Train: 86 [ 550/1251 ( 44%)]  Loss: 4.524 (4.37)  Time: 0.181s, 5660.90/s  (0.192s, 5331.05/s)  LR: 8.108e-04  Data: 0.023 (0.036)
Train: 86 [ 600/1251 ( 48%)]  Loss: 4.356 (4.37)  Time: 0.169s, 6071.35/s  (0.192s, 5338.81/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [ 650/1251 ( 52%)]  Loss: 4.634 (4.39)  Time: 0.153s, 6674.67/s  (0.192s, 5346.09/s)  LR: 8.108e-04  Data: 0.024 (0.036)
Train: 86 [ 700/1251 ( 56%)]  Loss: 4.720 (4.41)  Time: 0.176s, 5815.39/s  (0.192s, 5339.75/s)  LR: 8.108e-04  Data: 0.026 (0.036)
Train: 86 [ 750/1251 ( 60%)]  Loss: 4.459 (4.41)  Time: 0.167s, 6130.89/s  (0.192s, 5346.09/s)  LR: 8.108e-04  Data: 0.025 (0.036)
Train: 86 [ 800/1251 ( 64%)]  Loss: 4.685 (4.43)  Time: 0.186s, 5496.52/s  (0.192s, 5340.73/s)  LR: 8.108e-04  Data: 0.020 (0.037)
Train: 86 [ 850/1251 ( 68%)]  Loss: 4.617 (4.44)  Time: 0.202s, 5071.14/s  (0.192s, 5340.20/s)  LR: 8.108e-04  Data: 0.019 (0.037)
Train: 86 [ 900/1251 ( 72%)]  Loss: 4.360 (4.44)  Time: 0.153s, 6703.04/s  (0.192s, 5342.84/s)  LR: 8.108e-04  Data: 0.026 (0.037)
Train: 86 [ 950/1251 ( 76%)]  Loss: 4.337 (4.43)  Time: 0.177s, 5799.48/s  (0.192s, 5344.85/s)  LR: 8.108e-04  Data: 0.033 (0.037)
Train: 86 [1000/1251 ( 80%)]  Loss: 4.214 (4.42)  Time: 0.174s, 5875.94/s  (0.192s, 5342.47/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [1050/1251 ( 84%)]  Loss: 4.653 (4.43)  Time: 0.159s, 6424.05/s  (0.192s, 5337.13/s)  LR: 8.108e-04  Data: 0.025 (0.036)
Train: 86 [1100/1251 ( 88%)]  Loss: 4.389 (4.43)  Time: 0.239s, 4290.94/s  (0.192s, 5331.15/s)  LR: 8.108e-04  Data: 0.027 (0.036)
Train: 86 [1150/1251 ( 92%)]  Loss: 4.489 (4.43)  Time: 0.171s, 5971.96/s  (0.192s, 5333.73/s)  LR: 8.108e-04  Data: 0.029 (0.035)
Train: 86 [1200/1251 ( 96%)]  Loss: 4.278 (4.43)  Time: 0.160s, 6400.49/s  (0.192s, 5331.63/s)  LR: 8.108e-04  Data: 0.023 (0.035)
Train: 86 [1250/1251 (100%)]  Loss: 4.312 (4.42)  Time: 0.114s, 8982.99/s  (0.192s, 5343.89/s)  LR: 8.108e-04  Data: 0.000 (0.035)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.935 (1.935)  Loss:  1.2340 (1.2340)  Acc@1: 78.7109 (78.7109)  Acc@5: 94.1406 (94.1406)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.2591 (1.8772)  Acc@1: 79.4811 (62.5340)  Acc@5: 92.3349 (84.6000)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-67.pth.tar', 61.7560000415039)

Train: 87 [   0/1251 (  0%)]  Loss: 3.969 (3.97)  Time: 1.688s,  606.74/s  (1.688s,  606.74/s)  LR: 8.066e-04  Data: 1.544 (1.544)
Train: 87 [  50/1251 (  4%)]  Loss: 4.387 (4.18)  Time: 0.171s, 6004.10/s  (0.225s, 4548.60/s)  LR: 8.066e-04  Data: 0.024 (0.077)
Train: 87 [ 100/1251 (  8%)]  Loss: 4.474 (4.28)  Time: 0.178s, 5756.01/s  (0.208s, 4921.86/s)  LR: 8.066e-04  Data: 0.033 (0.062)
Train: 87 [ 150/1251 ( 12%)]  Loss: 4.484 (4.33)  Time: 0.174s, 5870.83/s  (0.200s, 5121.84/s)  LR: 8.066e-04  Data: 0.019 (0.054)
Train: 87 [ 200/1251 ( 16%)]  Loss: 4.364 (4.34)  Time: 0.181s, 5649.21/s  (0.198s, 5162.48/s)  LR: 8.066e-04  Data: 0.034 (0.053)
Train: 87 [ 250/1251 ( 20%)]  Loss: 4.394 (4.35)  Time: 0.176s, 5816.68/s  (0.196s, 5228.13/s)  LR: 8.066e-04  Data: 0.030 (0.050)
Train: 87 [ 300/1251 ( 24%)]  Loss: 4.562 (4.38)  Time: 0.181s, 5667.21/s  (0.196s, 5233.45/s)  LR: 8.066e-04  Data: 0.025 (0.047)
Train: 87 [ 350/1251 ( 28%)]  Loss: 4.142 (4.35)  Time: 0.173s, 5906.76/s  (0.195s, 5258.63/s)  LR: 8.066e-04  Data: 0.027 (0.044)
Train: 87 [ 400/1251 ( 32%)]  Loss: 4.414 (4.35)  Time: 0.167s, 6118.51/s  (0.195s, 5254.78/s)  LR: 8.066e-04  Data: 0.025 (0.042)
Train: 87 [ 450/1251 ( 36%)]  Loss: 4.756 (4.39)  Time: 0.168s, 6092.77/s  (0.194s, 5285.38/s)  LR: 8.066e-04  Data: 0.032 (0.040)
Train: 87 [ 500/1251 ( 40%)]  Loss: 4.500 (4.40)  Time: 0.173s, 5935.98/s  (0.194s, 5283.16/s)  LR: 8.066e-04  Data: 0.024 (0.039)
Train: 87 [ 550/1251 ( 44%)]  Loss: 4.377 (4.40)  Time: 0.177s, 5794.42/s  (0.193s, 5293.47/s)  LR: 8.066e-04  Data: 0.029 (0.038)
Train: 87 [ 600/1251 ( 48%)]  Loss: 4.598 (4.42)  Time: 0.181s, 5669.33/s  (0.193s, 5292.95/s)  LR: 8.066e-04  Data: 0.023 (0.037)
Train: 87 [ 650/1251 ( 52%)]  Loss: 4.031 (4.39)  Time: 0.170s, 6040.68/s  (0.193s, 5300.03/s)  LR: 8.066e-04  Data: 0.028 (0.036)
Train: 87 [ 700/1251 ( 56%)]  Loss: 4.295 (4.38)  Time: 0.173s, 5913.33/s  (0.193s, 5296.96/s)  LR: 8.066e-04  Data: 0.024 (0.036)
Train: 87 [ 750/1251 ( 60%)]  Loss: 4.631 (4.40)  Time: 0.162s, 6309.27/s  (0.193s, 5302.42/s)  LR: 8.066e-04  Data: 0.035 (0.035)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 87 [ 800/1251 ( 64%)]  Loss: 4.025 (4.38)  Time: 0.161s, 6372.19/s  (0.193s, 5317.35/s)  LR: 8.066e-04  Data: 0.031 (0.035)
Train: 87 [ 850/1251 ( 68%)]  Loss: 4.589 (4.39)  Time: 0.328s, 3119.46/s  (0.193s, 5308.36/s)  LR: 8.066e-04  Data: 0.032 (0.035)
Train: 87 [ 900/1251 ( 72%)]  Loss: 4.617 (4.40)  Time: 0.198s, 5169.58/s  (0.193s, 5308.43/s)  LR: 8.066e-04  Data: 0.026 (0.034)
Train: 87 [ 950/1251 ( 76%)]  Loss: 4.360 (4.40)  Time: 0.158s, 6475.09/s  (0.193s, 5308.06/s)  LR: 8.066e-04  Data: 0.028 (0.034)
Train: 87 [1000/1251 ( 80%)]  Loss: 4.104 (4.38)  Time: 0.168s, 6079.67/s  (0.193s, 5300.74/s)  LR: 8.066e-04  Data: 0.026 (0.034)
Train: 87 [1050/1251 ( 84%)]  Loss: 4.099 (4.37)  Time: 0.320s, 3198.08/s  (0.193s, 5307.19/s)  LR: 8.066e-04  Data: 0.036 (0.033)
Train: 87 [1100/1251 ( 88%)]  Loss: 4.214 (4.36)  Time: 0.173s, 5907.08/s  (0.193s, 5299.51/s)  LR: 8.066e-04  Data: 0.020 (0.033)
Train: 87 [1150/1251 ( 92%)]  Loss: 4.097 (4.35)  Time: 0.176s, 5826.04/s  (0.193s, 5299.62/s)  LR: 8.066e-04  Data: 0.020 (0.033)
Train: 87 [1200/1251 ( 96%)]  Loss: 4.327 (4.35)  Time: 0.314s, 3263.73/s  (0.193s, 5299.11/s)  LR: 8.066e-04  Data: 0.030 (0.033)
Train: 87 [1250/1251 (100%)]  Loss: 4.437 (4.36)  Time: 0.114s, 8999.78/s  (0.193s, 5314.22/s)  LR: 8.066e-04  Data: 0.000 (0.032)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.814 (1.814)  Loss:  1.1774 (1.1774)  Acc@1: 81.5430 (81.5430)  Acc@5: 95.5078 (95.5078)
Test: [  48/48]  Time: 0.019 (0.220)  Loss:  1.3291 (1.8957)  Acc@1: 78.1840 (62.9940)  Acc@5: 92.6887 (84.9180)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-76.pth.tar', 61.98000000732422)

Train: 88 [   0/1251 (  0%)]  Loss: 4.428 (4.43)  Time: 1.792s,  571.36/s  (1.792s,  571.36/s)  LR: 8.025e-04  Data: 1.660 (1.660)
Train: 88 [  50/1251 (  4%)]  Loss: 4.414 (4.42)  Time: 0.177s, 5799.52/s  (0.229s, 4478.76/s)  LR: 8.025e-04  Data: 0.025 (0.068)
Train: 88 [ 100/1251 (  8%)]  Loss: 4.535 (4.46)  Time: 0.173s, 5903.60/s  (0.209s, 4905.81/s)  LR: 8.025e-04  Data: 0.031 (0.048)
Train: 88 [ 150/1251 ( 12%)]  Loss: 4.665 (4.51)  Time: 0.176s, 5817.22/s  (0.203s, 5051.24/s)  LR: 8.025e-04  Data: 0.033 (0.042)
Train: 88 [ 200/1251 ( 16%)]  Loss: 4.592 (4.53)  Time: 0.162s, 6306.45/s  (0.199s, 5140.25/s)  LR: 8.025e-04  Data: 0.020 (0.040)
Train: 88 [ 250/1251 ( 20%)]  Loss: 4.185 (4.47)  Time: 0.164s, 6226.47/s  (0.197s, 5186.84/s)  LR: 8.025e-04  Data: 0.030 (0.041)
Train: 88 [ 300/1251 ( 24%)]  Loss: 4.261 (4.44)  Time: 0.189s, 5411.79/s  (0.195s, 5238.57/s)  LR: 8.025e-04  Data: 0.024 (0.041)
Train: 88 [ 350/1251 ( 28%)]  Loss: 3.981 (4.38)  Time: 0.181s, 5663.90/s  (0.196s, 5236.76/s)  LR: 8.025e-04  Data: 0.025 (0.042)
Train: 88 [ 400/1251 ( 32%)]  Loss: 4.137 (4.36)  Time: 0.161s, 6353.77/s  (0.194s, 5273.42/s)  LR: 8.025e-04  Data: 0.020 (0.042)
Train: 88 [ 450/1251 ( 36%)]  Loss: 4.575 (4.38)  Time: 0.169s, 6062.27/s  (0.194s, 5286.69/s)  LR: 8.025e-04  Data: 0.027 (0.042)
Train: 88 [ 500/1251 ( 40%)]  Loss: 4.242 (4.37)  Time: 0.166s, 6176.89/s  (0.193s, 5313.11/s)  LR: 8.025e-04  Data: 0.031 (0.042)
Train: 88 [ 550/1251 ( 44%)]  Loss: 4.184 (4.35)  Time: 0.195s, 5259.72/s  (0.193s, 5312.15/s)  LR: 8.025e-04  Data: 0.023 (0.042)
Train: 88 [ 600/1251 ( 48%)]  Loss: 4.567 (4.37)  Time: 0.169s, 6060.62/s  (0.193s, 5296.87/s)  LR: 8.025e-04  Data: 0.026 (0.043)
Train: 88 [ 650/1251 ( 52%)]  Loss: 4.425 (4.37)  Time: 0.178s, 5740.19/s  (0.193s, 5301.73/s)  LR: 8.025e-04  Data: 0.026 (0.043)
Train: 88 [ 700/1251 ( 56%)]  Loss: 3.992 (4.35)  Time: 0.198s, 5169.98/s  (0.193s, 5308.85/s)  LR: 8.025e-04  Data: 0.024 (0.043)
Train: 88 [ 750/1251 ( 60%)]  Loss: 4.626 (4.36)  Time: 0.177s, 5795.67/s  (0.193s, 5306.56/s)  LR: 8.025e-04  Data: 0.025 (0.043)
Train: 88 [ 800/1251 ( 64%)]  Loss: 4.449 (4.37)  Time: 0.174s, 5878.78/s  (0.193s, 5305.73/s)  LR: 8.025e-04  Data: 0.023 (0.043)
Train: 88 [ 850/1251 ( 68%)]  Loss: 4.588 (4.38)  Time: 0.170s, 6023.39/s  (0.193s, 5308.80/s)  LR: 8.025e-04  Data: 0.021 (0.044)
Train: 88 [ 900/1251 ( 72%)]  Loss: 4.533 (4.39)  Time: 0.162s, 6321.33/s  (0.193s, 5310.80/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [ 950/1251 ( 76%)]  Loss: 4.392 (4.39)  Time: 0.172s, 5943.27/s  (0.193s, 5306.71/s)  LR: 8.025e-04  Data: 0.027 (0.044)
Train: 88 [1000/1251 ( 80%)]  Loss: 4.364 (4.39)  Time: 0.166s, 6182.68/s  (0.193s, 5306.56/s)  LR: 8.025e-04  Data: 0.025 (0.044)
Train: 88 [1050/1251 ( 84%)]  Loss: 4.399 (4.39)  Time: 0.177s, 5769.42/s  (0.193s, 5311.38/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [1100/1251 ( 88%)]  Loss: 4.250 (4.38)  Time: 0.178s, 5767.38/s  (0.193s, 5310.55/s)  LR: 8.025e-04  Data: 0.026 (0.044)
Train: 88 [1150/1251 ( 92%)]  Loss: 3.788 (4.36)  Time: 0.259s, 3946.52/s  (0.193s, 5306.64/s)  LR: 8.025e-04  Data: 0.023 (0.044)
Train: 88 [1200/1251 ( 96%)]  Loss: 4.005 (4.34)  Time: 0.186s, 5505.93/s  (0.193s, 5301.83/s)  LR: 8.025e-04  Data: 0.027 (0.044)
Train: 88 [1250/1251 (100%)]  Loss: 4.430 (4.35)  Time: 0.113s, 9031.88/s  (0.193s, 5318.42/s)  LR: 8.025e-04  Data: 0.000 (0.043)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.863 (1.863)  Loss:  1.1548 (1.1548)  Acc@1: 79.6875 (79.6875)  Acc@5: 94.5312 (94.5312)
Test: [  48/48]  Time: 0.019 (0.217)  Loss:  1.2239 (1.8664)  Acc@1: 76.8868 (62.5100)  Acc@5: 91.6274 (84.6060)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-79.pth.tar', 61.984000012207034)

Train: 89 [   0/1251 (  0%)]  Loss: 4.337 (4.34)  Time: 1.858s,  551.22/s  (1.858s,  551.22/s)  LR: 7.983e-04  Data: 1.713 (1.713)
Train: 89 [  50/1251 (  4%)]  Loss: 4.313 (4.32)  Time: 0.166s, 6164.43/s  (0.223s, 4595.49/s)  LR: 7.983e-04  Data: 0.030 (0.076)
Train: 89 [ 100/1251 (  8%)]  Loss: 4.390 (4.35)  Time: 0.157s, 6534.34/s  (0.208s, 4922.15/s)  LR: 7.983e-04  Data: 0.027 (0.052)
Train: 89 [ 150/1251 ( 12%)]  Loss: 4.636 (4.42)  Time: 0.161s, 6353.14/s  (0.199s, 5150.69/s)  LR: 7.983e-04  Data: 0.033 (0.044)
Train: 89 [ 200/1251 ( 16%)]  Loss: 4.183 (4.37)  Time: 0.168s, 6107.30/s  (0.197s, 5202.74/s)  LR: 7.983e-04  Data: 0.025 (0.040)
Train: 89 [ 250/1251 ( 20%)]  Loss: 4.364 (4.37)  Time: 0.168s, 6092.42/s  (0.196s, 5236.45/s)  LR: 7.983e-04  Data: 0.034 (0.038)
Train: 89 [ 300/1251 ( 24%)]  Loss: 4.388 (4.37)  Time: 0.181s, 5652.80/s  (0.195s, 5260.51/s)  LR: 7.983e-04  Data: 0.028 (0.036)
Train: 89 [ 350/1251 ( 28%)]  Loss: 4.671 (4.41)  Time: 0.175s, 5851.42/s  (0.193s, 5297.68/s)  LR: 7.983e-04  Data: 0.030 (0.035)
Train: 89 [ 400/1251 ( 32%)]  Loss: 4.415 (4.41)  Time: 0.186s, 5494.44/s  (0.193s, 5305.11/s)  LR: 7.983e-04  Data: 0.026 (0.034)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 89 [ 450/1251 ( 36%)]  Loss: 4.201 (4.39)  Time: 0.187s, 5480.27/s  (0.193s, 5311.50/s)  LR: 7.983e-04  Data: 0.034 (0.034)
Train: 89 [ 500/1251 ( 40%)]  Loss: 4.199 (4.37)  Time: 0.158s, 6478.54/s  (0.192s, 5328.19/s)  LR: 7.983e-04  Data: 0.026 (0.033)
Train: 89 [ 550/1251 ( 44%)]  Loss: 4.503 (4.38)  Time: 0.158s, 6463.45/s  (0.193s, 5317.43/s)  LR: 7.983e-04  Data: 0.029 (0.033)
Train: 89 [ 600/1251 ( 48%)]  Loss: 4.352 (4.38)  Time: 0.189s, 5426.74/s  (0.192s, 5339.55/s)  LR: 7.983e-04  Data: 0.029 (0.032)
Train: 89 [ 650/1251 ( 52%)]  Loss: 4.753 (4.41)  Time: 0.182s, 5621.06/s  (0.193s, 5319.27/s)  LR: 7.983e-04  Data: 0.024 (0.032)
Train: 89 [ 700/1251 ( 56%)]  Loss: 4.051 (4.38)  Time: 0.165s, 6190.74/s  (0.193s, 5317.60/s)  LR: 7.983e-04  Data: 0.028 (0.032)
Train: 89 [ 750/1251 ( 60%)]  Loss: 4.581 (4.40)  Time: 0.527s, 1944.49/s  (0.193s, 5308.62/s)  LR: 7.983e-04  Data: 0.026 (0.031)
Train: 89 [ 800/1251 ( 64%)]  Loss: 4.327 (4.39)  Time: 0.171s, 5981.90/s  (0.192s, 5320.91/s)  LR: 7.983e-04  Data: 0.025 (0.031)
Train: 89 [ 850/1251 ( 68%)]  Loss: 4.138 (4.38)  Time: 0.150s, 6833.06/s  (0.192s, 5324.07/s)  LR: 7.983e-04  Data: 0.024 (0.031)
Train: 89 [ 900/1251 ( 72%)]  Loss: 4.090 (4.36)  Time: 0.181s, 5650.72/s  (0.192s, 5322.04/s)  LR: 7.983e-04  Data: 0.026 (0.031)
Train: 89 [ 950/1251 ( 76%)]  Loss: 4.191 (4.35)  Time: 0.163s, 6288.18/s  (0.192s, 5321.36/s)  LR: 7.983e-04  Data: 0.030 (0.031)
Train: 89 [1000/1251 ( 80%)]  Loss: 3.914 (4.33)  Time: 0.165s, 6193.81/s  (0.192s, 5319.57/s)  LR: 7.983e-04  Data: 0.025 (0.030)
Train: 89 [1050/1251 ( 84%)]  Loss: 4.245 (4.33)  Time: 0.178s, 5762.99/s  (0.193s, 5311.18/s)  LR: 7.983e-04  Data: 0.024 (0.030)
Train: 89 [1100/1251 ( 88%)]  Loss: 4.319 (4.33)  Time: 0.171s, 5993.61/s  (0.193s, 5315.66/s)  LR: 7.983e-04  Data: 0.023 (0.030)
Train: 89 [1150/1251 ( 92%)]  Loss: 4.568 (4.34)  Time: 0.151s, 6759.91/s  (0.193s, 5308.86/s)  LR: 7.983e-04  Data: 0.027 (0.030)
Train: 89 [1200/1251 ( 96%)]  Loss: 4.087 (4.33)  Time: 0.155s, 6605.03/s  (0.193s, 5308.92/s)  LR: 7.983e-04  Data: 0.027 (0.030)
Train: 89 [1250/1251 (100%)]  Loss: 4.176 (4.32)  Time: 0.114s, 9014.78/s  (0.192s, 5324.74/s)  LR: 7.983e-04  Data: 0.000 (0.030)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.784 (1.784)  Loss:  1.2485 (1.2485)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.7500 (93.7500)
Test: [  48/48]  Time: 0.019 (0.223)  Loss:  1.3580 (1.9880)  Acc@1: 77.7123 (61.8520)  Acc@5: 91.2736 (84.3260)
Train: 90 [   0/1251 (  0%)]  Loss: 4.020 (4.02)  Time: 1.798s,  569.53/s  (1.798s,  569.53/s)  LR: 7.941e-04  Data: 1.670 (1.670)
Train: 90 [  50/1251 (  4%)]  Loss: 4.368 (4.19)  Time: 0.192s, 5324.28/s  (0.221s, 4626.18/s)  LR: 7.941e-04  Data: 0.029 (0.064)
Train: 90 [ 100/1251 (  8%)]  Loss: 4.167 (4.18)  Time: 0.180s, 5687.16/s  (0.204s, 5017.09/s)  LR: 7.941e-04  Data: 0.025 (0.047)
Train: 90 [ 150/1251 ( 12%)]  Loss: 4.041 (4.15)  Time: 0.167s, 6149.40/s  (0.199s, 5145.91/s)  LR: 7.941e-04  Data: 0.026 (0.042)
Train: 90 [ 200/1251 ( 16%)]  Loss: 4.406 (4.20)  Time: 0.181s, 5665.50/s  (0.197s, 5210.59/s)  LR: 7.941e-04  Data: 0.031 (0.039)
Train: 90 [ 250/1251 ( 20%)]  Loss: 4.438 (4.24)  Time: 0.192s, 5341.42/s  (0.195s, 5238.68/s)  LR: 7.941e-04  Data: 0.027 (0.037)
Train: 90 [ 300/1251 ( 24%)]  Loss: 4.314 (4.25)  Time: 0.182s, 5622.11/s  (0.195s, 5261.28/s)  LR: 7.941e-04  Data: 0.025 (0.036)
Train: 90 [ 350/1251 ( 28%)]  Loss: 4.828 (4.32)  Time: 0.168s, 6099.61/s  (0.194s, 5275.09/s)  LR: 7.941e-04  Data: 0.029 (0.036)
Train: 90 [ 400/1251 ( 32%)]  Loss: 4.283 (4.32)  Time: 0.200s, 5107.71/s  (0.194s, 5285.04/s)  LR: 7.941e-04  Data: 0.025 (0.035)
Train: 90 [ 450/1251 ( 36%)]  Loss: 4.231 (4.31)  Time: 0.178s, 5762.78/s  (0.193s, 5305.10/s)  LR: 7.941e-04  Data: 0.041 (0.034)
Train: 90 [ 500/1251 ( 40%)]  Loss: 4.357 (4.31)  Time: 0.185s, 5543.06/s  (0.193s, 5314.81/s)  LR: 7.941e-04  Data: 0.029 (0.035)
Train: 90 [ 550/1251 ( 44%)]  Loss: 4.486 (4.33)  Time: 0.195s, 5251.59/s  (0.192s, 5335.80/s)  LR: 7.941e-04  Data: 0.023 (0.035)
Train: 90 [ 600/1251 ( 48%)]  Loss: 4.326 (4.33)  Time: 0.199s, 5148.32/s  (0.193s, 5315.28/s)  LR: 7.941e-04  Data: 0.029 (0.037)
Train: 90 [ 650/1251 ( 52%)]  Loss: 4.812 (4.36)  Time: 0.180s, 5678.73/s  (0.192s, 5336.39/s)  LR: 7.941e-04  Data: 0.053 (0.037)
Train: 90 [ 700/1251 ( 56%)]  Loss: 4.301 (4.36)  Time: 0.167s, 6132.42/s  (0.192s, 5339.86/s)  LR: 7.941e-04  Data: 0.020 (0.038)
Train: 90 [ 750/1251 ( 60%)]  Loss: 4.404 (4.36)  Time: 0.174s, 5871.56/s  (0.192s, 5345.46/s)  LR: 7.941e-04  Data: 0.030 (0.038)
Train: 90 [ 800/1251 ( 64%)]  Loss: 4.335 (4.36)  Time: 0.352s, 2908.33/s  (0.192s, 5337.49/s)  LR: 7.941e-04  Data: 0.029 (0.037)
Train: 90 [ 850/1251 ( 68%)]  Loss: 4.611 (4.37)  Time: 0.164s, 6227.64/s  (0.192s, 5333.51/s)  LR: 7.941e-04  Data: 0.036 (0.036)
Train: 90 [ 900/1251 ( 72%)]  Loss: 4.458 (4.38)  Time: 0.188s, 5460.79/s  (0.192s, 5336.56/s)  LR: 7.941e-04  Data: 0.026 (0.036)
Train: 90 [ 950/1251 ( 76%)]  Loss: 4.534 (4.39)  Time: 0.185s, 5529.59/s  (0.192s, 5334.58/s)  LR: 7.941e-04  Data: 0.029 (0.035)
Train: 90 [1000/1251 ( 80%)]  Loss: 4.323 (4.38)  Time: 0.175s, 5849.95/s  (0.192s, 5337.91/s)  LR: 7.941e-04  Data: 0.032 (0.035)
Train: 90 [1050/1251 ( 84%)]  Loss: 4.207 (4.37)  Time: 0.695s, 1474.28/s  (0.193s, 5316.52/s)  LR: 7.941e-04  Data: 0.038 (0.035)
Train: 90 [1100/1251 ( 88%)]  Loss: 4.582 (4.38)  Time: 0.180s, 5677.38/s  (0.192s, 5322.25/s)  LR: 7.941e-04  Data: 0.025 (0.034)
Train: 90 [1150/1251 ( 92%)]  Loss: 4.449 (4.39)  Time: 0.170s, 6020.07/s  (0.193s, 5314.65/s)  LR: 7.941e-04  Data: 0.033 (0.034)
Train: 90 [1200/1251 ( 96%)]  Loss: 3.974 (4.37)  Time: 0.173s, 5907.01/s  (0.193s, 5316.27/s)  LR: 7.941e-04  Data: 0.036 (0.034)
Train: 90 [1250/1251 (100%)]  Loss: 4.207 (4.36)  Time: 0.114s, 9015.31/s  (0.192s, 5331.57/s)  LR: 7.941e-04  Data: 0.000 (0.034)
Distributing BatchNorm running means and vars
Test: [   0/48]  Time: 1.867 (1.867)  Loss:  1.0878 (1.0878)  Acc@1: 82.2266 (82.2266)  Acc@5: 94.8242 (94.8242)
Test: [  48/48]  Time: 0.019 (0.218)  Loss:  1.1970 (1.8853)  Acc@1: 77.8302 (62.5980)  Acc@5: 92.2170 (84.5400)
Current checkpoints:
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-87.pth.tar', 62.9939999609375)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-85.pth.tar', 62.7100001171875)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-80.pth.tar', 62.602000041503906)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-90.pth.tar', 62.597999936523436)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-86.pth.tar', 62.53399987792969)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-88.pth.tar', 62.51000004394531)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-78.pth.tar', 62.31000014160156)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-81.pth.tar', 62.1679999584961)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-77.pth.tar', 62.140000134277344)
 ('./output/train/20220223-153746-mobilenetv2_100-224/checkpoint-83.pth.tar', 62.1360000390625)

Train: 91 [   0/1251 (  0%)]  Loss: 4.323 (4.32)  Time: 1.636s,  626.05/s  (1.636s,  626.05/s)  LR: 7.899e-04  Data: 1.504 (1.504)
Train: 91 [  50/1251 (  4%)]  Loss: 4.235 (4.28)  Time: 0.181s, 5646.61/s  (0.223s, 4582.04/s)  LR: 7.899e-04  Data: 0.033 (0.071)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Train: 91 [ 100/1251 (  8%)]  Loss: 4.438 (4.33)  Time: 0.185s, 5535.25/s  (0.206s, 4972.38/s)  LR: 7.899e-04  Data: 0.023 (0.050)
Train: 91 [ 150/1251 ( 12%)]  Loss: 4.466 (4.37)  Time: 0.158s, 6489.76/s  (0.200s, 5128.16/s)  LR: 7.899e-04  Data: 0.030 (0.042)
Train: 91 [ 200/1251 ( 16%)]  Loss: 4.504 (4.39)  Time: 0.267s, 3834.30/s  (0.197s, 5187.21/s)  LR: 7.899e-04  Data: 0.022 (0.039)
Train: 91 [ 250/1251 ( 20%)]  Loss: 4.461 (4.40)  Time: 0.169s, 6060.26/s  (0.196s, 5216.16/s)  LR: 7.899e-04  Data: 0.026 (0.037)
Train: 91 [ 300/1251 ( 24%)]  Loss: 4.128 (4.36)  Time: 0.173s, 5908.94/s  (0.195s, 5256.48/s)  LR: 7.899e-04  Data: 0.032 (0.035)
Train: 91 [ 350/1251 ( 28%)]  Loss: 4.446 (4.38)  Time: 0.167s, 6119.05/s  (0.195s, 5243.12/s)  LR: 7.899e-04  Data: 0.050 (0.035)
Train: 91 [ 400/1251 ( 32%)]  Loss: 3.965 (4.33)  Time: 0.161s, 6359.76/s  (0.194s, 5267.13/s)  LR: 7.899e-04  Data: 0.025 (0.034)
Train: 91 [ 450/1251 ( 36%)]  Loss: 4.490 (4.35)  Time: 0.166s, 6152.65/s  (0.194s, 5284.09/s)  LR: 7.899e-04  Data: 0.026 (0.033)
Train: 91 [ 500/1251 ( 40%)]  Loss: 4.189 (4.33)  Time: 0.175s, 5851.02/s  (0.194s, 5283.12/s)  LR: 7.899e-04  Data: 0.027 (0.033)
Train: 91 [ 550/1251 ( 44%)]  Loss: 4.698 (4.36)  Time: 0.158s, 6488.96/s  (0.194s, 5282.17/s)  LR: 7.899e-04  Data: 0.032 (0.032)
Train: 91 [ 600/1251 ( 48%)]  Loss: 4.382 (4.36)  Time: 0.162s, 6304.19/s  (0.194s, 5286.70/s)  LR: 7.899e-04  Data: 0.033 (0.032)
Train: 91 [ 650/1251 ( 52%)]  Loss: 4.736 (4.39)  Time: 0.165s, 6187.35/s  (0.193s, 5298.31/s)  LR: 7.899e-04  Data: 0.025 (0.032)
